[2017-05-03 17:41:23,495][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-03 17:41:23,495][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-03 17:41:23,495][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-03 17:41:23,495][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-03 17:41:23,495][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3c0f255560>
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f3c0f8d9758>
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-03 17:41:23,496][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-03 17:41:23,496][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-03 17:41:23,497][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-03 17:41:23,522][gdb3][DEBUG ] connection detected need for sudo
[2017-05-03 17:41:23,535][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-03 17:41:23,536][gdb3][DEBUG ] detect platform information from remote host
[2017-05-03 17:41:23,552][gdb3][DEBUG ] detect machine type
[2017-05-03 17:41:23,554][gdb3][DEBUG ] find the location of an executable
[2017-05-03 17:41:23,556][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-03 17:41:23,567][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-03 17:41:23,573][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-03 17:41:23,573][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-03 17:41:23,573][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-03 17:41:23,573][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-03 17:41:23,574][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-03 17:41:23,574][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-03 17:41:23,574][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-03 17:41:23,574][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-03 17:48:27,903][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-03 17:48:27,903][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6dbdcefe60>
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f6dbdcc4b18>
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-03 17:48:27,904][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-03 17:48:27,905][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-03 17:48:27,905][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-03 17:48:27,931][gdb3][DEBUG ] connection detected need for sudo
[2017-05-03 17:48:27,944][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-03 17:48:27,945][gdb3][DEBUG ] detect platform information from remote host
[2017-05-03 17:48:27,961][gdb3][DEBUG ] detect machine type
[2017-05-03 17:48:27,963][gdb3][DEBUG ] find the location of an executable
[2017-05-03 17:48:27,963][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-03 17:48:27,963][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-03 17:48:27,964][gdb3][DEBUG ] get remote short hostname
[2017-05-03 17:48:27,964][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-03 17:48:27,964][gdb3][DEBUG ] get remote short hostname
[2017-05-03 17:48:27,964][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-03 17:48:27,965][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-03 17:48:27,966][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-03 17:48:27,966][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-03 17:48:27,967][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-03 17:48:27,967][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-03 17:48:27,967][gdb3][DEBUG ] create the monitor keyring file
[2017-05-03 17:48:27,968][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-03 17:48:28,006][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-03 17:48:28,006][gdb3][DEBUG ] ceph-mon: set fsid to 14b01b52-dba4-4e04-a748-8e19384a107b
[2017-05-03 17:48:28,014][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-03 17:48:28,015][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-03 17:48:28,015][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-03 17:48:28,016][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-03 17:48:28,017][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-03 17:48:28,086][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-03 17:48:28,153][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-03 17:48:30,223][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-03 17:48:30,338][gdb3][DEBUG ] ********************************************************************************
[2017-05-03 17:48:30,338][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-03 17:48:30,339][gdb3][DEBUG ] {
[2017-05-03 17:48:30,339][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-03 17:48:30,339][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-03 17:48:30,339][gdb3][DEBUG ]   "features": {
[2017-05-03 17:48:30,339][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-03 17:48:30,339][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-03 17:48:30,339][gdb3][DEBUG ]       "kraken", 
[2017-05-03 17:48:30,339][gdb3][DEBUG ]       "luminous"
[2017-05-03 17:48:30,339][gdb3][DEBUG ]     ], 
[2017-05-03 17:48:30,339][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-03 17:48:30,339][gdb3][DEBUG ]     "required_mon": [
[2017-05-03 17:48:30,339][gdb3][DEBUG ]       "kraken", 
[2017-05-03 17:48:30,340][gdb3][DEBUG ]       "luminous"
[2017-05-03 17:48:30,340][gdb3][DEBUG ]     ]
[2017-05-03 17:48:30,340][gdb3][DEBUG ]   }, 
[2017-05-03 17:48:30,340][gdb3][DEBUG ]   "monmap": {
[2017-05-03 17:48:30,340][gdb3][DEBUG ]     "created": "2017-05-03 17:48:27.992976", 
[2017-05-03 17:48:30,340][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-03 17:48:30,340][gdb3][DEBUG ]     "features": {
[2017-05-03 17:48:30,340][gdb3][DEBUG ]       "optional": [], 
[2017-05-03 17:48:30,340][gdb3][DEBUG ]       "persistent": [
[2017-05-03 17:48:30,340][gdb3][DEBUG ]         "kraken", 
[2017-05-03 17:48:30,340][gdb3][DEBUG ]         "luminous"
[2017-05-03 17:48:30,340][gdb3][DEBUG ]       ]
[2017-05-03 17:48:30,340][gdb3][DEBUG ]     }, 
[2017-05-03 17:48:30,340][gdb3][DEBUG ]     "fsid": "14b01b52-dba4-4e04-a748-8e19384a107b", 
[2017-05-03 17:48:30,340][gdb3][DEBUG ]     "modified": "2017-05-03 17:48:28.237168", 
[2017-05-03 17:48:30,340][gdb3][DEBUG ]     "mons": [
[2017-05-03 17:48:30,341][gdb3][DEBUG ]       {
[2017-05-03 17:48:30,341][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-03 17:48:30,341][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-03 17:48:30,341][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-03 17:48:30,341][gdb3][DEBUG ]         "rank": 0
[2017-05-03 17:48:30,341][gdb3][DEBUG ]       }
[2017-05-03 17:48:30,341][gdb3][DEBUG ]     ]
[2017-05-03 17:48:30,341][gdb3][DEBUG ]   }, 
[2017-05-03 17:48:30,341][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-03 17:48:30,341][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-03 17:48:30,341][gdb3][DEBUG ]   "quorum": [
[2017-05-03 17:48:30,341][gdb3][DEBUG ]     0
[2017-05-03 17:48:30,341][gdb3][DEBUG ]   ], 
[2017-05-03 17:48:30,341][gdb3][DEBUG ]   "rank": 0, 
[2017-05-03 17:48:30,341][gdb3][DEBUG ]   "state": "leader", 
[2017-05-03 17:48:30,341][gdb3][DEBUG ]   "sync_provider": []
[2017-05-03 17:48:30,341][gdb3][DEBUG ] }
[2017-05-03 17:48:30,342][gdb3][DEBUG ] ********************************************************************************
[2017-05-03 17:48:30,342][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-03 17:48:30,342][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-03 17:48:30,408][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-03 17:48:30,423][gdb3][DEBUG ] connection detected need for sudo
[2017-05-03 17:48:30,436][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-03 17:48:30,437][gdb3][DEBUG ] detect platform information from remote host
[2017-05-03 17:48:30,453][gdb3][DEBUG ] detect machine type
[2017-05-03 17:48:30,455][gdb3][DEBUG ] find the location of an executable
[2017-05-03 17:48:30,456][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-03 17:48:30,521][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-03 17:48:30,521][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-03 17:48:30,521][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-03 17:48:30,523][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpLrVtUN
[2017-05-03 17:48:30,539][gdb3][DEBUG ] connection detected need for sudo
[2017-05-03 17:48:30,552][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-03 17:48:30,552][gdb3][DEBUG ] detect platform information from remote host
[2017-05-03 17:48:30,568][gdb3][DEBUG ] detect machine type
[2017-05-03 17:48:30,571][gdb3][DEBUG ] get remote short hostname
[2017-05-03 17:48:30,571][gdb3][DEBUG ] fetch remote file
[2017-05-03 17:48:30,572][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-03 17:48:30,638][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-03 17:48:30,804][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-03 17:48:30,971][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-03 17:48:31,137][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-03 17:48:31,303][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-03 17:48:31,469][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-03 17:48:31,635][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-03 17:48:31,801][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-03 17:48:31,967][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-03 17:48:32,133][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-03 17:48:32,299][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-03 17:48:32,299][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-03 17:48:32,299][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring
[2017-05-03 17:48:32,299][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-03 17:48:32,300][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-03 17:48:32,300][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-03 17:48:32,300][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpLrVtUN
[2017-05-03 17:48:42,148][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-03 17:48:42,148][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-03 17:48:42,149][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-03 17:48:42,149][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-03 17:48:42,149][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-03 17:48:42,149][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-03 17:48:42,149][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-03 17:48:42,149][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5cab71f518>
[2017-05-03 17:48:42,149][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-03 17:48:42,149][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-03 17:48:42,149][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f5cac036938>
[2017-05-03 17:48:42,149][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-03 17:48:42,149][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-03 17:48:42,149][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-03 17:48:42,175][gdb3][DEBUG ] connection detected need for sudo
[2017-05-03 17:48:42,189][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-03 17:48:42,189][gdb3][DEBUG ] detect platform information from remote host
[2017-05-03 17:48:42,205][gdb3][DEBUG ] detect machine type
[2017-05-03 17:48:42,208][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-03 17:50:01,731][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-03 17:50:01,731][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-03 17:50:01,731][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-03 17:50:01,731][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-03 17:50:01,731][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-03 17:50:01,732][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-03 17:50:01,732][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-03 17:50:01,732][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-03 17:50:01,732][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdd7b6ffe60>
[2017-05-03 17:50:01,732][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-03 17:50:01,732][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fdd7b6d4b18>
[2017-05-03 17:50:01,732][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-03 17:50:01,732][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-03 17:50:01,732][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-03 17:50:01,733][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-03 17:50:01,733][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-03 17:50:01,759][gdb3][DEBUG ] connection detected need for sudo
[2017-05-03 17:50:01,773][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-03 17:50:01,774][gdb3][DEBUG ] detect platform information from remote host
[2017-05-03 17:50:01,791][gdb3][DEBUG ] detect machine type
[2017-05-03 17:50:01,793][gdb3][DEBUG ] find the location of an executable
[2017-05-03 17:50:01,793][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-03 17:50:01,794][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-03 17:50:01,794][gdb3][DEBUG ] get remote short hostname
[2017-05-03 17:50:01,794][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-03 17:50:01,794][gdb3][DEBUG ] get remote short hostname
[2017-05-03 17:50:01,794][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-03 17:50:01,795][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-03 17:50:01,798][ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-03 17:50:01,798][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2017-05-03 17:50:14,626][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f45441f1e60>
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f45441c6b18>
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-03 17:50:14,627][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-03 17:50:14,628][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-03 17:50:14,628][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-03 17:50:14,628][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-03 17:50:14,655][gdb3][DEBUG ] connection detected need for sudo
[2017-05-03 17:50:14,669][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-03 17:50:14,669][gdb3][DEBUG ] detect platform information from remote host
[2017-05-03 17:50:14,686][gdb3][DEBUG ] detect machine type
[2017-05-03 17:50:14,688][gdb3][DEBUG ] find the location of an executable
[2017-05-03 17:50:14,689][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-03 17:50:14,689][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-03 17:50:14,689][gdb3][DEBUG ] get remote short hostname
[2017-05-03 17:50:14,689][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-03 17:50:14,689][gdb3][DEBUG ] get remote short hostname
[2017-05-03 17:50:14,690][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-03 17:50:14,690][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-03 17:50:14,692][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-03 17:50:14,692][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-03 17:50:14,692][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-03 17:50:14,693][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-03 17:50:14,694][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-03 17:50:14,769][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-03 17:50:14,837][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-03 17:50:16,850][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-03 17:50:16,915][gdb3][DEBUG ] ********************************************************************************
[2017-05-03 17:50:16,916][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-03 17:50:16,916][gdb3][DEBUG ] {
[2017-05-03 17:50:16,916][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-03 17:50:16,916][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-03 17:50:16,916][gdb3][DEBUG ]   "features": {
[2017-05-03 17:50:16,916][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-03 17:50:16,916][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-03 17:50:16,916][gdb3][DEBUG ]       "kraken", 
[2017-05-03 17:50:16,916][gdb3][DEBUG ]       "luminous"
[2017-05-03 17:50:16,916][gdb3][DEBUG ]     ], 
[2017-05-03 17:50:16,917][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-03 17:50:16,917][gdb3][DEBUG ]     "required_mon": [
[2017-05-03 17:50:16,917][gdb3][DEBUG ]       "kraken", 
[2017-05-03 17:50:16,917][gdb3][DEBUG ]       "luminous"
[2017-05-03 17:50:16,917][gdb3][DEBUG ]     ]
[2017-05-03 17:50:16,917][gdb3][DEBUG ]   }, 
[2017-05-03 17:50:16,917][gdb3][DEBUG ]   "monmap": {
[2017-05-03 17:50:16,917][gdb3][DEBUG ]     "created": "2017-05-03 17:48:27.992976", 
[2017-05-03 17:50:16,917][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-03 17:50:16,917][gdb3][DEBUG ]     "features": {
[2017-05-03 17:50:16,917][gdb3][DEBUG ]       "optional": [], 
[2017-05-03 17:50:16,917][gdb3][DEBUG ]       "persistent": [
[2017-05-03 17:50:16,917][gdb3][DEBUG ]         "kraken", 
[2017-05-03 17:50:16,917][gdb3][DEBUG ]         "luminous"
[2017-05-03 17:50:16,917][gdb3][DEBUG ]       ]
[2017-05-03 17:50:16,918][gdb3][DEBUG ]     }, 
[2017-05-03 17:50:16,918][gdb3][DEBUG ]     "fsid": "14b01b52-dba4-4e04-a748-8e19384a107b", 
[2017-05-03 17:50:16,918][gdb3][DEBUG ]     "modified": "2017-05-03 17:48:28.237168", 
[2017-05-03 17:50:16,918][gdb3][DEBUG ]     "mons": [
[2017-05-03 17:50:16,918][gdb3][DEBUG ]       {
[2017-05-03 17:50:16,918][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-03 17:50:16,918][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-03 17:50:16,918][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-03 17:50:16,918][gdb3][DEBUG ]         "rank": 0
[2017-05-03 17:50:16,918][gdb3][DEBUG ]       }
[2017-05-03 17:50:16,918][gdb3][DEBUG ]     ]
[2017-05-03 17:50:16,918][gdb3][DEBUG ]   }, 
[2017-05-03 17:50:16,918][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-03 17:50:16,918][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-03 17:50:16,918][gdb3][DEBUG ]   "quorum": [
[2017-05-03 17:50:16,919][gdb3][DEBUG ]     0
[2017-05-03 17:50:16,919][gdb3][DEBUG ]   ], 
[2017-05-03 17:50:16,919][gdb3][DEBUG ]   "rank": 0, 
[2017-05-03 17:50:16,919][gdb3][DEBUG ]   "state": "leader", 
[2017-05-03 17:50:16,919][gdb3][DEBUG ]   "sync_provider": []
[2017-05-03 17:50:16,919][gdb3][DEBUG ] }
[2017-05-03 17:50:16,919][gdb3][DEBUG ] ********************************************************************************
[2017-05-03 17:50:16,919][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-03 17:50:16,920][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-03 17:50:16,985][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-03 17:50:17,002][gdb3][DEBUG ] connection detected need for sudo
[2017-05-03 17:50:17,015][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-03 17:50:17,016][gdb3][DEBUG ] detect platform information from remote host
[2017-05-03 17:50:17,031][gdb3][DEBUG ] detect machine type
[2017-05-03 17:50:17,034][gdb3][DEBUG ] find the location of an executable
[2017-05-03 17:50:17,035][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-03 17:50:17,100][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-03 17:50:17,100][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-03 17:50:17,100][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-03 17:50:17,102][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpKYQpPz
[2017-05-03 17:50:17,116][gdb3][DEBUG ] connection detected need for sudo
[2017-05-03 17:50:17,130][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-03 17:50:17,130][gdb3][DEBUG ] detect platform information from remote host
[2017-05-03 17:50:17,146][gdb3][DEBUG ] detect machine type
[2017-05-03 17:50:17,149][gdb3][DEBUG ] get remote short hostname
[2017-05-03 17:50:17,149][gdb3][DEBUG ] fetch remote file
[2017-05-03 17:50:17,150][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-03 17:50:17,216][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-03 17:50:17,382][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-03 17:50:17,548][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-03 17:50:17,714][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-03 17:50:17,881][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-03 17:50:18,046][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-05-03 17:50:18,047][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-05-03 17:50:18,047][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-05-03 17:50:18,047][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-03 17:50:18,047][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-05-03 17:50:18,047][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-05-03 17:50:18,047][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpKYQpPz
[2017-05-03 17:50:31,957][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-03 17:50:31,958][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-03 17:50:31,958][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-03 17:50:31,958][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-03 17:50:31,958][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-03 17:50:31,958][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-03 17:50:31,958][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-03 17:50:31,958][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f86b143c518>
[2017-05-03 17:50:31,958][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-03 17:50:31,958][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-03 17:50:31,958][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f86b1d53938>
[2017-05-03 17:50:31,959][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-03 17:50:31,959][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-03 17:50:31,959][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-03 17:50:31,985][gdb3][DEBUG ] connection detected need for sudo
[2017-05-03 17:50:31,999][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-03 17:50:32,000][gdb3][DEBUG ] detect platform information from remote host
[2017-05-03 17:50:32,017][gdb3][DEBUG ] detect machine type
[2017-05-03 17:50:32,019][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-03 17:51:26,772][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-03 17:51:26,772][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-03 17:51:26,772][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f72d5900908>
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-03 17:51:26,773][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f72d5b56aa0>
[2017-05-03 17:51:26,774][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-03 17:51:26,774][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-03 17:51:26,774][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-03 17:51:26,774][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-03 17:51:27,026][gdb1][DEBUG ] connection detected need for sudo
[2017-05-03 17:51:27,255][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-03 17:51:27,256][gdb1][DEBUG ] detect platform information from remote host
[2017-05-03 17:51:27,274][gdb1][DEBUG ] detect machine type
[2017-05-03 17:51:27,277][gdb1][DEBUG ] find the location of an executable
[2017-05-03 17:51:27,278][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-03 17:51:27,278][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-03 17:51:27,279][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-03 17:51:27,281][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-03 17:51:27,282][gdb1][DEBUG ] find the location of an executable
[2017-05-03 17:51:27,284][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-03 17:51:27,404][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-03 17:51:27,419][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:27,419][gdb1][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-03 17:51:27,420][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-03 17:51:27,427][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-03 17:51:27,435][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-03 17:51:27,451][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:27,451][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:27,451][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:27,451][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-03 17:51:27,451][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-03 17:51:27,451][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-03 17:51:27,455][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-03 17:51:27,470][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-03 17:51:27,474][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-03 17:51:27,490][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:27,490][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-03 17:51:27,490][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:27,490][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-03 17:51:27,522][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-03 17:51:27,522][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-03 17:51:27,522][gdb1][WARNING] 10+0 records in
[2017-05-03 17:51:27,522][gdb1][WARNING] 10+0 records out
[2017-05-03 17:51:27,522][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00683663 s, 1.5 GB/s
[2017-05-03 17:51:27,522][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-03 17:51:27,636][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-03 17:51:27,637][gdb1][WARNING] 10+0 records in
[2017-05-03 17:51:27,637][gdb1][WARNING] 10+0 records out
[2017-05-03 17:51:27,637][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00514513 s, 2.0 GB/s
[2017-05-03 17:51:27,637][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-03 17:51:27,637][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-03 17:51:27,637][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-03 17:51:27,637][gdb1][WARNING] backup header from main header.
[2017-05-03 17:51:27,637][gdb1][WARNING] 
[2017-05-03 17:51:27,637][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-03 17:51:27,637][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-03 17:51:27,637][gdb1][WARNING] 
[2017-05-03 17:51:27,637][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-03 17:51:27,638][gdb1][WARNING] 
[2017-05-03 17:51:28,705][gdb1][DEBUG ] ****************************************************************************
[2017-05-03 17:51:28,705][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-03 17:51:28,705][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-03 17:51:28,705][gdb1][DEBUG ] ****************************************************************************
[2017-05-03 17:51:28,705][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-03 17:51:28,705][gdb1][DEBUG ] other utilities.
[2017-05-03 17:51:28,705][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-03 17:51:29,722][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-03 17:51:29,722][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-03 17:51:29,722][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-03 17:51:29,723][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:51:29,723][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 17:51:29,726][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:51:29,742][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:29,742][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-03 17:51:29,742][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:29,742][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-03 17:51:29,742][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:29,742][gdb1][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-03 17:51:29,742][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:4b3e9519-cd43-4955-9b4c-de188cc562bc --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-03 17:51:30,759][gdb1][DEBUG ] Setting name!
[2017-05-03 17:51:30,759][gdb1][DEBUG ] partNum is 0
[2017-05-03 17:51:30,759][gdb1][DEBUG ] REALLY setting name!
[2017-05-03 17:51:30,760][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-03 17:51:30,760][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-03 17:51:30,760][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:51:30,874][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 17:51:30,988][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:51:31,052][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:31,052][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:31,053][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-03 17:51:31,053][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:31,053][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:31,053][gdb1][WARNING] ptype_tobe_for_name: name = block
[2017-05-03 17:51:31,053][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:31,053][gdb1][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-03 17:51:31,053][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:a9c5773c-65a0-41af-96dd-069d8d0aa9fa --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-03 17:51:32,070][gdb1][DEBUG ] Setting name!
[2017-05-03 17:51:32,070][gdb1][DEBUG ] partNum is 1
[2017-05-03 17:51:32,070][gdb1][DEBUG ] REALLY setting name!
[2017-05-03 17:51:32,070][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-03 17:51:32,070][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-03 17:51:32,070][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:51:32,285][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 17:51:32,550][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:51:32,614][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:32,614][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:32,614][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-03 17:51:32,614][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/a9c5773c-65a0-41af-96dd-069d8d0aa9fa
[2017-05-03 17:51:32,615][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-03 17:51:33,632][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-03 17:51:33,632][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-03 17:51:33,632][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:51:33,846][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 17:51:34,111][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:51:34,127][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/a9c5773c-65a0-41af-96dd-069d8d0aa9fa
[2017-05-03 17:51:34,127][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-03 17:51:34,127][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-03 17:51:34,241][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-03 17:51:34,242][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-03 17:51:34,242][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-03 17:51:34,242][gdb1][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-03 17:51:34,242][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-03 17:51:34,242][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-03 17:51:34,242][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-03 17:51:34,242][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-03 17:51:34,242][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-03 17:51:34,242][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.gQ6Y9e with options noatime,inode64
[2017-05-03 17:51:34,242][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.gQ6Y9e
[2017-05-03 17:51:34,306][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.gQ6Y9e
[2017-05-03 17:51:34,306][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.gQ6Y9e/ceph_fsid.2391.tmp
[2017-05-03 17:51:34,307][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.gQ6Y9e/fsid.2391.tmp
[2017-05-03 17:51:34,307][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.gQ6Y9e/magic.2391.tmp
[2017-05-03 17:51:34,307][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.gQ6Y9e/block_uuid.2391.tmp
[2017-05-03 17:51:34,307][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.gQ6Y9e/block -> /dev/disk/by-partuuid/a9c5773c-65a0-41af-96dd-069d8d0aa9fa
[2017-05-03 17:51:34,307][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.gQ6Y9e/type.2391.tmp
[2017-05-03 17:51:34,307][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.gQ6Y9e
[2017-05-03 17:51:34,307][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.gQ6Y9e
[2017-05-03 17:51:34,307][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.gQ6Y9e
[2017-05-03 17:51:34,339][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:51:34,339][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-03 17:51:35,356][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-03 17:51:35,356][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-03 17:51:35,356][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:51:35,571][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 17:51:35,835][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:51:36,100][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-03 17:51:36,102][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-03 17:51:41,225][gdb1][INFO  ] checking OSD status...
[2017-05-03 17:51:41,225][gdb1][DEBUG ] find the location of an executable
[2017-05-03 17:51:41,227][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-03 17:51:41,343][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-03 17:52:38,894][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-03 17:52:38,894][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-03 17:52:38,894][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-03 17:52:38,894][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcefa9f3908>
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcefac49aa0>
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-03 17:52:38,895][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-03 17:52:38,896][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-03 17:52:38,896][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-03 17:52:39,139][gdb1][DEBUG ] connection detected need for sudo
[2017-05-03 17:52:39,367][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-03 17:52:39,367][gdb1][DEBUG ] detect platform information from remote host
[2017-05-03 17:52:39,383][gdb1][DEBUG ] detect machine type
[2017-05-03 17:52:39,387][gdb1][DEBUG ] find the location of an executable
[2017-05-03 17:52:39,388][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-03 17:52:39,388][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-03 17:52:39,388][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-03 17:52:39,392][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-03 17:52:39,392][gdb1][DEBUG ] create a keyring file
[2017-05-03 17:52:39,394][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-03 17:52:39,394][gdb1][DEBUG ] find the location of an executable
[2017-05-03 17:52:39,396][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-03 17:52:39,516][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-03 17:52:39,532][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:39,532][gdb1][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-03 17:52:39,532][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-03 17:52:39,536][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-03 17:52:39,552][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-03 17:52:39,555][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:39,555][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:39,556][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:39,556][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-03 17:52:39,556][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-03 17:52:39,556][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-03 17:52:39,572][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-03 17:52:39,579][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-03 17:52:39,587][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-03 17:52:39,595][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:39,595][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-03 17:52:39,595][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:39,595][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-03 17:52:39,627][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-03 17:52:39,627][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-03 17:52:39,691][gdb1][WARNING] 10+0 records in
[2017-05-03 17:52:39,691][gdb1][WARNING] 10+0 records out
[2017-05-03 17:52:39,691][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.0448202 s, 234 MB/s
[2017-05-03 17:52:39,691][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-03 17:52:39,691][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-03 17:52:39,693][gdb1][WARNING] 10+0 records in
[2017-05-03 17:52:39,693][gdb1][WARNING] 10+0 records out
[2017-05-03 17:52:39,693][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00654135 s, 1.6 GB/s
[2017-05-03 17:52:39,694][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-03 17:52:39,694][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-03 17:52:39,698][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-03 17:52:39,698][gdb1][WARNING] backup header from main header.
[2017-05-03 17:52:39,698][gdb1][WARNING] 
[2017-05-03 17:52:39,698][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-03 17:52:39,698][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-03 17:52:39,698][gdb1][WARNING] 
[2017-05-03 17:52:39,698][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-03 17:52:39,698][gdb1][WARNING] 
[2017-05-03 17:52:40,866][gdb1][DEBUG ] ****************************************************************************
[2017-05-03 17:52:40,866][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-03 17:52:40,866][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-03 17:52:40,866][gdb1][DEBUG ] ****************************************************************************
[2017-05-03 17:52:40,866][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-03 17:52:40,866][gdb1][DEBUG ] other utilities.
[2017-05-03 17:52:40,866][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-03 17:52:41,833][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-03 17:52:41,833][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-03 17:52:41,833][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-03 17:52:41,834][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:52:41,849][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 17:52:41,881][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:52:41,897][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:41,897][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-03 17:52:41,897][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:41,897][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-03 17:52:41,897][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:41,897][gdb1][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-03 17:52:41,897][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:f253e419-2b81-4baf-8b78-b042f1356a04 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-03 17:52:42,914][gdb1][DEBUG ] Setting name!
[2017-05-03 17:52:42,914][gdb1][DEBUG ] partNum is 0
[2017-05-03 17:52:42,915][gdb1][DEBUG ] REALLY setting name!
[2017-05-03 17:52:42,915][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-03 17:52:42,915][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-03 17:52:42,915][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:52:42,979][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 17:52:43,143][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:52:43,159][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:43,159][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:43,159][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-03 17:52:43,159][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:43,159][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:43,159][gdb1][WARNING] ptype_tobe_for_name: name = block
[2017-05-03 17:52:43,160][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:43,160][gdb1][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-03 17:52:43,160][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:53d4faef-42cb-46a9-b827-9b651534d739 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-03 17:52:44,177][gdb1][DEBUG ] Setting name!
[2017-05-03 17:52:44,177][gdb1][DEBUG ] partNum is 1
[2017-05-03 17:52:44,177][gdb1][DEBUG ] REALLY setting name!
[2017-05-03 17:52:44,177][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-03 17:52:44,177][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-03 17:52:44,177][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:52:44,392][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 17:52:44,656][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:52:44,871][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:44,871][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:44,871][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-03 17:52:44,871][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/53d4faef-42cb-46a9-b827-9b651534d739
[2017-05-03 17:52:44,871][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-03 17:52:45,888][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-03 17:52:45,888][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-03 17:52:45,888][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:52:46,103][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 17:52:46,317][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:52:46,532][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/53d4faef-42cb-46a9-b827-9b651534d739
[2017-05-03 17:52:46,532][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-03 17:52:46,532][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-03 17:52:46,646][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-03 17:52:46,646][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-03 17:52:46,646][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-03 17:52:46,646][gdb1][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-03 17:52:46,646][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-03 17:52:46,647][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-03 17:52:46,647][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-03 17:52:46,647][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-03 17:52:46,647][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-03 17:52:46,647][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.xCcS7n with options noatime,inode64
[2017-05-03 17:52:46,647][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.xCcS7n
[2017-05-03 17:52:46,647][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.xCcS7n
[2017-05-03 17:52:46,647][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.xCcS7n/ceph_fsid.3320.tmp
[2017-05-03 17:52:46,647][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.xCcS7n/fsid.3320.tmp
[2017-05-03 17:52:46,647][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.xCcS7n/magic.3320.tmp
[2017-05-03 17:52:46,647][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.xCcS7n/block_uuid.3320.tmp
[2017-05-03 17:52:46,647][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.xCcS7n/block -> /dev/disk/by-partuuid/53d4faef-42cb-46a9-b827-9b651534d739
[2017-05-03 17:52:46,647][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.xCcS7n/type.3320.tmp
[2017-05-03 17:52:46,647][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.xCcS7n
[2017-05-03 17:52:46,648][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.xCcS7n
[2017-05-03 17:52:46,648][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.xCcS7n
[2017-05-03 17:52:46,663][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 17:52:46,665][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-03 17:52:47,681][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-03 17:52:47,681][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-03 17:52:47,682][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-03 17:52:47,682][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-03 17:52:47,682][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-03 17:52:47,682][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:52:47,689][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 17:52:47,804][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 17:52:47,804][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-03 17:52:47,806][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-03 17:52:52,928][gdb1][INFO  ] checking OSD status...
[2017-05-03 17:52:52,928][gdb1][DEBUG ] find the location of an executable
[2017-05-03 17:52:52,931][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-03 17:52:53,046][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-03 18:07:46,281][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-03 18:07:46,281][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-03 18:07:46,281][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-03 18:07:46,281][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-03 18:07:46,281][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-03 18:07:46,281][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8493f11908>
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8494167aa0>
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-03 18:07:46,282][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-03 18:07:46,283][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-03 18:07:46,522][gdb0][DEBUG ] connection detected need for sudo
[2017-05-03 18:07:46,751][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-03 18:07:46,752][gdb0][DEBUG ] detect platform information from remote host
[2017-05-03 18:07:46,769][gdb0][DEBUG ] detect machine type
[2017-05-03 18:07:46,773][gdb0][DEBUG ] find the location of an executable
[2017-05-03 18:07:46,774][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-03 18:07:46,774][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-03 18:07:46,774][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-03 18:07:46,777][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-03 18:07:46,777][gdb0][DEBUG ] find the location of an executable
[2017-05-03 18:07:46,779][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-03 18:07:46,899][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-03 18:07:46,907][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:46,907][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-03 18:07:46,907][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-03 18:07:46,915][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-03 18:07:46,930][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-03 18:07:46,934][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:46,934][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:46,934][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:46,934][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-03 18:07:46,934][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-03 18:07:46,934][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-03 18:07:46,942][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-03 18:07:46,958][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-03 18:07:46,961][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-03 18:07:46,969][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:46,969][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-03 18:07:46,969][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:46,969][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-03 18:07:47,001][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-03 18:07:47,001][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-03 18:07:47,001][gdb0][WARNING] 10+0 records in
[2017-05-03 18:07:47,001][gdb0][WARNING] 10+0 records out
[2017-05-03 18:07:47,001][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00854844 s, 1.2 GB/s
[2017-05-03 18:07:47,001][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-03 18:07:47,116][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-03 18:07:47,117][gdb0][WARNING] 10+0 records in
[2017-05-03 18:07:47,117][gdb0][WARNING] 10+0 records out
[2017-05-03 18:07:47,117][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00523393 s, 2.0 GB/s
[2017-05-03 18:07:47,117][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-03 18:07:47,117][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-03 18:07:47,133][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-03 18:07:47,133][gdb0][WARNING] backup header from main header.
[2017-05-03 18:07:47,133][gdb0][WARNING] 
[2017-05-03 18:07:47,133][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-03 18:07:47,133][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-03 18:07:47,133][gdb0][WARNING] 
[2017-05-03 18:07:47,134][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-03 18:07:47,134][gdb0][WARNING] 
[2017-05-03 18:07:48,201][gdb0][DEBUG ] ****************************************************************************
[2017-05-03 18:07:48,201][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-03 18:07:48,201][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-03 18:07:48,201][gdb0][DEBUG ] ****************************************************************************
[2017-05-03 18:07:48,201][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-03 18:07:48,201][gdb0][DEBUG ] other utilities.
[2017-05-03 18:07:48,201][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-03 18:07:49,168][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-03 18:07:49,168][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-03 18:07:49,169][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-03 18:07:49,169][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:07:49,184][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 18:07:49,248][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:07:49,249][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:49,249][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-03 18:07:49,249][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:49,249][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-03 18:07:49,249][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:49,249][gdb0][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-03 18:07:49,249][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:8969dcfe-ea2b-4537-87b9-97aebe69946f --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-03 18:07:50,266][gdb0][DEBUG ] Setting name!
[2017-05-03 18:07:50,266][gdb0][DEBUG ] partNum is 0
[2017-05-03 18:07:50,266][gdb0][DEBUG ] REALLY setting name!
[2017-05-03 18:07:50,267][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-03 18:07:50,267][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-03 18:07:50,267][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:07:50,331][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 18:07:50,495][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:07:50,559][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:50,559][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:50,560][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-03 18:07:50,560][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:50,560][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:50,560][gdb0][WARNING] ptype_tobe_for_name: name = block
[2017-05-03 18:07:50,560][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:50,560][gdb0][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-03 18:07:50,560][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:c82af1e2-a24d-48bd-86a2-231745be0522 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-03 18:07:51,577][gdb0][DEBUG ] Setting name!
[2017-05-03 18:07:51,578][gdb0][DEBUG ] partNum is 1
[2017-05-03 18:07:51,578][gdb0][DEBUG ] REALLY setting name!
[2017-05-03 18:07:51,578][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-03 18:07:51,578][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-03 18:07:51,578][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:07:51,792][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 18:07:51,957][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:07:52,171][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:52,171][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:52,172][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-03 18:07:52,172][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/c82af1e2-a24d-48bd-86a2-231745be0522
[2017-05-03 18:07:52,172][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-03 18:07:53,189][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-03 18:07:53,189][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-03 18:07:53,189][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:07:53,403][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 18:07:53,618][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:07:53,682][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/c82af1e2-a24d-48bd-86a2-231745be0522
[2017-05-03 18:07:53,682][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-03 18:07:53,683][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-03 18:07:53,797][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-03 18:07:53,797][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-03 18:07:53,797][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-03 18:07:53,797][gdb0][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-03 18:07:53,797][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-03 18:07:53,797][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-03 18:07:53,797][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-03 18:07:53,797][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-03 18:07:53,798][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-03 18:07:53,798][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.hM7VfP with options noatime,inode64
[2017-05-03 18:07:53,798][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.hM7VfP
[2017-05-03 18:07:53,798][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.hM7VfP
[2017-05-03 18:07:53,798][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.hM7VfP/ceph_fsid.4958.tmp
[2017-05-03 18:07:53,798][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.hM7VfP/fsid.4958.tmp
[2017-05-03 18:07:53,798][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.hM7VfP/magic.4958.tmp
[2017-05-03 18:07:53,798][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.hM7VfP/block_uuid.4958.tmp
[2017-05-03 18:07:53,798][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.hM7VfP/block -> /dev/disk/by-partuuid/c82af1e2-a24d-48bd-86a2-231745be0522
[2017-05-03 18:07:53,798][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.hM7VfP/type.4958.tmp
[2017-05-03 18:07:53,798][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.hM7VfP
[2017-05-03 18:07:53,798][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.hM7VfP
[2017-05-03 18:07:53,799][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.hM7VfP
[2017-05-03 18:07:53,814][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:07:53,814][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-03 18:07:54,881][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-03 18:07:54,881][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-03 18:07:54,882][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:07:55,096][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 18:07:55,311][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:07:55,343][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-03 18:07:55,361][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-03 18:08:00,483][gdb0][INFO  ] checking OSD status...
[2017-05-03 18:08:00,483][gdb0][DEBUG ] find the location of an executable
[2017-05-03 18:08:00,486][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-03 18:08:00,601][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-03 18:08:32,031][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-03 18:08:32,031][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-03 18:08:32,031][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-03 18:08:32,031][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-03 18:08:32,031][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-03 18:08:32,031][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-03 18:08:32,031][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-03 18:08:32,031][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-03 18:08:32,031][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-03 18:08:32,031][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-03 18:08:32,031][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-03 18:08:32,032][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-03 18:08:32,032][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-03 18:08:32,032][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-03 18:08:32,032][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbaf7192908>
[2017-05-03 18:08:32,032][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-03 18:08:32,032][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-03 18:08:32,032][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fbaf73e8aa0>
[2017-05-03 18:08:32,032][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-03 18:08:32,032][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-03 18:08:32,032][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-03 18:08:32,032][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-03 18:08:32,271][gdb0][DEBUG ] connection detected need for sudo
[2017-05-03 18:08:32,495][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-03 18:08:32,495][gdb0][DEBUG ] detect platform information from remote host
[2017-05-03 18:08:32,511][gdb0][DEBUG ] detect machine type
[2017-05-03 18:08:32,515][gdb0][DEBUG ] find the location of an executable
[2017-05-03 18:08:32,516][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-03 18:08:32,517][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-03 18:08:32,517][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-03 18:08:32,519][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-03 18:08:32,519][gdb0][DEBUG ] create a keyring file
[2017-05-03 18:08:32,521][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-03 18:08:32,521][gdb0][DEBUG ] find the location of an executable
[2017-05-03 18:08:32,523][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-03 18:08:32,644][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-03 18:08:32,651][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:32,651][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-03 18:08:32,652][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-03 18:08:32,659][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-03 18:08:32,675][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-03 18:08:32,676][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:32,676][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:32,677][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:32,677][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-03 18:08:32,677][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-03 18:08:32,677][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-03 18:08:32,692][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-03 18:08:32,694][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-03 18:08:32,710][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-03 18:08:32,711][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:32,711][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-03 18:08:32,711][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:32,711][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-03 18:08:32,727][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-03 18:08:32,727][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-03 18:08:32,759][gdb0][WARNING] 10+0 records in
[2017-05-03 18:08:32,759][gdb0][WARNING] 10+0 records out
[2017-05-03 18:08:32,759][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.0286975 s, 365 MB/s
[2017-05-03 18:08:32,759][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-03 18:08:32,767][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-03 18:08:32,782][gdb0][WARNING] 10+0 records in
[2017-05-03 18:08:32,782][gdb0][WARNING] 10+0 records out
[2017-05-03 18:08:32,783][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00789581 s, 1.3 GB/s
[2017-05-03 18:08:32,783][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-03 18:08:32,783][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-03 18:08:32,783][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-03 18:08:32,783][gdb0][WARNING] backup header from main header.
[2017-05-03 18:08:32,783][gdb0][WARNING] 
[2017-05-03 18:08:32,783][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-03 18:08:32,783][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-03 18:08:32,783][gdb0][WARNING] 
[2017-05-03 18:08:32,783][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-03 18:08:32,783][gdb0][WARNING] 
[2017-05-03 18:08:33,951][gdb0][DEBUG ] ****************************************************************************
[2017-05-03 18:08:33,951][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-03 18:08:33,951][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-03 18:08:33,951][gdb0][DEBUG ] ****************************************************************************
[2017-05-03 18:08:33,951][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-03 18:08:33,952][gdb0][DEBUG ] other utilities.
[2017-05-03 18:08:33,952][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-03 18:08:34,969][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-03 18:08:34,969][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-03 18:08:34,969][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-03 18:08:34,969][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:08:34,969][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 18:08:34,977][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:08:34,992][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:34,993][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-03 18:08:34,993][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:34,993][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-03 18:08:34,993][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:34,993][gdb0][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-03 18:08:34,993][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:d054b67d-a956-4ada-86e7-53c849be0e92 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-03 18:08:36,010][gdb0][DEBUG ] Setting name!
[2017-05-03 18:08:36,010][gdb0][DEBUG ] partNum is 0
[2017-05-03 18:08:36,010][gdb0][DEBUG ] REALLY setting name!
[2017-05-03 18:08:36,010][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-03 18:08:36,011][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-03 18:08:36,011][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:08:36,125][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 18:08:36,239][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:08:36,303][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:36,303][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:36,303][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-03 18:08:36,303][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:36,303][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:36,304][gdb0][WARNING] ptype_tobe_for_name: name = block
[2017-05-03 18:08:36,304][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:36,304][gdb0][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-03 18:08:36,304][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:b0adf892-be17-48dc-b5e3-244f9a7dad42 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-03 18:08:37,321][gdb0][DEBUG ] Setting name!
[2017-05-03 18:08:37,321][gdb0][DEBUG ] partNum is 1
[2017-05-03 18:08:37,321][gdb0][DEBUG ] REALLY setting name!
[2017-05-03 18:08:37,321][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-03 18:08:37,321][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-03 18:08:37,321][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:08:37,536][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 18:08:37,700][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:08:37,915][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:37,915][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:37,915][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-03 18:08:37,915][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/b0adf892-be17-48dc-b5e3-244f9a7dad42
[2017-05-03 18:08:37,915][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-03 18:08:38,932][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-03 18:08:38,932][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-03 18:08:38,933][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:08:39,147][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 18:08:39,362][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:08:39,576][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/b0adf892-be17-48dc-b5e3-244f9a7dad42
[2017-05-03 18:08:39,576][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-03 18:08:39,577][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-03 18:08:39,691][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-03 18:08:39,691][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-03 18:08:39,691][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-03 18:08:39,691][gdb0][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-03 18:08:39,691][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-03 18:08:39,691][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-03 18:08:39,691][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-03 18:08:39,691][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-03 18:08:39,691][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-03 18:08:39,692][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.r1Eosn with options noatime,inode64
[2017-05-03 18:08:39,692][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.r1Eosn
[2017-05-03 18:08:39,692][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.r1Eosn
[2017-05-03 18:08:39,692][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.r1Eosn/ceph_fsid.5798.tmp
[2017-05-03 18:08:39,692][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.r1Eosn/fsid.5798.tmp
[2017-05-03 18:08:39,692][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.r1Eosn/magic.5798.tmp
[2017-05-03 18:08:39,692][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.r1Eosn/block_uuid.5798.tmp
[2017-05-03 18:08:39,692][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.r1Eosn/block -> /dev/disk/by-partuuid/b0adf892-be17-48dc-b5e3-244f9a7dad42
[2017-05-03 18:08:39,692][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.r1Eosn/type.5798.tmp
[2017-05-03 18:08:39,692][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.r1Eosn
[2017-05-03 18:08:39,693][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.r1Eosn
[2017-05-03 18:08:39,694][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.r1Eosn
[2017-05-03 18:08:39,726][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-03 18:08:39,726][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-03 18:08:40,743][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-03 18:08:40,743][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-03 18:08:40,743][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-03 18:08:40,743][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-03 18:08:40,743][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-03 18:08:40,743][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:08:40,743][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-03 18:08:40,857][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-03 18:08:40,858][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-03 18:08:40,861][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-03 18:08:45,983][gdb0][INFO  ] checking OSD status...
[2017-05-03 18:08:45,983][gdb0][DEBUG ] find the location of an executable
[2017-05-03 18:08:45,985][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-03 18:08:46,101][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-03 18:09:03,921][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-03 18:09:03,921][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-05-03 18:09:03,922][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-03 18:09:03,922][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-03 18:09:03,922][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-03 18:09:03,922][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-03 18:09:03,922][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-03 18:09:03,922][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f98d3bd1518>
[2017-05-03 18:09:03,922][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-03 18:09:03,922][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-05-03 18:09:03,922][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f98d44e8938>
[2017-05-03 18:09:03,922][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-03 18:09:03,922][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-03 18:09:03,922][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-03 18:09:04,164][gdb0][DEBUG ] connection detected need for sudo
[2017-05-03 18:09:04,391][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-03 18:09:04,391][gdb0][DEBUG ] detect platform information from remote host
[2017-05-03 18:09:04,407][gdb0][DEBUG ] detect machine type
[2017-05-03 18:09:04,411][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-03 18:09:04,414][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-03 18:09:04,639][gdb1][DEBUG ] connection detected need for sudo
[2017-05-03 18:09:04,867][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-03 18:09:04,867][gdb1][DEBUG ] detect platform information from remote host
[2017-05-03 18:09:04,883][gdb1][DEBUG ] detect machine type
[2017-05-03 18:09:04,887][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-03 18:09:04,889][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-03 18:09:04,904][gdb3][DEBUG ] connection detected need for sudo
[2017-05-03 18:09:04,919][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-03 18:09:04,919][gdb3][DEBUG ] detect platform information from remote host
[2017-05-03 18:09:04,936][gdb3][DEBUG ] detect machine type
[2017-05-03 18:09:04,938][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 01:33:45,060][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 01:33:45,060][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-14 01:33:45,060][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 01:33:45,060][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 01:33:45,060][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 01:33:45,060][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 01:33:45,060][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 01:33:45,060][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc3f5821560>
[2017-05-14 01:33:45,060][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 01:33:45,061][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-14 01:33:45,061][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-14 01:33:45,061][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fc3f5ea5758>
[2017-05-14 01:33:45,061][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-14 01:33:45,061][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 01:33:45,061][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-14 01:33:45,061][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 01:33:45,061][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-14 01:33:45,061][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-14 01:33:45,061][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-14 01:33:45,087][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 01:33:45,101][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 01:33:45,102][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 01:33:45,119][gdb3][DEBUG ] detect machine type
[2017-05-14 01:33:45,121][gdb3][DEBUG ] find the location of an executable
[2017-05-14 01:33:45,123][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-14 01:33:45,134][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-14 01:33:45,140][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-14 01:33:45,141][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-14 01:33:45,141][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-14 01:33:45,141][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-14 01:33:45,141][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-14 01:33:45,141][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-14 01:33:45,141][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-14 01:33:45,141][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-14 01:34:42,281][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 01:34:42,282][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-14 01:34:42,282][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 01:34:42,282][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 01:34:42,282][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 01:34:42,282][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 01:34:42,282][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-14 01:34:42,282][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 01:34:42,282][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa7f6935e60>
[2017-05-14 01:34:42,282][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 01:34:42,282][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fa7f690ab18>
[2017-05-14 01:34:42,282][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 01:34:42,283][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-14 01:34:42,283][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 01:34:42,283][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-14 01:34:42,283][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-14 01:34:42,310][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 01:34:42,324][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 01:34:42,324][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 01:34:42,341][gdb3][DEBUG ] detect machine type
[2017-05-14 01:34:42,343][gdb3][DEBUG ] find the location of an executable
[2017-05-14 01:34:42,343][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-14 01:34:42,343][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-14 01:34:42,343][gdb3][DEBUG ] get remote short hostname
[2017-05-14 01:34:42,344][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-14 01:34:42,344][gdb3][DEBUG ] get remote short hostname
[2017-05-14 01:34:42,344][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-14 01:34:42,345][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 01:34:42,346][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-14 01:34:42,346][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-14 01:34:42,347][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-14 01:34:42,347][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-14 01:34:42,347][gdb3][DEBUG ] create the monitor keyring file
[2017-05-14 01:34:42,348][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-14 01:34:42,386][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-14 01:34:42,387][gdb3][DEBUG ] ceph-mon: set fsid to 293cf4c0-5136-436a-9d8c-bd1a6ff9e8b4
[2017-05-14 01:34:42,394][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-14 01:34:42,394][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-14 01:34:42,395][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-14 01:34:42,395][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-14 01:34:42,396][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-14 01:34:42,467][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-14 01:34:42,535][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-14 01:34:44,605][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 01:34:44,670][gdb3][DEBUG ] ********************************************************************************
[2017-05-14 01:34:44,670][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-14 01:34:44,671][gdb3][DEBUG ] {
[2017-05-14 01:34:44,671][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-14 01:34:44,671][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-14 01:34:44,671][gdb3][DEBUG ]   "features": {
[2017-05-14 01:34:44,671][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-14 01:34:44,671][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-14 01:34:44,671][gdb3][DEBUG ]       "kraken", 
[2017-05-14 01:34:44,671][gdb3][DEBUG ]       "luminous"
[2017-05-14 01:34:44,671][gdb3][DEBUG ]     ], 
[2017-05-14 01:34:44,671][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-14 01:34:44,671][gdb3][DEBUG ]     "required_mon": [
[2017-05-14 01:34:44,671][gdb3][DEBUG ]       "kraken", 
[2017-05-14 01:34:44,671][gdb3][DEBUG ]       "luminous"
[2017-05-14 01:34:44,671][gdb3][DEBUG ]     ]
[2017-05-14 01:34:44,671][gdb3][DEBUG ]   }, 
[2017-05-14 01:34:44,672][gdb3][DEBUG ]   "monmap": {
[2017-05-14 01:34:44,672][gdb3][DEBUG ]     "created": "2017-05-14 01:34:42.372623", 
[2017-05-14 01:34:44,672][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-14 01:34:44,672][gdb3][DEBUG ]     "features": {
[2017-05-14 01:34:44,672][gdb3][DEBUG ]       "optional": [], 
[2017-05-14 01:34:44,672][gdb3][DEBUG ]       "persistent": [
[2017-05-14 01:34:44,672][gdb3][DEBUG ]         "kraken", 
[2017-05-14 01:34:44,672][gdb3][DEBUG ]         "luminous"
[2017-05-14 01:34:44,672][gdb3][DEBUG ]       ]
[2017-05-14 01:34:44,672][gdb3][DEBUG ]     }, 
[2017-05-14 01:34:44,672][gdb3][DEBUG ]     "fsid": "293cf4c0-5136-436a-9d8c-bd1a6ff9e8b4", 
[2017-05-14 01:34:44,672][gdb3][DEBUG ]     "modified": "2017-05-14 01:34:42.622505", 
[2017-05-14 01:34:44,672][gdb3][DEBUG ]     "mons": [
[2017-05-14 01:34:44,672][gdb3][DEBUG ]       {
[2017-05-14 01:34:44,672][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-14 01:34:44,672][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-14 01:34:44,673][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-14 01:34:44,673][gdb3][DEBUG ]         "rank": 0
[2017-05-14 01:34:44,673][gdb3][DEBUG ]       }
[2017-05-14 01:34:44,673][gdb3][DEBUG ]     ]
[2017-05-14 01:34:44,673][gdb3][DEBUG ]   }, 
[2017-05-14 01:34:44,673][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-14 01:34:44,673][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-14 01:34:44,673][gdb3][DEBUG ]   "quorum": [
[2017-05-14 01:34:44,673][gdb3][DEBUG ]     0
[2017-05-14 01:34:44,673][gdb3][DEBUG ]   ], 
[2017-05-14 01:34:44,673][gdb3][DEBUG ]   "rank": 0, 
[2017-05-14 01:34:44,673][gdb3][DEBUG ]   "state": "leader", 
[2017-05-14 01:34:44,673][gdb3][DEBUG ]   "sync_provider": []
[2017-05-14 01:34:44,673][gdb3][DEBUG ] }
[2017-05-14 01:34:44,673][gdb3][DEBUG ] ********************************************************************************
[2017-05-14 01:34:44,673][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-14 01:34:44,674][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 01:34:44,740][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-14 01:34:44,755][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 01:34:44,768][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 01:34:44,769][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 01:34:44,785][gdb3][DEBUG ] detect machine type
[2017-05-14 01:34:44,788][gdb3][DEBUG ] find the location of an executable
[2017-05-14 01:34:44,789][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 01:34:44,854][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-14 01:34:44,854][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-14 01:34:44,855][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-14 01:34:44,856][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpls7L_f
[2017-05-14 01:34:44,872][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 01:34:44,885][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 01:34:44,886][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 01:34:44,902][gdb3][DEBUG ] detect machine type
[2017-05-14 01:34:44,904][gdb3][DEBUG ] get remote short hostname
[2017-05-14 01:34:44,905][gdb3][DEBUG ] fetch remote file
[2017-05-14 01:34:44,906][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 01:34:44,972][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-14 01:34:45,138][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-14 01:34:45,304][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-14 01:34:45,471][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-14 01:34:45,637][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-14 01:34:45,803][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-14 01:34:45,969][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-14 01:34:46,136][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-14 01:34:46,302][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-14 01:34:46,468][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-14 01:34:46,634][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.client.admin.keyring' and backing up old key as 'ceph.client.admin.keyring-20170514013446'
[2017-05-14 01:34:46,635][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mds.keyring' and backing up old key as 'ceph.bootstrap-mds.keyring-20170514013446'
[2017-05-14 01:34:46,636][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170514013446'
[2017-05-14 01:34:46,636][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-14 01:34:46,637][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-osd.keyring' and backing up old key as 'ceph.bootstrap-osd.keyring-20170514013446'
[2017-05-14 01:34:46,638][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-rgw.keyring' and backing up old key as 'ceph.bootstrap-rgw.keyring-20170514013446'
[2017-05-14 01:34:46,638][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpls7L_f
[2017-05-14 01:35:05,227][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 01:35:05,227][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-14 01:35:05,227][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 01:35:05,227][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 01:35:05,227][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 01:35:05,227][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 01:35:05,227][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 01:35:05,227][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa7e8778518>
[2017-05-14 01:35:05,227][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 01:35:05,228][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-14 01:35:05,228][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fa7e908f938>
[2017-05-14 01:35:05,228][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 01:35:05,228][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 01:35:05,228][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-14 01:35:05,254][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 01:35:05,268][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 01:35:05,269][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 01:35:05,286][gdb3][DEBUG ] detect machine type
[2017-05-14 01:35:05,288][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 01:41:15,385][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 01:41:15,385][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-14 01:41:15,385][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 01:41:15,385][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 01:41:15,385][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff7c5627908>
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff7c587daa0>
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 01:41:15,386][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 01:41:15,387][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-14 01:41:15,387][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-14 01:41:15,629][gdb0][DEBUG ] connection detected need for sudo
[2017-05-14 01:41:15,857][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-14 01:41:15,857][gdb0][DEBUG ] detect platform information from remote host
[2017-05-14 01:41:15,873][gdb0][DEBUG ] detect machine type
[2017-05-14 01:41:15,878][gdb0][DEBUG ] find the location of an executable
[2017-05-14 01:41:15,878][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 01:41:15,878][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-14 01:41:15,879][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 01:41:15,881][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-14 01:41:15,881][gdb0][DEBUG ] create a keyring file
[2017-05-14 01:41:15,883][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-14 01:41:15,883][gdb0][DEBUG ] find the location of an executable
[2017-05-14 01:41:15,886][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-14 01:41:16,006][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-14 01:41:16,014][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:16,014][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-14 01:41:16,014][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-14 01:41:16,022][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-14 01:41:16,029][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-14 01:41:16,045][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:16,045][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:16,045][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:16,045][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-14 01:41:16,048][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-14 01:41:16,056][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-14 01:41:16,072][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-14 01:41:16,073][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:16,073][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-14 01:41:16,073][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:16,073][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-14 01:41:16,089][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-14 01:41:16,096][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-14 01:41:16,097][gdb0][WARNING] backup header from main header.
[2017-05-14 01:41:16,097][gdb0][WARNING] 
[2017-05-14 01:41:17,164][gdb0][DEBUG ] ****************************************************************************
[2017-05-14 01:41:17,164][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-14 01:41:17,164][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-14 01:41:17,164][gdb0][DEBUG ] ****************************************************************************
[2017-05-14 01:41:17,164][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-14 01:41:17,164][gdb0][DEBUG ] other utilities.
[2017-05-14 01:41:17,164][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-14 01:41:18,131][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-14 01:41:18,131][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-14 01:41:18,131][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-14 01:41:18,131][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:41:18,147][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:41:18,179][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:41:18,187][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:18,187][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-14 01:41:18,188][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:18,188][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-14 01:41:18,188][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:18,188][gdb0][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-14 01:41:18,188][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:d092559d-0277-48ad-8d5e-6062b06134ac --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-14 01:41:19,205][gdb0][DEBUG ] Setting name!
[2017-05-14 01:41:19,205][gdb0][DEBUG ] partNum is 0
[2017-05-14 01:41:19,205][gdb0][DEBUG ] REALLY setting name!
[2017-05-14 01:41:19,205][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-14 01:41:19,206][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 01:41:19,206][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:41:19,320][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:41:19,434][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:41:19,466][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:19,466][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:19,466][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-14 01:41:19,466][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:19,466][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:19,466][gdb0][WARNING] ptype_tobe_for_name: name = block
[2017-05-14 01:41:19,466][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:19,466][gdb0][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-14 01:41:19,466][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:1e39a873-9dce-4db8-91af-3c729005dd12 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-14 01:41:20,534][gdb0][DEBUG ] Setting name!
[2017-05-14 01:41:20,534][gdb0][DEBUG ] partNum is 1
[2017-05-14 01:41:20,534][gdb0][DEBUG ] REALLY setting name!
[2017-05-14 01:41:20,534][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-14 01:41:20,534][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 01:41:20,534][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:41:20,749][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:41:20,963][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:41:21,178][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:21,178][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:21,178][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-14 01:41:21,178][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/1e39a873-9dce-4db8-91af-3c729005dd12
[2017-05-14 01:41:21,178][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-14 01:41:22,195][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-14 01:41:22,195][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 01:41:22,196][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:41:22,410][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:41:22,524][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:41:22,739][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/1e39a873-9dce-4db8-91af-3c729005dd12
[2017-05-14 01:41:22,739][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-14 01:41:22,739][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-14 01:41:22,853][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-14 01:41:22,853][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-14 01:41:22,853][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-14 01:41:22,853][gdb0][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-14 01:41:22,853][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-14 01:41:22,853][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-14 01:41:22,854][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-14 01:41:22,854][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-14 01:41:22,854][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-14 01:41:22,854][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.RepkO4 with options noatime,inode64
[2017-05-14 01:41:22,854][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.RepkO4
[2017-05-14 01:41:22,854][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.RepkO4
[2017-05-14 01:41:22,854][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.RepkO4/ceph_fsid.5514.tmp
[2017-05-14 01:41:22,855][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.RepkO4/fsid.5514.tmp
[2017-05-14 01:41:22,855][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.RepkO4/magic.5514.tmp
[2017-05-14 01:41:22,855][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.RepkO4/block_uuid.5514.tmp
[2017-05-14 01:41:22,855][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.RepkO4/block -> /dev/disk/by-partuuid/1e39a873-9dce-4db8-91af-3c729005dd12
[2017-05-14 01:41:22,855][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.RepkO4/type.5514.tmp
[2017-05-14 01:41:22,855][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.RepkO4
[2017-05-14 01:41:22,855][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.RepkO4
[2017-05-14 01:41:22,855][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.RepkO4
[2017-05-14 01:41:22,871][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:41:22,871][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-14 01:41:23,938][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-14 01:41:23,938][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-14 01:41:23,938][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-14 01:41:23,939][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-14 01:41:23,939][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 01:41:23,939][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:41:23,939][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:41:24,003][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:41:24,003][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-14 01:41:24,009][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-14 01:41:29,131][gdb0][INFO  ] checking OSD status...
[2017-05-14 01:41:29,131][gdb0][DEBUG ] find the location of an executable
[2017-05-14 01:41:29,134][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-14 01:41:29,199][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-14 01:47:01,301][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 01:47:01,301][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-14 01:47:01,301][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8f3690a908>
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-14 01:47:01,302][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8f36b60aa0>
[2017-05-14 01:47:01,303][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 01:47:01,303][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 01:47:01,303][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-14 01:47:01,303][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-14 01:47:01,560][gdb1][DEBUG ] connection detected need for sudo
[2017-05-14 01:47:01,789][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-14 01:47:01,790][gdb1][DEBUG ] detect platform information from remote host
[2017-05-14 01:47:01,806][gdb1][DEBUG ] detect machine type
[2017-05-14 01:47:01,810][gdb1][DEBUG ] find the location of an executable
[2017-05-14 01:47:01,811][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 01:47:01,811][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-14 01:47:01,811][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 01:47:01,814][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-14 01:47:01,814][gdb1][DEBUG ] create a keyring file
[2017-05-14 01:47:01,816][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-14 01:47:01,816][gdb1][DEBUG ] find the location of an executable
[2017-05-14 01:47:01,818][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-14 01:47:01,989][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-14 01:47:01,989][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:01,989][gdb1][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-14 01:47:01,989][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-14 01:47:01,989][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-14 01:47:01,990][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-14 01:47:01,990][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:01,990][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:01,990][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:01,990][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-14 01:47:01,991][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-14 01:47:02,007][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-14 01:47:02,010][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-14 01:47:02,026][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:02,026][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-14 01:47:02,026][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:02,026][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-14 01:47:02,026][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-14 01:47:02,027][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-14 01:47:02,028][gdb1][WARNING] backup header from main header.
[2017-05-14 01:47:02,028][gdb1][WARNING] 
[2017-05-14 01:47:03,044][gdb1][DEBUG ] ****************************************************************************
[2017-05-14 01:47:03,045][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-14 01:47:03,045][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-14 01:47:03,045][gdb1][DEBUG ] ****************************************************************************
[2017-05-14 01:47:03,045][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-14 01:47:03,045][gdb1][DEBUG ] other utilities.
[2017-05-14 01:47:03,045][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-14 01:47:04,062][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-14 01:47:04,062][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 01:47:04,062][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-14 01:47:04,062][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:47:04,078][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:47:04,110][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:47:04,126][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:04,126][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-14 01:47:04,126][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:04,126][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-14 01:47:04,126][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:04,126][gdb1][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-14 01:47:04,126][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:8160fcfd-1ce4-43d0-a576-767834b2c432 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-14 01:47:05,143][gdb1][DEBUG ] Setting name!
[2017-05-14 01:47:05,143][gdb1][DEBUG ] partNum is 0
[2017-05-14 01:47:05,143][gdb1][DEBUG ] REALLY setting name!
[2017-05-14 01:47:05,143][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 01:47:05,143][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 01:47:05,143][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:47:05,257][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:47:05,422][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:47:05,438][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:05,438][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:05,438][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-14 01:47:05,438][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:05,438][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:05,438][gdb1][WARNING] ptype_tobe_for_name: name = block
[2017-05-14 01:47:05,438][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:05,438][gdb1][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-14 01:47:05,438][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:d14dc9f5-111a-408b-84d8-326cbc9a2073 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-14 01:47:06,455][gdb1][DEBUG ] Setting name!
[2017-05-14 01:47:06,455][gdb1][DEBUG ] partNum is 1
[2017-05-14 01:47:06,456][gdb1][DEBUG ] REALLY setting name!
[2017-05-14 01:47:06,456][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 01:47:06,456][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 01:47:06,456][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:47:06,670][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:47:06,935][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:47:07,200][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:07,200][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:07,200][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-14 01:47:07,200][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/d14dc9f5-111a-408b-84d8-326cbc9a2073
[2017-05-14 01:47:07,200][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-14 01:47:08,217][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 01:47:08,217][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 01:47:08,217][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:47:08,432][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:47:08,646][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:47:08,861][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/d14dc9f5-111a-408b-84d8-326cbc9a2073
[2017-05-14 01:47:08,861][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-14 01:47:08,861][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-14 01:47:08,925][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-14 01:47:08,926][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-14 01:47:08,926][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-14 01:47:08,926][gdb1][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-14 01:47:08,926][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-14 01:47:08,926][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-14 01:47:08,926][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-14 01:47:08,926][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-14 01:47:08,926][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-14 01:47:08,926][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.yaYOrD with options noatime,inode64
[2017-05-14 01:47:08,926][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.yaYOrD
[2017-05-14 01:47:08,958][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.yaYOrD
[2017-05-14 01:47:08,958][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.yaYOrD/ceph_fsid.10513.tmp
[2017-05-14 01:47:08,958][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.yaYOrD/fsid.10513.tmp
[2017-05-14 01:47:08,962][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.yaYOrD/magic.10513.tmp
[2017-05-14 01:47:08,965][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.yaYOrD/block_uuid.10513.tmp
[2017-05-14 01:47:08,965][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.yaYOrD/block -> /dev/disk/by-partuuid/d14dc9f5-111a-408b-84d8-326cbc9a2073
[2017-05-14 01:47:08,967][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.yaYOrD/type.10513.tmp
[2017-05-14 01:47:08,970][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.yaYOrD
[2017-05-14 01:47:08,971][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.yaYOrD
[2017-05-14 01:47:08,971][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.yaYOrD
[2017-05-14 01:47:09,035][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:09,035][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-14 01:47:10,052][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-14 01:47:10,052][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-14 01:47:10,052][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-14 01:47:10,052][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 01:47:10,052][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 01:47:10,052][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:47:10,052][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:47:10,166][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:47:10,170][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-14 01:47:10,188][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-14 01:47:15,260][gdb1][INFO  ] checking OSD status...
[2017-05-14 01:47:15,260][gdb1][DEBUG ] find the location of an executable
[2017-05-14 01:47:15,263][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-14 01:47:15,378][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-14 01:47:58,075][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 01:47:58,075][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-14 01:47:58,075][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2d38e54908>
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-14 01:47:58,076][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2d390aaaa0>
[2017-05-14 01:47:58,077][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 01:47:58,077][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 01:47:58,077][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-14 01:47:58,077][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-14 01:47:58,318][gdb1][DEBUG ] connection detected need for sudo
[2017-05-14 01:47:58,545][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-14 01:47:58,545][gdb1][DEBUG ] detect platform information from remote host
[2017-05-14 01:47:58,561][gdb1][DEBUG ] detect machine type
[2017-05-14 01:47:58,565][gdb1][DEBUG ] find the location of an executable
[2017-05-14 01:47:58,566][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 01:47:58,566][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-14 01:47:58,566][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 01:47:58,569][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-14 01:47:58,569][gdb1][DEBUG ] create a keyring file
[2017-05-14 01:47:58,571][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-14 01:47:58,571][gdb1][DEBUG ] find the location of an executable
[2017-05-14 01:47:58,573][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-14 01:47:58,693][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-14 01:47:58,709][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:58,709][gdb1][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-14 01:47:58,709][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-14 01:47:58,716][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-14 01:47:58,724][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-14 01:47:58,740][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:58,740][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:58,740][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:58,740][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-14 01:47:58,740][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-14 01:47:58,740][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-14 01:47:58,744][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-14 01:47:58,759][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-14 01:47:58,763][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-14 01:47:58,779][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:58,779][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-14 01:47:58,779][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:47:58,779][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-14 01:47:58,787][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-14 01:47:58,788][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-14 01:47:58,804][gdb1][WARNING] 10+0 records in
[2017-05-14 01:47:58,804][gdb1][WARNING] 10+0 records out
[2017-05-14 01:47:58,804][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00494608 s, 2.1 GB/s
[2017-05-14 01:47:58,804][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-14 01:47:58,836][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-14 01:47:58,950][gdb1][WARNING] 10+0 records in
[2017-05-14 01:47:58,950][gdb1][WARNING] 10+0 records out
[2017-05-14 01:47:58,950][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.113243 s, 92.6 MB/s
[2017-05-14 01:47:58,951][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-14 01:47:58,951][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-14 01:47:58,954][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-14 01:47:58,954][gdb1][WARNING] backup header from main header.
[2017-05-14 01:47:58,954][gdb1][WARNING] 
[2017-05-14 01:47:58,955][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-14 01:47:58,956][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-14 01:47:58,956][gdb1][WARNING] 
[2017-05-14 01:47:58,956][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-14 01:47:58,956][gdb1][WARNING] 
[2017-05-14 01:47:59,973][gdb1][DEBUG ] ****************************************************************************
[2017-05-14 01:47:59,973][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-14 01:47:59,973][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-14 01:47:59,973][gdb1][DEBUG ] ****************************************************************************
[2017-05-14 01:47:59,974][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-14 01:47:59,974][gdb1][DEBUG ] other utilities.
[2017-05-14 01:47:59,974][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-14 01:48:00,991][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-14 01:48:00,991][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 01:48:00,991][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-14 01:48:00,991][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:48:00,999][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:48:01,030][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:48:01,046][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:48:01,046][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-14 01:48:01,046][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:48:01,046][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-14 01:48:01,046][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:48:01,046][gdb1][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-14 01:48:01,047][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:4113656f-3940-402a-8494-e2d803cdcbb8 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-14 01:48:02,064][gdb1][DEBUG ] Setting name!
[2017-05-14 01:48:02,064][gdb1][DEBUG ] partNum is 0
[2017-05-14 01:48:02,064][gdb1][DEBUG ] REALLY setting name!
[2017-05-14 01:48:02,064][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 01:48:02,064][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 01:48:02,064][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:48:02,178][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:48:02,343][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:48:02,375][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:48:02,375][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:48:02,375][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-14 01:48:02,375][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:48:02,375][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:48:02,375][gdb1][WARNING] ptype_tobe_for_name: name = block
[2017-05-14 01:48:02,375][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:48:02,375][gdb1][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-14 01:48:02,375][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:234f6cb7-fb49-44fa-89e5-388efe90ae66 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-14 01:48:03,392][gdb1][DEBUG ] Setting name!
[2017-05-14 01:48:03,393][gdb1][DEBUG ] partNum is 1
[2017-05-14 01:48:03,393][gdb1][DEBUG ] REALLY setting name!
[2017-05-14 01:48:03,393][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 01:48:03,393][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 01:48:03,393][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:48:03,607][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:48:03,872][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:48:03,904][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:48:03,904][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:48:03,904][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-14 01:48:03,904][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/234f6cb7-fb49-44fa-89e5-388efe90ae66
[2017-05-14 01:48:03,904][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-14 01:48:04,921][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 01:48:04,922][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 01:48:04,922][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:48:05,136][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:48:05,401][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:48:05,615][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/234f6cb7-fb49-44fa-89e5-388efe90ae66
[2017-05-14 01:48:05,616][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-14 01:48:05,616][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-14 01:48:05,648][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-14 01:48:05,648][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-14 01:48:05,648][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-14 01:48:05,648][gdb1][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-14 01:48:05,648][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-14 01:48:05,648][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-14 01:48:05,648][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-14 01:48:05,648][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-14 01:48:05,649][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-14 01:48:05,649][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.W6ppKC with options noatime,inode64
[2017-05-14 01:48:05,649][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.W6ppKC
[2017-05-14 01:48:05,713][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.W6ppKC
[2017-05-14 01:48:05,713][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.W6ppKC/ceph_fsid.11421.tmp
[2017-05-14 01:48:05,713][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.W6ppKC/fsid.11421.tmp
[2017-05-14 01:48:05,713][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.W6ppKC/magic.11421.tmp
[2017-05-14 01:48:05,713][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.W6ppKC/block_uuid.11421.tmp
[2017-05-14 01:48:05,713][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.W6ppKC/block -> /dev/disk/by-partuuid/234f6cb7-fb49-44fa-89e5-388efe90ae66
[2017-05-14 01:48:05,713][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.W6ppKC/type.11421.tmp
[2017-05-14 01:48:05,713][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.W6ppKC
[2017-05-14 01:48:05,713][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.W6ppKC
[2017-05-14 01:48:05,713][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.W6ppKC
[2017-05-14 01:48:05,729][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 01:48:05,729][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-14 01:48:06,796][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-14 01:48:06,797][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-14 01:48:06,797][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-14 01:48:06,797][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 01:48:06,797][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 01:48:06,797][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:48:06,797][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 01:48:06,829][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 01:48:06,845][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-14 01:48:06,863][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-14 01:48:11,985][gdb1][INFO  ] checking OSD status...
[2017-05-14 01:48:11,986][gdb1][DEBUG ] find the location of an executable
[2017-05-14 01:48:11,989][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-14 01:48:12,104][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-14 02:05:05,816][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:05:05,816][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-14 02:05:05,816][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:05:05,817][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:05:05,817][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:05:05,817][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 02:05:05,817][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:05:05,817][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efc95687fc8>
[2017-05-14 02:05:05,817][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:05:05,817][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-14 02:05:05,817][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7efc95f9a1b8>
[2017-05-14 02:05:05,817][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:05:05,817][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:05:05,817][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-14 02:05:05,817][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-14 02:05:05,817][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-14 02:05:05,817][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-14 02:05:05,843][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:05:05,858][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:05:05,858][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:05:05,875][gdb3][DEBUG ] detect machine type
[2017-05-14 02:05:05,877][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 02:05:05,877][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-14 02:05:05,878][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-14 02:05:05,916][gdb3][DEBUG ] Reading package lists...
[2017-05-14 02:05:06,081][gdb3][DEBUG ] Building dependency tree...
[2017-05-14 02:05:06,081][gdb3][DEBUG ] Reading state information...
[2017-05-14 02:05:06,195][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-14 02:05:06,195][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-14 02:05:06,195][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-14 02:05:06,196][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-14 02:05:06,196][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-14 02:05:06,196][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 ntp python-blinker python-cephfs
[2017-05-14 02:05:06,196][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-05-14 02:05:06,196][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-05-14 02:05:06,196][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-05-14 02:05:06,196][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-05-14 02:05:06,196][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-05-14 02:05:06,196][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-14 02:05:06,212][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-14 02:05:06,212][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-14 02:05:06,326][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 26 not upgraded.
[2017-05-14 02:05:06,326][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-14 02:05:06,390][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 79236 files and directories currently installed.)
[2017-05-14 02:05:06,391][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-14 02:05:06,558][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-14 02:05:06,622][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-14 02:05:06,687][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-14 02:05:06,901][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-14 02:05:06,966][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-14 02:05:07,131][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-14 02:05:07,195][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-14 02:05:07,227][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-14 02:05:07,393][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-14 02:05:07,457][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-14 02:05:07,465][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-14 02:05:07,629][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-14 02:05:07,661][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-14 02:05:07,830][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-14 02:05:07,862][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-14 02:05:07,878][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-14 02:05:07,992][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-14 02:05:08,808][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-14 02:05:08,975][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:05:08,975][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-14 02:05:08,976][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:05:08,976][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:05:08,976][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:05:08,976][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 02:05:08,976][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:05:08,976][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f032b2ad710>
[2017-05-14 02:05:08,976][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:05:08,976][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-14 02:05:08,976][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f032bbba230>
[2017-05-14 02:05:08,976][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:05:08,976][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:05:08,976][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-14 02:05:09,003][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:05:09,017][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:05:09,017][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:05:09,034][gdb3][DEBUG ] detect machine type
[2017-05-14 02:05:09,036][gdb3][DEBUG ] find the location of an executable
[2017-05-14 02:05:09,052][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:05:09,066][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:05:09,066][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:05:09,082][gdb3][DEBUG ] detect machine type
[2017-05-14 02:05:09,085][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 02:05:09,085][gdb3][INFO  ] purging data on gdb3
[2017-05-14 02:05:09,086][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-14 02:05:09,099][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-14 02:05:09,270][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:05:09,270][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-14 02:05:09,270][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:05:09,270][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:05:09,270][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:05:09,270][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 02:05:09,271][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:05:09,271][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f63dcf5c9e0>
[2017-05-14 02:05:09,271][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:05:09,271][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f63dd81f848>
[2017-05-14 02:05:09,271][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:05:09,271][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:05:28,576][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:05:28,576][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-14 02:05:28,576][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:05:28,576][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:05:28,576][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:05:28,576][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 02:05:28,576][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:05:28,576][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5252a31560>
[2017-05-14 02:05:28,577][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:05:28,577][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-14 02:05:28,577][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-14 02:05:28,577][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f52530b5758>
[2017-05-14 02:05:28,577][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-14 02:05:28,577][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:05:28,577][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-14 02:05:28,577][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:05:28,577][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-14 02:05:28,577][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-14 02:05:28,577][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-14 02:05:28,603][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:05:28,617][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:05:28,617][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:05:28,633][gdb3][DEBUG ] detect machine type
[2017-05-14 02:05:28,636][gdb3][DEBUG ] find the location of an executable
[2017-05-14 02:05:28,637][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-14 02:05:28,648][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-14 02:05:28,654][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-14 02:05:28,654][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-14 02:05:28,655][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-14 02:05:28,655][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-14 02:05:28,655][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-14 02:05:28,655][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-14 02:05:28,655][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-14 02:05:28,655][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-14 02:07:06,321][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe090213e60>
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fe0901e8b18>
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-14 02:07:06,322][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:07:06,323][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-14 02:07:06,323][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-14 02:07:06,349][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:07:06,364][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:07:06,364][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:07:06,380][gdb3][DEBUG ] detect machine type
[2017-05-14 02:07:06,383][gdb3][DEBUG ] find the location of an executable
[2017-05-14 02:07:06,383][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-14 02:07:06,383][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-14 02:07:06,383][gdb3][DEBUG ] get remote short hostname
[2017-05-14 02:07:06,384][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-14 02:07:06,384][gdb3][DEBUG ] get remote short hostname
[2017-05-14 02:07:06,384][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-14 02:07:06,385][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 02:07:06,386][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-14 02:07:06,386][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-14 02:07:06,387][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-14 02:07:06,387][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-14 02:07:06,387][gdb3][DEBUG ] create the monitor keyring file
[2017-05-14 02:07:06,388][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-14 02:07:06,426][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-14 02:07:06,427][gdb3][DEBUG ] ceph-mon: set fsid to ae990821-d2fc-42a7-9cf0-32545c6072e7
[2017-05-14 02:07:06,430][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-14 02:07:06,432][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-14 02:07:06,432][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-14 02:07:06,433][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-14 02:07:06,434][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-14 02:07:06,503][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-14 02:07:06,570][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-14 02:07:08,640][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 02:07:08,705][gdb3][DEBUG ] ********************************************************************************
[2017-05-14 02:07:08,706][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-14 02:07:08,706][gdb3][DEBUG ] {
[2017-05-14 02:07:08,706][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-14 02:07:08,706][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-14 02:07:08,706][gdb3][DEBUG ]   "features": {
[2017-05-14 02:07:08,706][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-14 02:07:08,706][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-14 02:07:08,706][gdb3][DEBUG ]       "kraken", 
[2017-05-14 02:07:08,706][gdb3][DEBUG ]       "luminous"
[2017-05-14 02:07:08,706][gdb3][DEBUG ]     ], 
[2017-05-14 02:07:08,706][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-14 02:07:08,707][gdb3][DEBUG ]     "required_mon": [
[2017-05-14 02:07:08,707][gdb3][DEBUG ]       "kraken", 
[2017-05-14 02:07:08,707][gdb3][DEBUG ]       "luminous"
[2017-05-14 02:07:08,707][gdb3][DEBUG ]     ]
[2017-05-14 02:07:08,707][gdb3][DEBUG ]   }, 
[2017-05-14 02:07:08,707][gdb3][DEBUG ]   "monmap": {
[2017-05-14 02:07:08,707][gdb3][DEBUG ]     "created": "2017-05-14 02:07:06.411594", 
[2017-05-14 02:07:08,707][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-14 02:07:08,707][gdb3][DEBUG ]     "features": {
[2017-05-14 02:07:08,707][gdb3][DEBUG ]       "optional": [], 
[2017-05-14 02:07:08,707][gdb3][DEBUG ]       "persistent": [
[2017-05-14 02:07:08,707][gdb3][DEBUG ]         "kraken", 
[2017-05-14 02:07:08,707][gdb3][DEBUG ]         "luminous"
[2017-05-14 02:07:08,707][gdb3][DEBUG ]       ]
[2017-05-14 02:07:08,707][gdb3][DEBUG ]     }, 
[2017-05-14 02:07:08,707][gdb3][DEBUG ]     "fsid": "ae990821-d2fc-42a7-9cf0-32545c6072e7", 
[2017-05-14 02:07:08,707][gdb3][DEBUG ]     "modified": "2017-05-14 02:07:06.660160", 
[2017-05-14 02:07:08,708][gdb3][DEBUG ]     "mons": [
[2017-05-14 02:07:08,708][gdb3][DEBUG ]       {
[2017-05-14 02:07:08,708][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-14 02:07:08,708][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-14 02:07:08,708][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-14 02:07:08,708][gdb3][DEBUG ]         "rank": 0
[2017-05-14 02:07:08,708][gdb3][DEBUG ]       }
[2017-05-14 02:07:08,708][gdb3][DEBUG ]     ]
[2017-05-14 02:07:08,708][gdb3][DEBUG ]   }, 
[2017-05-14 02:07:08,708][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-14 02:07:08,708][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-14 02:07:08,708][gdb3][DEBUG ]   "quorum": [
[2017-05-14 02:07:08,708][gdb3][DEBUG ]     0
[2017-05-14 02:07:08,708][gdb3][DEBUG ]   ], 
[2017-05-14 02:07:08,708][gdb3][DEBUG ]   "rank": 0, 
[2017-05-14 02:07:08,708][gdb3][DEBUG ]   "state": "leader", 
[2017-05-14 02:07:08,708][gdb3][DEBUG ]   "sync_provider": []
[2017-05-14 02:07:08,709][gdb3][DEBUG ] }
[2017-05-14 02:07:08,709][gdb3][DEBUG ] ********************************************************************************
[2017-05-14 02:07:08,709][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-14 02:07:08,709][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 02:07:08,775][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-14 02:07:08,792][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:07:08,805][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:07:08,805][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:07:08,822][gdb3][DEBUG ] detect machine type
[2017-05-14 02:07:08,824][gdb3][DEBUG ] find the location of an executable
[2017-05-14 02:07:08,825][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 02:07:08,890][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-14 02:07:08,890][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-14 02:07:08,891][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-14 02:07:08,892][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmp2nJvuk
[2017-05-14 02:07:08,907][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:07:08,921][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:07:08,921][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:07:08,938][gdb3][DEBUG ] detect machine type
[2017-05-14 02:07:08,941][gdb3][DEBUG ] get remote short hostname
[2017-05-14 02:07:08,941][gdb3][DEBUG ] fetch remote file
[2017-05-14 02:07:08,942][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 02:07:09,008][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-14 02:07:09,174][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-14 02:07:09,341][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-14 02:07:09,507][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-14 02:07:09,673][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-14 02:07:09,840][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-14 02:07:10,006][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-14 02:07:10,172][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-14 02:07:10,339][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-14 02:07:10,505][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-14 02:07:10,670][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-14 02:07:10,670][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-14 02:07:10,671][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170514020710'
[2017-05-14 02:07:10,671][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-14 02:07:10,671][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-14 02:07:10,671][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-14 02:07:10,671][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp2nJvuk
[2017-05-14 02:14:57,433][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:14:57,433][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5de2c10908>
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:14:57,434][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-14 02:14:57,435][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5de2e66aa0>
[2017-05-14 02:14:57,435][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:14:57,435][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:14:57,435][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-14 02:14:57,435][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-14 02:14:57,682][gdb1][DEBUG ] connection detected need for sudo
[2017-05-14 02:14:57,910][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-14 02:14:57,910][gdb1][DEBUG ] detect platform information from remote host
[2017-05-14 02:14:57,927][gdb1][DEBUG ] detect machine type
[2017-05-14 02:14:57,931][gdb1][DEBUG ] find the location of an executable
[2017-05-14 02:14:57,932][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 02:14:57,932][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-14 02:14:57,933][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 02:14:57,935][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-14 02:14:57,935][gdb1][DEBUG ] create a keyring file
[2017-05-14 02:14:57,937][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-14 02:14:57,937][gdb1][DEBUG ] find the location of an executable
[2017-05-14 02:14:57,940][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-14 02:14:58,110][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-14 02:14:58,111][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:14:58,111][gdb1][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-14 02:14:58,111][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-14 02:14:58,111][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-14 02:14:58,111][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-14 02:14:58,111][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:14:58,111][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:14:58,111][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:14:58,111][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-14 02:14:58,115][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-14 02:14:58,130][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-14 02:14:58,134][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-14 02:14:58,150][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:14:58,150][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-14 02:14:58,150][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:14:58,150][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-14 02:14:58,150][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-14 02:14:58,150][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-14 02:14:58,150][gdb1][WARNING] backup header from main header.
[2017-05-14 02:14:58,150][gdb1][WARNING] 
[2017-05-14 02:14:59,167][gdb1][DEBUG ] ****************************************************************************
[2017-05-14 02:14:59,168][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-14 02:14:59,168][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-14 02:14:59,168][gdb1][DEBUG ] ****************************************************************************
[2017-05-14 02:14:59,168][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-14 02:14:59,168][gdb1][DEBUG ] other utilities.
[2017-05-14 02:14:59,168][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-14 02:15:00,186][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-14 02:15:00,186][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:15:00,186][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-14 02:15:00,186][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:15:00,187][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:15:00,219][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:15:00,227][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:15:00,227][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-14 02:15:00,227][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:15:00,228][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-14 02:15:00,228][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:15:00,228][gdb1][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-14 02:15:00,228][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:0b8b4eda-171f-4fd7-90c7-80ce1dba4a38 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-14 02:15:01,295][gdb1][DEBUG ] Setting name!
[2017-05-14 02:15:01,295][gdb1][DEBUG ] partNum is 0
[2017-05-14 02:15:01,295][gdb1][DEBUG ] REALLY setting name!
[2017-05-14 02:15:01,295][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:15:01,295][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 02:15:01,295][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:15:01,359][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:15:01,474][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:15:01,538][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:15:01,538][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:15:01,538][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-14 02:15:01,538][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:15:01,538][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:15:01,538][gdb1][WARNING] ptype_tobe_for_name: name = block
[2017-05-14 02:15:01,538][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:15:01,538][gdb1][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-14 02:15:01,538][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:d206c91b-f410-434f-b7be-8226e8426dae --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-14 02:15:02,556][gdb1][DEBUG ] Setting name!
[2017-05-14 02:15:02,556][gdb1][DEBUG ] partNum is 1
[2017-05-14 02:15:02,556][gdb1][DEBUG ] REALLY setting name!
[2017-05-14 02:15:02,556][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:15:02,556][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 02:15:02,556][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:15:02,771][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:15:03,035][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:15:03,250][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:15:03,250][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:15:03,250][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-14 02:15:03,250][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/d206c91b-f410-434f-b7be-8226e8426dae
[2017-05-14 02:15:03,251][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-14 02:15:04,268][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:15:04,268][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 02:15:04,268][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:15:04,482][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:15:04,647][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:15:04,711][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/d206c91b-f410-434f-b7be-8226e8426dae
[2017-05-14 02:15:04,711][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-14 02:15:04,711][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-14 02:15:04,775][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-14 02:15:04,775][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-14 02:15:04,775][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-14 02:15:04,776][gdb1][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-14 02:15:04,776][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-14 02:15:04,776][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-14 02:15:04,776][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-14 02:15:04,776][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-14 02:15:04,776][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-14 02:15:04,776][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.JUKw4Y with options noatime,inode64
[2017-05-14 02:15:04,776][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.JUKw4Y
[2017-05-14 02:15:04,808][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.JUKw4Y
[2017-05-14 02:15:04,808][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.JUKw4Y/ceph_fsid.12922.tmp
[2017-05-14 02:15:04,808][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.JUKw4Y/fsid.12922.tmp
[2017-05-14 02:15:04,824][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.JUKw4Y/magic.12922.tmp
[2017-05-14 02:15:04,824][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.JUKw4Y/block_uuid.12922.tmp
[2017-05-14 02:15:04,824][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.JUKw4Y/block -> /dev/disk/by-partuuid/d206c91b-f410-434f-b7be-8226e8426dae
[2017-05-14 02:15:04,825][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.JUKw4Y/type.12922.tmp
[2017-05-14 02:15:04,829][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.JUKw4Y
[2017-05-14 02:15:04,829][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.JUKw4Y
[2017-05-14 02:15:04,829][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.JUKw4Y
[2017-05-14 02:15:04,893][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:15:04,893][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-14 02:15:05,910][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-14 02:15:05,910][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-14 02:15:05,910][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-14 02:15:05,910][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:15:05,910][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 02:15:05,910][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:15:05,910][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:15:05,974][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:15:05,975][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-14 02:15:05,985][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-14 02:15:11,106][gdb1][INFO  ] checking OSD status...
[2017-05-14 02:15:11,106][gdb1][DEBUG ] find the location of an executable
[2017-05-14 02:15:11,109][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-14 02:15:11,224][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 15:53:26,366][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:53:26,367][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 15:53:26,367][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:53:26,368][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:53:26,368][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:53:26,368][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 15:53:26,368][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:53:26,368][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faa7c76f0e0>
[2017-05-31 15:53:26,368][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:53:26,368][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 15:53:26,368][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7faa7d07c1b8>
[2017-05-31 15:53:26,368][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:53:26,368][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:53:26,368][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 15:53:26,368][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 15:53:26,368][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 15:53:26,368][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 15:53:26,406][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:53:26,422][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:53:26,423][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:53:26,439][gdb3][DEBUG ] detect machine type
[2017-05-31 15:53:26,441][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 15:53:26,441][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 15:53:26,442][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 15:53:26,482][gdb3][DEBUG ] Reading package lists...
[2017-05-31 15:53:26,646][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 15:53:26,647][gdb3][DEBUG ] Reading state information...
[2017-05-31 15:53:26,711][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 15:53:26,711][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 15:53:26,711][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 15:53:26,711][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 15:53:26,711][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 15:53:26,711][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 15:53:26,712][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 15:53:26,712][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 15:53:26,712][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 15:53:26,712][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 15:53:26,712][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 15:53:26,712][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 15:53:26,712][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 15:53:26,744][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 15:53:26,744][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 15:53:26,909][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 15:53:26,909][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 15:53:26,909][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 15:53:26,913][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 15:53:27,128][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 15:53:27,192][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 15:53:27,224][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 15:53:27,441][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 15:53:27,555][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 15:53:27,720][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 15:53:27,784][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 15:53:27,784][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 15:53:27,949][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 15:53:28,014][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 15:53:28,046][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 15:53:28,211][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 15:53:28,212][gdb3][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-05-31 15:53:28,215][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 15:53:28,380][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 15:53:28,444][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 15:53:28,445][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 15:53:28,559][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 15:53:29,375][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 15:53:29,541][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f88807cb758>
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f88810d8230>
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:53:29,542][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:53:29,542][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 15:53:29,569][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:53:29,583][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:53:29,584][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:53:29,600][gdb3][DEBUG ] detect machine type
[2017-05-31 15:53:29,603][gdb3][DEBUG ] find the location of an executable
[2017-05-31 15:53:29,619][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:53:29,633][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:53:29,633][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:53:29,650][gdb3][DEBUG ] detect machine type
[2017-05-31 15:53:29,653][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 15:53:29,653][gdb3][INFO  ] purging data on gdb3
[2017-05-31 15:53:29,653][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 15:53:29,666][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 15:53:29,838][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:53:29,838][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 15:53:29,838][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:53:29,838][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:53:29,838][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:53:29,838][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 15:53:29,838][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:53:29,839][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f353c9dba28>
[2017-05-31 15:53:29,839][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:53:29,839][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f353d29e848>
[2017-05-31 15:53:29,839][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:53:29,839][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:53:42,609][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:53:42,609][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 15:53:42,609][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:53:42,609][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:53:42,609][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:53:42,609][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 15:53:42,609][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:53:42,610][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1a3dcde5a8>
[2017-05-31 15:53:42,610][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:53:42,610][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 15:53:42,610][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 15:53:42,610][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f1a3e362758>
[2017-05-31 15:53:42,610][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 15:53:42,610][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:53:42,610][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 15:53:42,610][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:53:42,610][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 15:53:42,610][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 15:53:42,610][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 15:53:42,637][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:53:42,652][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:53:42,652][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:53:42,669][gdb3][DEBUG ] detect machine type
[2017-05-31 15:53:42,671][gdb3][DEBUG ] find the location of an executable
[2017-05-31 15:53:42,673][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 15:53:42,684][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 15:53:42,690][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 15:53:42,690][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 15:53:42,690][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 15:53:42,690][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 15:53:42,691][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 15:53:42,691][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 15:53:42,691][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 15:53:42,691][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 15:53:42,860][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:53:42,860][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 15:53:42,860][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:53:42,860][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:53:42,860][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:53:42,860][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 15:53:42,860][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 15:53:42,860][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:53:42,860][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f45b9d26ea8>
[2017-05-31 15:53:42,861][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:53:42,861][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f45b9cfab18>
[2017-05-31 15:53:42,861][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:53:42,861][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 15:53:42,861][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:53:42,861][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 15:53:42,862][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 15:53:42,889][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:53:42,903][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:53:42,904][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:53:42,921][gdb3][DEBUG ] detect machine type
[2017-05-31 15:53:42,924][gdb3][DEBUG ] find the location of an executable
[2017-05-31 15:53:42,924][ceph_deploy.mon][ERROR ] ceph needs to be installed in remote host: gdb3
[2017-05-31 15:53:42,924][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2017-05-31 15:54:32,794][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:54:32,795][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 15:54:32,795][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:54:32,795][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:54:32,795][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:54:32,795][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 15:54:32,795][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:54:32,795][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffba56925a8>
[2017-05-31 15:54:32,796][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:54:32,796][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 15:54:32,796][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 15:54:32,796][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7ffba5d16758>
[2017-05-31 15:54:32,796][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 15:54:32,796][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:54:32,796][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 15:54:32,796][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:54:32,796][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 15:54:32,796][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 15:54:32,797][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 15:54:32,823][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:54:32,837][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:54:32,837][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:54:32,854][gdb3][DEBUG ] detect machine type
[2017-05-31 15:54:32,857][gdb3][DEBUG ] find the location of an executable
[2017-05-31 15:54:32,858][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 15:54:32,869][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 15:54:32,875][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 15:54:32,875][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 15:54:32,875][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 15:54:32,875][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 15:54:32,876][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 15:54:32,876][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 15:54:32,876][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 15:54:32,876][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 15:54:33,041][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:54:33,041][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 15:54:33,041][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:54:33,042][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:54:33,042][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:54:33,042][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 15:54:33,042][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 15:54:33,042][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:54:33,042][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa556dd0ea8>
[2017-05-31 15:54:33,042][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:54:33,042][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fa556da4b18>
[2017-05-31 15:54:33,042][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:54:33,042][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 15:54:33,042][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:54:33,043][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 15:54:33,043][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 15:54:33,069][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:54:33,083][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:54:33,084][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:54:33,100][gdb3][DEBUG ] detect machine type
[2017-05-31 15:54:33,103][gdb3][DEBUG ] find the location of an executable
[2017-05-31 15:54:33,103][ceph_deploy.mon][ERROR ] ceph needs to be installed in remote host: gdb3
[2017-05-31 15:54:33,103][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2017-05-31 15:55:29,918][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:55:29,918][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-05-31 15:55:29,918][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:55:29,918][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:55:29,918][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:55:29,918][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 15:55:29,918][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 15:55:29,918][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:55:29,918][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd351ce6ea8>
[2017-05-31 15:55:29,919][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:55:29,919][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fd351cbab18>
[2017-05-31 15:55:29,919][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:55:29,919][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 15:55:29,919][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:55:29,919][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 15:55:29,919][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 15:55:29,946][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:55:29,960][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:55:29,961][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:55:29,977][gdb3][DEBUG ] detect machine type
[2017-05-31 15:55:29,980][gdb3][DEBUG ] find the location of an executable
[2017-05-31 15:55:29,980][ceph_deploy.mon][ERROR ] ceph needs to be installed in remote host: gdb3
[2017-05-31 15:55:29,981][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2017-05-31 15:57:16,388][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:57:16,388][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f31a005d5a8>
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f31a06e1758>
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:57:16,389][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 15:57:16,390][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 15:57:16,390][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 15:57:16,416][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:57:16,430][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:57:16,430][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:57:16,447][gdb3][DEBUG ] detect machine type
[2017-05-31 15:57:16,449][gdb3][DEBUG ] find the location of an executable
[2017-05-31 15:57:16,450][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 15:57:16,461][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 15:57:16,468][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 15:57:16,468][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 15:57:16,468][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 15:57:16,468][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 15:57:16,468][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 15:57:16,468][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 15:57:16,468][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 15:57:16,469][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 15:57:16,633][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:57:16,633][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 15:57:16,633][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:57:16,633][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:57:16,634][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:57:16,634][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 15:57:16,634][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 15:57:16,634][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:57:16,634][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff24bcc3ea8>
[2017-05-31 15:57:16,634][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:57:16,634][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7ff24bc97b18>
[2017-05-31 15:57:16,634][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:57:16,634][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 15:57:16,634][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:57:16,635][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 15:57:16,635][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 15:57:16,661][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:57:16,675][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:57:16,676][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:57:16,692][gdb3][DEBUG ] detect machine type
[2017-05-31 15:57:16,694][gdb3][DEBUG ] find the location of an executable
[2017-05-31 15:57:16,695][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 15:57:16,695][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 15:57:16,695][gdb3][DEBUG ] get remote short hostname
[2017-05-31 15:57:16,695][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 15:57:16,695][gdb3][DEBUG ] get remote short hostname
[2017-05-31 15:57:16,696][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 15:57:16,696][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 15:57:16,697][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 15:57:16,698][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 15:57:16,698][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 15:57:16,698][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 15:57:16,699][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 15:57:16,700][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 15:57:16,770][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 15:57:16,770][gdb3][DEBUG ] ceph-mon: set fsid to 17b5ee5e-3750-48df-b89b-863508cf4dab
[2017-05-31 15:57:16,770][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 15:57:16,770][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 15:57:16,771][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 15:57:16,771][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 15:57:16,772][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 15:57:16,842][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 15:57:16,909][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 15:57:18,948][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 15:57:19,063][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 15:57:19,063][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 15:57:19,064][gdb3][DEBUG ] {
[2017-05-31 15:57:19,064][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 15:57:19,064][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 15:57:19,064][gdb3][DEBUG ]   "features": {
[2017-05-31 15:57:19,064][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 15:57:19,064][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 15:57:19,064][gdb3][DEBUG ]       "kraken", 
[2017-05-31 15:57:19,064][gdb3][DEBUG ]       "luminous"
[2017-05-31 15:57:19,064][gdb3][DEBUG ]     ], 
[2017-05-31 15:57:19,064][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 15:57:19,064][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 15:57:19,064][gdb3][DEBUG ]       "kraken", 
[2017-05-31 15:57:19,064][gdb3][DEBUG ]       "luminous"
[2017-05-31 15:57:19,064][gdb3][DEBUG ]     ]
[2017-05-31 15:57:19,064][gdb3][DEBUG ]   }, 
[2017-05-31 15:57:19,065][gdb3][DEBUG ]   "monmap": {
[2017-05-31 15:57:19,065][gdb3][DEBUG ]     "created": "2017-05-31 15:57:16.742625", 
[2017-05-31 15:57:19,065][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 15:57:19,065][gdb3][DEBUG ]     "features": {
[2017-05-31 15:57:19,065][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 15:57:19,065][gdb3][DEBUG ]       "persistent": [
[2017-05-31 15:57:19,065][gdb3][DEBUG ]         "kraken", 
[2017-05-31 15:57:19,065][gdb3][DEBUG ]         "luminous"
[2017-05-31 15:57:19,065][gdb3][DEBUG ]       ]
[2017-05-31 15:57:19,065][gdb3][DEBUG ]     }, 
[2017-05-31 15:57:19,065][gdb3][DEBUG ]     "fsid": "17b5ee5e-3750-48df-b89b-863508cf4dab", 
[2017-05-31 15:57:19,065][gdb3][DEBUG ]     "modified": "2017-05-31 15:57:16.988033", 
[2017-05-31 15:57:19,065][gdb3][DEBUG ]     "mons": [
[2017-05-31 15:57:19,065][gdb3][DEBUG ]       {
[2017-05-31 15:57:19,065][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 15:57:19,065][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 15:57:19,065][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 15:57:19,065][gdb3][DEBUG ]         "rank": 0
[2017-05-31 15:57:19,066][gdb3][DEBUG ]       }
[2017-05-31 15:57:19,066][gdb3][DEBUG ]     ]
[2017-05-31 15:57:19,066][gdb3][DEBUG ]   }, 
[2017-05-31 15:57:19,066][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 15:57:19,066][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 15:57:19,066][gdb3][DEBUG ]   "quorum": [
[2017-05-31 15:57:19,066][gdb3][DEBUG ]     0
[2017-05-31 15:57:19,066][gdb3][DEBUG ]   ], 
[2017-05-31 15:57:19,066][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 15:57:19,066][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 15:57:19,066][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 15:57:19,066][gdb3][DEBUG ] }
[2017-05-31 15:57:19,066][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 15:57:19,066][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 15:57:19,067][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 15:57:19,132][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 15:57:19,150][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:57:19,164][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:57:19,164][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:57:19,180][gdb3][DEBUG ] detect machine type
[2017-05-31 15:57:19,183][gdb3][DEBUG ] find the location of an executable
[2017-05-31 15:57:19,184][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 15:57:19,249][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 15:57:19,249][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 15:57:19,249][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 15:57:19,251][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpVZW_cu
[2017-05-31 15:57:19,266][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 15:57:19,280][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 15:57:19,281][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 15:57:19,298][gdb3][DEBUG ] detect machine type
[2017-05-31 15:57:19,300][gdb3][DEBUG ] get remote short hostname
[2017-05-31 15:57:19,301][gdb3][DEBUG ] fetch remote file
[2017-05-31 15:57:19,302][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 15:57:19,368][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 15:57:19,534][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 15:57:19,700][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 15:57:19,866][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 15:57:20,032][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 15:57:20,199][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 15:57:20,365][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 15:57:20,531][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 15:57:20,697][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 15:57:20,864][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 15:57:21,029][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 15:57:21,029][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 15:57:21,030][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170531155721'
[2017-05-31 15:57:21,030][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 15:57:21,031][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 15:57:21,031][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 15:57:21,031][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpVZW_cu
[2017-05-31 15:57:34,991][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:57:34,991][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-31 15:57:34,991][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:57:34,991][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:57:34,991][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fefed14c950>
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fefed3a2aa0>
[2017-05-31 15:57:34,992][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:57:34,993][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:57:34,993][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 15:57:34,993][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-31 15:57:35,250][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 15:57:35,496][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 15:57:35,496][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 15:57:35,514][gdb0][DEBUG ] detect machine type
[2017-05-31 15:57:35,518][gdb0][DEBUG ] find the location of an executable
[2017-05-31 15:57:35,519][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 15:57:35,519][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 15:57:35,519][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 15:57:35,522][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-31 15:57:35,522][gdb0][DEBUG ] find the location of an executable
[2017-05-31 15:57:35,524][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 15:57:35,645][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 15:57:35,652][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:35,652][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 15:57:35,652][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 15:57:35,668][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 15:57:35,672][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 15:57:35,687][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:35,687][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:35,687][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:35,687][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-31 15:57:35,688][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-31 15:57:35,688][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-31 15:57:35,691][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-31 15:57:35,707][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-31 15:57:35,708][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-31 15:57:35,724][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:35,724][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-31 15:57:35,724][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:35,724][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-31 15:57:35,732][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-31 15:57:35,732][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-31 15:57:35,747][gdb0][WARNING] 10+0 records in
[2017-05-31 15:57:35,748][gdb0][WARNING] 10+0 records out
[2017-05-31 15:57:35,748][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00751732 s, 1.4 GB/s
[2017-05-31 15:57:35,748][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-31 15:57:35,862][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-31 15:57:35,862][gdb0][WARNING] 10+0 records in
[2017-05-31 15:57:35,862][gdb0][WARNING] 10+0 records out
[2017-05-31 15:57:35,862][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.0053806 s, 1.9 GB/s
[2017-05-31 15:57:35,862][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-31 15:57:35,863][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-31 15:57:35,878][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-31 15:57:35,878][gdb0][WARNING] backup header from main header.
[2017-05-31 15:57:35,878][gdb0][WARNING] 
[2017-05-31 15:57:35,878][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-31 15:57:35,879][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-31 15:57:35,879][gdb0][WARNING] 
[2017-05-31 15:57:35,879][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-31 15:57:35,879][gdb0][WARNING] 
[2017-05-31 15:57:36,946][gdb0][DEBUG ] ****************************************************************************
[2017-05-31 15:57:36,946][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-31 15:57:36,947][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-31 15:57:36,947][gdb0][DEBUG ] ****************************************************************************
[2017-05-31 15:57:36,947][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-31 15:57:36,947][gdb0][DEBUG ] other utilities.
[2017-05-31 15:57:36,947][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-31 15:57:37,964][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-31 15:57:37,964][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 15:57:37,964][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-31 15:57:37,964][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:57:37,965][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 15:57:37,980][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:57:37,996][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:37,996][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-31 15:57:37,996][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:37,996][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-31 15:57:37,996][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:37,996][gdb0][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-31 15:57:37,996][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:125662fa-736e-4d7c-b01f-7cb1fe013195 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-31 15:57:39,014][gdb0][DEBUG ] Setting name!
[2017-05-31 15:57:39,014][gdb0][DEBUG ] partNum is 0
[2017-05-31 15:57:39,014][gdb0][DEBUG ] REALLY setting name!
[2017-05-31 15:57:39,014][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 15:57:39,014][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 15:57:39,014][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:57:39,128][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 15:57:39,343][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:57:39,343][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:39,343][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:39,343][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-31 15:57:39,343][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:39,344][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:39,344][gdb0][WARNING] ptype_tobe_for_name: name = block
[2017-05-31 15:57:39,344][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:39,344][gdb0][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-31 15:57:39,344][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:aa1838e5-9942-4b15-80a7-4bfb30d90ede --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-31 15:57:40,411][gdb0][DEBUG ] Setting name!
[2017-05-31 15:57:40,412][gdb0][DEBUG ] partNum is 1
[2017-05-31 15:57:40,412][gdb0][DEBUG ] REALLY setting name!
[2017-05-31 15:57:40,412][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 15:57:40,412][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 15:57:40,412][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:57:40,576][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 15:57:40,791][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:57:41,056][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:41,056][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:41,056][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-31 15:57:41,056][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/aa1838e5-9942-4b15-80a7-4bfb30d90ede
[2017-05-31 15:57:41,056][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-31 15:57:42,073][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 15:57:42,074][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 15:57:42,074][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:57:42,238][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 15:57:42,503][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:57:42,617][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/aa1838e5-9942-4b15-80a7-4bfb30d90ede
[2017-05-31 15:57:42,617][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-31 15:57:42,617][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-31 15:57:42,681][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-31 15:57:42,681][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-31 15:57:42,682][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-31 15:57:42,682][gdb0][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-31 15:57:42,682][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-31 15:57:42,682][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-31 15:57:42,682][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-31 15:57:42,682][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-31 15:57:42,682][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-31 15:57:42,682][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.sSgOAA with options noatime,inode64
[2017-05-31 15:57:42,682][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.sSgOAA
[2017-05-31 15:57:42,690][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.sSgOAA
[2017-05-31 15:57:42,690][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.sSgOAA/ceph_fsid.1765.tmp
[2017-05-31 15:57:42,693][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.sSgOAA/fsid.1765.tmp
[2017-05-31 15:57:42,696][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.sSgOAA/magic.1765.tmp
[2017-05-31 15:57:42,698][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.sSgOAA/block_uuid.1765.tmp
[2017-05-31 15:57:42,701][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.sSgOAA/block -> /dev/disk/by-partuuid/aa1838e5-9942-4b15-80a7-4bfb30d90ede
[2017-05-31 15:57:42,701][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.sSgOAA/type.1765.tmp
[2017-05-31 15:57:42,702][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.sSgOAA
[2017-05-31 15:57:42,706][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.sSgOAA
[2017-05-31 15:57:42,706][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.sSgOAA
[2017-05-31 15:57:42,738][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:57:42,738][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-31 15:57:43,755][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 15:57:43,755][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 15:57:43,755][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:57:43,970][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 15:57:44,235][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:57:44,250][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-31 15:57:44,260][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 15:57:49,382][gdb0][INFO  ] checking OSD status...
[2017-05-31 15:57:49,383][gdb0][DEBUG ] find the location of an executable
[2017-05-31 15:57:49,385][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 15:57:49,500][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 15:59:17,143][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:59:17,143][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-31 15:59:17,143][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:59:17,143][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8501522950>
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8501778aa0>
[2017-05-31 15:59:17,144][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:59:17,145][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:59:17,145][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 15:59:17,145][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-31 15:59:17,387][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 15:59:17,619][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 15:59:17,619][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 15:59:17,635][gdb0][DEBUG ] detect machine type
[2017-05-31 15:59:17,639][gdb0][DEBUG ] find the location of an executable
[2017-05-31 15:59:17,640][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 15:59:17,640][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 15:59:17,640][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 15:59:17,643][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 15:59:17,643][gdb0][DEBUG ] create a keyring file
[2017-05-31 15:59:17,644][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-31 15:59:17,644][gdb0][DEBUG ] find the location of an executable
[2017-05-31 15:59:17,646][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 15:59:17,767][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 15:59:17,774][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:17,775][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 15:59:17,775][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 15:59:17,790][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 15:59:17,792][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 15:59:17,807][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:17,807][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:17,808][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:17,808][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-31 15:59:17,811][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-31 15:59:17,827][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-31 15:59:17,830][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-31 15:59:17,838][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:17,839][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-31 15:59:17,839][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:17,839][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-31 15:59:17,840][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-31 15:59:17,844][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-31 15:59:17,844][gdb0][WARNING] backup header from main header.
[2017-05-31 15:59:17,844][gdb0][WARNING] 
[2017-05-31 15:59:18,912][gdb0][DEBUG ] ****************************************************************************
[2017-05-31 15:59:18,912][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-31 15:59:18,912][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-31 15:59:18,912][gdb0][DEBUG ] ****************************************************************************
[2017-05-31 15:59:18,912][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-31 15:59:18,912][gdb0][DEBUG ] other utilities.
[2017-05-31 15:59:18,912][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-31 15:59:19,930][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-31 15:59:19,930][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 15:59:19,930][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-31 15:59:19,930][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:59:19,930][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 15:59:19,930][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:59:19,962][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:19,962][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-31 15:59:19,962][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:19,963][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-31 15:59:19,963][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:19,963][gdb0][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-31 15:59:19,963][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:f838824d-0914-4135-9985-31765be8d31c --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-31 15:59:20,980][gdb0][DEBUG ] Setting name!
[2017-05-31 15:59:20,980][gdb0][DEBUG ] partNum is 0
[2017-05-31 15:59:20,980][gdb0][DEBUG ] REALLY setting name!
[2017-05-31 15:59:20,980][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 15:59:20,980][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 15:59:20,981][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:59:21,044][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 15:59:21,159][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:59:21,175][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:21,175][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:21,175][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-31 15:59:21,175][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:21,175][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:21,175][gdb0][WARNING] ptype_tobe_for_name: name = block
[2017-05-31 15:59:21,175][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:21,175][gdb0][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-31 15:59:21,175][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:f57c494d-1fc2-4170-aa7c-90ea5ef4cab6 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-31 15:59:22,193][gdb0][DEBUG ] Setting name!
[2017-05-31 15:59:22,193][gdb0][DEBUG ] partNum is 1
[2017-05-31 15:59:22,193][gdb0][DEBUG ] REALLY setting name!
[2017-05-31 15:59:22,193][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 15:59:22,193][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 15:59:22,193][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:59:22,408][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 15:59:22,623][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:59:22,837][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:22,837][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:22,837][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-31 15:59:22,838][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/f57c494d-1fc2-4170-aa7c-90ea5ef4cab6
[2017-05-31 15:59:22,838][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-31 15:59:23,855][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 15:59:23,855][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 15:59:23,855][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:59:24,070][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 15:59:24,234][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:59:24,449][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/f57c494d-1fc2-4170-aa7c-90ea5ef4cab6
[2017-05-31 15:59:24,449][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-31 15:59:24,449][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-31 15:59:24,563][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-31 15:59:24,563][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-31 15:59:24,564][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-31 15:59:24,564][gdb0][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-31 15:59:24,564][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-31 15:59:24,564][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-31 15:59:24,564][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-31 15:59:24,564][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-31 15:59:24,564][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-31 15:59:24,564][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.MIh09C with options noatime,inode64
[2017-05-31 15:59:24,564][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.MIh09C
[2017-05-31 15:59:24,564][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.MIh09C
[2017-05-31 15:59:24,565][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.MIh09C/ceph_fsid.2686.tmp
[2017-05-31 15:59:24,565][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.MIh09C/fsid.2686.tmp
[2017-05-31 15:59:24,565][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.MIh09C/magic.2686.tmp
[2017-05-31 15:59:24,568][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.MIh09C/block_uuid.2686.tmp
[2017-05-31 15:59:24,569][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.MIh09C/block -> /dev/disk/by-partuuid/f57c494d-1fc2-4170-aa7c-90ea5ef4cab6
[2017-05-31 15:59:24,570][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.MIh09C/type.2686.tmp
[2017-05-31 15:59:24,573][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.MIh09C
[2017-05-31 15:59:24,576][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.MIh09C
[2017-05-31 15:59:24,576][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.MIh09C
[2017-05-31 15:59:24,608][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:24,608][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-31 15:59:25,626][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-31 15:59:25,626][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-31 15:59:25,626][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-31 15:59:25,626][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 15:59:25,626][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 15:59:25,626][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:59:25,626][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 15:59:25,690][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 15:59:25,706][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-31 15:59:25,724][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 15:59:30,846][gdb0][INFO  ] checking OSD status...
[2017-05-31 15:59:30,846][gdb0][DEBUG ] find the location of an executable
[2017-05-31 15:59:30,848][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 15:59:30,964][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 15:59:51,230][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 15:59:51,230][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-31 15:59:51,230][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 15:59:51,230][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 15:59:51,230][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 15:59:51,230][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f122af3c950>
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f122b192aa0>
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 15:59:51,231][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 15:59:51,232][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-31 15:59:51,475][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 15:59:51,671][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 15:59:51,671][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 15:59:51,689][gdb1][DEBUG ] detect machine type
[2017-05-31 15:59:51,692][gdb1][DEBUG ] find the location of an executable
[2017-05-31 15:59:51,693][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 15:59:51,693][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 15:59:51,693][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 15:59:51,696][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 15:59:51,696][gdb1][DEBUG ] create a keyring file
[2017-05-31 15:59:51,697][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-31 15:59:51,698][gdb1][DEBUG ] find the location of an executable
[2017-05-31 15:59:51,699][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 15:59:51,870][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 15:59:51,870][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:51,870][gdb1][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 15:59:51,870][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 15:59:51,870][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 15:59:51,870][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 15:59:51,871][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:51,871][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:51,871][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 15:59:51,871][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 15:59:51,871][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 15:59:51,871][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 15:59:51,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 15:59:51,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 15:59:51,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2029, in main
[2017-05-31 15:59:51,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2018, in prepare
[2017-05-31 15:59:51,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2090, in prepare_locked
[2017-05-31 15:59:51,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2816, in prepare
[2017-05-31 15:59:51,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2984, in prepare_device
[2017-05-31 15:59:51,872][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2878, in prepare_device
[2017-05-31 15:59:51,872][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2841, in sanity_checks
[2017-05-31 15:59:51,872][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-05-31 15:59:51,872][gdb1][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-05-31 15:59:51,876][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 15:59:51,876][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/local/bin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 15:59:51,876][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:01:14,442][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:01:14,442][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-31 16:01:14,442][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa966947950>
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:01:14,443][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa966b9daa0>
[2017-05-31 16:01:14,444][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:01:14,444][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:01:14,444][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:01:14,444][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-31 16:01:14,679][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:01:14,907][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:01:14,907][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:01:14,923][gdb0][DEBUG ] detect machine type
[2017-05-31 16:01:14,927][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:01:14,928][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:01:14,928][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 16:01:14,928][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:01:14,931][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-31 16:01:14,931][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:01:14,933][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:01:15,053][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:01:15,061][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:01:15,061][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:01:15,061][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:01:15,077][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:01:15,078][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:01:15,094][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:01:15,094][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:01:15,094][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:01:15,094][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 16:01:15,094][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 16:01:15,094][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 16:01:15,094][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 16:01:15,094][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 16:01:15,094][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 16:01:15,094][gdb0][WARNING]     args.func(args)
[2017-05-31 16:01:15,095][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 16:01:15,095][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 16:01:15,095][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2018, in prepare
[2017-05-31 16:01:15,095][gdb0][WARNING]     self.prepare_locked()
[2017-05-31 16:01:15,095][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2090, in prepare_locked
[2017-05-31 16:01:15,095][gdb0][WARNING]     self.data.prepare(*to_prepare_list)
[2017-05-31 16:01:15,095][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2816, in prepare
[2017-05-31 16:01:15,095][gdb0][WARNING]     self.prepare_device(*to_prepare_list)
[2017-05-31 16:01:15,095][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2984, in prepare_device
[2017-05-31 16:01:15,095][gdb0][WARNING]     super(PrepareBluestoreData, self).prepare_device(*to_prepare_list)
[2017-05-31 16:01:15,095][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2878, in prepare_device
[2017-05-31 16:01:15,095][gdb0][WARNING]     self.sanity_checks()
[2017-05-31 16:01:15,095][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2841, in sanity_checks
[2017-05-31 16:01:15,096][gdb0][WARNING]     check_partitions=not self.args.dmcrypt)
[2017-05-31 16:01:15,096][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-05-31 16:01:15,096][gdb0][WARNING]     raise Error('Device is mounted', partition)
[2017-05-31 16:01:15,096][gdb0][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-05-31 16:01:15,103][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 16:01:15,104][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:01:15,104][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:02:57,400][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:02:57,400][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb1
[2017-05-31 16:02:57,400][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:02:57,400][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:02:57,400][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:02:57,400][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:02:57,400][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:02:57,401][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2e9f78f0e0>
[2017-05-31 16:02:57,401][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:02:57,401][ceph_deploy.cli][INFO  ]  host                          : ['gdb1']
[2017-05-31 16:02:57,401][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f2ea009c1b8>
[2017-05-31 16:02:57,401][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:02:57,401][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:02:57,401][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 16:02:57,401][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 16:02:57,401][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb1
[2017-05-31 16:02:57,401][ceph_deploy.install][DEBUG ] Detecting platform for host gdb1 ...
[2017-05-31 16:02:57,639][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:02:57,867][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:02:57,867][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:02:57,884][gdb1][DEBUG ] detect machine type
[2017-05-31 16:02:57,887][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:02:57,888][gdb1][INFO  ] Purging Ceph on gdb1
[2017-05-31 16:02:57,889][gdb1][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 16:02:57,927][gdb1][DEBUG ] Reading package lists...
[2017-05-31 16:02:58,092][gdb1][DEBUG ] Building dependency tree...
[2017-05-31 16:02:58,092][gdb1][DEBUG ] Reading state information...
[2017-05-31 16:02:58,206][gdb1][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 16:02:58,206][gdb1][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 16:02:58,206][gdb1][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   bc ca-certificates-java ceph-fuse cmake cmake-data cython cython3
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   default-jdk default-jdk-headless default-jre default-jre-headless dh-systemd
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   docutils-common fontconfig fonts-font-awesome fonts-lato icu-devtools
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   java-common javahelper javascript-common jq junit4 libaio-dev libarchive13
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   libasound2 libasound2-data libasyncns0 libatk1.0-0 libatk1.0-data
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   libatomic-ops-dev libavahi-client3 libavahi-common-data libavahi-common3
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   libbabeltrace-ctf-dev libbabeltrace-dev libblkid-dev libcairo2 libcephfs2
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   libcups2 libcurl3 libdatrie1 libdrm-amdgpu1 libdrm-intel1 libdrm-nouveau2
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   libdrm-radeon1 libexpat1-dev libfcgi-dev libfcgi0ldbl libflac8 libfuse-dev
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   libgdk-pixbuf2.0-0 libgdk-pixbuf2.0-common libgif7 libgl1-mesa-dri
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   libgl1-mesa-glx libglapi-mesa libgoogle-perftools-dev libgoogle-perftools4
[2017-05-31 16:02:58,207][gdb1][DEBUG ]   libgraphite2-3 libgtk2.0-0 libgtk2.0-common libhamcrest-java libharfbuzz0b
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libibverbs-dev libicu-dev libjs-jquery libjs-modernizr libjs-sphinxdoc
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libjs-underscore libjsoncpp1 libkeyutils-dev liblcms2-2 libleveldb-dev
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libleveldb1v5 libllvm3.8 liblttng-ust-dev liblttng-ust-python-agent0
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libnspr4-dev libnss3-dev libogg0 libonig2 libpango-1.0-0 libpangocairo-1.0-0
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libpangoft2-1.0-0 libpciaccess0 libpcre16-3 libpcre3-dev libpcre32-3
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libpcrecpp0v5 libpixman-1-0 libpulse0 libpython-all-dev libpython-dev
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libpython2.7 libpython2.7-dev libpython3-all-dev libpython3-dev
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libpython3.5-dev libradosstriper1 librgw2 libselinux1-dev libsepol1-dev
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libsnappy-dev libsnappy1v5 libsndfile1 libssl-dev libtcmalloc-minimal4
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libthai-data libthai0 libudev-dev libunwind-dev libunwind8 libunwind8-dev
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   liburcu-dev libvorbis0a libvorbisenc2 libx11-xcb1 libxcb-dri2-0
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-render0 libxcb-shm0
[2017-05-31 16:02:58,208][gdb1][DEBUG ]   libxcb-sync1 libxcomposite1 libxcursor1 libxdamage1 libxfixes3 libxi6
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   libxinerama1 libxml2-dev libxrandr2 libxrender1 libxshmfence1 libxtst6
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   libxxf86vm1 linux-aws-headers-4.4.0-1013 linux-headers-4.4.0-1013-aws
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   linux-image-4.4.0-1013-aws openjdk-8-jdk openjdk-8-jdk-headless
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   openjdk-8-jre openjdk-8-jre-headless python-alabaster python-all
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   python-all-dev python-babel python-babel-localedata python-blinker
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   python-dev python-docutils python-enum34 python-flask python-idna
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   python-ipaddress python-itsdangerous python-jinja2 python-markupsafe
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   python-ndg-httpsclient python-nose python-openssl python-pyasn1
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   python-pygments python-pyinotify python-rados python-rbd python-requests
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   python-rgw python-roman python-six python-sphinx python-sphinx-rtd-theme
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   python-tz python-urllib3 python-werkzeug python2.7-dev python3-all
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   python3-all-dev python3-dev python3.5-dev sphinx-common
[2017-05-31 16:02:58,209][gdb1][DEBUG ]   sphinx-rtd-theme-common uuid-dev valgrind x11-common xfslibs-dev xmlstarlet
[2017-05-31 16:02:58,210][gdb1][DEBUG ]   yasm
[2017-05-31 16:02:58,210][gdb1][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 16:02:58,225][gdb1][DEBUG ] The following packages will be REMOVED:
[2017-05-31 16:02:58,225][gdb1][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 16:02:58,390][gdb1][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 43 not upgraded.
[2017-05-31 16:02:58,390][gdb1][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 16:02:58,405][gdb1][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 122129 files and directories currently installed.)
[2017-05-31 16:02:58,409][gdb1][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 16:02:58,623][gdb1][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 16:02:58,738][gdb1][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 16:02:58,754][gdb1][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 16:02:59,019][gdb1][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 16:02:59,133][gdb1][DEBUG ] dpkg: warning: while removing ceph-osd, directory '/var/lib/ceph/osd' not empty so not removed
[2017-05-31 16:02:59,133][gdb1][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 16:02:59,297][gdb1][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 16:02:59,362][gdb1][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 16:02:59,426][gdb1][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 16:02:59,640][gdb1][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 16:02:59,672][gdb1][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 16:02:59,704][gdb1][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 16:02:59,869][gdb1][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 16:02:59,876][gdb1][DEBUG ] dpkg: warning: while removing ceph-base, directory '/usr/share/doc/ceph' not empty so not removed
[2017-05-31 16:02:59,877][gdb1][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[2017-05-31 16:02:59,877][gdb1][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-05-31 16:02:59,909][gdb1][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 16:03:00,073][gdb1][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 16:03:00,187][gdb1][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 16:03:00,187][gdb1][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 16:03:00,302][gdb1][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 16:03:01,219][gdb1][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 16:03:22,594][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb1
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb1ee19c758>
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ]  host                          : ['gdb1']
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fb1eeaa9230>
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:03:22,595][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:03:22,595][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb1
[2017-05-31 16:03:22,836][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:03:23,031][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:03:23,032][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:03:23,048][gdb1][DEBUG ] detect machine type
[2017-05-31 16:03:23,052][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:03:23,280][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:03:23,510][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:03:23,511][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:03:23,528][gdb1][DEBUG ] detect machine type
[2017-05-31 16:03:23,532][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:03:23,532][gdb1][INFO  ] purging data on gdb1
[2017-05-31 16:03:23,533][gdb1][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:03:23,554][gdb1][WARNING] OSDs may still be mounted, trying to unmount them
[2017-05-31 16:03:23,555][gdb1][INFO  ] Running command: sudo find /var/lib/ceph -mindepth 1 -maxdepth 2 -type d -exec umount {} ;
[2017-05-31 16:03:23,566][gdb1][WARNING] umount: /var/lib/ceph/osd: not mounted
[2017-05-31 16:03:23,599][gdb1][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:03:23,606][gdb1][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 16:05:35,716][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:05:35,716][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-31 16:05:35,716][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:05:35,716][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd060c55950>
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:05:35,717][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd060eabaa0>
[2017-05-31 16:05:35,718][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:05:35,718][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:05:35,718][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:05:35,718][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-31 16:05:35,964][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:05:36,191][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:05:36,192][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:05:36,208][gdb1][DEBUG ] detect machine type
[2017-05-31 16:05:36,211][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:05:36,212][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:05:36,212][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 16:05:36,212][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:05:36,215][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:05:36,215][gdb1][DEBUG ] create a keyring file
[2017-05-31 16:05:36,217][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-31 16:05:36,217][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:05:36,219][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:05:36,340][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:05:36,355][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:36,356][gdb1][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:05:36,356][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:05:36,470][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:05:36,470][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:05:36,470][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:36,470][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:36,470][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:36,470][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-31 16:05:36,470][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-31 16:05:36,471][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-31 16:05:36,471][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-31 16:05:36,471][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-31 16:05:36,474][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-31 16:05:36,490][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:36,490][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-31 16:05:36,490][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:36,490][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-31 16:05:36,506][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-31 16:05:36,506][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-31 16:05:36,522][gdb1][WARNING] 10+0 records in
[2017-05-31 16:05:36,522][gdb1][WARNING] 10+0 records out
[2017-05-31 16:05:36,522][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00729276 s, 1.4 GB/s
[2017-05-31 16:05:36,522][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-31 16:05:36,636][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-31 16:05:36,639][gdb1][WARNING] 10+0 records in
[2017-05-31 16:05:36,640][gdb1][WARNING] 10+0 records out
[2017-05-31 16:05:36,640][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00479976 s, 2.2 GB/s
[2017-05-31 16:05:36,640][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-31 16:05:36,640][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-31 16:05:36,647][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-31 16:05:36,647][gdb1][WARNING] backup header from main header.
[2017-05-31 16:05:36,648][gdb1][WARNING] 
[2017-05-31 16:05:37,715][gdb1][DEBUG ] ****************************************************************************
[2017-05-31 16:05:37,715][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-31 16:05:37,715][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-31 16:05:37,715][gdb1][DEBUG ] ****************************************************************************
[2017-05-31 16:05:37,715][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-31 16:05:37,716][gdb1][DEBUG ] other utilities.
[2017-05-31 16:05:37,716][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-31 16:05:38,733][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-31 16:05:38,733][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:05:38,733][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-31 16:05:38,733][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:05:38,733][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:05:38,741][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:05:38,757][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:38,757][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-31 16:05:38,757][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:38,757][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-31 16:05:38,757][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:38,757][gdb1][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-31 16:05:38,757][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:bb73a7f1-b0a3-4633-99e7-dc629f500f64 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-31 16:05:39,774][gdb1][DEBUG ] Setting name!
[2017-05-31 16:05:39,775][gdb1][DEBUG ] partNum is 0
[2017-05-31 16:05:39,775][gdb1][DEBUG ] REALLY setting name!
[2017-05-31 16:05:39,775][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:05:39,775][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:05:39,775][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:05:39,889][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:05:40,003][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:05:40,035][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:40,035][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:40,036][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-31 16:05:40,036][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:40,036][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:40,036][gdb1][WARNING] ptype_tobe_for_name: name = block
[2017-05-31 16:05:40,036][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:40,036][gdb1][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-31 16:05:40,036][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:403cc3b0-6941-4c00-adcc-ca770f4b31b2 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-31 16:05:41,053][gdb1][DEBUG ] Setting name!
[2017-05-31 16:05:41,054][gdb1][DEBUG ] partNum is 1
[2017-05-31 16:05:41,054][gdb1][DEBUG ] REALLY setting name!
[2017-05-31 16:05:41,054][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:05:41,054][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:05:41,054][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:05:41,319][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:05:41,483][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:05:41,698][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:41,698][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:41,698][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-31 16:05:41,698][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/403cc3b0-6941-4c00-adcc-ca770f4b31b2
[2017-05-31 16:05:41,698][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-31 16:05:42,716][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:05:42,716][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:05:42,716][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:05:42,931][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:05:43,145][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:05:43,209][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/403cc3b0-6941-4c00-adcc-ca770f4b31b2
[2017-05-31 16:05:43,210][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-31 16:05:43,210][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-31 16:05:43,274][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-31 16:05:43,274][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-31 16:05:43,274][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-31 16:05:43,274][gdb1][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-31 16:05:43,274][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-31 16:05:43,274][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-31 16:05:43,274][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-31 16:05:43,274][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-31 16:05:43,274][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-31 16:05:43,274][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.znYOBa with options noatime,inode64
[2017-05-31 16:05:43,275][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.znYOBa
[2017-05-31 16:05:43,290][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.znYOBa
[2017-05-31 16:05:43,290][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.znYOBa/ceph_fsid.5143.tmp
[2017-05-31 16:05:43,290][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.znYOBa/fsid.5143.tmp
[2017-05-31 16:05:43,294][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.znYOBa/magic.5143.tmp
[2017-05-31 16:05:43,295][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.znYOBa/block_uuid.5143.tmp
[2017-05-31 16:05:43,296][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.znYOBa/block -> /dev/disk/by-partuuid/403cc3b0-6941-4c00-adcc-ca770f4b31b2
[2017-05-31 16:05:43,297][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.znYOBa/type.5143.tmp
[2017-05-31 16:05:43,301][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.znYOBa
[2017-05-31 16:05:43,302][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.znYOBa
[2017-05-31 16:05:43,302][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.znYOBa
[2017-05-31 16:05:43,333][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:05:43,334][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-31 16:05:44,351][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-31 16:05:44,351][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-31 16:05:44,351][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-31 16:05:44,351][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:05:44,351][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:05:44,351][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:05:44,352][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:05:44,516][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:05:44,516][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-31 16:05:44,522][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:05:49,644][gdb1][INFO  ] checking OSD status...
[2017-05-31 16:05:49,644][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:05:49,647][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 16:05:49,762][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 16:19:40,162][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:19:40,163][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 16:19:40,163][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:19:40,163][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:19:40,163][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:19:40,163][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:19:40,163][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:19:40,163][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f60a4326518>
[2017-05-31 16:19:40,163][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:19:40,163][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 16:19:40,163][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f60a4c3d938>
[2017-05-31 16:19:40,163][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:19:40,164][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:19:40,164][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 16:19:40,190][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:19:40,204][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:19:40,204][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:19:40,221][gdb3][DEBUG ] detect machine type
[2017-05-31 16:19:40,223][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:21:06,959][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:21:06,959][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb1
[2017-05-31 16:21:06,959][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:21:06,959][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:21:06,959][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:21:06,960][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:21:06,960][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:21:06,960][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f77c02fcfc8>
[2017-05-31 16:21:06,960][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:21:06,960][ceph_deploy.cli][INFO  ]  host                          : ['gdb1']
[2017-05-31 16:21:06,960][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f77c0c0f1b8>
[2017-05-31 16:21:06,960][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:21:06,960][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:21:06,960][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 16:21:06,960][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 16:21:06,960][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb1
[2017-05-31 16:21:06,960][ceph_deploy.install][DEBUG ] Detecting platform for host gdb1 ...
[2017-05-31 16:21:07,203][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:21:07,431][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:21:07,431][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:21:07,447][gdb1][DEBUG ] detect machine type
[2017-05-31 16:21:07,451][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:21:07,451][gdb1][INFO  ] Purging Ceph on gdb1
[2017-05-31 16:21:07,452][gdb1][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 16:21:07,490][gdb1][DEBUG ] Reading package lists...
[2017-05-31 16:21:07,655][gdb1][DEBUG ] Building dependency tree...
[2017-05-31 16:21:07,655][gdb1][DEBUG ] Reading state information...
[2017-05-31 16:21:07,719][gdb1][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 16:21:07,719][gdb1][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 16:21:07,719][gdb1][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 16:21:07,719][gdb1][DEBUG ]   bc ca-certificates-java ceph-fuse cmake cmake-data cython cython3
[2017-05-31 16:21:07,719][gdb1][DEBUG ]   default-jdk default-jdk-headless default-jre default-jre-headless dh-systemd
[2017-05-31 16:21:07,719][gdb1][DEBUG ]   docutils-common fontconfig fonts-font-awesome fonts-lato icu-devtools
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   java-common javahelper javascript-common jq junit4 libaio-dev libarchive13
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   libasound2 libasound2-data libasyncns0 libatk1.0-0 libatk1.0-data
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   libatomic-ops-dev libavahi-client3 libavahi-common-data libavahi-common3
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   libbabeltrace-ctf-dev libbabeltrace-dev libblkid-dev libcairo2 libcephfs2
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   libcups2 libcurl3 libdatrie1 libdrm-amdgpu1 libdrm-intel1 libdrm-nouveau2
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   libdrm-radeon1 libexpat1-dev libfcgi-dev libfcgi0ldbl libflac8 libfuse-dev
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   libgdk-pixbuf2.0-0 libgdk-pixbuf2.0-common libgif7 libgl1-mesa-dri
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   libgl1-mesa-glx libglapi-mesa libgoogle-perftools-dev libgoogle-perftools4
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   libgraphite2-3 libgtk2.0-0 libgtk2.0-common libhamcrest-java libharfbuzz0b
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   libibverbs-dev libicu-dev libjs-jquery libjs-modernizr libjs-sphinxdoc
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   libjs-underscore libjsoncpp1 libkeyutils-dev liblcms2-2 libleveldb-dev
[2017-05-31 16:21:07,720][gdb1][DEBUG ]   libleveldb1v5 libllvm3.8 liblttng-ust-dev liblttng-ust-python-agent0
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   libnspr4-dev libnss3-dev libogg0 libonig2 libpango-1.0-0 libpangocairo-1.0-0
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   libpangoft2-1.0-0 libpciaccess0 libpcre16-3 libpcre3-dev libpcre32-3
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   libpcrecpp0v5 libpixman-1-0 libpulse0 libpython-all-dev libpython-dev
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   libpython2.7 libpython2.7-dev libpython3-all-dev libpython3-dev
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   libpython3.5-dev libradosstriper1 librgw2 libselinux1-dev libsepol1-dev
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   libsnappy-dev libsnappy1v5 libsndfile1 libssl-dev libtcmalloc-minimal4
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   libthai-data libthai0 libudev-dev libunwind-dev libunwind8 libunwind8-dev
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   liburcu-dev libvorbis0a libvorbisenc2 libx11-xcb1 libxcb-dri2-0
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-render0 libxcb-shm0
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   libxcb-sync1 libxcomposite1 libxcursor1 libxdamage1 libxfixes3 libxi6
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   libxinerama1 libxml2-dev libxrandr2 libxrender1 libxshmfence1 libxtst6
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   libxxf86vm1 linux-aws-headers-4.4.0-1013 linux-headers-4.4.0-1013-aws
[2017-05-31 16:21:07,721][gdb1][DEBUG ]   linux-image-4.4.0-1013-aws openjdk-8-jdk openjdk-8-jdk-headless
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   openjdk-8-jre openjdk-8-jre-headless python-alabaster python-all
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   python-all-dev python-babel python-babel-localedata python-blinker
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   python-dev python-docutils python-enum34 python-flask python-idna
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   python-ipaddress python-itsdangerous python-jinja2 python-markupsafe
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   python-ndg-httpsclient python-nose python-openssl python-pyasn1
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   python-pygments python-pyinotify python-rados python-rbd python-requests
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   python-rgw python-roman python-six python-sphinx python-sphinx-rtd-theme
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   python-tz python-urllib3 python-werkzeug python2.7-dev python3-all
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   python3-all-dev python3-dev python3.5-dev sphinx-common
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   sphinx-rtd-theme-common uuid-dev valgrind x11-common xfslibs-dev xmlstarlet
[2017-05-31 16:21:07,722][gdb1][DEBUG ]   yasm
[2017-05-31 16:21:07,723][gdb1][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 16:21:07,786][gdb1][DEBUG ] The following packages will be REMOVED:
[2017-05-31 16:21:07,787][gdb1][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 16:21:07,901][gdb1][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 43 not upgraded.
[2017-05-31 16:21:07,901][gdb1][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 16:21:07,965][gdb1][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 122129 files and directories currently installed.)
[2017-05-31 16:21:07,965][gdb1][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 16:21:08,129][gdb1][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 16:21:08,244][gdb1][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 16:21:08,308][gdb1][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 16:21:08,522][gdb1][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 16:21:08,637][gdb1][DEBUG ] dpkg: warning: while removing ceph-osd, directory '/var/lib/ceph/osd' not empty so not removed
[2017-05-31 16:21:08,637][gdb1][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 16:21:08,801][gdb1][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 16:21:08,916][gdb1][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 16:21:09,080][gdb1][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 16:21:09,144][gdb1][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 16:21:09,309][gdb1][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 16:21:09,316][gdb1][DEBUG ] dpkg: warning: while removing ceph-base, directory '/usr/share/doc/ceph' not empty so not removed
[2017-05-31 16:21:09,316][gdb1][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[2017-05-31 16:21:09,317][gdb1][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-05-31 16:21:09,348][gdb1][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 16:21:09,513][gdb1][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 16:21:09,577][gdb1][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 16:21:09,577][gdb1][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 16:21:09,691][gdb1][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 16:21:10,558][gdb1][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 16:21:10,724][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:21:10,725][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb1
[2017-05-31 16:21:10,725][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:21:10,725][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:21:10,725][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:21:10,725][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:21:10,725][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:21:10,725][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fde47835710>
[2017-05-31 16:21:10,725][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:21:10,725][ceph_deploy.cli][INFO  ]  host                          : ['gdb1']
[2017-05-31 16:21:10,725][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fde48142230>
[2017-05-31 16:21:10,726][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:21:10,726][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:21:10,726][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb1
[2017-05-31 16:21:10,967][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:21:11,195][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:21:11,195][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:21:11,211][gdb1][DEBUG ] detect machine type
[2017-05-31 16:21:11,215][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:21:11,411][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:21:11,639][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:21:11,639][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:21:11,656][gdb1][DEBUG ] detect machine type
[2017-05-31 16:21:11,659][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:21:11,660][gdb1][INFO  ] purging data on gdb1
[2017-05-31 16:21:11,661][gdb1][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:21:11,674][gdb1][WARNING] OSDs may still be mounted, trying to unmount them
[2017-05-31 16:21:11,675][gdb1][INFO  ] Running command: sudo find /var/lib/ceph -mindepth 1 -maxdepth 2 -type d -exec umount {} ;
[2017-05-31 16:21:11,685][gdb1][WARNING] umount: /var/lib/ceph/osd: not mounted
[2017-05-31 16:21:11,686][gdb1][WARNING] umount: /var/lib/ceph/osd/ceph-1: target is busy
[2017-05-31 16:21:11,686][gdb1][WARNING]         (In some cases useful info about processes that
[2017-05-31 16:21:11,686][gdb1][WARNING]          use the device is found by lsof(8) or fuser(1).)
[2017-05-31 16:21:11,687][gdb1][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:21:11,693][gdb1][WARNING] rm: skipping '/var/lib/ceph/osd/ceph-1', since it's on a different device
[2017-05-31 16:21:11,693][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 16:21:11,693][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: rm -rf --one-file-system -- /var/lib/ceph

[2017-05-31 16:21:11,865][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:21:11,866][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 16:21:11,866][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:21:11,866][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:21:11,866][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:21:11,866][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:21:11,866][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:21:11,866][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8c97b219e0>
[2017-05-31 16:21:11,866][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:21:11,866][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f8c983e4848>
[2017-05-31 16:21:11,866][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:21:11,866][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:21:42,310][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb1
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9efa7d8fc8>
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ]  host                          : ['gdb1']
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f9efb0eb1b8>
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:21:42,311][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:21:42,311][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 16:21:42,312][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 16:21:42,312][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb1
[2017-05-31 16:21:42,312][ceph_deploy.install][DEBUG ] Detecting platform for host gdb1 ...
[2017-05-31 16:21:42,555][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:21:42,786][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:21:42,787][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:21:42,804][gdb1][DEBUG ] detect machine type
[2017-05-31 16:21:42,807][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:21:42,807][gdb1][INFO  ] Purging Ceph on gdb1
[2017-05-31 16:21:42,809][gdb1][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 16:21:42,847][gdb1][DEBUG ] Reading package lists...
[2017-05-31 16:21:43,012][gdb1][DEBUG ] Building dependency tree...
[2017-05-31 16:21:43,012][gdb1][DEBUG ] Reading state information...
[2017-05-31 16:21:43,076][gdb1][DEBUG ] Package 'ceph' is not installed, so not removed
[2017-05-31 16:21:43,076][gdb1][DEBUG ] Package 'ceph-common' is not installed, so not removed
[2017-05-31 16:21:43,076][gdb1][DEBUG ] Package 'ceph-mds' is not installed, so not removed
[2017-05-31 16:21:43,076][gdb1][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 16:21:43,076][gdb1][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 16:21:43,076][gdb1][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   bc ca-certificates-java ceph-fuse cmake cmake-data cython cython3
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   default-jdk default-jdk-headless default-jre default-jre-headless dh-systemd
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   docutils-common fontconfig fonts-font-awesome fonts-lato icu-devtools
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   java-common javahelper javascript-common jq junit4 libaio-dev libarchive13
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   libasound2 libasound2-data libasyncns0 libatk1.0-0 libatk1.0-data
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   libatomic-ops-dev libavahi-client3 libavahi-common-data libavahi-common3
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   libbabeltrace-ctf-dev libbabeltrace-dev libblkid-dev libcairo2 libcephfs2
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   libcups2 libcurl3 libdatrie1 libdrm-amdgpu1 libdrm-intel1 libdrm-nouveau2
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   libdrm-radeon1 libexpat1-dev libfcgi-dev libfcgi0ldbl libflac8 libfuse-dev
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   libgdk-pixbuf2.0-0 libgdk-pixbuf2.0-common libgif7 libgl1-mesa-dri
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   libgl1-mesa-glx libglapi-mesa libgoogle-perftools-dev libgoogle-perftools4
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   libgraphite2-3 libgtk2.0-0 libgtk2.0-common libhamcrest-java libharfbuzz0b
[2017-05-31 16:21:43,077][gdb1][DEBUG ]   libibverbs-dev libicu-dev libjs-jquery libjs-modernizr libjs-sphinxdoc
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libjs-underscore libjsoncpp1 libkeyutils-dev liblcms2-2 libleveldb-dev
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libleveldb1v5 libllvm3.8 liblttng-ust-dev liblttng-ust-python-agent0
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libnspr4-dev libnss3-dev libogg0 libonig2 libpango-1.0-0 libpangocairo-1.0-0
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libpangoft2-1.0-0 libpciaccess0 libpcre16-3 libpcre3-dev libpcre32-3
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libpcrecpp0v5 libpixman-1-0 libpulse0 libpython-all-dev libpython-dev
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libpython2.7 libpython2.7-dev libpython3-all-dev libpython3-dev
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libpython3.5-dev libradosstriper1 librgw2 libselinux1-dev libsepol1-dev
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libsnappy-dev libsnappy1v5 libsndfile1 libssl-dev libtcmalloc-minimal4
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libthai-data libthai0 libudev-dev libunwind-dev libunwind8 libunwind8-dev
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   liburcu-dev libvorbis0a libvorbisenc2 libx11-xcb1 libxcb-dri2-0
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-render0 libxcb-shm0
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libxcb-sync1 libxcomposite1 libxcursor1 libxdamage1 libxfixes3 libxi6
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libxinerama1 libxml2-dev libxrandr2 libxrender1 libxshmfence1 libxtst6
[2017-05-31 16:21:43,078][gdb1][DEBUG ]   libxxf86vm1 linux-aws-headers-4.4.0-1013 linux-headers-4.4.0-1013-aws
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   linux-image-4.4.0-1013-aws openjdk-8-jdk openjdk-8-jdk-headless
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   openjdk-8-jre openjdk-8-jre-headless python-alabaster python-all
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   python-all-dev python-babel python-babel-localedata python-blinker
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   python-dev python-docutils python-enum34 python-flask python-idna
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   python-ipaddress python-itsdangerous python-jinja2 python-markupsafe
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   python-ndg-httpsclient python-nose python-openssl python-pyasn1
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   python-pygments python-pyinotify python-rados python-rbd python-requests
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   python-rgw python-roman python-six python-sphinx python-sphinx-rtd-theme
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   python-tz python-urllib3 python-werkzeug python2.7-dev python3-all
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   python3-all-dev python3-dev python3.5-dev sphinx-common
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   sphinx-rtd-theme-common uuid-dev valgrind x11-common xfslibs-dev xmlstarlet
[2017-05-31 16:21:43,079][gdb1][DEBUG ]   yasm
[2017-05-31 16:21:43,079][gdb1][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 16:21:43,080][gdb1][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.
[2017-05-31 16:21:43,243][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb1
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f42cb0e9710>
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ]  host                          : ['gdb1']
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f42cb9f6230>
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:21:43,244][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:21:43,244][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb1
[2017-05-31 16:21:43,487][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:21:43,714][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:21:43,715][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:21:43,731][gdb1][DEBUG ] detect machine type
[2017-05-31 16:21:43,735][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:21:43,967][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:21:44,199][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:21:44,199][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:21:44,215][gdb1][DEBUG ] detect machine type
[2017-05-31 16:21:44,219][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:21:44,219][gdb1][INFO  ] purging data on gdb1
[2017-05-31 16:21:44,221][gdb1][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:21:44,235][gdb1][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 16:21:44,406][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:21:44,407][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 16:21:44,407][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:21:44,407][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:21:44,407][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:21:44,407][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:21:44,407][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:21:44,407][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8d474269e0>
[2017-05-31 16:21:44,407][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:21:44,407][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f8d47ce9848>
[2017-05-31 16:21:44,407][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:21:44,407][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:22:06,644][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:22:06,644][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd779d42908>
[2017-05-31 16:22:06,645][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:22:06,646][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:22:06,646][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd779f98aa0>
[2017-05-31 16:22:06,646][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:22:06,646][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:22:06,646][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:22:06,646][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-31 16:22:06,646][ceph_deploy][ERROR ] RuntimeError: bootstrap-osd keyring not found; run 'gatherkeys'

[2017-05-31 16:22:51,996][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:22:51,996][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-31 16:22:51,996][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:22:51,996][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:22:51,996][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4cc2d5a908>
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f4cc2fb0aa0>
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:22:51,997][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:22:51,998][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-31 16:22:51,998][ceph_deploy][ERROR ] RuntimeError: bootstrap-osd keyring not found; run 'gatherkeys'

[2017-05-31 16:24:44,345][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:24:44,345][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-31 16:24:44,345][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:24:44,345][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:24:44,345][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:24:44,345][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-31 16:24:44,345][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:24:44,345][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:24:44,345][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffa6b231908>
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ffa6b487aa0>
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:24:44,346][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:24:44,347][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-31 16:24:44,347][ceph_deploy][ERROR ] RuntimeError: bootstrap-osd keyring not found; run 'gatherkeys'

[2017-05-31 16:27:15,279][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:27:15,279][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 16:27:15,279][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:27:15,279][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:27:15,279][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:27:15,279][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:27:15,280][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:27:15,280][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0ffbefcfc8>
[2017-05-31 16:27:15,280][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:27:15,280][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 16:27:15,280][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f0ffc80f1b8>
[2017-05-31 16:27:15,280][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:27:15,280][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:27:15,280][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 16:27:15,280][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 16:27:15,280][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 16:27:15,280][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 16:27:15,306][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:27:15,321][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:27:15,321][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:27:15,337][gdb3][DEBUG ] detect machine type
[2017-05-31 16:27:15,340][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:27:15,340][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 16:27:15,341][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 16:27:15,379][gdb3][DEBUG ] Reading package lists...
[2017-05-31 16:27:15,543][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 16:27:15,544][gdb3][DEBUG ] Reading state information...
[2017-05-31 16:27:15,608][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 16:27:15,608][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 16:27:15,608][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 16:27:15,608][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 16:27:15,608][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 16:27:15,609][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 16:27:15,609][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 16:27:15,609][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 16:27:15,609][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 16:27:15,609][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 16:27:15,609][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 16:27:15,609][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 16:27:15,609][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 16:27:15,641][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 16:27:15,641][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 16:27:15,756][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 16:27:15,756][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 16:27:15,820][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 16:27:15,820][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 16:27:15,986][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 16:27:16,100][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 16:27:16,100][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 16:27:16,314][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 16:27:16,428][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 16:27:16,594][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 16:27:16,659][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 16:27:16,667][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 16:27:16,833][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 16:27:16,898][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 16:27:16,936][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 16:27:17,104][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 16:27:17,111][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 16:27:17,276][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 16:27:17,340][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 16:27:17,340][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 16:27:17,454][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 16:27:18,271][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 16:27:18,436][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:27:18,436][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 16:27:18,436][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:27:18,436][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:27:18,436][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:27:18,437][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:27:18,437][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:27:18,437][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9bb24f2710>
[2017-05-31 16:27:18,437][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:27:18,437][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 16:27:18,437][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f9bb2dff230>
[2017-05-31 16:27:18,437][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:27:18,437][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:27:18,437][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 16:27:18,463][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:27:18,477][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:27:18,478][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:27:18,494][gdb3][DEBUG ] detect machine type
[2017-05-31 16:27:18,497][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:27:18,512][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:27:18,527][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:27:18,528][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:27:18,544][gdb3][DEBUG ] detect machine type
[2017-05-31 16:27:18,547][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:27:18,547][gdb3][INFO  ] purging data on gdb3
[2017-05-31 16:27:18,548][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:27:18,561][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 16:27:18,732][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:27:18,732][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 16:27:18,733][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:27:18,733][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:27:18,733][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:27:18,733][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:27:18,733][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:27:18,733][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5fa085d9e0>
[2017-05-31 16:27:18,733][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:27:18,733][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f5fa1120848>
[2017-05-31 16:27:18,733][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:27:18,733][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:28:11,941][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:28:11,941][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb0
[2017-05-31 16:28:11,941][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:28:11,941][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:28:11,941][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:28:11,941][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:28:11,942][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:28:11,942][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe519e6cfc8>
[2017-05-31 16:28:11,942][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:28:11,942][ceph_deploy.cli][INFO  ]  host                          : ['gdb0']
[2017-05-31 16:28:11,942][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fe51a77f1b8>
[2017-05-31 16:28:11,942][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:28:11,942][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:28:11,942][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 16:28:11,942][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 16:28:11,942][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb0
[2017-05-31 16:28:11,942][ceph_deploy.install][DEBUG ] Detecting platform for host gdb0 ...
[2017-05-31 16:28:12,178][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:28:12,406][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:28:12,406][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:28:12,422][gdb0][DEBUG ] detect machine type
[2017-05-31 16:28:12,425][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:28:12,425][gdb0][INFO  ] Purging Ceph on gdb0
[2017-05-31 16:28:12,426][gdb0][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 16:28:12,748][gdb0][DEBUG ] Reading package lists...
[2017-05-31 16:28:12,862][gdb0][DEBUG ] Building dependency tree...
[2017-05-31 16:28:12,862][gdb0][DEBUG ] Reading state information...
[2017-05-31 16:28:12,976][gdb0][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 16:28:12,977][gdb0][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 16:28:12,977][gdb0][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 16:28:12,977][gdb0][DEBUG ]   ceph-fuse javascript-common libaio1 libcephfs2 libgoogle-perftools4
[2017-05-31 16:28:12,977][gdb0][DEBUG ]   libibverbs1 libjs-jquery libleveldb1v5 liblttng-ust-ctl2 liblttng-ust0
[2017-05-31 16:28:12,977][gdb0][DEBUG ]   libnspr4 libnss3 libnss3-nssdb libopts25 libpython2.7 librados2
[2017-05-31 16:28:12,977][gdb0][DEBUG ]   libradosstriper1 librbd1 librgw2 libsnappy1v5 libtcmalloc-minimal4
[2017-05-31 16:28:12,977][gdb0][DEBUG ]   libunwind8 liburcu4 linux-aws-headers-4.4.0-1013
[2017-05-31 16:28:12,977][gdb0][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 16:28:12,977][gdb0][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 16:28:12,977][gdb0][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 16:28:12,977][gdb0][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 16:28:12,977][gdb0][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 16:28:12,977][gdb0][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 16:28:12,978][gdb0][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 16:28:12,978][gdb0][DEBUG ] The following packages will be REMOVED:
[2017-05-31 16:28:12,978][gdb0][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 16:28:13,343][gdb0][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 16:28:13,343][gdb0][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 16:28:13,758][gdb0][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104462 files and directories currently installed.)
[2017-05-31 16:28:13,759][gdb0][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 16:28:13,923][gdb0][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 16:28:14,037][gdb0][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 16:28:14,101][gdb0][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 16:28:14,316][gdb0][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 16:28:14,430][gdb0][DEBUG ] dpkg: warning: while removing ceph-osd, directory '/var/lib/ceph/osd' not empty so not removed
[2017-05-31 16:28:14,431][gdb0][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 16:28:14,595][gdb0][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 16:28:14,659][gdb0][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 16:28:14,723][gdb0][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 16:28:14,888][gdb0][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 16:28:14,920][gdb0][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 16:28:14,984][gdb0][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 16:28:15,098][gdb0][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 16:28:15,130][gdb0][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-05-31 16:28:15,162][gdb0][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 16:28:15,327][gdb0][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 16:28:15,441][gdb0][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 16:28:15,441][gdb0][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 16:28:15,606][gdb0][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 16:28:16,724][gdb0][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 16:28:16,889][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:28:16,889][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb0
[2017-05-31 16:28:16,889][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:28:16,889][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:28:16,889][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:28:16,889][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:28:16,889][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:28:16,890][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f018f184710>
[2017-05-31 16:28:16,890][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:28:16,890][ceph_deploy.cli][INFO  ]  host                          : ['gdb0']
[2017-05-31 16:28:16,890][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f018fa91230>
[2017-05-31 16:28:16,890][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:28:16,890][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:28:16,890][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb0
[2017-05-31 16:28:17,134][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:28:17,366][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:28:17,366][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:28:17,382][gdb0][DEBUG ] detect machine type
[2017-05-31 16:28:17,385][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:28:17,614][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:28:17,837][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:28:17,838][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:28:17,854][gdb0][DEBUG ] detect machine type
[2017-05-31 16:28:17,857][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:28:17,857][gdb0][INFO  ] purging data on gdb0
[2017-05-31 16:28:17,859][gdb0][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:28:17,871][gdb0][WARNING] OSDs may still be mounted, trying to unmount them
[2017-05-31 16:28:17,872][gdb0][INFO  ] Running command: sudo find /var/lib/ceph -mindepth 1 -maxdepth 2 -type d -exec umount {} ;
[2017-05-31 16:28:17,883][gdb0][WARNING] umount: /var/lib/ceph/osd: not mounted
[2017-05-31 16:28:17,883][gdb0][WARNING] umount: /var/lib/ceph/osd/ceph-0: target is busy
[2017-05-31 16:28:17,883][gdb0][WARNING]         (In some cases useful info about processes that
[2017-05-31 16:28:17,883][gdb0][WARNING]          use the device is found by lsof(8) or fuser(1).)
[2017-05-31 16:28:17,884][gdb0][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:28:17,890][gdb0][WARNING] rm: skipping '/var/lib/ceph/osd/ceph-0', since it's on a different device
[2017-05-31 16:28:17,890][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 16:28:17,891][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: rm -rf --one-file-system -- /var/lib/ceph

[2017-05-31 16:28:18,065][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:28:18,065][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 16:28:18,067][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:28:18,067][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:28:18,067][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:28:18,067][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:28:18,067][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:28:18,067][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efeccbfe9e0>
[2017-05-31 16:28:18,067][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:28:18,067][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7efecd4c1848>
[2017-05-31 16:28:18,067][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:28:18,067][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:28:52,998][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb0
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8c5b02ffc8>
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ]  host                          : ['gdb0']
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f8c5b9421b8>
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:28:52,999][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:28:52,999][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 16:28:53,000][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 16:28:53,000][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb0
[2017-05-31 16:28:53,000][ceph_deploy.install][DEBUG ] Detecting platform for host gdb0 ...
[2017-05-31 16:28:53,239][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:28:53,462][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:28:53,462][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:28:53,478][gdb0][DEBUG ] detect machine type
[2017-05-31 16:28:53,481][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:28:53,481][gdb0][INFO  ] Purging Ceph on gdb0
[2017-05-31 16:28:53,483][gdb0][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 16:28:53,521][gdb0][DEBUG ] Reading package lists...
[2017-05-31 16:28:53,686][gdb0][DEBUG ] Building dependency tree...
[2017-05-31 16:28:53,686][gdb0][DEBUG ] Reading state information...
[2017-05-31 16:28:53,750][gdb0][DEBUG ] Package 'ceph' is not installed, so not removed
[2017-05-31 16:28:53,750][gdb0][DEBUG ] Package 'ceph-common' is not installed, so not removed
[2017-05-31 16:28:53,750][gdb0][DEBUG ] Package 'ceph-mds' is not installed, so not removed
[2017-05-31 16:28:53,750][gdb0][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 16:28:53,750][gdb0][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 16:28:53,750][gdb0][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 16:28:53,750][gdb0][DEBUG ]   ceph-fuse javascript-common libaio1 libcephfs2 libgoogle-perftools4
[2017-05-31 16:28:53,751][gdb0][DEBUG ]   libibverbs1 libjs-jquery libleveldb1v5 liblttng-ust-ctl2 liblttng-ust0
[2017-05-31 16:28:53,751][gdb0][DEBUG ]   libnspr4 libnss3 libnss3-nssdb libopts25 libpython2.7 librados2
[2017-05-31 16:28:53,751][gdb0][DEBUG ]   libradosstriper1 librbd1 librgw2 libsnappy1v5 libtcmalloc-minimal4
[2017-05-31 16:28:53,751][gdb0][DEBUG ]   libunwind8 liburcu4 linux-aws-headers-4.4.0-1013
[2017-05-31 16:28:53,751][gdb0][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 16:28:53,751][gdb0][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 16:28:53,751][gdb0][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 16:28:53,751][gdb0][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 16:28:53,751][gdb0][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 16:28:53,751][gdb0][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 16:28:53,751][gdb0][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 16:28:53,751][gdb0][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.
[2017-05-31 16:28:53,917][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:28:53,918][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb0
[2017-05-31 16:28:53,918][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:28:53,918][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:28:53,918][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:28:53,918][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:28:53,918][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:28:53,918][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f338086d710>
[2017-05-31 16:28:53,918][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:28:53,918][ceph_deploy.cli][INFO  ]  host                          : ['gdb0']
[2017-05-31 16:28:53,918][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f338117a230>
[2017-05-31 16:28:53,919][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:28:53,919][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:28:53,919][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb0
[2017-05-31 16:28:54,158][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:28:54,390][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:28:54,390][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:28:54,406][gdb0][DEBUG ] detect machine type
[2017-05-31 16:28:54,410][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:28:54,602][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:28:54,829][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:28:54,830][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:28:54,846][gdb0][DEBUG ] detect machine type
[2017-05-31 16:28:54,849][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:28:54,849][gdb0][INFO  ] purging data on gdb0
[2017-05-31 16:28:54,851][gdb0][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:28:54,865][gdb0][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 16:28:55,038][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:28:55,039][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 16:28:55,039][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:28:55,039][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:28:55,039][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:28:55,039][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:28:55,039][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:28:55,039][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8e7d5379e0>
[2017-05-31 16:28:55,039][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:28:55,039][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f8e7ddfa848>
[2017-05-31 16:28:55,039][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:28:55,039][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:29:02,054][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:29:02,054][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb1
[2017-05-31 16:29:02,054][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:29:02,054][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:29:02,054][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:29:02,054][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:29:02,054][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:29:02,054][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fccf2487fc8>
[2017-05-31 16:29:02,054][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:29:02,055][ceph_deploy.cli][INFO  ]  host                          : ['gdb1']
[2017-05-31 16:29:02,055][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fccf2d9a1b8>
[2017-05-31 16:29:02,055][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:29:02,055][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:29:02,055][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 16:29:02,055][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 16:29:02,055][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb1
[2017-05-31 16:29:02,055][ceph_deploy.install][DEBUG ] Detecting platform for host gdb1 ...
[2017-05-31 16:29:02,304][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:29:02,499][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:29:02,499][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:29:02,516][gdb1][DEBUG ] detect machine type
[2017-05-31 16:29:02,519][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:29:02,520][gdb1][INFO  ] Purging Ceph on gdb1
[2017-05-31 16:29:02,521][gdb1][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 16:29:02,559][gdb1][DEBUG ] Reading package lists...
[2017-05-31 16:29:02,723][gdb1][DEBUG ] Building dependency tree...
[2017-05-31 16:29:02,724][gdb1][DEBUG ] Reading state information...
[2017-05-31 16:29:02,838][gdb1][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 16:29:02,838][gdb1][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 16:29:02,838][gdb1][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 16:29:02,838][gdb1][DEBUG ]   bc ca-certificates-java ceph-fuse cmake cmake-data cython cython3
[2017-05-31 16:29:02,838][gdb1][DEBUG ]   default-jdk default-jdk-headless default-jre default-jre-headless dh-systemd
[2017-05-31 16:29:02,838][gdb1][DEBUG ]   docutils-common fontconfig fonts-font-awesome fonts-lato icu-devtools
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   java-common javahelper javascript-common jq junit4 libaio-dev libarchive13
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   libasound2 libasound2-data libasyncns0 libatk1.0-0 libatk1.0-data
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   libatomic-ops-dev libavahi-client3 libavahi-common-data libavahi-common3
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   libbabeltrace-ctf-dev libbabeltrace-dev libblkid-dev libcairo2 libcephfs2
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   libcups2 libcurl3 libdatrie1 libdrm-amdgpu1 libdrm-intel1 libdrm-nouveau2
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   libdrm-radeon1 libexpat1-dev libfcgi-dev libfcgi0ldbl libflac8 libfuse-dev
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   libgdk-pixbuf2.0-0 libgdk-pixbuf2.0-common libgif7 libgl1-mesa-dri
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   libgl1-mesa-glx libglapi-mesa libgoogle-perftools-dev libgoogle-perftools4
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   libgraphite2-3 libgtk2.0-0 libgtk2.0-common libhamcrest-java libharfbuzz0b
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   libibverbs-dev libicu-dev libjs-jquery libjs-modernizr libjs-sphinxdoc
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   libjs-underscore libjsoncpp1 libkeyutils-dev liblcms2-2 libleveldb-dev
[2017-05-31 16:29:02,839][gdb1][DEBUG ]   libleveldb1v5 libllvm3.8 liblttng-ust-dev liblttng-ust-python-agent0
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   libnspr4-dev libnss3-dev libogg0 libonig2 libpango-1.0-0 libpangocairo-1.0-0
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   libpangoft2-1.0-0 libpciaccess0 libpcre16-3 libpcre3-dev libpcre32-3
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   libpcrecpp0v5 libpixman-1-0 libpulse0 libpython-all-dev libpython-dev
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   libpython2.7 libpython2.7-dev libpython3-all-dev libpython3-dev
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   libpython3.5-dev libradosstriper1 librgw2 libselinux1-dev libsepol1-dev
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   libsnappy-dev libsnappy1v5 libsndfile1 libssl-dev libtcmalloc-minimal4
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   libthai-data libthai0 libudev-dev libunwind-dev libunwind8 libunwind8-dev
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   liburcu-dev libvorbis0a libvorbisenc2 libx11-xcb1 libxcb-dri2-0
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-render0 libxcb-shm0
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   libxcb-sync1 libxcomposite1 libxcursor1 libxdamage1 libxfixes3 libxi6
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   libxinerama1 libxml2-dev libxrandr2 libxrender1 libxshmfence1 libxtst6
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   libxxf86vm1 linux-aws-headers-4.4.0-1013 linux-headers-4.4.0-1013-aws
[2017-05-31 16:29:02,840][gdb1][DEBUG ]   linux-image-4.4.0-1013-aws openjdk-8-jdk openjdk-8-jdk-headless
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   openjdk-8-jre openjdk-8-jre-headless python-alabaster python-all
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   python-all-dev python-babel python-babel-localedata python-blinker
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   python-dev python-docutils python-enum34 python-flask python-idna
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   python-ipaddress python-itsdangerous python-jinja2 python-markupsafe
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   python-ndg-httpsclient python-nose python-openssl python-pyasn1
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   python-pygments python-pyinotify python-rados python-rbd python-requests
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   python-rgw python-roman python-six python-sphinx python-sphinx-rtd-theme
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   python-tz python-urllib3 python-werkzeug python2.7-dev python3-all
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   python3-all-dev python3-dev python3.5-dev sphinx-common
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   sphinx-rtd-theme-common uuid-dev valgrind x11-common xfslibs-dev xmlstarlet
[2017-05-31 16:29:02,841][gdb1][DEBUG ]   yasm
[2017-05-31 16:29:02,841][gdb1][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 16:29:02,845][gdb1][DEBUG ] The following packages will be REMOVED:
[2017-05-31 16:29:02,845][gdb1][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 16:29:03,009][gdb1][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 43 not upgraded.
[2017-05-31 16:29:03,009][gdb1][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 16:29:03,041][gdb1][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 122129 files and directories currently installed.)
[2017-05-31 16:29:03,041][gdb1][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 16:29:03,206][gdb1][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 16:29:03,320][gdb1][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 16:29:03,384][gdb1][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 16:29:03,599][gdb1][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 16:29:03,713][gdb1][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 16:29:03,878][gdb1][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 16:29:04,042][gdb1][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 16:29:04,206][gdb1][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 16:29:04,321][gdb1][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 16:29:04,535][gdb1][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 16:29:04,536][gdb1][DEBUG ] dpkg: warning: while removing ceph-base, directory '/usr/share/doc/ceph' not empty so not removed
[2017-05-31 16:29:04,543][gdb1][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 16:29:04,708][gdb1][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 16:29:04,772][gdb1][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 16:29:04,936][gdb1][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 16:29:05,803][gdb1][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 16:29:05,969][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:29:05,969][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb1
[2017-05-31 16:29:05,969][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:29:05,969][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:29:05,969][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:29:05,969][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:29:05,970][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:29:05,970][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa36e5ab710>
[2017-05-31 16:29:05,970][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:29:05,970][ceph_deploy.cli][INFO  ]  host                          : ['gdb1']
[2017-05-31 16:29:05,970][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fa36eeb8230>
[2017-05-31 16:29:05,970][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:29:05,970][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:29:05,970][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb1
[2017-05-31 16:29:06,207][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:29:06,439][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:29:06,440][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:29:06,456][gdb1][DEBUG ] detect machine type
[2017-05-31 16:29:06,460][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:29:06,656][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:29:06,883][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:29:06,883][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:29:06,899][gdb1][DEBUG ] detect machine type
[2017-05-31 16:29:06,903][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:29:06,903][gdb1][INFO  ] purging data on gdb1
[2017-05-31 16:29:06,904][gdb1][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:29:06,918][gdb1][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 16:29:07,089][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:29:07,089][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 16:29:07,090][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:29:07,090][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:29:07,090][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:29:07,090][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:29:07,090][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:29:07,090][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4b4d8549e0>
[2017-05-31 16:29:07,090][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:29:07,090][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f4b4e117848>
[2017-05-31 16:29:07,090][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:29:07,090][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:29:50,888][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:29:50,888][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 16:29:50,888][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:29:50,888][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:29:50,888][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2de252a560>
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f2de2bae758>
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:29:50,889][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 16:29:50,889][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 16:29:50,890][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 16:29:50,916][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:29:50,930][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:29:50,930][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:29:50,947][gdb3][DEBUG ] detect machine type
[2017-05-31 16:29:50,949][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:29:50,951][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 16:29:50,962][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 16:29:50,968][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 16:29:50,968][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 16:29:50,968][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 16:29:50,968][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 16:29:50,969][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 16:29:50,969][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 16:29:50,969][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 16:29:50,969][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 16:29:51,136][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:29:51,136][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 16:29:51,136][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:29:51,136][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:29:51,136][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:29:51,136][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:29:51,136][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 16:29:51,137][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:29:51,137][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcbe9e49e60>
[2017-05-31 16:29:51,137][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:29:51,137][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fcbe9e1eb18>
[2017-05-31 16:29:51,137][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:29:51,137][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 16:29:51,137][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:29:51,138][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 16:29:51,138][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 16:29:51,164][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:29:51,179][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:29:51,179][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:29:51,196][gdb3][DEBUG ] detect machine type
[2017-05-31 16:29:51,198][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:29:51,198][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 16:29:51,199][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 16:29:51,199][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:29:51,199][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 16:29:51,199][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:29:51,199][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 16:29:51,200][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:29:51,201][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 16:29:51,202][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 16:29:51,202][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 16:29:51,202][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 16:29:51,202][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 16:29:51,204][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 16:29:51,242][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 16:29:51,242][gdb3][DEBUG ] ceph-mon: set fsid to e27e70dc-fca4-4880-9bc9-872a64d43c2a
[2017-05-31 16:29:51,246][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 16:29:51,250][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 16:29:51,250][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 16:29:51,251][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 16:29:51,252][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:29:51,319][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 16:29:51,387][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 16:29:53,457][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:29:53,522][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 16:29:53,523][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 16:29:53,523][gdb3][DEBUG ] {
[2017-05-31 16:29:53,523][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 16:29:53,524][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 16:29:53,524][gdb3][DEBUG ]   "features": {
[2017-05-31 16:29:53,524][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 16:29:53,524][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 16:29:53,524][gdb3][DEBUG ]       "kraken", 
[2017-05-31 16:29:53,524][gdb3][DEBUG ]       "luminous"
[2017-05-31 16:29:53,524][gdb3][DEBUG ]     ], 
[2017-05-31 16:29:53,524][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 16:29:53,524][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 16:29:53,524][gdb3][DEBUG ]       "kraken", 
[2017-05-31 16:29:53,524][gdb3][DEBUG ]       "luminous"
[2017-05-31 16:29:53,524][gdb3][DEBUG ]     ]
[2017-05-31 16:29:53,524][gdb3][DEBUG ]   }, 
[2017-05-31 16:29:53,524][gdb3][DEBUG ]   "monmap": {
[2017-05-31 16:29:53,525][gdb3][DEBUG ]     "created": "2017-05-31 16:29:51.227226", 
[2017-05-31 16:29:53,525][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 16:29:53,525][gdb3][DEBUG ]     "features": {
[2017-05-31 16:29:53,525][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 16:29:53,525][gdb3][DEBUG ]       "persistent": [
[2017-05-31 16:29:53,525][gdb3][DEBUG ]         "kraken", 
[2017-05-31 16:29:53,525][gdb3][DEBUG ]         "luminous"
[2017-05-31 16:29:53,525][gdb3][DEBUG ]       ]
[2017-05-31 16:29:53,525][gdb3][DEBUG ]     }, 
[2017-05-31 16:29:53,526][gdb3][DEBUG ]     "fsid": "e27e70dc-fca4-4880-9bc9-872a64d43c2a", 
[2017-05-31 16:29:53,526][gdb3][DEBUG ]     "modified": "2017-05-31 16:29:51.477546", 
[2017-05-31 16:29:53,526][gdb3][DEBUG ]     "mons": [
[2017-05-31 16:29:53,526][gdb3][DEBUG ]       {
[2017-05-31 16:29:53,526][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 16:29:53,526][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 16:29:53,526][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 16:29:53,526][gdb3][DEBUG ]         "rank": 0
[2017-05-31 16:29:53,526][gdb3][DEBUG ]       }
[2017-05-31 16:29:53,526][gdb3][DEBUG ]     ]
[2017-05-31 16:29:53,527][gdb3][DEBUG ]   }, 
[2017-05-31 16:29:53,527][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 16:29:53,527][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 16:29:53,527][gdb3][DEBUG ]   "quorum": [
[2017-05-31 16:29:53,527][gdb3][DEBUG ]     0
[2017-05-31 16:29:53,527][gdb3][DEBUG ]   ], 
[2017-05-31 16:29:53,527][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 16:29:53,527][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 16:29:53,527][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 16:29:53,527][gdb3][DEBUG ] }
[2017-05-31 16:29:53,527][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 16:29:53,527][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 16:29:53,528][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:29:53,593][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 16:29:53,610][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:29:53,624][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:29:53,625][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:29:53,641][gdb3][DEBUG ] detect machine type
[2017-05-31 16:29:53,643][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:29:53,644][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:29:53,709][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 16:29:53,710][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 16:29:53,710][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 16:29:53,712][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmph_9GxO
[2017-05-31 16:29:53,728][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:29:53,742][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:29:53,743][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:29:53,759][gdb3][DEBUG ] detect machine type
[2017-05-31 16:29:53,762][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:29:53,762][gdb3][DEBUG ] fetch remote file
[2017-05-31 16:29:53,763][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:29:53,829][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 16:29:53,996][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 16:29:54,162][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 16:29:54,328][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 16:29:54,494][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 16:29:54,661][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 16:29:54,827][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 16:29:54,993][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 16:29:55,159][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 16:29:55,326][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 16:29:55,491][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 16:29:55,491][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 16:29:55,491][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170531162955'
[2017-05-31 16:29:55,492][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 16:29:55,492][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 16:29:55,492][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 16:29:55,492][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmph_9GxO
[2017-05-31 16:29:55,675][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f478fcc3518>
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f47905da938>
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:29:55,676][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:29:55,676][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 16:29:55,703][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:29:55,717][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:29:55,718][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:29:55,734][gdb3][DEBUG ] detect machine type
[2017-05-31 16:29:55,737][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:30:43,059][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:30:43,059][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb:/dev/xvdb
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  disk                          : [('gdb', '/dev/xvdb', None)]
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1a561ea908>
[2017-05-31 16:30:43,060][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:30:43,061][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:30:43,061][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1a56440aa0>
[2017-05-31 16:30:43,061][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:30:43,061][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:30:43,061][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:30:43,061][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb:/dev/xvdb:
[2017-05-31 16:30:43,164][ceph_deploy.osd][ERROR ] connecting to host: gdb resulted in errors: HostNotFound gdb
[2017-05-31 16:30:43,164][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:30:44,908][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:30:44,908][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-31 16:30:44,908][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:30:44,908][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:30:44,908][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:30:44,908][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-31 16:30:44,908][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:30:44,908][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:30:44,908][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:30:44,908][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:30:44,908][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:30:44,909][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:30:44,909][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:30:44,909][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:30:44,909][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcafc483908>
[2017-05-31 16:30:44,909][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:30:44,909][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:30:44,909][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcafc6d9aa0>
[2017-05-31 16:30:44,909][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:30:44,909][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:30:44,909][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:30:44,909][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-31 16:30:45,155][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:30:45,382][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:30:45,383][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:30:45,398][gdb0][DEBUG ] detect machine type
[2017-05-31 16:30:45,402][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:30:45,403][ceph_deploy.osd][ERROR ] ceph needs to be installed in remote host: gdb0
[2017-05-31 16:30:45,403][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:30:54,767][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:30:54,768][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-31 16:30:54,768][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:30:54,768][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:30:54,768][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:30:54,768][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-31 16:30:54,768][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:30:54,768][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:30:54,768][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:30:54,768][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:30:54,768][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:30:54,768][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:30:54,769][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:30:54,769][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:30:54,769][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc3ee7dc908>
[2017-05-31 16:30:54,769][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:30:54,769][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:30:54,769][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc3eea32aa0>
[2017-05-31 16:30:54,769][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:30:54,769][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:30:54,769][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:30:54,769][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-31 16:30:55,015][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:30:55,243][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:30:55,243][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:30:55,259][gdb1][DEBUG ] detect machine type
[2017-05-31 16:30:55,263][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:30:55,264][ceph_deploy.osd][ERROR ] ceph needs to be installed in remote host: gdb1
[2017-05-31 16:30:55,264][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:31:21,125][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:31:21,125][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-31 16:31:21,125][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdabf82e908>
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:31:21,126][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fdabfa84aa0>
[2017-05-31 16:31:21,127][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:31:21,127][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:31:21,127][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:31:21,127][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-31 16:31:21,372][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:31:21,603][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:31:21,603][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:31:21,620][gdb1][DEBUG ] detect machine type
[2017-05-31 16:31:21,624][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:31:21,624][ceph_deploy.osd][ERROR ] ceph needs to be installed in remote host: gdb1
[2017-05-31 16:31:21,625][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:31:58,801][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:31:58,801][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-31 16:31:58,801][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:31:58,801][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:31:58,801][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:31:58,801][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-31 16:31:58,801][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:31:58,801][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:31:58,801][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:31:58,801][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:31:58,801][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:31:58,802][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:31:58,802][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:31:58,802][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:31:58,802][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0b33d7d908>
[2017-05-31 16:31:58,802][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:31:58,802][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:31:58,802][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0b33fd3aa0>
[2017-05-31 16:31:58,802][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:31:58,802][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:31:58,802][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:31:58,802][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-31 16:31:59,042][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:31:59,273][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:31:59,274][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:31:59,290][gdb0][DEBUG ] detect machine type
[2017-05-31 16:31:59,293][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:31:59,294][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:31:59,294][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 16:31:59,294][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:31:59,297][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:31:59,297][gdb0][DEBUG ] create a keyring file
[2017-05-31 16:31:59,299][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-31 16:31:59,299][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:31:59,301][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:31:59,421][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:31:59,429][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:31:59,429][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:31:59,429][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:31:59,436][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:31:59,444][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:31:59,452][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:31:59,452][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:31:59,452][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:31:59,452][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-31 16:31:59,468][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-31 16:31:59,469][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-31 16:31:59,477][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-31 16:31:59,492][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:31:59,493][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-31 16:31:59,493][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:31:59,493][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-31 16:31:59,493][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-31 16:31:59,493][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-31 16:31:59,493][gdb0][WARNING] backup header from main header.
[2017-05-31 16:31:59,493][gdb0][WARNING] 
[2017-05-31 16:32:00,712][gdb0][DEBUG ] ****************************************************************************
[2017-05-31 16:32:00,712][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-31 16:32:00,712][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-31 16:32:00,712][gdb0][DEBUG ] ****************************************************************************
[2017-05-31 16:32:00,712][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-31 16:32:00,712][gdb0][DEBUG ] other utilities.
[2017-05-31 16:32:00,712][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-31 16:32:01,730][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-31 16:32:01,730][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:32:01,730][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-31 16:32:01,730][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:01,730][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:32:01,738][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:01,753][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:01,754][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-31 16:32:01,754][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:01,754][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-31 16:32:01,754][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:01,754][gdb0][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-31 16:32:01,754][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:248bc6e7-79dd-4b48-8fe7-f2f193ad19fa --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-31 16:32:02,822][gdb0][DEBUG ] Setting name!
[2017-05-31 16:32:02,822][gdb0][DEBUG ] partNum is 0
[2017-05-31 16:32:02,822][gdb0][DEBUG ] REALLY setting name!
[2017-05-31 16:32:02,822][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:32:02,822][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:32:02,822][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:02,854][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:32:02,968][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:02,984][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:02,984][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:02,984][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-31 16:32:02,984][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:02,986][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:02,986][gdb0][WARNING] ptype_tobe_for_name: name = block
[2017-05-31 16:32:02,986][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:02,986][gdb0][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-31 16:32:02,986][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:7fa22143-361b-4930-9927-81c6ba2940e3 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-31 16:32:04,003][gdb0][DEBUG ] Setting name!
[2017-05-31 16:32:04,004][gdb0][DEBUG ] partNum is 1
[2017-05-31 16:32:04,004][gdb0][DEBUG ] REALLY setting name!
[2017-05-31 16:32:04,004][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:32:04,004][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:32:04,004][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:04,218][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:32:04,383][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:04,598][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:04,598][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:04,598][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-31 16:32:04,598][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/7fa22143-361b-4930-9927-81c6ba2940e3
[2017-05-31 16:32:04,598][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-31 16:32:05,615][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:32:05,616][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:32:05,616][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:05,830][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:32:05,995][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:06,011][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/7fa22143-361b-4930-9927-81c6ba2940e3
[2017-05-31 16:32:06,011][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-31 16:32:06,011][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-31 16:32:06,125][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-31 16:32:06,125][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-31 16:32:06,125][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-31 16:32:06,125][gdb0][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-31 16:32:06,126][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-31 16:32:06,126][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-31 16:32:06,126][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-31 16:32:06,126][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-31 16:32:06,126][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-31 16:32:06,126][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.zsDt4v with options noatime,inode64
[2017-05-31 16:32:06,126][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.zsDt4v
[2017-05-31 16:32:06,126][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.zsDt4v
[2017-05-31 16:32:06,126][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zsDt4v/ceph_fsid.6482.tmp
[2017-05-31 16:32:06,126][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zsDt4v/fsid.6482.tmp
[2017-05-31 16:32:06,126][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zsDt4v/magic.6482.tmp
[2017-05-31 16:32:06,130][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zsDt4v/block_uuid.6482.tmp
[2017-05-31 16:32:06,130][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.zsDt4v/block -> /dev/disk/by-partuuid/7fa22143-361b-4930-9927-81c6ba2940e3
[2017-05-31 16:32:06,131][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zsDt4v/type.6482.tmp
[2017-05-31 16:32:06,134][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zsDt4v
[2017-05-31 16:32:06,134][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.zsDt4v
[2017-05-31 16:32:06,135][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.zsDt4v
[2017-05-31 16:32:06,199][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:06,199][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-31 16:32:07,216][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-31 16:32:07,216][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-31 16:32:07,216][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-31 16:32:07,216][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:32:07,216][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:32:07,217][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:07,217][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:32:07,281][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:07,281][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-31 16:32:07,283][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:32:12,405][gdb0][INFO  ] checking OSD status...
[2017-05-31 16:32:12,405][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:32:12,408][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 16:32:12,473][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 16:32:14,926][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:32:14,927][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:32:14,928][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe3cc22b908>
[2017-05-31 16:32:14,928][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:32:14,928][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:32:14,928][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe3cc481aa0>
[2017-05-31 16:32:14,928][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:32:14,928][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:32:14,928][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:32:14,928][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-31 16:32:15,167][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:32:15,395][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:32:15,396][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:32:15,412][gdb1][DEBUG ] detect machine type
[2017-05-31 16:32:15,416][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:32:15,416][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:32:15,416][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 16:32:15,417][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:32:15,419][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:32:15,419][gdb1][DEBUG ] create a keyring file
[2017-05-31 16:32:15,421][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-31 16:32:15,421][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:32:15,423][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:32:15,543][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:32:15,559][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:15,559][gdb1][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:32:15,559][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:32:15,567][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:32:15,574][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:32:15,590][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:15,590][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:15,590][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:15,590][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-31 16:32:15,594][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-31 16:32:15,601][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-31 16:32:15,609][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-31 16:32:15,624][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:15,624][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-31 16:32:15,624][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:15,625][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-31 16:32:15,625][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-31 16:32:15,625][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-31 16:32:15,625][gdb1][WARNING] backup header from main header.
[2017-05-31 16:32:15,625][gdb1][WARNING] 
[2017-05-31 16:32:16,793][gdb1][DEBUG ] ****************************************************************************
[2017-05-31 16:32:16,793][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-31 16:32:16,793][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-31 16:32:16,793][gdb1][DEBUG ] ****************************************************************************
[2017-05-31 16:32:16,793][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-31 16:32:16,793][gdb1][DEBUG ] other utilities.
[2017-05-31 16:32:16,793][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-31 16:32:17,760][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-31 16:32:17,761][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:32:17,762][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-31 16:32:17,762][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:17,778][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:32:17,809][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:17,817][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:17,817][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-31 16:32:17,817][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:17,817][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-31 16:32:17,817][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:17,818][gdb1][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-31 16:32:17,818][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:6094852d-5cc2-4c8e-9bca-631e038497cf --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-31 16:32:18,835][gdb1][DEBUG ] Setting name!
[2017-05-31 16:32:18,835][gdb1][DEBUG ] partNum is 0
[2017-05-31 16:32:18,835][gdb1][DEBUG ] REALLY setting name!
[2017-05-31 16:32:18,835][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:32:18,835][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:32:18,835][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:18,950][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:32:19,014][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:19,046][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:19,046][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:19,046][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-31 16:32:19,046][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:19,046][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:19,046][gdb1][WARNING] ptype_tobe_for_name: name = block
[2017-05-31 16:32:19,046][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:19,046][gdb1][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-31 16:32:19,046][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:e56ddcae-4211-4d8a-9079-c40d9b6f9541 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-31 16:32:20,064][gdb1][DEBUG ] Setting name!
[2017-05-31 16:32:20,064][gdb1][DEBUG ] partNum is 1
[2017-05-31 16:32:20,064][gdb1][DEBUG ] REALLY setting name!
[2017-05-31 16:32:20,064][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:32:20,064][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:32:20,065][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:20,279][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:32:20,544][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:20,759][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:20,759][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:20,759][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-31 16:32:20,759][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/e56ddcae-4211-4d8a-9079-c40d9b6f9541
[2017-05-31 16:32:20,759][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-31 16:32:21,776][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:32:21,777][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:32:21,777][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:21,991][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:32:22,156][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:22,171][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/e56ddcae-4211-4d8a-9079-c40d9b6f9541
[2017-05-31 16:32:22,172][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-31 16:32:22,172][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-31 16:32:22,286][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-31 16:32:22,286][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-31 16:32:22,286][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-31 16:32:22,286][gdb1][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-31 16:32:22,286][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-31 16:32:22,286][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-31 16:32:22,287][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-31 16:32:22,287][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-31 16:32:22,287][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-31 16:32:22,287][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.Vm1MJ2 with options noatime,inode64
[2017-05-31 16:32:22,287][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.Vm1MJ2
[2017-05-31 16:32:22,287][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.Vm1MJ2
[2017-05-31 16:32:22,287][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Vm1MJ2/ceph_fsid.9960.tmp
[2017-05-31 16:32:22,287][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Vm1MJ2/fsid.9960.tmp
[2017-05-31 16:32:22,287][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Vm1MJ2/magic.9960.tmp
[2017-05-31 16:32:22,288][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Vm1MJ2/block_uuid.9960.tmp
[2017-05-31 16:32:22,292][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.Vm1MJ2/block -> /dev/disk/by-partuuid/e56ddcae-4211-4d8a-9079-c40d9b6f9541
[2017-05-31 16:32:22,292][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Vm1MJ2/type.9960.tmp
[2017-05-31 16:32:22,293][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Vm1MJ2
[2017-05-31 16:32:22,297][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.Vm1MJ2
[2017-05-31 16:32:22,297][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Vm1MJ2
[2017-05-31 16:32:22,361][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:32:22,361][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-31 16:32:23,378][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-31 16:32:23,378][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-31 16:32:23,378][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-31 16:32:23,378][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:32:23,379][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:32:23,379][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:23,379][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:32:23,543][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:32:23,551][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-31 16:32:23,569][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:32:28,691][gdb1][INFO  ] checking OSD status...
[2017-05-31 16:32:28,691][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:32:28,694][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 16:32:28,809][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
