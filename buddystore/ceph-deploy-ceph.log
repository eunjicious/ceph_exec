[2017-05-31 16:34:09,790][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:34:09,790][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 16:34:09,790][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:34:09,790][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f407344afc8>
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f4073d5d1b8>
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:34:09,791][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 16:34:09,791][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 16:34:09,791][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 16:34:09,791][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 16:34:09,818][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:34:09,833][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:34:09,834][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:34:09,850][gdb3][DEBUG ] detect machine type
[2017-05-31 16:34:09,853][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:34:09,853][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 16:34:09,854][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 16:34:09,893][gdb3][DEBUG ] Reading package lists...
[2017-05-31 16:34:10,057][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 16:34:10,057][gdb3][DEBUG ] Reading state information...
[2017-05-31 16:34:10,171][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 16:34:10,172][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 16:34:10,172][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 16:34:10,172][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 16:34:10,172][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 16:34:10,172][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 16:34:10,173][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 16:34:10,173][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 16:34:10,174][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 16:34:10,288][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 16:34:10,288][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 16:34:10,356][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 16:34:10,357][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 16:34:10,521][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 16:34:10,635][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 16:34:10,635][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 16:34:10,850][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 16:34:10,964][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 16:34:11,129][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 16:34:11,193][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 16:34:11,209][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 16:34:11,375][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 16:34:11,439][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 16:34:11,455][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 16:34:11,624][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 16:34:11,656][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 16:34:11,821][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 16:34:11,885][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 16:34:11,885][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 16:34:12,000][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 16:34:12,816][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 16:34:12,983][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe8d7689710>
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fe8d7f96230>
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:34:12,984][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 16:34:13,011][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:34:13,026][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:34:13,026][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:34:13,043][gdb3][DEBUG ] detect machine type
[2017-05-31 16:34:13,045][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:34:13,061][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:34:13,076][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:34:13,077][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:34:13,093][gdb3][DEBUG ] detect machine type
[2017-05-31 16:34:13,096][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:34:13,096][gdb3][INFO  ] purging data on gdb3
[2017-05-31 16:34:13,097][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:34:13,110][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 16:34:13,281][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f329227f9e0>
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f3292b42848>
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:34:13,283][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:35:36,302][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:35:36,302][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6a720a6560>
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f6a7272a758>
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 16:35:36,304][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 16:35:36,305][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 16:35:36,331][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:35:36,345][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:35:36,345][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:35:36,362][gdb3][DEBUG ] detect machine type
[2017-05-31 16:35:36,364][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:35:36,365][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 16:35:36,377][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 16:35:36,383][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 16:35:36,383][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 16:35:36,383][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 16:35:36,384][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 16:35:36,384][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 16:35:36,384][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 16:35:36,384][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 16:35:36,384][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 16:35:36,550][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:35:36,550][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 16:35:36,550][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:35:36,550][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa11d0dce60>
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fa11d0b1b18>
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 16:35:36,552][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:35:36,552][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 16:35:36,552][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 16:35:36,579][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:35:36,592][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:35:36,593][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:35:36,609][gdb3][DEBUG ] detect machine type
[2017-05-31 16:35:36,612][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:35:36,612][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 16:35:36,612][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 16:35:36,612][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:35:36,613][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 16:35:36,613][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:35:36,613][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 16:35:36,614][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:35:36,615][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 16:35:36,615][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 16:35:36,616][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 16:35:36,616][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 16:35:36,616][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 16:35:36,617][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 16:35:36,656][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 16:35:36,656][gdb3][DEBUG ] ceph-mon: set fsid to e52d1ea0-de58-48ed-b1fd-547c31561e69
[2017-05-31 16:35:36,663][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 16:35:36,663][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 16:35:36,664][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 16:35:36,664][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 16:35:36,665][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:35:36,733][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 16:35:36,805][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 16:35:38,875][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:35:38,940][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 16:35:38,940][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 16:35:38,940][gdb3][DEBUG ] {
[2017-05-31 16:35:38,940][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]   "features": {
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 16:35:38,941][gdb3][DEBUG ]       "kraken", 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]       "luminous"
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     ], 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 16:35:38,941][gdb3][DEBUG ]       "kraken", 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]       "luminous"
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     ]
[2017-05-31 16:35:38,941][gdb3][DEBUG ]   }, 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]   "monmap": {
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "created": "2017-05-31 16:35:36.641607", 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]     "features": {
[2017-05-31 16:35:38,942][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]       "persistent": [
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "kraken", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "luminous"
[2017-05-31 16:35:38,942][gdb3][DEBUG ]       ]
[2017-05-31 16:35:38,942][gdb3][DEBUG ]     }, 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]     "fsid": "e52d1ea0-de58-48ed-b1fd-547c31561e69", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]     "modified": "2017-05-31 16:35:36.894213", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]     "mons": [
[2017-05-31 16:35:38,942][gdb3][DEBUG ]       {
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "rank": 0
[2017-05-31 16:35:38,942][gdb3][DEBUG ]       }
[2017-05-31 16:35:38,943][gdb3][DEBUG ]     ]
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   }, 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "quorum": [
[2017-05-31 16:35:38,943][gdb3][DEBUG ]     0
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   ], 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 16:35:38,943][gdb3][DEBUG ] }
[2017-05-31 16:35:38,943][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 16:35:38,943][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 16:35:38,944][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:35:39,009][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 16:35:39,025][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:35:39,039][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:35:39,039][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:35:39,056][gdb3][DEBUG ] detect machine type
[2017-05-31 16:35:39,058][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:35:39,060][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:35:39,125][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 16:35:39,125][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 16:35:39,125][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 16:35:39,127][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmphK0O18
[2017-05-31 16:35:39,143][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:35:39,158][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:35:39,158][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:35:39,175][gdb3][DEBUG ] detect machine type
[2017-05-31 16:35:39,177][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:35:39,177][gdb3][DEBUG ] fetch remote file
[2017-05-31 16:35:39,179][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:35:39,245][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 16:35:39,411][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 16:35:39,577][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 16:35:39,744][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 16:35:39,960][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 16:35:40,127][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 16:35:40,293][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 16:35:40,459][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 16:35:40,625][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 16:35:40,792][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 16:35:40,957][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 16:35:40,957][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 16:35:40,957][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring
[2017-05-31 16:35:40,958][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 16:35:40,958][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 16:35:40,958][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 16:35:40,958][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmphK0O18
[2017-05-31 16:35:41,140][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:35:41,140][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8159351518>
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f8159c68938>
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:35:41,141][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 16:35:41,168][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:35:41,182][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:35:41,182][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:35:41,198][gdb3][DEBUG ] detect machine type
[2017-05-31 16:35:41,201][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:36:04,282][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:36:04,282][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff46122a908>
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:36:04,284][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:36:04,284][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff461480aa0>
[2017-05-31 16:36:04,284][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:36:04,284][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:36:04,284][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:36:04,284][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-31 16:36:04,526][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:36:04,718][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:36:04,719][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:36:04,734][gdb0][DEBUG ] detect machine type
[2017-05-31 16:36:04,738][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:36:04,739][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:36:04,739][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 16:36:04,739][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:36:04,741][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-31 16:36:04,741][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:36:04,743][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:36:04,864][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:36:04,867][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:36:04,867][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:36:04,867][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:36:04,883][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:36:04,884][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:36:04,900][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:36:04,900][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:36:04,900][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:36:04,900][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 16:36:04,901][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 16:36:04,901][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 16:36:04,901][gdb0][WARNING]     args.func(args)
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 16:36:04,901][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2018, in prepare
[2017-05-31 16:36:04,901][gdb0][WARNING]     self.prepare_locked()
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2090, in prepare_locked
[2017-05-31 16:36:04,901][gdb0][WARNING]     self.data.prepare(*to_prepare_list)
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2816, in prepare
[2017-05-31 16:36:04,902][gdb0][WARNING]     self.prepare_device(*to_prepare_list)
[2017-05-31 16:36:04,902][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2984, in prepare_device
[2017-05-31 16:36:04,902][gdb0][WARNING]     super(PrepareBluestoreData, self).prepare_device(*to_prepare_list)
[2017-05-31 16:36:04,902][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2878, in prepare_device
[2017-05-31 16:36:04,902][gdb0][WARNING]     self.sanity_checks()
[2017-05-31 16:36:04,902][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2841, in sanity_checks
[2017-05-31 16:36:04,902][gdb0][WARNING]     check_partitions=not self.args.dmcrypt)
[2017-05-31 16:36:04,902][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-05-31 16:36:04,902][gdb0][WARNING]     raise Error('Device is mounted', partition)
[2017-05-31 16:36:04,902][gdb0][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-05-31 16:36:04,906][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 16:36:04,906][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:36:04,906][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:39:15,638][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7ac7e0b908>
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7ac8061aa0>
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:39:15,640][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-31 16:39:15,875][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:39:16,106][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:39:16,106][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:39:16,122][gdb0][DEBUG ] detect machine type
[2017-05-31 16:39:16,126][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:39:16,126][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:39:16,126][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 16:39:16,127][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:39:16,129][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:39:16,129][gdb0][DEBUG ] create a keyring file
[2017-05-31 16:39:16,131][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-31 16:39:16,131][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:39:16,133][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:39:16,254][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:39:16,257][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:16,257][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:39:16,257][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:39:16,265][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:39:16,281][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:39:16,282][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:16,282][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:16,282][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:16,282][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 16:39:16,282][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 16:39:16,283][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 16:39:16,283][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 16:39:16,283][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 16:39:16,283][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 16:39:16,283][gdb0][WARNING]     args.func(args)
[2017-05-31 16:39:16,284][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 16:39:16,284][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 16:39:16,284][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2018, in prepare
[2017-05-31 16:39:16,284][gdb0][WARNING]     self.prepare_locked()
[2017-05-31 16:39:16,284][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2090, in prepare_locked
[2017-05-31 16:39:16,284][gdb0][WARNING]     self.data.prepare(*to_prepare_list)
[2017-05-31 16:39:16,285][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2816, in prepare
[2017-05-31 16:39:16,285][gdb0][WARNING]     self.prepare_device(*to_prepare_list)
[2017-05-31 16:39:16,285][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2984, in prepare_device
[2017-05-31 16:39:16,285][gdb0][WARNING]     super(PrepareBluestoreData, self).prepare_device(*to_prepare_list)
[2017-05-31 16:39:16,285][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2878, in prepare_device
[2017-05-31 16:39:16,285][gdb0][WARNING]     self.sanity_checks()
[2017-05-31 16:39:16,285][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2841, in sanity_checks
[2017-05-31 16:39:16,285][gdb0][WARNING]     check_partitions=not self.args.dmcrypt)
[2017-05-31 16:39:16,285][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-05-31 16:39:16,285][gdb0][WARNING]     raise Error('Device is mounted', partition)
[2017-05-31 16:39:16,286][gdb0][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-05-31 16:39:16,293][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 16:39:16,293][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:39:16,293][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:39:39,078][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5011019908>
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f501126faa0>
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:39:39,080][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-31 16:39:39,322][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:39:39,549][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:39:39,550][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:39:39,565][gdb0][DEBUG ] detect machine type
[2017-05-31 16:39:39,569][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:39:39,570][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:39:39,570][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 16:39:39,570][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:39:39,572][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:39:39,572][gdb0][DEBUG ] create a keyring file
[2017-05-31 16:39:39,574][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-31 16:39:39,574][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:39:39,576][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:39:39,696][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:39:39,700][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,700][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:39:39,700][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:39:39,716][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:39:39,717][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:39:39,733][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,733][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,733][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,733][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-31 16:39:39,734][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-31 16:39:39,742][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-31 16:39:39,758][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-31 16:39:39,759][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,759][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-31 16:39:39,759][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,759][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-31 16:39:39,760][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-31 16:39:39,764][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-31 16:39:39,764][gdb0][WARNING] backup header from main header.
[2017-05-31 16:39:39,764][gdb0][WARNING] 
[2017-05-31 16:39:40,831][gdb0][DEBUG ] ****************************************************************************
[2017-05-31 16:39:40,831][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-31 16:39:40,832][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-31 16:39:40,832][gdb0][DEBUG ] ****************************************************************************
[2017-05-31 16:39:40,832][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-31 16:39:40,832][gdb0][DEBUG ] other utilities.
[2017-05-31 16:39:40,832][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-31 16:39:41,849][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-31 16:39:41,849][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:39:41,850][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-31 16:39:41,850][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:41,853][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:39:41,885][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:41,888][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:41,888][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-31 16:39:41,889][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:41,889][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-31 16:39:41,889][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:41,889][gdb0][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-31 16:39:41,889][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:7e7e4d4e-fb27-4154-a27a-e2f06d039b76 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-31 16:39:42,906][gdb0][DEBUG ] Setting name!
[2017-05-31 16:39:42,906][gdb0][DEBUG ] partNum is 0
[2017-05-31 16:39:42,906][gdb0][DEBUG ] REALLY setting name!
[2017-05-31 16:39:42,906][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:39:42,907][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:39:42,907][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:42,971][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:39:43,085][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] ptype_tobe_for_name: name = block
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-31 16:39:43,117][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:8d4465c8-eb33-4fae-bb6c-98aeb5b9cc73 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-31 16:39:44,135][gdb0][DEBUG ] Setting name!
[2017-05-31 16:39:44,135][gdb0][DEBUG ] partNum is 1
[2017-05-31 16:39:44,135][gdb0][DEBUG ] REALLY setting name!
[2017-05-31 16:39:44,135][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:39:44,135][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:39:44,135][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:44,350][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:39:44,615][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:44,830][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:44,830][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:44,830][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-31 16:39:44,830][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/8d4465c8-eb33-4fae-bb6c-98aeb5b9cc73
[2017-05-31 16:39:44,830][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-31 16:39:45,847][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:39:45,848][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:39:45,848][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:46,062][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:39:46,277][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:46,492][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/8d4465c8-eb33-4fae-bb6c-98aeb5b9cc73
[2017-05-31 16:39:46,492][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-31 16:39:46,492][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-31 16:39:46,556][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-31 16:39:46,556][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-31 16:39:46,556][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-31 16:39:46,556][gdb0][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-31 16:39:46,556][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-31 16:39:46,556][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-31 16:39:46,556][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-31 16:39:46,557][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-31 16:39:46,557][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-31 16:39:46,557][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.umpsaG with options noatime,inode64
[2017-05-31 16:39:46,557][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.umpsaG
[2017-05-31 16:39:46,572][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.umpsaG
[2017-05-31 16:39:46,576][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG/ceph_fsid.8078.tmp
[2017-05-31 16:39:46,579][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG/fsid.8078.tmp
[2017-05-31 16:39:46,587][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG/magic.8078.tmp
[2017-05-31 16:39:46,587][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG/block_uuid.8078.tmp
[2017-05-31 16:39:46,588][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.umpsaG/block -> /dev/disk/by-partuuid/8d4465c8-eb33-4fae-bb6c-98aeb5b9cc73
[2017-05-31 16:39:46,589][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG/type.8078.tmp
[2017-05-31 16:39:46,591][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG
[2017-05-31 16:39:46,594][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.umpsaG
[2017-05-31 16:39:46,594][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.umpsaG
[2017-05-31 16:39:46,626][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:46,626][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-31 16:39:47,693][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:39:47,694][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:39:47,694][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:47,858][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:39:48,073][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:48,338][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-31 16:39:48,340][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:39:53,462][gdb0][INFO  ] checking OSD status...
[2017-05-31 16:39:53,462][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:39:53,465][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 16:39:53,530][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 16:40:00,896][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6d2b948908>
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6d2bb9eaa0>
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:40:00,898][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-31 16:40:01,144][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:40:01,379][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:40:01,379][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:40:01,396][gdb1][DEBUG ] detect machine type
[2017-05-31 16:40:01,400][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:40:01,401][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:40:01,401][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 16:40:01,401][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:40:01,403][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:40:01,404][gdb1][DEBUG ] create a keyring file
[2017-05-31 16:40:01,405][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-31 16:40:01,405][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:40:01,407][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:40:01,578][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:40:01,578][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,578][gdb1][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:40:01,578][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:40:01,578][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:40:01,579][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:40:01,579][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,579][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,579][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,579][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-31 16:40:01,579][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-31 16:40:01,586][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-31 16:40:01,594][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-31 16:40:01,601][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,602][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-31 16:40:01,602][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,602][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-31 16:40:01,602][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-31 16:40:01,605][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-31 16:40:01,605][gdb1][WARNING] backup header from main header.
[2017-05-31 16:40:01,605][gdb1][WARNING] 
[2017-05-31 16:40:02,673][gdb1][DEBUG ] ****************************************************************************
[2017-05-31 16:40:02,673][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-31 16:40:02,673][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-31 16:40:02,673][gdb1][DEBUG ] ****************************************************************************
[2017-05-31 16:40:02,673][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-31 16:40:02,674][gdb1][DEBUG ] other utilities.
[2017-05-31 16:40:02,674][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-31 16:40:03,641][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-31 16:40:03,641][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:40:03,641][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-31 16:40:03,641][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:03,657][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:40:03,689][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:03,692][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:03,692][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-31 16:40:03,692][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:03,693][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-31 16:40:03,693][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:03,693][gdb1][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-31 16:40:03,693][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:03829711-0e84-467b-b718-52c76eb24320 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-31 16:40:04,710][gdb1][DEBUG ] Setting name!
[2017-05-31 16:40:04,710][gdb1][DEBUG ] partNum is 0
[2017-05-31 16:40:04,710][gdb1][DEBUG ] REALLY setting name!
[2017-05-31 16:40:04,711][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:40:04,711][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:40:04,711][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:04,775][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:40:04,889][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] ptype_tobe_for_name: name = block
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-31 16:40:04,906][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:ac2260d3-0972-46d4-9786-44e431798028 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-31 16:40:05,923][gdb1][DEBUG ] Setting name!
[2017-05-31 16:40:05,923][gdb1][DEBUG ] partNum is 1
[2017-05-31 16:40:05,923][gdb1][DEBUG ] REALLY setting name!
[2017-05-31 16:40:05,923][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:40:05,924][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:40:05,924][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:06,138][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:40:06,353][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:06,567][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:06,568][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:06,568][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-31 16:40:06,568][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/ac2260d3-0972-46d4-9786-44e431798028
[2017-05-31 16:40:06,568][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-31 16:40:07,585][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:40:07,585][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:40:07,585][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:07,800][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:40:07,965][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:08,179][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/ac2260d3-0972-46d4-9786-44e431798028
[2017-05-31 16:40:08,179][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-31 16:40:08,179][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-31 16:40:08,243][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-31 16:40:08,244][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-31 16:40:08,244][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-31 16:40:08,244][gdb1][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-31 16:40:08,244][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-31 16:40:08,244][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-31 16:40:08,244][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-31 16:40:08,244][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-31 16:40:08,244][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-31 16:40:08,244][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.KBPkbB with options noatime,inode64
[2017-05-31 16:40:08,244][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.KBPkbB
[2017-05-31 16:40:08,276][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.KBPkbB
[2017-05-31 16:40:08,276][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB/ceph_fsid.11280.tmp
[2017-05-31 16:40:08,277][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB/fsid.11280.tmp
[2017-05-31 16:40:08,278][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB/magic.11280.tmp
[2017-05-31 16:40:08,281][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB/block_uuid.11280.tmp
[2017-05-31 16:40:08,282][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.KBPkbB/block -> /dev/disk/by-partuuid/ac2260d3-0972-46d4-9786-44e431798028
[2017-05-31 16:40:08,283][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB/type.11280.tmp
[2017-05-31 16:40:08,286][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB
[2017-05-31 16:40:08,287][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.KBPkbB
[2017-05-31 16:40:08,287][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.KBPkbB
[2017-05-31 16:40:08,351][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:08,351][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-31 16:40:09,369][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-31 16:40:09,369][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-31 16:40:09,369][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-31 16:40:09,369][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:40:09,369][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:40:09,369][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:09,369][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:40:09,534][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:09,537][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-31 16:40:09,555][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:40:14,677][gdb1][INFO  ] checking OSD status...
[2017-05-31 16:40:14,677][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:40:14,680][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 16:40:14,795][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 16:46:47,903][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:46:47,903][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 16:46:47,903][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:46:47,903][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7a17388fc8>
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f7a17c9b1b8>
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:46:47,904][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 16:46:47,904][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 16:46:47,904][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 16:46:47,904][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 16:46:47,931][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:46:47,945][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:46:47,946][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:46:47,962][gdb3][DEBUG ] detect machine type
[2017-05-31 16:46:47,965][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:46:47,965][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 16:46:47,966][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 16:46:48,004][gdb3][DEBUG ] Reading package lists...
[2017-05-31 16:46:48,168][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 16:46:48,168][gdb3][DEBUG ] Reading state information...
[2017-05-31 16:46:48,232][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 16:46:48,233][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 16:46:48,233][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 16:46:48,233][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 16:46:48,297][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 16:46:48,298][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 16:46:48,412][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 16:46:48,412][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 16:46:48,444][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 16:46:48,444][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 16:46:48,608][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 16:46:48,722][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 16:46:48,754][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 16:46:48,972][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 16:46:49,091][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 16:46:49,258][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 16:46:49,322][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 16:46:49,338][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 16:46:49,502][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 16:46:49,617][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 16:46:49,619][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 16:46:49,787][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 16:46:49,787][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 16:46:49,951][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 16:46:50,017][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 16:46:50,017][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 16:46:50,131][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 16:46:50,948][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 16:46:51,114][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbea0df9710>
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fbea1706230>
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:46:51,116][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 16:46:51,142][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:46:51,156][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:46:51,157][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:46:51,173][gdb3][DEBUG ] detect machine type
[2017-05-31 16:46:51,176][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:46:51,192][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:46:51,206][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:46:51,207][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:46:51,223][gdb3][DEBUG ] detect machine type
[2017-05-31 16:46:51,226][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:46:51,226][gdb3][INFO  ] purging data on gdb3
[2017-05-31 16:46:51,227][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:46:51,240][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 16:46:51,412][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc825dd89e0>
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fc82669b848>
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:19,509][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:47:19,509][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7bd7c8b560>
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f7bd830f758>
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 16:47:19,511][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:19,511][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 16:47:19,511][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 16:47:19,511][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 16:47:19,537][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:19,551][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:47:19,552][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:19,568][gdb3][DEBUG ] detect machine type
[2017-05-31 16:47:19,571][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:47:19,572][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 16:47:19,583][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 16:47:19,589][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 16:47:19,758][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f04d9566e60>
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f04d953bb18>
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:47:19,760][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 16:47:19,760][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:19,760][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 16:47:19,760][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 16:47:19,787][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:19,801][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:47:19,802][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:19,818][gdb3][DEBUG ] detect machine type
[2017-05-31 16:47:19,821][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:47:19,821][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 16:47:19,821][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 16:47:19,821][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:47:19,822][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 16:47:19,822][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:47:19,822][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 16:47:19,823][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:47:19,824][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 16:47:19,824][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 16:47:19,825][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 16:47:19,825][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 16:47:19,825][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 16:47:19,826][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 16:47:19,864][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 16:47:19,865][gdb3][DEBUG ] ceph-mon: set fsid to da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 16:47:19,868][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 16:47:19,871][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 16:47:19,872][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 16:47:19,872][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 16:47:19,873][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:47:19,941][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 16:47:20,013][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 16:47:22,083][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:47:22,148][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 16:47:22,149][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 16:47:22,149][gdb3][DEBUG ] {
[2017-05-31 16:47:22,149][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 16:47:22,149][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]   "features": {
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 16:47:22,150][gdb3][DEBUG ]       "kraken", 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]       "luminous"
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     ], 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 16:47:22,150][gdb3][DEBUG ]       "kraken", 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]       "luminous"
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     ]
[2017-05-31 16:47:22,151][gdb3][DEBUG ]   }, 
[2017-05-31 16:47:22,151][gdb3][DEBUG ]   "monmap": {
[2017-05-31 16:47:22,151][gdb3][DEBUG ]     "created": "2017-05-31 16:47:19.850297", 
[2017-05-31 16:47:22,151][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 16:47:22,151][gdb3][DEBUG ]     "features": {
[2017-05-31 16:47:22,151][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 16:47:22,151][gdb3][DEBUG ]       "persistent": [
[2017-05-31 16:47:22,151][gdb3][DEBUG ]         "kraken", 
[2017-05-31 16:47:22,151][gdb3][DEBUG ]         "luminous"
[2017-05-31 16:47:22,152][gdb3][DEBUG ]       ]
[2017-05-31 16:47:22,152][gdb3][DEBUG ]     }, 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]     "fsid": "da581514-b214-4b7a-baef-020d0e69b258", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]     "modified": "2017-05-31 16:47:20.102387", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]     "mons": [
[2017-05-31 16:47:22,152][gdb3][DEBUG ]       {
[2017-05-31 16:47:22,152][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]         "rank": 0
[2017-05-31 16:47:22,152][gdb3][DEBUG ]       }
[2017-05-31 16:47:22,152][gdb3][DEBUG ]     ]
[2017-05-31 16:47:22,152][gdb3][DEBUG ]   }, 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 16:47:22,153][gdb3][DEBUG ]   "quorum": [
[2017-05-31 16:47:22,153][gdb3][DEBUG ]     0
[2017-05-31 16:47:22,153][gdb3][DEBUG ]   ], 
[2017-05-31 16:47:22,153][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 16:47:22,153][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 16:47:22,153][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 16:47:22,153][gdb3][DEBUG ] }
[2017-05-31 16:47:22,153][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 16:47:22,153][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 16:47:22,154][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:47:22,219][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 16:47:22,234][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:22,249][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:47:22,249][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:22,266][gdb3][DEBUG ] detect machine type
[2017-05-31 16:47:22,269][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:47:22,270][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:47:22,335][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 16:47:22,335][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 16:47:22,335][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 16:47:22,337][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpUI2eIB
[2017-05-31 16:47:22,352][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:22,367][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:47:22,367][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:22,384][gdb3][DEBUG ] detect machine type
[2017-05-31 16:47:22,386][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:47:22,387][gdb3][DEBUG ] fetch remote file
[2017-05-31 16:47:22,388][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:47:22,454][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 16:47:22,620][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 16:47:22,786][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 16:47:22,952][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 16:47:23,120][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 16:47:23,286][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 16:47:23,452][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 16:47:23,618][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 16:47:23,785][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 16:47:23,951][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 16:47:24,116][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 16:47:24,116][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 16:47:24,116][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170531164724'
[2017-05-31 16:47:24,117][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 16:47:24,117][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 16:47:24,117][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 16:47:24,117][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpUI2eIB
[2017-05-31 16:47:24,301][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:47:24,301][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 16:47:24,301][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcf888e5518>
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fcf891fc938>
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:24,302][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 16:47:24,329][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:24,343][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:47:24,344][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:24,360][gdb3][DEBUG ] detect machine type
[2017-05-31 16:47:24,363][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:47:32,107][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create gdb0:/mnt/memstore
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2b31291908>
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2b314e7aa0>
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 16:47:32,108][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 16:47:32,350][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:32,578][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:47:32,578][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:32,594][gdb0][DEBUG ] detect machine type
[2017-05-31 16:47:32,598][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:47:32,598][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:47:32,599][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 16:47:32,599][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:47:32,601][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:47:32,601][gdb0][DEBUG ] create a keyring file
[2017-05-31 16:47:32,603][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate True
[2017-05-31 16:47:32,603][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:47:32,605][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 16:47:32,725][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:47:32,728][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:47:32,744][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:47:32,760][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:47:32,767][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 16:47:32,783][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 16:47:32,791][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.9239.tmp
[2017-05-31 16:47:32,792][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.9239.tmp
[2017-05-31 16:47:32,795][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.9239.tmp
[2017-05-31 16:47:32,813][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:47:37,885][gdb0][INFO  ] checking OSD status...
[2017-05-31 16:47:37,885][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:47:37,887][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 16:47:37,952][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 16:47:53,911][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create gdb1:/mnt/memstore
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f15482da908>
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1548530aa0>
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 16:47:53,913][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 16:47:54,155][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:54,383][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:47:54,383][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:54,400][gdb1][DEBUG ] detect machine type
[2017-05-31 16:47:54,404][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:47:54,405][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:47:54,405][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 16:47:54,405][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:47:54,407][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:47:54,407][gdb1][DEBUG ] create a keyring file
[2017-05-31 16:47:54,409][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate True
[2017-05-31 16:47:54,409][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:47:54,411][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 16:47:54,582][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 16:47:54,582][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2029, in main
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2025, in factory
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 16:47:54,583][gdb1][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-05-31 16:47:54,583][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 16:47:54,583][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 16:47:54,583][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:48:36,389][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create gdb1:/mnt/memstore
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa51bbda908>
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:48:36,391][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:48:36,391][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa51be30aa0>
[2017-05-31 16:48:36,391][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:48:36,391][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:48:36,391][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 16:48:36,391][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 16:48:36,632][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:48:36,862][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:48:36,863][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:48:36,879][gdb1][DEBUG ] detect machine type
[2017-05-31 16:48:36,883][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:48:36,884][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:48:36,884][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 16:48:36,885][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:48:36,887][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate True
[2017-05-31 16:48:36,887][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:48:36,889][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 16:48:37,009][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:48:37,025][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:48:37,041][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:48:37,056][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:48:37,064][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 16:48:37,080][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 16:48:37,080][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.12243.tmp
[2017-05-31 16:48:37,081][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.12243.tmp
[2017-05-31 16:48:37,084][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.12243.tmp
[2017-05-31 16:48:37,102][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:48:42,174][gdb1][INFO  ] checking OSD status...
[2017-05-31 16:48:42,175][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:48:42,177][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 16:48:42,292][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 17:07:50,298][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd029a82fc8>
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 17:07:50,299][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fd02a3951b8>
[2017-05-31 17:07:50,299][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:07:50,299][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:07:50,299][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 17:07:50,299][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 17:07:50,299][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 17:07:50,299][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 17:07:50,325][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:07:50,339][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:07:50,339][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:07:50,356][gdb3][DEBUG ] detect machine type
[2017-05-31 17:07:50,358][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:07:50,358][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 17:07:50,359][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 17:07:50,397][gdb3][DEBUG ] Reading package lists...
[2017-05-31 17:07:50,562][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 17:07:50,562][gdb3][DEBUG ] Reading state information...
[2017-05-31 17:07:50,626][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 17:07:50,626][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 17:07:50,626][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 17:07:50,626][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 17:07:50,628][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 17:07:50,628][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 17:07:50,660][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 17:07:50,660][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 17:07:50,774][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 17:07:50,774][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 17:07:50,838][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 17:07:50,838][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 17:07:51,003][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 17:07:51,066][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 17:07:51,131][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 17:07:51,346][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 17:07:51,410][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 17:07:51,574][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 17:07:51,639][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 17:07:51,671][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 17:07:51,836][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 17:07:51,900][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 17:07:51,916][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 17:07:52,080][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 17:07:52,112][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 17:07:52,276][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 17:07:52,308][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 17:07:52,316][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 17:07:52,430][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 17:07:53,246][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 17:07:53,410][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:07:53,410][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 17:07:53,410][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:07:53,410][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:07:53,410][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:07:53,410][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f32ed785710>
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f32ee092230>
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:07:53,411][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 17:07:53,437][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:07:53,451][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:07:53,451][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:07:53,468][gdb3][DEBUG ] detect machine type
[2017-05-31 17:07:53,470][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:07:53,485][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:07:53,500][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:07:53,500][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:07:53,517][gdb3][DEBUG ] detect machine type
[2017-05-31 17:07:53,519][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:07:53,519][gdb3][INFO  ] purging data on gdb3
[2017-05-31 17:07:53,520][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 17:07:53,533][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 17:07:53,702][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc6007329e0>
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fc600ff5848>
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:11,875][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe3424ff560>
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fe342b83758>
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 17:08:11,876][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 17:08:11,876][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 17:08:11,903][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:11,917][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:08:11,917][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:11,934][gdb3][DEBUG ] detect machine type
[2017-05-31 17:08:11,936][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:08:11,937][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 17:08:11,949][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 17:08:11,955][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 17:08:11,955][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 17:08:11,955][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 17:08:11,955][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 17:08:11,955][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 17:08:11,955][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 17:08:11,956][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 17:08:11,956][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 17:08:12,122][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:12,122][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 17:08:12,122][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:12,122][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f350509de60>
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f3505072b18>
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:12,124][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 17:08:12,124][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 17:08:12,150][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:12,164][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:08:12,165][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:12,181][gdb3][DEBUG ] detect machine type
[2017-05-31 17:08:12,183][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:08:12,184][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 17:08:12,184][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 17:08:12,184][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:08:12,184][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 17:08:12,184][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:08:12,185][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 17:08:12,185][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:08:12,186][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 17:08:12,187][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 17:08:12,187][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 17:08:12,187][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 17:08:12,188][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 17:08:12,189][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 17:08:12,227][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 17:08:12,227][gdb3][DEBUG ] ceph-mon: set fsid to ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:08:12,230][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 17:08:12,234][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 17:08:12,234][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 17:08:12,234][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 17:08:12,235][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 17:08:12,305][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 17:08:12,373][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 17:08:14,443][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:08:14,508][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 17:08:14,508][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 17:08:14,508][gdb3][DEBUG ] {
[2017-05-31 17:08:14,508][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 17:08:14,508][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 17:08:14,508][gdb3][DEBUG ]   "features": {
[2017-05-31 17:08:14,508][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "kraken", 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "luminous"
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     ], 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "kraken", 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "luminous"
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     ]
[2017-05-31 17:08:14,509][gdb3][DEBUG ]   }, 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]   "monmap": {
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "created": "2017-05-31 17:08:12.212153", 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "features": {
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "persistent": [
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "kraken", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "luminous"
[2017-05-31 17:08:14,510][gdb3][DEBUG ]       ]
[2017-05-31 17:08:14,510][gdb3][DEBUG ]     }, 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]     "fsid": "ca3e0a81-2fd3-4069-bf21-8407a86a4dd3", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]     "modified": "2017-05-31 17:08:12.455156", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]     "mons": [
[2017-05-31 17:08:14,510][gdb3][DEBUG ]       {
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "rank": 0
[2017-05-31 17:08:14,510][gdb3][DEBUG ]       }
[2017-05-31 17:08:14,510][gdb3][DEBUG ]     ]
[2017-05-31 17:08:14,510][gdb3][DEBUG ]   }, 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 17:08:14,511][gdb3][DEBUG ]   "quorum": [
[2017-05-31 17:08:14,511][gdb3][DEBUG ]     0
[2017-05-31 17:08:14,511][gdb3][DEBUG ]   ], 
[2017-05-31 17:08:14,511][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 17:08:14,511][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 17:08:14,511][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 17:08:14,511][gdb3][DEBUG ] }
[2017-05-31 17:08:14,511][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 17:08:14,511][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 17:08:14,512][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:08:14,577][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 17:08:14,592][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:14,607][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:08:14,607][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:14,624][gdb3][DEBUG ] detect machine type
[2017-05-31 17:08:14,627][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:08:14,628][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:08:14,693][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 17:08:14,693][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 17:08:14,693][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 17:08:14,695][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpU1Oj8a
[2017-05-31 17:08:14,710][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:14,724][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:08:14,724][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:14,740][gdb3][DEBUG ] detect machine type
[2017-05-31 17:08:14,743][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:08:14,743][gdb3][DEBUG ] fetch remote file
[2017-05-31 17:08:14,744][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:08:14,810][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 17:08:14,976][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 17:08:15,142][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 17:08:15,308][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 17:08:15,475][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 17:08:15,641][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 17:08:15,807][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 17:08:15,973][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 17:08:16,139][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 17:08:16,306][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 17:08:16,471][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 17:08:16,471][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 17:08:16,471][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170531170816'
[2017-05-31 17:08:16,472][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 17:08:16,472][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 17:08:16,472][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 17:08:16,472][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpU1Oj8a
[2017-05-31 17:08:16,654][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:16,654][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 17:08:16,654][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb6f184f518>
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fb6f2166938>
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:16,655][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 17:08:16,681][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:16,696][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:08:16,697][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:16,713][gdb3][DEBUG ] detect machine type
[2017-05-31 17:08:16,715][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:08:30,514][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0ee519a908>
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0ee53f0aa0>
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:08:30,515][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:08:30,758][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:30,982][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:08:30,982][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:30,998][gdb0][DEBUG ] detect machine type
[2017-05-31 17:08:31,002][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:31,003][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:08:31,003][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:08:31,003][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:08:31,005][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 17:08:31,005][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:08:41,595][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd prepare gdb0:/mnt/memstore
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd061813908>
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd061a69aa0>
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:08:41,596][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:08:41,830][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:42,054][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:08:42,054][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:42,070][gdb0][DEBUG ] detect machine type
[2017-05-31 17:08:42,074][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:42,074][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:08:42,075][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:08:42,075][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:08:42,077][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:08:42,077][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:42,079][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:08:42,199][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:08:42,203][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:08:42,219][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:08:42,234][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:08:42,242][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:08:42,258][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:08:47,266][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:08:47,267][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:47,269][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:08:47,334][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:08:58,043][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd activate gdb0:/mnt/memstore
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f59abf02908>
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f59ac158aa0>
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:08:58,045][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:08:58,282][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:58,469][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:08:58,470][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:58,486][gdb0][DEBUG ] detect machine type
[2017-05-31 17:08:58,489][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:58,490][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:08:58,490][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:08:58,490][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:08:58,491][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:58,492][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:08:58,612][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:08:58,613][gdb0][WARNING] activate: Cluster uuid is da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:08:58,613][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:08:58,614][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:08:58,614][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:08:58,614][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:08:58,615][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:08:58,615][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:08:58,615][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:08:58,615][gdb0][WARNING]     args.func(args)
[2017-05-31 17:08:58,616][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:08:58,616][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:08:58,616][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:08:58,616][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:08:58,616][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 17:08:58,617][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 17:08:58,617][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:08:58,624][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:08:58,625][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:21:03,164][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd activate gdb0:/mnt/memstore
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fae890dc908>
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fae89332aa0>
[2017-05-31 17:21:03,166][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:21:03,166][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:21:03,166][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:21:03,166][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:21:03,406][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:21:03,637][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:21:03,638][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:21:03,654][gdb0][DEBUG ] detect machine type
[2017-05-31 17:21:03,657][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:03,658][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:21:03,658][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:21:03,658][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:21:03,658][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:03,660][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:21:03,780][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:21:03,781][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:21:03,781][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:21:03,782][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:21:03,782][gdb0][WARNING] activate: OSD uuid is 6a42016a-cb50-44b5-a78b-c0660b317c1f
[2017-05-31 17:21:03,782][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:21:03,782][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 6a42016a-cb50-44b5-a78b-c0660b317c1f
[2017-05-31 17:21:03,846][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:21:03,846][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:21:03,846][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:21:03,847][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:21:03,847][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:21:03,847][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:21:03,847][gdb0][WARNING]     args.func(args)
[2017-05-31 17:21:03,847][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:21:03,847][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:21:03,847][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:21:03,847][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:21:03,847][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:21:03,848][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:21:03,849][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:21:03,849][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:21:03,849][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:21:03.832088 7f463d537700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:21:03,849][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:21:03,849][gdb0][WARNING] 
[2017-05-31 17:21:03,856][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:21:03,857][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:21:24,941][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd prepare gdb0:/mnt/memstore
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe5b1696908>
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe5b18ecaa0>
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:21:24,943][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:21:25,186][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:21:25,410][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:21:25,410][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:21:25,426][gdb0][DEBUG ] detect machine type
[2017-05-31 17:21:25,430][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:25,430][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:21:25,430][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:21:25,431][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:21:25,433][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:21:25,433][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:25,435][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:21:25,555][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:21:25,559][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:21:25,575][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:21:25,590][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:21:25,598][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:21:25,614][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:21:30,626][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:21:30,627][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:30,629][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:21:30,694][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:21:37,853][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd activate gdb0:/mnt/memstore
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faaaf868908>
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7faaafabeaa0>
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:21:37,854][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:21:38,098][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:21:38,325][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:21:38,326][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:21:38,342][gdb0][DEBUG ] detect machine type
[2017-05-31 17:21:38,345][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:38,346][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:21:38,346][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:21:38,346][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:21:38,346][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:38,348][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:21:38,469][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:21:38,469][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:21:38,469][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:21:38,472][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:21:38,472][gdb0][WARNING] activate: OSD uuid is 6a42016a-cb50-44b5-a78b-c0660b317c1f
[2017-05-31 17:21:38,473][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:21:38,473][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 6a42016a-cb50-44b5-a78b-c0660b317c1f
[2017-05-31 17:21:38,537][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:21:38,537][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:21:38,537][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:21:38,537][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:21:38,537][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:21:38,537][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:21:38,537][gdb0][WARNING]     args.func(args)
[2017-05-31 17:21:38,537][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:21:38,537][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:21:38,537][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:21:38,537][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:21:38,538][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:21:38,538][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:21:38,538][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:21:38,538][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:21:38,538][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:21:38.521237 7fad53326700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:21:38,538][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:21:38,538][gdb0][WARNING] 
[2017-05-31 17:21:38,546][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:21:38,546][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:22:33,708][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:22:33,708][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd prepare gdb0:/mnt/memstore
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc9a79d4908>
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc9a7c2aaa0>
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:22:33,711][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:22:33,711][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:22:33,955][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:22:34,182][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:22:34,183][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:22:34,199][gdb0][DEBUG ] detect machine type
[2017-05-31 17:22:34,203][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:22:34,204][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:22:34,204][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:22:34,204][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:22:34,207][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:22:34,207][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:22:34,209][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:22:34,329][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:22:34,329][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:22:34,329][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:22:34,329][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:22:34,329][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:22:34,329][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:22:34,330][gdb0][WARNING]     args.func(args)
[2017-05-31 17:22:34,330][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:22:34,330][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:22:34,330][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:22:34,330][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:22:34,330][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:22:34,330][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:22:34,330][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:22:34,330][gdb0][WARNING]     self.set_type()
[2017-05-31 17:22:34,330][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:22:34,330][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:22:34,330][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-05-31 17:22:34,334][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:22:34,334][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:22:34,334][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:23:25,827][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd prepare gdb0:/home/ubuntu/memstore/
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/home/ubuntu/memstore/', None)]
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7ac59bd908>
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7ac5c13aa0>
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:23:25,829][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/home/ubuntu/memstore/:
[2017-05-31 17:23:26,067][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:23:26,291][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:23:26,291][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:23:26,307][gdb0][DEBUG ] detect machine type
[2017-05-31 17:23:26,311][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:23:26,312][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:23:26,312][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:23:26,312][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:23:26,315][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /home/ubuntu/memstore/ journal None activate False
[2017-05-31 17:23:26,315][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:23:26,317][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /home/ubuntu/memstore/
[2017-05-31 17:23:26,437][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:23:26,445][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:23:26,460][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:23:26,476][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:23:26,484][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:23:26,500][gdb0][WARNING] populate_data_path: Preparing osd data dir /home/ubuntu/memstore/
[2017-05-31 17:23:26,507][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /home/ubuntu/memstore/ceph_fsid.10517.tmp
[2017-05-31 17:23:26,508][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /home/ubuntu/memstore/fsid.10517.tmp
[2017-05-31 17:23:26,512][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /home/ubuntu/memstore/magic.10517.tmp
[2017-05-31 17:23:31,533][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:23:31,533][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:23:31,535][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:23:31,600][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:23:41,212][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:23:41,212][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd activate gdb0:/home/ubuntu/memstore/
[2017-05-31 17:23:41,212][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9bd0d86908>
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9bd0fdcaa0>
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/home/ubuntu/memstore/', None)]
[2017-05-31 17:23:41,213][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/home/ubuntu/memstore/:
[2017-05-31 17:23:41,459][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:23:41,686][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:23:41,687][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:23:41,703][gdb0][DEBUG ] detect machine type
[2017-05-31 17:23:41,707][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:23:41,707][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:23:41,707][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /home/ubuntu/memstore/
[2017-05-31 17:23:41,707][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:23:41,708][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:23:41,710][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /home/ubuntu/memstore/
[2017-05-31 17:23:41,830][gdb0][WARNING] main_activate: path = /home/ubuntu/memstore/
[2017-05-31 17:23:41,830][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:23:41,830][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:23:41,838][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:23:41,838][gdb0][WARNING] activate: OSD uuid is c85613d8-2b0e-4250-af41-052f3f698198
[2017-05-31 17:23:41,838][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:23:41,838][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise c85613d8-2b0e-4250-af41-052f3f698198
[2017-05-31 17:23:41,952][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:23:41,953][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:23:41,953][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:23:41,953][gdb0][WARNING]     args.func(args)
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:23:41,953][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:23:41,953][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:23:41,953][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:23:41,954][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:23:41,954][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:23:41,954][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:23:41.891883 7f56b0bc8700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:23:41,954][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:23:41,954][gdb0][WARNING] 
[2017-05-31 17:23:41,954][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:23:41,954][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /home/ubuntu/memstore/

[2017-05-31 17:24:50,365][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd46bc40518>
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fd46c557938>
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:24:50,367][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:24:50,367][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 17:24:50,611][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:24:50,835][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:24:50,835][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:24:50,851][gdb0][DEBUG ] detect machine type
[2017-05-31 17:24:50,855][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:24:55,871][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:24:55,871][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:24:55,871][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:24:55,871][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f68cdf29518>
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f68ce840938>
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:24:55,872][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:24:56,112][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:24:56,339][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:24:56,339][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:24:56,355][gdb1][DEBUG ] detect machine type
[2017-05-31 17:24:56,359][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:24:56,361][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 17:24:56,361][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 17:25:01,810][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:25:01,810][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd activate gdb0:/home/ubuntu/memstore/
[2017-05-31 17:25:01,810][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe68c37c908>
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe68c5d2aa0>
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/home/ubuntu/memstore/', None)]
[2017-05-31 17:25:01,811][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/home/ubuntu/memstore/:
[2017-05-31 17:25:02,051][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:25:02,283][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:25:02,283][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:25:02,300][gdb0][DEBUG ] detect machine type
[2017-05-31 17:25:02,304][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:25:02,305][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:25:02,305][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /home/ubuntu/memstore/
[2017-05-31 17:25:02,305][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:25:02,305][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:25:02,307][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /home/ubuntu/memstore/
[2017-05-31 17:25:02,428][gdb0][WARNING] main_activate: path = /home/ubuntu/memstore/
[2017-05-31 17:25:02,428][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:25:02,428][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:25:02,436][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:25:02,436][gdb0][WARNING] activate: OSD uuid is c85613d8-2b0e-4250-af41-052f3f698198
[2017-05-31 17:25:02,436][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:25:02,436][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise c85613d8-2b0e-4250-af41-052f3f698198
[2017-05-31 17:25:02,550][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:25:02,550][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:25:02,550][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:25:02,550][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:25:02,551][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:25:02,551][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:25:02,551][gdb0][WARNING]     args.func(args)
[2017-05-31 17:25:02,551][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:25:02,551][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:25:02,551][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:25:02,551][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:25:02,551][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:25:02,551][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:25:02,551][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:25:02,551][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:25:02,551][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:25:02.489541 7f7c2ad08700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:25:02,551][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:25:02,552][gdb0][WARNING] 
[2017-05-31 17:25:02,552][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:25:02,552][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /home/ubuntu/memstore/

[2017-05-31 17:26:09,147][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:09,428][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:26:09,428][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:09,428][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:09,428][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:09,428][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fedc15c8518>
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fedc1edf938>
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:09,429][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:26:09,675][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:09,867][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:26:09,867][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:09,883][gdb1][DEBUG ] detect machine type
[2017-05-31 17:26:09,887][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:26:09,889][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 17:26:09,889][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 17:26:13,872][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:13,872][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 17:26:13,872][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbea7d08518>
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fbea861f938>
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:13,873][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 17:26:14,115][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:14,366][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:26:14,367][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:14,383][gdb0][DEBUG ] detect machine type
[2017-05-31 17:26:14,387][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:26:17,810][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faa512c5518>
[2017-05-31 17:26:17,811][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:17,811][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 17:26:17,811][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7faa51bdc938>
[2017-05-31 17:26:17,811][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:17,811][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:17,811][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 17:26:18,051][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:18,282][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:26:18,283][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:18,299][gdb0][DEBUG ] detect machine type
[2017-05-31 17:26:18,302][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:26:31,577][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f82035a7518>
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:26:31,578][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f8203ebe938>
[2017-05-31 17:26:31,578][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:31,578][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:31,578][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:26:31,819][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:32,047][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:26:32,047][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:32,063][gdb1][DEBUG ] detect machine type
[2017-05-31 17:26:32,067][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:26:32,069][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 17:26:32,069][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 17:26:37,804][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f262edd4518>
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:37,805][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:26:37,805][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f262f6eb938>
[2017-05-31 17:26:37,805][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:37,805][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:37,805][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:26:38,048][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:38,275][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:26:38,275][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:38,291][gdb1][DEBUG ] detect machine type
[2017-05-31 17:26:38,295][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:26:41,218][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbd2a5e0518>
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fbd2aef7938>
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:41,220][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:41,220][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:26:41,463][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:41,691][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:26:41,691][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:41,708][gdb1][DEBUG ] detect machine type
[2017-05-31 17:26:41,712][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:28:07,417][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:28:07,417][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd create --zap-disk gdb0:/mnt/memstore
[2017-05-31 17:28:07,417][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:28:07,417][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:28:07,417][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff7512f6908>
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff75154caa0>
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:28:07,419][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:28:07,419][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 17:28:07,419][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:28:07,660][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:28:07,890][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:28:07,891][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:28:07,907][gdb0][DEBUG ] detect machine type
[2017-05-31 17:28:07,910][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:28:07,911][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:28:07,911][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:28:07,911][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:28:07,913][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate True
[2017-05-31 17:28:07,914][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:28:07,916][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:28:08,036][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:28:08,036][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:28:08,036][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:28:08,037][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:28:08,037][gdb0][WARNING]     args.func(args)
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:28:08,037][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:28:08,037][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:28:08,037][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:28:08,037][gdb0][WARNING]     self.set_type()
[2017-05-31 17:28:08,038][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:28:08,038][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:28:08,038][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-05-31 17:28:08,039][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:28:08,039][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:28:08,039][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:28:36,224][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd create gdb0:/mnt/memstore
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f304ebe3908>
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f304ee39aa0>
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:28:36,226][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:28:36,467][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:28:36,699][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:28:36,699][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:28:36,716][gdb0][DEBUG ] detect machine type
[2017-05-31 17:28:36,719][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:28:36,720][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:28:36,720][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:28:36,720][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:28:36,722][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate True
[2017-05-31 17:28:36,723][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:28:36,724][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:28:36,845][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:28:36,845][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:28:36,845][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:28:36,845][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:28:36,845][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:28:36,845][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:28:36,845][gdb0][WARNING]     args.func(args)
[2017-05-31 17:28:36,845][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:28:36,845][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:28:36,845][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:28:36,845][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:28:36,846][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:28:36,846][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:28:36,846][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:28:36,846][gdb0][WARNING]     self.set_type()
[2017-05-31 17:28:36,846][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:28:36,846][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:28:36,846][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-05-31 17:28:36,850][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:28:36,850][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:28:36,850][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:29:39,243][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd create gdb0:/mnt/memstore1
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore1', None)]
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8e4a597908>
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8e4a7edaa0>
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:29:39,245][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore1:
[2017-05-31 17:29:39,484][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:29:39,714][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:29:39,715][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:29:39,731][gdb0][DEBUG ] detect machine type
[2017-05-31 17:29:39,734][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:29:39,735][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:29:39,735][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:29:39,736][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:29:39,738][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore1 journal None activate True
[2017-05-31 17:29:39,738][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:29:39,740][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore1
[2017-05-31 17:29:39,860][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:29:39,860][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:29:39,860][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:29:39,861][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:29:39,861][gdb0][WARNING]     args.func(args)
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:29:39,861][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:29:39,861][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:29:39,861][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:29:39,861][gdb0][WARNING]     self.set_type()
[2017-05-31 17:29:39,862][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:29:39,862][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:29:39,862][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore1'
[2017-05-31 17:29:39,863][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:29:39,863][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore1
[2017-05-31 17:29:39,863][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:31:18,609][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore0
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore0', None)]
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f24ec147908>
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f24ec39daa0>
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:31:18,611][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore0:
[2017-05-31 17:31:18,850][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:31:19,077][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:31:19,078][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:31:19,093][gdb0][DEBUG ] detect machine type
[2017-05-31 17:31:19,096][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:19,097][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:31:19,097][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:31:19,097][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:31:19,100][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore0 journal None activate False
[2017-05-31 17:31:19,100][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:19,101][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore0
[2017-05-31 17:31:19,222][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:31:19,222][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:31:19,222][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:31:19,222][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:31:19,222][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:31:19,222][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:31:19,222][gdb0][WARNING]     args.func(args)
[2017-05-31 17:31:19,223][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:31:19,223][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:31:19,223][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:31:19,223][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:31:19,223][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:31:19,223][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:31:19,223][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:31:19,223][gdb0][WARNING]     self.set_type()
[2017-05-31 17:31:19,223][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:31:19,223][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:31:19,223][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore0'
[2017-05-31 17:31:19,224][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:31:19,224][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore0
[2017-05-31 17:31:19,224][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:31:27,340][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0a00ca5908>
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0a00efbaa0>
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:31:27,342][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:31:27,582][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:31:27,810][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:31:27,810][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:31:27,826][gdb0][DEBUG ] detect machine type
[2017-05-31 17:31:27,829][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:27,830][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:31:27,830][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:31:27,831][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:31:27,833][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:31:27,833][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:27,835][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:31:27,955][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:31:27,955][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:31:27,955][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:31:27,955][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:31:27,956][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:31:27,956][gdb0][WARNING]     args.func(args)
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:31:27,956][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:31:27,956][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:31:27,956][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:31:27,956][gdb0][WARNING]     self.set_type()
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:31:27,956][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:31:27,957][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-05-31 17:31:27,957][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:31:27,957][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:31:27,957][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:31:45,369][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:31:45,369][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:31:45,369][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:31:45,369][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f511e360908>
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f511e5b6aa0>
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:31:45,371][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:31:45,371][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:31:45,371][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:31:45,606][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:31:45,834][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:31:45,834][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:31:45,850][gdb0][DEBUG ] detect machine type
[2017-05-31 17:31:45,853][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:45,854][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:31:45,854][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:31:45,854][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:31:45,857][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:31:45,857][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:45,859][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:31:45,979][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:31:45,983][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:31:45,999][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:31:46,014][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:31:46,022][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:31:46,038][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 17:31:46,053][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.11500.tmp
[2017-05-31 17:31:46,054][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.11500.tmp
[2017-05-31 17:31:46,054][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.11500.tmp
[2017-05-31 17:31:51,071][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:31:51,071][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:51,073][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:31:51,239][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:31:59,940][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:31:59,940][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:31:59,940][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fabd52b2908>
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fabd5508aa0>
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:31:59,942][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:32:00,183][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:32:00,410][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:32:00,410][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:32:00,426][gdb0][DEBUG ] detect machine type
[2017-05-31 17:32:00,429][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:32:00,430][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:32:00,430][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:32:00,430][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:32:00,430][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:32:00,432][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:32:00,552][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:32:00,553][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:32:00,553][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:32:00,556][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:32:00,556][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:32:00,556][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:32:00,556][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:32:00,671][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:32:00,671][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:32:00,671][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:32:00,671][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:32:00,671][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:32:00,671][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:32:00,671][gdb0][WARNING]     args.func(args)
[2017-05-31 17:32:00,671][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:32:00,671][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:32:00,671][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:32:00,672][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:32:00,672][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:32:00,672][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:32:00,672][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:32:00,672][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:32:00,672][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:32:00.611189 7fd518948700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:32:00,672][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:32:00,672][gdb0][WARNING] 
[2017-05-31 17:32:00,672][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:32:00,672][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:32:37,303][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:32:37,303][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:32:37,303][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:32:37,303][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:32:37,303][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:32:37,303][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb869074908>
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb8692caaa0>
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:32:37,304][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:32:37,546][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:32:37,773][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:32:37,774][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:32:37,790][gdb0][DEBUG ] detect machine type
[2017-05-31 17:32:37,793][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:32:37,794][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:32:37,794][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:32:37,794][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:32:37,794][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:32:37,796][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:32:37,916][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:32:37,917][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:32:37,917][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:32:37,920][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:32:37,920][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:32:37,920][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:32:37,920][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:32:37,984][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:32:37,984][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:32:37,985][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:32:37,985][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:32:37,985][gdb0][WARNING]     args.func(args)
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:32:37,985][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:32:37,985][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:32:37,985][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:32:37,986][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:32:37,986][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:32:37.970899 7f301d34a700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:32:37,986][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:32:37,986][gdb0][WARNING] 
[2017-05-31 17:32:37,993][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:32:37,994][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:33:01,166][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f80bcdce908>
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f80bd024aa0>
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:33:01,168][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:33:01,407][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:33:01,630][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:33:01,631][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:33:01,647][gdb0][DEBUG ] detect machine type
[2017-05-31 17:33:01,651][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:33:01,652][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:33:01,652][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:33:01,652][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:33:01,652][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:33:01,654][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:33:01,775][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:33:01,775][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:33:01,775][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:33:01,778][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:33:01,779][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:33:01,779][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:33:01,779][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:33:01,843][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:33:01,843][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:33:01,843][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:33:01,843][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:33:01,843][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:33:01,844][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:33:01,844][gdb0][WARNING]     args.func(args)
[2017-05-31 17:33:01,844][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:33:01,844][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:33:01,844][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:33:01,844][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:33:01,845][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:33:01,846][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:33:01,846][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:33:01,846][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:33:01,846][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:33:01.830834 7f8a312d6700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:33:01,846][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:33:01,846][gdb0][WARNING] 
[2017-05-31 17:33:01,862][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:33:01,862][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:35:20,726][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f31fc0bf908>
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f31fc315aa0>
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:35:20,727][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:35:20,966][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:35:21,190][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:35:21,190][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:35:21,206][gdb0][DEBUG ] detect machine type
[2017-05-31 17:35:21,209][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:21,210][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:35:21,210][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:35:21,210][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:35:21,210][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:21,213][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:35:21,333][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:35:21,333][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:35:21,333][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:35:21,335][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:35:21,335][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:21,335][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:35:21,335][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:21,449][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:35:21,450][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:35:21,450][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:35:21,450][gdb0][WARNING]     args.func(args)
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:35:21,450][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:35:21,450][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:35:21,450][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:35:21,451][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:35:21,451][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:35:21,451][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:35:21.391200 7fcb3940a700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:35:21,451][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:35:21,451][gdb0][WARNING] 
[2017-05-31 17:35:21,451][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:35:21,451][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:35:43,815][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6ae6d2f908>
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6ae6f85aa0>
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:35:43,817][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:35:43,817][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:35:43,817][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:35:44,054][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:35:44,281][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:35:44,282][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:35:44,298][gdb0][DEBUG ] detect machine type
[2017-05-31 17:35:44,301][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:44,302][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:35:44,302][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:35:44,302][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:35:44,302][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:44,304][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:35:44,424][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:35:44,424][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:35:44,425][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:35:44,426][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:35:44,426][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:44,426][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:35:44,426][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:44,490][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:35:44,490][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:35:44,491][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:35:44,491][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:35:44,491][gdb0][WARNING]     args.func(args)
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:35:44,491][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:35:44,491][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:35:44,491][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:35:44,495][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:35:44,495][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:35:44.478096 7f0159824700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:35:44,495][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:35:44,495][gdb0][WARNING] 
[2017-05-31 17:35:44,503][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:35:44,503][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:35:47,483][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f21d1e41908>
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f21d2097aa0>
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:35:47,485][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:35:47,722][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:35:47,949][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:35:47,950][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:35:47,965][gdb0][DEBUG ] detect machine type
[2017-05-31 17:35:47,969][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:47,970][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:35:47,970][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:35:47,970][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:35:47,972][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:35:47,972][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:47,974][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:35:48,095][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:35:48,098][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:35:48,114][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:35:48,130][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:35:48,137][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:35:48,145][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:35:53,166][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:35:53,166][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:53,168][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:35:53,333][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:35:57,208][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb58bf81908>
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:35:57,209][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb58c1d7aa0>
[2017-05-31 17:35:57,209][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:35:57,209][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:35:57,209][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:35:57,209][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:35:57,450][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:35:57,677][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:35:57,678][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:35:57,693][gdb0][DEBUG ] detect machine type
[2017-05-31 17:35:57,697][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:57,698][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:35:57,698][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:35:57,698][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:35:57,698][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:57,700][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:35:57,820][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:35:57,821][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:35:57,821][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:35:57,821][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:35:57,821][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:57,822][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:35:57,822][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:57,937][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:35:57,937][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:35:57,937][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:35:57,937][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:35:57,937][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:35:57,937][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:35:57,937][gdb0][WARNING]     args.func(args)
[2017-05-31 17:35:57,937][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:35:57,937][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:35:57,937][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:35:57,938][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:35:57,938][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:35:57,938][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:35:57,938][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:35:57,938][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:35:57,938][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:35:57.877212 7f9978f6b700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:35:57,938][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:35:57,938][gdb0][WARNING] 
[2017-05-31 17:35:57,938][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:35:57,938][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:37:04,926][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f67b3b90908>
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f67b3de6aa0>
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:37:04,928][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:37:05,166][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:05,393][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:37:05,394][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:05,409][gdb0][DEBUG ] detect machine type
[2017-05-31 17:37:05,412][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:05,413][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:37:05,413][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:37:05,413][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:37:05,416][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:37:05,416][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:05,418][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:37:05,538][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:37:05,541][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:05,557][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:05,573][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:05,580][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:37:05,596][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:37:10,605][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:37:10,605][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:10,607][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:37:10,773][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:37:14,106][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fde7b9cf908>
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fde7bc25aa0>
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:37:14,108][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:37:14,346][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:14,577][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:37:14,578][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:14,594][gdb0][DEBUG ] detect machine type
[2017-05-31 17:37:14,597][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:14,598][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:37:14,598][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:37:14,598][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:37:14,598][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:14,600][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:37:14,721][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:37:14,721][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:37:14,721][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:37:14,722][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:37:14,722][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:37:14,722][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:37:14,722][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:37:14,786][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:37:14,786][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:37:14,787][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:37:14,787][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:37:14,787][gdb0][WARNING]     args.func(args)
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:37:14,787][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:37:14,787][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:37:14,787][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:37:14,788][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:37:14,788][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:37:14.773746 7fd04a885700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:37:14,788][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:37:14,788][gdb0][WARNING] 
[2017-05-31 17:37:14,795][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:37:14,795][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:37:26,260][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb120b14e60>
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fb120ae9b18>
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 17:37:26,262][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:37:26,262][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 17:37:26,262][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 17:37:26,289][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:26,303][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:37:26,304][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:26,320][gdb3][DEBUG ] detect machine type
[2017-05-31 17:37:26,323][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:37:26,323][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 17:37:26,323][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 17:37:26,323][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:37:26,324][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 17:37:26,324][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:37:26,324][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 17:37:26,325][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:37:26,326][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 17:37:26,326][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 17:37:26,327][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 17:37:26,327][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 17:37:26,328][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 17:37:26,401][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 17:37:26,469][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 17:37:28,482][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:37:28,547][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 17:37:28,547][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 17:37:28,548][gdb3][DEBUG ] {
[2017-05-31 17:37:28,548][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]   "features": {
[2017-05-31 17:37:28,548][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 17:37:28,548][gdb3][DEBUG ]       "kraken", 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]       "luminous"
[2017-05-31 17:37:28,548][gdb3][DEBUG ]     ], 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 17:37:28,549][gdb3][DEBUG ]       "kraken", 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]       "luminous"
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     ]
[2017-05-31 17:37:28,549][gdb3][DEBUG ]   }, 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]   "monmap": {
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     "created": "2017-05-31 17:08:12.212153", 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     "features": {
[2017-05-31 17:37:28,549][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]       "persistent": [
[2017-05-31 17:37:28,549][gdb3][DEBUG ]         "kraken", 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]         "luminous"
[2017-05-31 17:37:28,549][gdb3][DEBUG ]       ]
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     }, 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     "fsid": "ca3e0a81-2fd3-4069-bf21-8407a86a4dd3", 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     "modified": "2017-05-31 17:08:12.455156", 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]     "mons": [
[2017-05-31 17:37:28,550][gdb3][DEBUG ]       {
[2017-05-31 17:37:28,550][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]         "rank": 0
[2017-05-31 17:37:28,550][gdb3][DEBUG ]       }
[2017-05-31 17:37:28,550][gdb3][DEBUG ]     ]
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   }, 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   "quorum": [
[2017-05-31 17:37:28,550][gdb3][DEBUG ]     0
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   ], 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 17:37:28,551][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 17:37:28,551][gdb3][DEBUG ] }
[2017-05-31 17:37:28,551][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 17:37:28,551][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 17:37:28,552][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:37:28,617][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 17:37:28,634][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:28,649][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:37:28,649][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:28,666][gdb3][DEBUG ] detect machine type
[2017-05-31 17:37:28,668][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:37:28,669][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:37:28,734][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 17:37:28,735][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 17:37:28,735][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 17:37:28,737][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpVN2kfG
[2017-05-31 17:37:28,752][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:28,766][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:37:28,767][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:28,783][gdb3][DEBUG ] detect machine type
[2017-05-31 17:37:28,785][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:37:28,786][gdb3][DEBUG ] fetch remote file
[2017-05-31 17:37:28,787][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:37:28,853][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 17:37:29,019][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 17:37:29,185][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 17:37:29,351][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 17:37:29,518][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 17:37:29,683][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-05-31 17:37:29,683][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-05-31 17:37:29,683][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-05-31 17:37:29,684][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 17:37:29,684][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-05-31 17:37:29,684][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-05-31 17:37:29,684][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpVN2kfG
[2017-05-31 17:37:37,308][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:37:37,308][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 17:37:37,308][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:37:37,308][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:37:37,308][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:37:37,308][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb432d88518>
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fb43369f938>
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:37:37,309][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 17:37:37,550][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:37,774][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:37:37,774][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:37,790][gdb0][DEBUG ] detect machine type
[2017-05-31 17:37:37,793][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:37:52,886][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f78bf918908>
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f78bfb6eaa0>
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:37:52,888][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:37:53,127][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:53,354][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:37:53,354][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:53,370][gdb0][DEBUG ] detect machine type
[2017-05-31 17:37:53,374][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:53,375][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:37:53,375][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:37:53,375][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:37:53,377][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:37:53,377][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:53,379][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:37:53,499][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:37:53,507][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:53,523][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:53,538][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:53,546][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:37:53,562][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:37:58,570][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:37:58,571][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:58,573][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:37:58,738][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:38:04,191][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:38:04,191][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd5b2840908>
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd5b2a96aa0>
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:38:04,193][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:38:04,434][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:38:04,666][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:38:04,666][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:38:04,682][gdb0][DEBUG ] detect machine type
[2017-05-31 17:38:04,685][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:04,686][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:38:04,686][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:38:04,686][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:38:04,686][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:04,688][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:38:04,808][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:38:04,808][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:38:04,809][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:38:04,812][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:38:04,812][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:04,812][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:38:04,812][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:04,876][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:38:04,876][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:38:04,876][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:38:04,877][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:38:04,877][gdb0][WARNING]     args.func(args)
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:38:04,877][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:38:04,877][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:38:04,877][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:38:04,877][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:38:04,878][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:38:04.863263 7fe323196700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:38:04,878][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:38:04,878][gdb0][WARNING] 
[2017-05-31 17:38:04,885][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:38:04,885][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:38:38,724][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0cb5e89908>
[2017-05-31 17:38:38,725][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:38:38,725][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0cb60dfaa0>
[2017-05-31 17:38:38,725][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:38:38,725][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:38:38,725][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:38:38,725][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:38:38,962][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:38:39,190][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:38:39,190][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:38:39,206][gdb0][DEBUG ] detect machine type
[2017-05-31 17:38:39,210][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:39,211][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:38:39,211][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:38:39,211][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:38:39,211][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:39,213][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:38:39,333][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:38:39,333][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:38:39,333][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:38:39,337][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:38:39,337][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:39,337][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:38:39,337][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:39,401][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:38:39,401][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:38:39,401][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:38:39,401][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:38:39,401][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:38:39,402][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:38:39,402][gdb0][WARNING]     args.func(args)
[2017-05-31 17:38:39,402][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:38:39,402][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:38:39,402][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:38:39,402][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:38:39,402][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:38:39,402][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:38:39,402][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:38:39,402][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:38:39,402][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:38:39.386582 7f2459254700 -1 auth: unable to find a keyring on /var/lib/ceph/bootstrap-osd/ceph.keyring: (2) No such file or directory
[2017-05-31 17:38:39,402][gdb0][WARNING] 2017-05-31 17:38:39.386595 7f2459254700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication
[2017-05-31 17:38:39,402][gdb0][WARNING] 2017-05-31 17:38:39.386596 7f2459254700  0 librados: client.bootstrap-osd initialization error (2) No such file or directory
[2017-05-31 17:38:39,403][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:38:39,403][gdb0][WARNING] 
[2017-05-31 17:38:39,410][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:38:39,410][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:38:42,692][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:38:42,692][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:38:42,692][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa8cdb3a908>
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa8cdd90aa0>
[2017-05-31 17:38:42,694][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:38:42,694][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:38:42,694][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:38:42,694][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:38:42,934][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:38:43,161][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:38:43,162][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:38:43,178][gdb0][DEBUG ] detect machine type
[2017-05-31 17:38:43,181][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:43,182][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:38:43,182][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:38:43,182][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:38:43,185][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 17:38:43,185][gdb0][DEBUG ] create a keyring file
[2017-05-31 17:38:43,187][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:38:43,187][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:43,189][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:38:43,309][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:38:43,313][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:38:43,328][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:38:43,344][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:38:43,352][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:38:43,359][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:38:48,380][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:38:48,380][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:48,383][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:38:48,548][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:38:58,883][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcc7933f908>
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcc79595aa0>
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:38:58,886][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:38:59,126][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:38:59,358][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:38:59,358][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:38:59,374][gdb0][DEBUG ] detect machine type
[2017-05-31 17:38:59,378][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:59,378][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:38:59,378][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:38:59,379][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:38:59,379][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:59,381][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:38:59,501][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:38:59,501][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:38:59,501][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:38:59,505][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:38:59,505][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:59,505][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:38:59,505][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:59,670][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.13365.tmp
[2017-05-31 17:38:59,670][gdb0][WARNING] activate: OSD id is 0
[2017-05-31 17:38:59,670][gdb0][WARNING] activate: Initializing OSD...
[2017-05-31 17:38:59,670][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 17:38:59,784][gdb0][WARNING] got monmap epoch 2
[2017-05-31 17:38:59,800][gdb0][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid 1b3b5a35-7de7-489c-8108-1493f28b991b --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 17:38:59,832][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 17:38:59,832][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 17:38:59,832][gdb0][WARNING] activate: Authorizing OSD key...
[2017-05-31 17:38:59,832][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 17:38:59,997][gdb0][WARNING] added key for osd.0
[2017-05-31 17:38:59,997][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.13365.tmp
[2017-05-31 17:38:59,997][gdb0][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-05-31 17:38:59,997][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:38:59,997][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:38:59,998][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:38:59,998][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:38:59,998][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:38:59,998][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:38:59,998][gdb0][WARNING]     args.func(args)
[2017-05-31 17:38:59,998][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:38:59,998][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:38:59,998][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3543, in activate_dir
[2017-05-31 17:38:59,999][gdb0][WARNING]     old = os.readlink(canonical)
[2017-05-31 17:38:59,999][gdb0][WARNING] OSError: [Errno 22] Invalid argument: '/var/lib/ceph/osd/ceph-0'
[2017-05-31 17:39:00,006][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:39:00,007][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:41:18,574][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:41:18,574][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc749669908>
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:41:18,576][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc7498bfaa0>
[2017-05-31 17:41:18,576][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:41:18,576][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:41:18,576][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:41:18,576][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:41:18,818][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:41:19,037][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:41:19,038][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:41:19,053][gdb0][DEBUG ] detect machine type
[2017-05-31 17:41:19,057][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:19,058][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:41:19,058][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:41:19,058][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:41:19,060][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:41:19,060][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:19,062][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:41:19,183][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:41:19,186][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:41:19,202][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:41:19,218][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:41:19,225][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:41:19,241][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 17:41:19,241][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.13634.tmp
[2017-05-31 17:41:19,242][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.13634.tmp
[2017-05-31 17:41:19,246][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.13634.tmp
[2017-05-31 17:41:24,266][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:41:24,267][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:24,269][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:41:24,435][gdb0][WARNING] there is 1 OSD down
[2017-05-31 17:41:24,435][gdb0][WARNING] there is 1 OSD out
[2017-05-31 17:41:24,435][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:41:36,573][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5dc56f3908>
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5dc5949aa0>
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:41:36,575][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:41:36,575][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:41:36,575][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:41:36,814][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:41:37,046][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:41:37,046][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:41:37,062][gdb0][DEBUG ] detect machine type
[2017-05-31 17:41:37,065][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:37,066][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:41:37,066][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:41:37,066][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:41:37,067][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:37,068][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:41:37,189][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:41:37,189][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:41:37,189][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:41:37,192][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:41:37,193][gdb0][WARNING] activate: OSD uuid is a824912d-c2ff-4ce5-b541-4675a0141fef
[2017-05-31 17:41:37,193][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:41:37,193][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise a824912d-c2ff-4ce5-b541-4675a0141fef
[2017-05-31 17:41:37,407][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.13755.tmp
[2017-05-31 17:41:37,407][gdb0][WARNING] activate: OSD id is 1
[2017-05-31 17:41:37,407][gdb0][WARNING] activate: Initializing OSD...
[2017-05-31 17:41:37,408][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 17:41:37,572][gdb0][WARNING] got monmap epoch 2
[2017-05-31 17:41:37,572][gdb0][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 1 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid a824912d-c2ff-4ce5-b541-4675a0141fef --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 17:41:37,572][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 17:41:37,572][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 17:41:37,572][gdb0][WARNING] activate: Authorizing OSD key...
[2017-05-31 17:41:37,572][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.1 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 17:41:37,737][gdb0][WARNING] added key for osd.1
[2017-05-31 17:41:37,737][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.13755.tmp
[2017-05-31 17:41:37,737][gdb0][WARNING] activate: ceph osd.1 data dir is ready at /mnt/memstore
[2017-05-31 17:41:37,737][gdb0][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-1 -> /mnt/memstore
[2017-05-31 17:41:37,737][gdb0][WARNING] start_daemon: Starting ceph osd.1...
[2017-05-31 17:41:37,737][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1
[2017-05-31 17:41:37,737][gdb0][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2017-05-31 17:41:37,801][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1 --runtime
[2017-05-31 17:41:37,833][gdb0][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@1
[2017-05-31 17:41:37,833][gdb0][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 17:41:37,897][gdb0][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@1
[2017-05-31 17:41:42,967][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:41:42,967][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:42,969][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:41:43,135][gdb0][WARNING] there is 1 OSD down
[2017-05-31 17:41:43,135][gdb0][WARNING] there is 1 OSD out
[2017-05-31 17:41:43,137][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 17:54:07,021][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:54:07,021][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa99b391908>
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa99b5e7aa0>
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:54:07,023][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:54:07,263][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:54:07,490][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:54:07,490][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:54:07,506][gdb0][DEBUG ] detect machine type
[2017-05-31 17:54:07,510][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:54:07,511][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:54:07,511][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:54:07,511][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:54:07,511][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:54:07,513][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:54:07,634][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:54:07,634][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:54:07,634][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:54:07,638][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:54:07,638][gdb0][WARNING] activate: OSD uuid is a824912d-c2ff-4ce5-b541-4675a0141fef
[2017-05-31 17:54:07,638][gdb0][WARNING] activate: OSD id is 1
[2017-05-31 17:54:07,638][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 17:54:07,638][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 17:54:07,639][gdb0][WARNING] activate: ceph osd.1 data dir is ready at /mnt/memstore
[2017-05-31 17:54:07,639][gdb0][WARNING] start_daemon: Starting ceph osd.1...
[2017-05-31 17:54:07,639][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1
[2017-05-31 17:54:07,655][gdb0][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2017-05-31 17:54:07,719][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1 --runtime
[2017-05-31 17:54:07,751][gdb0][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@1
[2017-05-31 17:54:07,751][gdb0][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 17:54:07,815][gdb0][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@1
[2017-05-31 17:54:12,821][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:54:12,821][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:54:12,823][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:54:12,989][gdb0][WARNING] there is 1 OSD down
[2017-05-31 17:54:12,989][gdb0][WARNING] there is 1 OSD out
[2017-05-31 17:54:12,991][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 17:54:19,274][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0a43521908>
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0a43777aa0>
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:54:19,276][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:54:19,523][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:54:19,755][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:54:19,755][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:54:19,772][gdb1][DEBUG ] detect machine type
[2017-05-31 17:54:19,776][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:54:19,776][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:54:19,777][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 17:54:19,777][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:54:19,779][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 17:54:19,779][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:54:19,782][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:54:19,952][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:54:19,952][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:54:19,952][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:54:19,953][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:54:19,960][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:54:19,968][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:54:24,989][gdb1][INFO  ] checking OSD status...
[2017-05-31 17:54:24,989][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:54:24,991][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:54:25,157][gdb1][WARNING] there is 1 OSD down
[2017-05-31 17:54:25,157][gdb1][WARNING] there is 1 OSD out
[2017-05-31 17:54:25,157][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 17:55:02,565][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:55:02,565][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4a8d45f908>
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f4a8d6b5aa0>
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:55:02,567][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:55:02,807][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:55:03,030][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:55:03,031][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:55:03,047][gdb1][DEBUG ] detect machine type
[2017-05-31 17:55:03,051][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:03,051][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:55:03,051][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:55:03,052][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:55:03,052][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:03,053][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:55:03,174][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:55:03,174][gdb1][WARNING] activate: Cluster uuid is da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:03,174][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:55:03,190][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 17:55:03,190][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3609, in activate
[2017-05-31 17:55:03,190][gdb1][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:03,192][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:55:03,192][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:55:25,577][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7b0446a518>
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:55:25,578][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:55:25,578][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f7b04d81938>
[2017-05-31 17:55:25,578][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:55:25,578][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:55:25,578][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:55:25,819][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:55:26,051][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:55:26,052][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:55:26,068][gdb1][DEBUG ] detect machine type
[2017-05-31 17:55:26,072][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:55:27,598][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2e1b214908>
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2e1b46aaa0>
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:55:27,600][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:55:27,839][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:55:28,067][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:55:28,068][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:55:28,084][gdb1][DEBUG ] detect machine type
[2017-05-31 17:55:28,088][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:28,089][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:55:28,089][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:55:28,089][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:55:28,089][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:28,091][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:55:28,212][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:55:28,212][gdb1][WARNING] activate: Cluster uuid is da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:28,212][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:55:28,228][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 17:55:28,228][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3609, in activate
[2017-05-31 17:55:28,229][gdb1][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:28,236][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:55:28,236][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:55:42,010][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:55:42,010][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd366180908>
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:55:42,012][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:55:42,012][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd3663d6aa0>
[2017-05-31 17:55:42,012][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:55:42,012][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:55:42,012][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:55:42,012][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:55:42,259][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:55:42,450][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:55:42,451][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:55:42,467][gdb1][DEBUG ] detect machine type
[2017-05-31 17:55:42,471][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:42,471][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:55:42,472][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 17:55:42,472][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:55:42,474][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 17:55:42,474][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:42,476][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:55:42,647][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:55:42,647][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:55:42,647][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:55:42,647][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:55:42,651][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:55:42,666][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:55:47,679][gdb1][INFO  ] checking OSD status...
[2017-05-31 17:55:47,679][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:47,682][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:55:47,847][gdb1][WARNING] there is 1 OSD down
[2017-05-31 17:55:47,847][gdb1][WARNING] there is 1 OSD out
[2017-05-31 17:55:47,848][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 17:55:56,173][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efc390cd908>
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7efc39323aa0>
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:55:56,174][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:55:56,419][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:55:56,619][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:55:56,620][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:55:56,636][gdb1][DEBUG ] detect machine type
[2017-05-31 17:55:56,640][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:56,641][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:55:56,641][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:55:56,641][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:55:56,641][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:56,643][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:55:56,813][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:55:56,814][gdb1][WARNING] activate: Cluster uuid is da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:56,814][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:55:56,814][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 17:55:56,814][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3609, in activate
[2017-05-31 17:55:56,814][gdb1][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:56,815][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:55:56,815][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:57:12,883][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:57:12,883][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 17:57:12,883][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:57:12,883][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f98fa502908>
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f98fa758aa0>
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:57:12,885][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:57:12,885][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:57:12,885][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:57:13,127][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:57:13,363][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:57:13,363][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:57:13,380][gdb1][DEBUG ] detect machine type
[2017-05-31 17:57:13,384][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:57:13,384][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:57:13,384][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 17:57:13,385][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:57:13,387][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 17:57:13,387][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:57:13,389][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:57:13,509][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:57:13,525][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:57:13,541][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:57:13,548][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:57:13,564][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:57:13,580][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 17:57:13,580][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.13708.tmp
[2017-05-31 17:57:13,580][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.13708.tmp
[2017-05-31 17:57:13,583][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.13708.tmp
[2017-05-31 17:57:18,604][gdb1][INFO  ] checking OSD status...
[2017-05-31 17:57:18,604][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:57:18,607][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:57:18,772][gdb1][WARNING] there is 1 OSD down
[2017-05-31 17:57:18,773][gdb1][WARNING] there is 1 OSD out
[2017-05-31 17:57:18,773][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 17:57:23,589][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:57:23,589][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1db2621908>
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1db2877aa0>
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:57:23,591][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:57:23,831][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:57:24,062][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:57:24,063][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:57:24,079][gdb1][DEBUG ] detect machine type
[2017-05-31 17:57:24,083][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:57:24,084][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:57:24,084][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:57:24,084][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:57:24,084][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:57:24,086][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:57:24,257][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:57:24,257][gdb1][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:57:24,257][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:57:24,257][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 17:57:24,257][gdb1][WARNING] activate: OSD uuid is 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:57:24,257][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:57:24,258][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:57:24,289][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 17:57:24,290][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:57:24,290][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:57:24.280299 7fa85e850700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:57:24,290][gdb1][WARNING] error connecting to the cluster
[2017-05-31 17:57:24,290][gdb1][WARNING] 
[2017-05-31 17:57:24,298][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:57:24,298][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:58:07,650][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa3a7716908>
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa3a796caa0>
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:58:07,651][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:58:07,887][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:58:08,082][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:58:08,083][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:58:08,099][gdb1][DEBUG ] detect machine type
[2017-05-31 17:58:08,103][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:58:08,103][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:58:08,103][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:58:08,103][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:58:08,104][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:58:08,105][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:58:08,226][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:58:08,226][gdb1][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:58:08,226][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:58:08,242][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 17:58:08,242][gdb1][WARNING] activate: OSD uuid is 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:58:08,242][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:58:08,242][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:58:08,306][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 17:58:08,306][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 17:58:08,306][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:58:08,307][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:58:08.298544 7f68f30e8700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:58:08,307][gdb1][WARNING] error connecting to the cluster
[2017-05-31 17:58:08,307][gdb1][WARNING] 
[2017-05-31 17:58:08,315][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:58:08,315][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:59:27,881][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:59:27,881][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4023cc6518>
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f40245dd938>
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:59:27,882][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:59:28,123][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:59:28,346][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:59:28,347][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:59:28,363][gdb1][DEBUG ] detect machine type
[2017-05-31 17:59:28,367][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:59:34,857][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:59:34,857][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 17:59:34,857][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:59:34,857][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:59:34,857][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffbf28de908>
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ffbf2b34aa0>
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:59:34,859][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:59:35,099][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:59:35,330][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:59:35,331][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:59:35,347][gdb1][DEBUG ] detect machine type
[2017-05-31 17:59:35,350][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:59:35,351][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:59:35,351][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 17:59:35,352][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:59:35,354][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 17:59:35,354][gdb1][DEBUG ] create a keyring file
[2017-05-31 17:59:35,356][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 17:59:35,356][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:59:35,358][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:59:35,479][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:59:35,494][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:59:35,510][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:59:35,518][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:59:35,533][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:59:35,541][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:59:40,562][gdb1][INFO  ] checking OSD status...
[2017-05-31 17:59:40,562][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:59:40,565][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:59:40,730][gdb1][WARNING] there is 1 OSD down
[2017-05-31 17:59:40,730][gdb1][WARNING] there is 1 OSD out
[2017-05-31 17:59:40,730][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 17:59:53,587][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1e2cdae908>
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1e2d004aa0>
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:59:53,588][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:59:53,832][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:59:54,058][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:59:54,059][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:59:54,075][gdb1][DEBUG ] detect machine type
[2017-05-31 17:59:54,079][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:59:54,080][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:59:54,080][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:59:54,080][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:59:54,080][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:59:54,082][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:59:54,202][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:59:54,202][gdb1][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:59:54,202][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:59:54,218][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 17:59:54,218][gdb1][WARNING] activate: OSD uuid is 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:59:54,218][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:59:54,218][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:59:54,884][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.14229.tmp
[2017-05-31 17:59:54,884][gdb1][WARNING] activate: OSD id is 2
[2017-05-31 17:59:54,885][gdb1][WARNING] activate: Initializing OSD...
[2017-05-31 17:59:54,885][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 17:59:54,999][gdb1][WARNING] got monmap epoch 2
[2017-05-31 17:59:55,006][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 2 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid 101ab77c-5335-474c-915b-a82ed5ca89ba --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 17:59:55,038][gdb1][WARNING] activate: Marking with init system systemd
[2017-05-31 17:59:55,038][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 17:59:55,040][gdb1][WARNING] activate: Authorizing OSD key...
[2017-05-31 17:59:55,040][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.2 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 17:59:55,204][gdb1][WARNING] added key for osd.2
[2017-05-31 17:59:55,204][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.14229.tmp
[2017-05-31 17:59:55,204][gdb1][WARNING] activate: ceph osd.2 data dir is ready at /mnt/memstore
[2017-05-31 17:59:55,204][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-2 -> /mnt/memstore
[2017-05-31 17:59:55,204][gdb1][WARNING] start_daemon: Starting ceph osd.2...
[2017-05-31 17:59:55,205][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@2
[2017-05-31 17:59:55,212][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@2.service.
[2017-05-31 17:59:55,276][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@2 --runtime
[2017-05-31 17:59:55,308][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@2
[2017-05-31 17:59:55,309][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@2.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 17:59:55,373][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@2
[2017-05-31 18:00:00,493][gdb1][INFO  ] checking OSD status...
[2017-05-31 18:00:00,493][gdb1][DEBUG ] find the location of an executable
[2017-05-31 18:00:00,495][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:00:00,661][gdb1][WARNING] there is 1 OSD down
[2017-05-31 18:00:00,661][gdb1][WARNING] there is 1 OSD out
[2017-05-31 18:00:00,663][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 18:45:31,589][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:45:31,589][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 18:45:31,589][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:45:31,589][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:45:31,589][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f84a3f46fc8>
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f84a48591b8>
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:45:31,590][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 18:45:31,590][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 18:45:31,590][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 18:45:31,590][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 18:45:31,617][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:31,631][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:31,631][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:31,648][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:31,650][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:45:31,650][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 18:45:31,652][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 18:45:31,690][gdb3][DEBUG ] Reading package lists...
[2017-05-31 18:45:31,854][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 18:45:31,854][gdb3][DEBUG ] Reading state information...
[2017-05-31 18:45:31,969][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 18:45:31,969][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 18:45:31,969][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 18:45:31,969][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 18:45:31,969][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 18:45:31,969][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 18:45:31,969][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 18:45:31,969][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 18:45:31,970][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 18:45:31,970][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 18:45:31,970][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 18:45:31,970][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 18:45:31,970][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 18:45:31,970][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 18:45:31,970][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 18:45:32,084][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 18:45:32,084][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 18:45:32,149][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 18:45:32,149][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 18:45:32,313][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 18:45:32,427][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 18:45:32,459][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 18:45:32,675][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 18:45:32,791][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 18:45:32,960][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 18:45:33,026][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 18:45:33,029][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 18:45:33,200][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 18:45:33,264][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 18:45:33,279][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 18:45:33,447][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 18:45:33,478][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 18:45:33,643][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 18:45:33,675][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 18:45:33,690][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 18:45:33,805][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 18:45:34,671][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 18:45:34,838][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f106bbfe710>
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f106c50b230>
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:45:34,839][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 18:45:34,866][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:34,880][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:34,881][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:34,897][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:34,900][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:45:34,916][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:34,931][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:34,931][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:34,948][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:34,950][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:45:34,950][gdb3][INFO  ] purging data on gdb3
[2017-05-31 18:45:34,951][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 18:45:34,964][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 18:45:35,137][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0d360b29e0>
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:45:35,138][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f0d36975848>
[2017-05-31 18:45:35,138][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:45:35,138][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:45:56,148][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:45:56,148][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 18:45:56,148][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:45:56,148][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc886d8f560>
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fc887413758>
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 18:45:56,149][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 18:45:56,150][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 18:45:56,176][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:56,190][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:56,191][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:56,207][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:56,210][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:45:56,211][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 18:45:56,222][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 18:45:56,229][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 18:45:56,394][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6db4199e60>
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f6db416eb18>
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 18:45:56,396][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:45:56,396][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 18:45:56,396][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 18:45:56,422][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:56,437][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:56,437][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:56,454][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:56,457][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:45:56,457][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 18:45:56,457][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 18:45:56,457][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:45:56,457][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 18:45:56,458][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:45:56,458][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 18:45:56,458][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:45:56,460][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 18:45:56,460][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 18:45:56,460][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 18:45:56,461][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 18:45:56,461][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 18:45:56,462][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 18:45:56,500][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 18:45:56,500][gdb3][DEBUG ] ceph-mon: set fsid to c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:45:56,504][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 18:45:56,505][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 18:45:56,506][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 18:45:56,506][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 18:45:56,507][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 18:45:56,577][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 18:45:56,644][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 18:45:58,715][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:45:58,780][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 18:45:58,780][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 18:45:58,781][gdb3][DEBUG ] {
[2017-05-31 18:45:58,781][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 18:45:58,781][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 18:45:58,782][gdb3][DEBUG ]   "features": {
[2017-05-31 18:45:58,782][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 18:45:58,782][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 18:45:58,782][gdb3][DEBUG ]       "kraken", 
[2017-05-31 18:45:58,782][gdb3][DEBUG ]       "luminous"
[2017-05-31 18:45:58,782][gdb3][DEBUG ]     ], 
[2017-05-31 18:45:58,782][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 18:45:58,782][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 18:45:58,782][gdb3][DEBUG ]       "kraken", 
[2017-05-31 18:45:58,783][gdb3][DEBUG ]       "luminous"
[2017-05-31 18:45:58,783][gdb3][DEBUG ]     ]
[2017-05-31 18:45:58,783][gdb3][DEBUG ]   }, 
[2017-05-31 18:45:58,783][gdb3][DEBUG ]   "monmap": {
[2017-05-31 18:45:58,783][gdb3][DEBUG ]     "created": "2017-05-31 18:45:56.486262", 
[2017-05-31 18:45:58,783][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 18:45:58,783][gdb3][DEBUG ]     "features": {
[2017-05-31 18:45:58,783][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 18:45:58,784][gdb3][DEBUG ]       "persistent": [
[2017-05-31 18:45:58,784][gdb3][DEBUG ]         "kraken", 
[2017-05-31 18:45:58,784][gdb3][DEBUG ]         "luminous"
[2017-05-31 18:45:58,784][gdb3][DEBUG ]       ]
[2017-05-31 18:45:58,784][gdb3][DEBUG ]     }, 
[2017-05-31 18:45:58,784][gdb3][DEBUG ]     "fsid": "c676bc10-14a8-4f7c-93e4-c9f0324506d5", 
[2017-05-31 18:45:58,784][gdb3][DEBUG ]     "modified": "2017-05-31 18:45:56.730163", 
[2017-05-31 18:45:58,784][gdb3][DEBUG ]     "mons": [
[2017-05-31 18:45:58,784][gdb3][DEBUG ]       {
[2017-05-31 18:45:58,784][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]         "rank": 0
[2017-05-31 18:45:58,785][gdb3][DEBUG ]       }
[2017-05-31 18:45:58,785][gdb3][DEBUG ]     ]
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   }, 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "quorum": [
[2017-05-31 18:45:58,785][gdb3][DEBUG ]     0
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   ], 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 18:45:58,785][gdb3][DEBUG ] }
[2017-05-31 18:45:58,785][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 18:45:58,786][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 18:45:58,787][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:45:58,852][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 18:45:58,868][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:58,882][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:58,883][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:58,899][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:58,902][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:45:58,903][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:45:58,968][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 18:45:58,969][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 18:45:58,969][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 18:45:58,970][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpDGdpWF
[2017-05-31 18:45:58,986][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:59,001][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:59,002][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:59,018][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:59,021][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:45:59,021][gdb3][DEBUG ] fetch remote file
[2017-05-31 18:45:59,023][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:45:59,089][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 18:45:59,255][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 18:45:59,421][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 18:45:59,587][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 18:45:59,754][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 18:45:59,920][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 18:46:00,086][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 18:46:00,253][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 18:46:00,419][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 18:46:00,585][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 18:46:00,751][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 18:46:00,751][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 18:46:00,751][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170531184600'
[2017-05-31 18:46:00,751][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 18:46:00,751][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 18:46:00,752][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 18:46:00,752][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpDGdpWF
[2017-05-31 18:46:00,936][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:46:00,936][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 18:46:00,936][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:46:00,936][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc602b75518>
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fc60348c938>
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:46:00,937][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 18:46:00,964][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:46:00,979][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:46:00,979][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:46:00,996][gdb3][DEBUG ] detect machine type
[2017-05-31 18:46:00,999][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:47:29,974][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9869f33518>
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f986a84a938>
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:29,975][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:47:30,222][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:30,449][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:47:30,450][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:30,466][gdb0][DEBUG ] detect machine type
[2017-05-31 18:47:30,469][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:47:30,471][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:47:30,471][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 18:47:30,649][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faca145a908>
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7faca16b0aa0>
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:47:30,651][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:47:30,886][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:31,118][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:47:31,118][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:31,134][gdb0][DEBUG ] detect machine type
[2017-05-31 18:47:31,138][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:47:31,139][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:47:31,139][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:47:31,139][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:47:31,141][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:47:31,141][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 18:47:31,315][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:31,315][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:47:31,315][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0e1cda0908>
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0e1cff6aa0>
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:47:31,317][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:47:31,554][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:31,782][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:47:31,782][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:31,799][gdb0][DEBUG ] detect machine type
[2017-05-31 18:47:31,802][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:47:31,803][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:47:31,803][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:47:31,803][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:47:31,803][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:47:31,806][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:47:31,926][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:47:31,926][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 18:47:31,926][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:47:31,928][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 18:47:31,928][gdb0][WARNING] activate: OSD uuid is a824912d-c2ff-4ce5-b541-4675a0141fef
[2017-05-31 18:47:31,928][gdb0][WARNING] activate: OSD id is 1
[2017-05-31 18:47:31,928][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 18:47:31,929][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 18:47:31,932][gdb0][WARNING] activate: ceph osd.1 data dir is ready at /mnt/memstore
[2017-05-31 18:47:31,932][gdb0][WARNING] start_daemon: Starting ceph osd.1...
[2017-05-31 18:47:31,932][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1
[2017-05-31 18:47:31,936][gdb0][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2017-05-31 18:47:32,000][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1 --runtime
[2017-05-31 18:47:32,064][gdb0][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@1
[2017-05-31 18:47:32,064][gdb0][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 18:47:32,096][gdb0][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@1
[2017-05-31 18:47:37,215][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:47:37,216][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:47:37,218][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:47:37,335][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 18:47:55,868][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0173bb3518>
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f01744ca938>
[2017-05-31 18:47:55,870][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:55,870][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:55,870][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 18:47:56,115][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:56,346][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 18:47:56,347][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:56,363][gdb1][DEBUG ] detect machine type
[2017-05-31 18:47:56,366][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:47:56,368][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:47:56,369][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 18:47:56,548][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:56,548][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb06843d908>
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:56,550][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:47:56,550][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb068693aa0>
[2017-05-31 18:47:56,550][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:56,550][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:56,550][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:47:56,550][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 18:47:56,795][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:57,026][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 18:47:57,027][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:57,043][gdb1][DEBUG ] detect machine type
[2017-05-31 18:47:57,046][gdb1][DEBUG ] find the location of an executable
[2017-05-31 18:47:57,047][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:47:57,047][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 18:47:57,048][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:47:57,049][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:47:57,049][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 18:47:57,222][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:57,222][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 18:47:57,222][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feedcb72908>
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7feedcdc8aa0>
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 18:47:57,224][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 18:47:57,463][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:57,695][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 18:47:57,696][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:57,712][gdb1][DEBUG ] detect machine type
[2017-05-31 18:47:57,716][gdb1][DEBUG ] find the location of an executable
[2017-05-31 18:47:57,716][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:47:57,717][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 18:47:57,717][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:47:57,717][gdb1][DEBUG ] find the location of an executable
[2017-05-31 18:47:57,719][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:47:57,839][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:47:57,840][gdb1][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 18:47:57,840][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:47:57,855][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 18:47:57,856][gdb1][WARNING] activate: OSD uuid is 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 18:47:57,856][gdb1][WARNING] activate: OSD id is 2
[2017-05-31 18:47:57,856][gdb1][WARNING] activate: Marking with init system systemd
[2017-05-31 18:47:57,856][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 18:47:57,856][gdb1][WARNING] activate: ceph osd.2 data dir is ready at /mnt/memstore
[2017-05-31 18:47:57,856][gdb1][WARNING] start_daemon: Starting ceph osd.2...
[2017-05-31 18:47:57,856][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@2
[2017-05-31 18:47:57,859][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@2.service.
[2017-05-31 18:47:57,923][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@2 --runtime
[2017-05-31 18:47:58,038][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@2
[2017-05-31 18:47:58,038][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@2.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 18:47:58,102][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@2
[2017-05-31 18:48:03,222][gdb1][INFO  ] checking OSD status...
[2017-05-31 18:48:03,222][gdb1][DEBUG ] find the location of an executable
[2017-05-31 18:48:03,224][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:48:03,342][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 18:49:54,207][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fceed5d2518>
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1']
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fceedee9938>
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:49:54,208][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:49:54,446][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:49:54,678][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:49:54,678][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:49:54,694][gdb0][DEBUG ] detect machine type
[2017-05-31 18:49:54,697][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:49:54,699][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:49:54,699][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 18:49:54,931][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 18:49:55,160][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 18:49:55,160][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 18:49:55,177][gdb1][DEBUG ] detect machine type
[2017-05-31 18:49:55,181][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:49:55,182][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:49:55,183][ceph_deploy][ERROR ] GenericError: Failed to configure 2 admin hosts

[2017-05-31 18:50:28,701][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:50:28,701][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-05-31 18:50:28,701][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:50:28,701][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7d0e56e518>
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f7d0ee85938>
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:50:28,702][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:50:28,942][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:50:29,170][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:50:29,170][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:50:29,186][gdb0][DEBUG ] detect machine type
[2017-05-31 18:50:29,190][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:50:29,365][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe3e2b4a908>
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe3e2da0aa0>
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:50:29,368][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:50:29,610][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:50:29,838][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:50:29,838][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:50:29,854][gdb0][DEBUG ] detect machine type
[2017-05-31 18:50:29,858][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:50:29,859][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:50:29,859][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:50:29,859][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:50:29,861][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:50:29,861][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:50:29,863][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:50:29,984][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:50:29,987][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:50:30,003][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:50:30,018][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:50:30,026][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:50:30,042][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 18:50:35,048][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:50:35,049][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:50:35,051][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:50:35,217][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:50:35,382][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:50:35,382][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:50:35,382][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc329b70908>
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc329dc6aa0>
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:50:35,383][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:50:35,630][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:50:35,826][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:50:35,826][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:50:35,842][gdb0][DEBUG ] detect machine type
[2017-05-31 18:50:35,846][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:50:35,847][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:50:35,847][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:50:35,847][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:50:35,847][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:50:35,849][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:50:35,970][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:50:35,970][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 18:50:35,970][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:50:35,974][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:50:35,974][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:50:35,974][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:50:35,974][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:50:35,974][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:50:35,975][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:50:35,975][gdb0][WARNING]     args.func(args)
[2017-05-31 18:50:35,975][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:50:35,975][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:50:35,975][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:50:35,975][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:50:35,975][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:50:35,975][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:50:35,976][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 18:50:35,983][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:50:35,983][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:51:44,351][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:51:44,351][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-05-31 18:51:44,351][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:51:44,351][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f80e4e73518>
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f80e578a938>
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:51:44,352][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:51:44,594][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:51:44,826][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:51:44,826][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:51:44,843][gdb0][DEBUG ] detect machine type
[2017-05-31 18:51:44,846][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:52:04,467][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:52:04,467][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memsto
[2017-05-31 18:52:04,467][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:52:04,467][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:52:04,467][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:52:04,467][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memsto', None)]
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fceef85e908>
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fceefab4aa0>
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:52:04,469][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memsto:
[2017-05-31 18:52:04,702][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:52:04,930][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:52:04,931][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:52:04,947][gdb0][DEBUG ] detect machine type
[2017-05-31 18:52:04,950][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:04,951][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:52:04,951][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:52:04,951][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:52:04,954][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memsto journal None activate False
[2017-05-31 18:52:04,954][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:04,956][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memsto
[2017-05-31 18:52:05,076][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:52:05,076][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:52:05,076][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:52:05,076][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:52:05,077][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:52:05,077][gdb0][WARNING]     args.func(args)
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 18:52:05,077][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 18:52:05,077][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 18:52:05,077][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 18:52:05,077][gdb0][WARNING]     self.set_type()
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 18:52:05,077][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 18:52:05,077][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memsto'
[2017-05-31 18:52:05,078][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:52:05,078][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memsto
[2017-05-31 18:52:05,078][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 18:52:07,388][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:52:07,388][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:52:07,388][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd0298fd908>
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd029b53aa0>
[2017-05-31 18:52:07,390][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:52:07,390][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:52:07,390][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:52:07,390][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:52:07,630][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:52:07,822][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:52:07,822][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:52:07,838][gdb0][DEBUG ] detect machine type
[2017-05-31 18:52:07,842][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:07,843][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:52:07,843][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:52:07,843][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:52:07,845][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:52:07,846][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:07,847][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:52:07,968][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:52:07,971][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:07,987][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:08,003][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:08,018][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:52:08,026][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 18:52:08,034][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.15711.tmp
[2017-05-31 18:52:08,037][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.15711.tmp
[2017-05-31 18:52:08,045][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.15711.tmp
[2017-05-31 18:52:13,057][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:52:13,057][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:13,060][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:52:13,225][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:52:21,817][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:52:21,817][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:52:21,817][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:52:21,817][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:52:21,817][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa09dd7f908>
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa09dfd5aa0>
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:52:21,818][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:52:22,050][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:52:22,278][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:52:22,278][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:52:22,294][gdb0][DEBUG ] detect machine type
[2017-05-31 18:52:22,298][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:22,298][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:52:22,299][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:52:22,299][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:52:22,299][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:22,301][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:52:22,421][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:52:22,421][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:52:22,421][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:52:22,425][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 18:52:22,425][gdb0][WARNING] activate: OSD uuid is c0f127b4-e253-45f0-9f51-71ce307580a4
[2017-05-31 18:52:22,425][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 18:52:22,425][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise c0f127b4-e253-45f0-9f51-71ce307580a4
[2017-05-31 18:52:22,489][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:52:22,489][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:52:22,489][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:52:22,489][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:52:22,489][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:52:22,489][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:52:22,490][gdb0][WARNING]     args.func(args)
[2017-05-31 18:52:22,490][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:52:22,490][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:52:22,490][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:52:22,490][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:52:22,490][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 18:52:22,490][gdb0][WARNING]     keyring=keyring,
[2017-05-31 18:52:22,490][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 18:52:22,490][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 18:52:22,490][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 18:52:22.476913 7f087fdec700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 18:52:22,490][gdb0][WARNING] error connecting to the cluster
[2017-05-31 18:52:22,490][gdb0][WARNING] 
[2017-05-31 18:52:22,498][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:52:22,498][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:52:40,085][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb19a84b908>
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb19aaa1aa0>
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:52:40,087][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:52:40,326][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:52:40,558][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:52:40,558][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:52:40,574][gdb0][DEBUG ] detect machine type
[2017-05-31 18:52:40,578][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:40,579][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:52:40,579][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:52:40,579][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:52:40,581][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 18:52:40,581][gdb0][DEBUG ] create a keyring file
[2017-05-31 18:52:40,583][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:52:40,583][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:40,585][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:52:40,706][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:52:40,709][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:40,725][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:40,741][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:40,748][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:52:40,764][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 18:52:45,772][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:52:45,773][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:45,775][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:52:45,940][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:52:48,018][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:52:48,018][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:52:48,018][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:52:48,018][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:52:48,018][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb44c146908>
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb44c39caa0>
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:52:48,019][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:52:48,258][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:52:48,454][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:52:48,455][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:52:48,471][gdb0][DEBUG ] detect machine type
[2017-05-31 18:52:48,474][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:48,475][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:52:48,475][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:52:48,475][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:52:48,475][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:48,477][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:52:48,598][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:52:48,598][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:52:48,598][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:52:48,606][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 18:52:48,606][gdb0][WARNING] activate: OSD uuid is c0f127b4-e253-45f0-9f51-71ce307580a4
[2017-05-31 18:52:48,606][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 18:52:48,606][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise c0f127b4-e253-45f0-9f51-71ce307580a4
[2017-05-31 18:52:48,770][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.16053.tmp
[2017-05-31 18:52:48,771][gdb0][WARNING] activate: OSD id is 0
[2017-05-31 18:52:48,771][gdb0][WARNING] activate: Initializing OSD...
[2017-05-31 18:52:48,771][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 18:52:48,885][gdb0][WARNING] got monmap epoch 2
[2017-05-31 18:52:48,893][gdb0][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid c0f127b4-e253-45f0-9f51-71ce307580a4 --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 18:52:48,924][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 18:52:48,925][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 18:52:48,928][gdb0][WARNING] activate: Authorizing OSD key...
[2017-05-31 18:52:48,928][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 18:52:49,092][gdb0][WARNING] added key for osd.0
[2017-05-31 18:52:49,093][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.16053.tmp
[2017-05-31 18:52:49,093][gdb0][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-05-31 18:52:49,093][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:52:49,093][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:52:49,093][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:52:49,093][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:52:49,093][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:52:49,093][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:52:49,093][gdb0][WARNING]     args.func(args)
[2017-05-31 18:52:49,093][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:52:49,093][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:52:49,094][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3543, in activate_dir
[2017-05-31 18:52:49,094][gdb0][WARNING]     old = os.readlink(canonical)
[2017-05-31 18:52:49,094][gdb0][WARNING] OSError: [Errno 22] Invalid argument: '/var/lib/ceph/osd/ceph-0'
[2017-05-31 18:52:49,094][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:52:49,094][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:53:23,583][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:53:23,583][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3854a70fc8>
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f38553831b8>
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:53:23,584][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 18:53:23,584][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 18:53:23,584][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 18:53:23,585][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 18:53:23,610][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:53:23,624][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:53:23,625][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:53:23,641][gdb3][DEBUG ] detect machine type
[2017-05-31 18:53:23,644][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:53:23,644][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 18:53:23,645][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 18:53:23,683][gdb3][DEBUG ] Reading package lists...
[2017-05-31 18:53:23,847][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 18:53:23,847][gdb3][DEBUG ] Reading state information...
[2017-05-31 18:53:23,911][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 18:53:23,911][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 18:53:23,911][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 18:53:23,912][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 18:53:23,976][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 18:53:23,976][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 18:53:24,091][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 18:53:24,091][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 18:53:24,123][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 18:53:24,123][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 18:53:24,287][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 18:53:24,402][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 18:53:24,405][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 18:53:24,621][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 18:53:24,737][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 18:53:24,905][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 18:53:24,969][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 18:53:25,000][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 18:53:25,167][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 18:53:25,232][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 18:53:25,233][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 18:53:25,397][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 18:53:25,429][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 18:53:25,593][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 18:53:25,657][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 18:53:25,665][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 18:53:25,779][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 18:53:26,595][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 18:53:26,761][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:53:26,761][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 18:53:26,761][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:53:26,761][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:53:26,761][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6c68cf3710>
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f6c69600230>
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:53:26,762][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 18:53:26,788][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:53:26,802][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:53:26,802][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:53:26,819][gdb3][DEBUG ] detect machine type
[2017-05-31 18:53:26,821][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:53:26,837][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:53:26,850][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:53:26,851][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:53:26,867][gdb3][DEBUG ] detect machine type
[2017-05-31 18:53:26,869][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:53:26,869][gdb3][INFO  ] purging data on gdb3
[2017-05-31 18:53:26,870][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 18:53:26,883][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 18:53:27,052][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa6a31b89e0>
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fa6a3a7b848>
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:53:59,636][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:53:59,636][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1af7e04560>
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f1af8488758>
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 18:53:59,638][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:53:59,638][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 18:53:59,638][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 18:53:59,638][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 18:53:59,664][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:53:59,678][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:53:59,678][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:53:59,695][gdb3][DEBUG ] detect machine type
[2017-05-31 18:53:59,697][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:53:59,698][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 18:53:59,710][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 18:53:59,716][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 18:53:59,716][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 18:53:59,716][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 18:53:59,716][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 18:53:59,716][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 18:53:59,716][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 18:53:59,717][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 18:53:59,717][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 18:53:59,881][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f73b3696e60>
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f73b366bb18>
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:53:59,883][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 18:53:59,883][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 18:53:59,909][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:53:59,923][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:53:59,924][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:53:59,940][gdb3][DEBUG ] detect machine type
[2017-05-31 18:53:59,942][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:53:59,943][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 18:53:59,943][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 18:53:59,943][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:53:59,943][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 18:53:59,943][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:53:59,944][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 18:53:59,944][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:53:59,946][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 18:53:59,946][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 18:53:59,946][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 18:53:59,947][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 18:53:59,947][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 18:53:59,948][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 18:53:59,986][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 18:53:59,986][gdb3][DEBUG ] ceph-mon: set fsid to cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 18:53:59,988][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 18:53:59,989][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 18:53:59,989][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 18:53:59,990][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 18:53:59,991][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 18:54:00,060][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 18:54:00,129][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 18:54:02,171][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:54:02,236][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 18:54:02,236][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 18:54:02,237][gdb3][DEBUG ] {
[2017-05-31 18:54:02,237][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]   "features": {
[2017-05-31 18:54:02,237][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 18:54:02,237][gdb3][DEBUG ]       "kraken", 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]       "luminous"
[2017-05-31 18:54:02,237][gdb3][DEBUG ]     ], 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 18:54:02,237][gdb3][DEBUG ]       "kraken", 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]       "luminous"
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     ]
[2017-05-31 18:54:02,238][gdb3][DEBUG ]   }, 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]   "monmap": {
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "created": "2017-05-31 18:53:59.971511", 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "features": {
[2017-05-31 18:54:02,238][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]       "persistent": [
[2017-05-31 18:54:02,238][gdb3][DEBUG ]         "kraken", 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]         "luminous"
[2017-05-31 18:54:02,238][gdb3][DEBUG ]       ]
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     }, 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "fsid": "cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab", 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "modified": "2017-05-31 18:54:00.211295", 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "mons": [
[2017-05-31 18:54:02,238][gdb3][DEBUG ]       {
[2017-05-31 18:54:02,238][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]         "rank": 0
[2017-05-31 18:54:02,239][gdb3][DEBUG ]       }
[2017-05-31 18:54:02,239][gdb3][DEBUG ]     ]
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   }, 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "quorum": [
[2017-05-31 18:54:02,239][gdb3][DEBUG ]     0
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   ], 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 18:54:02,239][gdb3][DEBUG ] }
[2017-05-31 18:54:02,239][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 18:54:02,239][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 18:54:02,240][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:54:02,306][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 18:54:02,320][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:54:02,334][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:54:02,335][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:54:02,351][gdb3][DEBUG ] detect machine type
[2017-05-31 18:54:02,353][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:54:02,354][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:54:02,420][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 18:54:02,420][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 18:54:02,420][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 18:54:02,422][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpSV9u_P
[2017-05-31 18:54:02,437][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:54:02,451][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:54:02,452][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:54:02,468][gdb3][DEBUG ] detect machine type
[2017-05-31 18:54:02,470][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:54:02,471][gdb3][DEBUG ] fetch remote file
[2017-05-31 18:54:02,472][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:54:02,538][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 18:54:02,704][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 18:54:02,870][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 18:54:03,036][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 18:54:03,253][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 18:54:03,419][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 18:54:03,586][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 18:54:03,752][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 18:54:03,918][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 18:54:04,084][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 18:54:04,249][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170531185404'
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpSV9u_P
[2017-05-31 18:54:04,431][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:54:04,431][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 18:54:04,431][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc3e268f518>
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fc3e2fa6938>
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:54:04,432][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 18:54:04,459][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:54:04,473][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:54:04,473][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:54:04,490][gdb3][DEBUG ] detect machine type
[2017-05-31 18:54:04,492][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:54:45,775][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f25debd2518>
[2017-05-31 18:54:45,776][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:54:45,776][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 18:54:45,776][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f25df4e9938>
[2017-05-31 18:54:45,776][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:54:45,776][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:54:45,776][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 18:54:45,803][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:54:45,818][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:54:45,818][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:54:45,835][gdb3][DEBUG ] detect machine type
[2017-05-31 18:54:45,838][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:55:44,967][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f83cffec518>
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f83d0903938>
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:55:44,969][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:55:45,210][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:55:45,442][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:55:45,442][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:55:45,458][gdb0][DEBUG ] detect machine type
[2017-05-31 18:55:45,461][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:55:45,463][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:55:45,464][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 18:55:56,020][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7551d43518>
[2017-05-31 18:55:56,021][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:55:56,021][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:55:56,021][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f755265a938>
[2017-05-31 18:55:56,021][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:55:56,021][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:55:56,021][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:55:56,262][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:55:56,494][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:55:56,494][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:55:56,510][gdb0][DEBUG ] detect machine type
[2017-05-31 18:55:56,513][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:56:09,705][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:56:09,705][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:56:09,705][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:56:09,705][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:56:09,705][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:56:09,705][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feadbb8b908>
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7feadbde1aa0>
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:56:09,707][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:56:09,942][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:56:10,174][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:56:10,174][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:56:10,190][gdb0][DEBUG ] detect machine type
[2017-05-31 18:56:10,194][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:10,194][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:56:10,194][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:56:10,195][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:56:10,197][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 18:56:10,197][gdb0][DEBUG ] create a keyring file
[2017-05-31 18:56:10,199][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:56:10,199][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:10,201][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:56:10,322][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:56:10,323][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:56:10,339][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:56:10,355][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:56:10,362][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:56:10,378][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 18:56:15,387][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:56:15,387][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:15,389][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:56:15,555][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:56:24,144][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:56:24,144][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:56:24,144][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:56:24,144][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:56:24,144][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f896b4a4908>
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f896b6faaa0>
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:56:24,145][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:56:24,386][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:56:24,610][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:56:24,610][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:56:24,626][gdb0][DEBUG ] detect machine type
[2017-05-31 18:56:24,629][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:24,630][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:56:24,630][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:56:24,630][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:56:24,631][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:24,632][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:56:24,753][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:56:24,753][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:56:24,753][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:56:24,757][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:56:24,757][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:56:24,757][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:56:24,757][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:56:24,757][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:56:24,758][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:56:24,758][gdb0][WARNING]     args.func(args)
[2017-05-31 18:56:24,758][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:56:24,758][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:56:24,758][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:56:24,758][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:56:24,758][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:56:24,758][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:56:24,758][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:56:24,766][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:56:24,766][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:56:52,810][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:56:52,810][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff039de8518>
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7ff03a6ff938>
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:56:52,811][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:56:53,054][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:56:53,278][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:56:53,279][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:56:53,295][gdb0][DEBUG ] detect machine type
[2017-05-31 18:56:53,298][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:56:57,584][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:56:57,584][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0178db6908>
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f017900caa0>
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:56:57,586][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:56:57,826][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:56:58,053][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:56:58,054][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:56:58,069][gdb0][DEBUG ] detect machine type
[2017-05-31 18:56:58,073][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:58,074][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:56:58,074][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:56:58,074][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:56:58,075][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:58,076][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:56:58,197][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:56:58,197][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:56:58,197][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:56:58,198][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:56:58,198][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:56:58,198][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:56:58,199][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:56:58,199][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:56:58,199][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:56:58,199][gdb0][WARNING]     args.func(args)
[2017-05-31 18:56:58,199][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:56:58,200][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:56:58,200][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:56:58,200][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:56:58,200][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:56:58,200][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:56:58,200][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:56:58,216][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:56:58,216][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:57:09,851][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:57:09,851][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7c2bbe7518>
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f7c2c4fe938>
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:57:09,852][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 18:57:09,878][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:57:09,892][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:57:09,893][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:57:09,909][gdb3][DEBUG ] detect machine type
[2017-05-31 18:57:09,911][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:57:16,149][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa7d3366908>
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa7d35bcaa0>
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:57:16,151][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:57:16,386][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:57:16,613][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:57:16,614][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:57:16,629][gdb0][DEBUG ] detect machine type
[2017-05-31 18:57:16,633][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:16,633][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:57:16,634][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:57:16,634][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:57:16,634][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:16,636][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:57:16,756][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:57:16,756][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:57:16,757][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:57:16,760][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:57:16,760][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:57:16,760][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:57:16,761][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:57:16,761][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:57:16,761][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:57:16,761][gdb0][WARNING]     args.func(args)
[2017-05-31 18:57:16,761][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:57:16,761][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:57:16,761][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:57:16,761][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:57:16,761][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:57:16,761][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:57:16,761][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:57:16,769][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:57:16,769][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:57:21,647][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:57:21,647][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:57:21,647][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:57:21,647][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efcda9f6908>
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7efcdac4caa0>
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:57:21,649][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:57:21,649][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:57:21,649][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:57:21,890][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:57:22,118][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:57:22,118][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:57:22,134][gdb0][DEBUG ] detect machine type
[2017-05-31 18:57:22,137][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:22,138][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:57:22,138][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:57:22,138][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:57:22,140][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:57:22,141][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:22,142][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:57:22,263][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:57:22,266][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:57:22,282][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:57:22,297][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:57:22,305][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:57:22,321][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 18:57:27,329][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:57:27,330][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:27,332][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:57:27,497][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:57:29,326][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:57:29,326][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:57:29,326][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:57:29,326][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:57:29,326][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:57:29,326][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efd3714f908>
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7efd373a5aa0>
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:57:29,327][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:57:29,562][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:57:29,786][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:57:29,786][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:57:29,802][gdb0][DEBUG ] detect machine type
[2017-05-31 18:57:29,805][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:29,806][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:57:29,806][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:57:29,806][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:57:29,806][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:29,808][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:57:29,929][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:57:29,929][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:57:29,929][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:57:29,930][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:57:29,930][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:57:29,931][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:57:29,931][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:57:29,931][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:57:29,931][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:57:29,931][gdb0][WARNING]     args.func(args)
[2017-05-31 18:57:29,931][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:57:29,931][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:57:29,932][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:57:29,932][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:57:29,932][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:57:29,932][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:57:29,932][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:57:29,948][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:57:29,948][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:58:02,594][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f37bf68c518>
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f37bffa3938>
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:58:02,596][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:58:02,838][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:58:03,062][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:58:03,062][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:58:03,078][gdb0][DEBUG ] detect machine type
[2017-05-31 18:58:03,082][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:58:05,043][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:58:05,043][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:58:05,043][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9982fa1908>
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f99831f7aa0>
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:58:05,045][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:58:05,045][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:58:05,045][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:58:05,286][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:58:05,509][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:58:05,510][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:58:05,526][gdb0][DEBUG ] detect machine type
[2017-05-31 18:58:05,529][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:58:05,530][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:58:05,530][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:58:05,530][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:58:05,533][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:58:05,533][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:58:05,534][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:58:05,655][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:58:05,658][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:58:05,674][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:58:05,690][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:58:05,697][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:58:05,713][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 18:58:10,722][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:58:10,722][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:58:10,724][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:58:10,889][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:58:14,417][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fea3fd2a908>
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fea3ff80aa0>
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:58:14,418][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:58:14,658][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:58:14,881][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:58:14,882][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:58:14,898][gdb0][DEBUG ] detect machine type
[2017-05-31 18:58:14,901][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:58:14,902][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:58:14,902][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:58:14,902][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:58:14,902][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:58:14,904][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:58:15,025][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:58:15,025][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:58:15,025][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:58:15,028][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:58:15,028][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:58:15,029][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:58:15,029][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:58:15,029][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:58:15,029][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:58:15,029][gdb0][WARNING]     args.func(args)
[2017-05-31 18:58:15,029][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:58:15,029][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:58:15,030][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:58:15,030][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:58:15,030][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:58:15,030][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:58:15,030][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:58:15,046][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:58:15,046][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:00:23,548][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2fbebd7908>
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2fbee2daa0>
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:00:23,550][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:00:23,791][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:00:24,022][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:00:24,022][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:00:24,038][gdb0][DEBUG ] detect machine type
[2017-05-31 19:00:24,042][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:00:24,042][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:00:24,043][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 19:00:24,043][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:00:24,045][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 19:00:24,045][gdb0][DEBUG ] create a keyring file
[2017-05-31 19:00:24,047][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 19:00:24,047][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:00:24,049][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:00:24,169][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:00:24,171][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:00:24,186][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:00:24,202][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:00:24,210][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:00:24,225][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 19:00:29,234][gdb0][INFO  ] checking OSD status...
[2017-05-31 19:00:29,234][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:00:29,237][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:00:29,402][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 19:00:34,139][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff7156d0908>
[2017-05-31 19:00:34,140][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:00:34,140][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff715926aa0>
[2017-05-31 19:00:34,140][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:00:34,140][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:00:34,140][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:00:34,140][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:00:34,378][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:00:34,605][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:00:34,606][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:00:34,621][gdb0][DEBUG ] detect machine type
[2017-05-31 19:00:34,625][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:00:34,626][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:00:34,626][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 19:00:34,626][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:00:34,626][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:00:34,628][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:00:34,748][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:00:34,749][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 19:00:34,749][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:00:34,752][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 19:00:34,752][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 19:00:34,753][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:00:34,753][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:00:34,753][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 19:00:34,753][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:00:34,753][gdb0][WARNING]     args.func(args)
[2017-05-31 19:00:34,753][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:00:34,753][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 19:00:34,754][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:00:34,754][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 19:00:34,754][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 19:00:34,754][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 19:00:34,754][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 19:00:34,762][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:00:34,762][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:01:30,099][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb097308908>
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb09755eaa0>
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:01:30,101][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:01:30,348][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:01:30,579][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:01:30,579][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:01:30,596][gdb1][DEBUG ] detect machine type
[2017-05-31 19:01:30,600][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:01:30,600][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:01:30,601][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 19:01:30,601][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:01:30,603][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 19:01:30,603][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 19:01:40,792][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:01:40,792][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbbd048a518>
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fbbd0da1938>
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:01:40,793][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 19:01:41,031][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:01:41,263][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:01:41,263][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:01:41,280][gdb1][DEBUG ] detect machine type
[2017-05-31 19:01:41,283][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:01:41,285][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 19:01:41,285][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 19:01:48,568][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0433916518>
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f043422d938>
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:01:48,569][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 19:01:48,811][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:01:49,047][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:01:49,047][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:01:49,063][gdb1][DEBUG ] detect machine type
[2017-05-31 19:01:49,067][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:01:52,711][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7a9172a908>
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7a91980aa0>
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:01:52,713][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:01:52,959][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:01:53,190][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:01:53,191][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:01:53,207][gdb1][DEBUG ] detect machine type
[2017-05-31 19:01:53,211][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:01:53,211][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:01:53,211][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 19:01:53,212][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:01:53,214][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 19:01:53,214][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:01:53,216][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:01:53,336][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:01:53,352][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:01:53,368][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:01:53,383][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:01:53,391][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:01:53,407][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 19:01:53,407][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.16092.tmp
[2017-05-31 19:01:53,407][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.16092.tmp
[2017-05-31 19:01:53,411][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.16092.tmp
[2017-05-31 19:01:58,432][gdb1][INFO  ] checking OSD status...
[2017-05-31 19:01:58,432][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:01:58,434][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:01:58,600][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 19:02:05,404][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:02:05,404][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 19:02:05,404][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:02:05,404][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fef4e246908>
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fef4e49caa0>
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:02:05,405][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:02:05,643][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:02:05,874][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:02:05,875][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:02:05,891][gdb1][DEBUG ] detect machine type
[2017-05-31 19:02:05,895][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:05,895][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:02:05,895][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 19:02:05,896][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:02:05,896][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:05,898][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:02:06,018][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:02:06,018][gdb1][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:02:06,018][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:02:06,034][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 19:02:06,034][gdb1][WARNING] activate: OSD uuid is 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:06,034][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:02:06,034][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:06,099][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 19:02:06,099][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 19:02:06,099][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:02:06,099][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:02:06,099][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:02:06,099][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:02:06,100][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:02:06,100][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 19:02:06,100][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 19:02:06,100][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 19:02:06.089590 7f162bcf1700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 19:02:06,100][gdb1][WARNING] error connecting to the cluster
[2017-05-31 19:02:06,100][gdb1][WARNING] 
[2017-05-31 19:02:06,108][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:02:06,108][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:02:26,603][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:02:26,603][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 19:02:26,603][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:02:26,603][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:02:26,603][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:02:26,603][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f31d72c1908>
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f31d7517aa0>
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:02:26,604][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:02:26,851][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:02:27,083][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:02:27,084][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:02:27,100][gdb1][DEBUG ] detect machine type
[2017-05-31 19:02:27,104][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:27,105][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:02:27,105][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 19:02:27,105][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:02:27,105][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:27,107][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:02:27,278][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:02:27,278][gdb1][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:02:27,278][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:02:27,278][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 19:02:27,278][gdb1][WARNING] activate: OSD uuid is 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:27,278][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:02:27,278][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:27,310][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 19:02:27,311][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 19:02:27,311][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:02:27,311][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:02:27,311][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:02:27,312][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:02:27,312][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:02:27,312][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 19:02:27,312][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 19:02:27,312][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 19:02:27.301230 7fc80c4df700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 19:02:27,312][gdb1][WARNING] error connecting to the cluster
[2017-05-31 19:02:27,312][gdb1][WARNING] 
[2017-05-31 19:02:27,320][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:02:27,320][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:02:38,060][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:02:38,060][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f14bb70e908>
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:02:38,062][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f14bb964aa0>
[2017-05-31 19:02:38,062][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:02:38,062][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:02:38,062][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:02:38,062][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:02:38,303][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:02:38,535][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:02:38,535][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:02:38,551][gdb1][DEBUG ] detect machine type
[2017-05-31 19:02:38,555][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:38,556][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:02:38,556][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 19:02:38,556][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:02:38,558][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 19:02:38,558][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:38,560][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:02:38,680][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:02:38,696][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:02:38,712][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:02:38,728][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:02:38,735][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:02:38,751][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 19:02:43,760][gdb1][INFO  ] checking OSD status...
[2017-05-31 19:02:43,760][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:43,762][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:02:43,928][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 19:02:45,629][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:02:45,629][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 19:02:45,629][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:02:45,629][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:02:45,629][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:02:45,629][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8b71056908>
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8b712acaa0>
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:02:45,630][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:02:45,867][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:02:46,099][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:02:46,099][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:02:46,115][gdb1][DEBUG ] detect machine type
[2017-05-31 19:02:46,119][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:46,120][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:02:46,120][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 19:02:46,120][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:02:46,120][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:46,122][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:02:46,243][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:02:46,243][gdb1][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:02:46,243][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:02:46,259][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 19:02:46,259][gdb1][WARNING] activate: OSD uuid is 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:46,259][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:02:46,259][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:46,373][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 19:02:46,373][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 19:02:46,373][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 19:02:46,374][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 19:02:46.315195 7ff02b813700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 19:02:46,374][gdb1][WARNING] error connecting to the cluster
[2017-05-31 19:02:46,374][gdb1][WARNING] 
[2017-05-31 19:02:46,374][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:02:46,374][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:03:30,541][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:03:30,541][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 19:03:30,541][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:03:30,541][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:03:30,541][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcc71562908>
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcc717b8aa0>
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:03:30,543][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:03:30,783][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:03:31,015][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:03:31,015][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:03:31,031][gdb1][DEBUG ] detect machine type
[2017-05-31 19:03:31,035][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:03:31,035][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:03:31,036][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 19:03:31,036][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:03:31,038][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 19:03:31,038][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:03:31,040][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:03:31,160][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:03:31,176][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:03:31,192][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:03:31,208][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:03:31,215][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:03:31,231][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 19:03:36,239][gdb1][INFO  ] checking OSD status...
[2017-05-31 19:03:36,240][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:03:36,242][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:03:36,408][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 19:03:41,172][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:03:41,172][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7d45f48908>
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7d4619eaa0>
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:03:41,174][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:03:41,415][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:03:41,647][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:03:41,647][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:03:41,664][gdb1][DEBUG ] detect machine type
[2017-05-31 19:03:41,667][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:03:41,668][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:03:41,668][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 19:03:41,668][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:03:41,668][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:03:41,670][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:03:41,790][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:03:41,791][gdb1][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:03:41,791][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:03:41,806][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 19:03:41,806][gdb1][WARNING] activate: OSD uuid is 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:03:41,807][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:03:41,807][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:03:41,871][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 19:03:41,871][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 19:03:41,872][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 19:03:41.861160 7f9a5c447700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 19:03:41,872][gdb1][WARNING] error connecting to the cluster
[2017-05-31 19:03:41,872][gdb1][WARNING] 
[2017-05-31 19:03:41,879][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:03:41,880][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:04:13,941][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:04:13,941][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-05-31 19:04:13,941][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f24f8549518>
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f24f8e60938>
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:04:13,942][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 19:04:14,184][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:04:14,415][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:04:14,415][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:04:14,432][gdb1][DEBUG ] detect machine type
[2017-05-31 19:04:14,435][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:04:18,227][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa794a1a908>
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa794c70aa0>
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:04:18,229][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:04:18,471][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:04:18,703][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:04:18,704][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:04:18,720][gdb1][DEBUG ] detect machine type
[2017-05-31 19:04:18,724][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:18,725][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:04:18,725][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 19:04:18,725][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:04:18,727][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 19:04:18,727][gdb1][DEBUG ] create a keyring file
[2017-05-31 19:04:18,729][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 19:04:18,729][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:18,731][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:04:18,901][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:04:18,902][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:04:18,902][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:04:18,902][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:04:18,909][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:04:18,925][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 19:04:23,934][gdb1][INFO  ] checking OSD status...
[2017-05-31 19:04:23,934][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:23,936][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:04:24,102][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 19:04:27,131][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f10404af908>
[2017-05-31 19:04:27,132][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:04:27,132][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1040705aa0>
[2017-05-31 19:04:27,132][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:04:27,132][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:04:27,132][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:04:27,132][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:04:27,375][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:04:27,606][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:04:27,607][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:04:27,623][gdb1][DEBUG ] detect machine type
[2017-05-31 19:04:27,627][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:27,628][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:04:27,628][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 19:04:27,628][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:04:27,628][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:27,630][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:04:27,751][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:04:27,751][gdb1][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:04:27,751][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:04:27,767][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 19:04:27,767][gdb1][WARNING] activate: OSD uuid is 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:04:27,767][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:04:27,767][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:04:27,931][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.17048.tmp
[2017-05-31 19:04:27,931][gdb1][WARNING] activate: OSD id is 0
[2017-05-31 19:04:27,931][gdb1][WARNING] activate: Initializing OSD...
[2017-05-31 19:04:27,931][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 19:04:28,096][gdb1][WARNING] got monmap epoch 2
[2017-05-31 19:04:28,096][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid 7c61a5ba-3130-4674-af68-e74e94fe90f6 --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 19:04:28,128][gdb1][WARNING] activate: Marking with init system systemd
[2017-05-31 19:04:28,128][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 19:04:28,128][gdb1][WARNING] activate: Authorizing OSD key...
[2017-05-31 19:04:28,128][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 19:04:28,292][gdb1][WARNING] added key for osd.0
[2017-05-31 19:04:28,293][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.17048.tmp
[2017-05-31 19:04:28,293][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-05-31 19:04:28,293][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-0 -> /mnt/memstore
[2017-05-31 19:04:28,293][gdb1][WARNING] start_daemon: Starting ceph osd.0...
[2017-05-31 19:04:28,293][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-05-31 19:04:28,293][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-05-31 19:04:28,357][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-05-31 19:04:28,389][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-05-31 19:04:28,392][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 19:04:28,456][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-05-31 19:04:33,576][gdb1][INFO  ] checking OSD status...
[2017-05-31 19:04:33,576][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:33,578][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:04:33,746][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 19:04:48,602][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2e86347518>
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 19:04:48,603][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f2e86c5e938>
[2017-05-31 19:04:48,603][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:04:48,603][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:04:48,603][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 19:04:48,842][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:04:49,070][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:04:49,070][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:04:49,086][gdb0][DEBUG ] detect machine type
[2017-05-31 19:04:49,090][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:05:02,677][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f56878ce908>
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5687b24aa0>
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:05:02,679][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:05:02,919][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:05:03,150][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:05:03,151][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:05:03,167][gdb0][DEBUG ] detect machine type
[2017-05-31 19:05:03,170][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:05:03,171][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:05:03,171][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 19:05:03,171][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:05:03,174][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 19:05:03,174][gdb0][DEBUG ] create a keyring file
[2017-05-31 19:05:03,176][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 19:05:03,176][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:05:03,178][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:05:03,298][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:05:03,306][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:05:03,321][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:05:03,329][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:05:03,345][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:05:03,352][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 19:05:08,373][gdb0][INFO  ] checking OSD status...
[2017-05-31 19:05:08,373][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:05:08,376][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:05:08,541][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 19:05:18,167][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:05:18,167][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 19:05:18,167][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:05:18,167][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f46d5fcb908>
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f46d6221aa0>
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:05:18,168][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:05:18,406][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:05:18,630][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:05:18,630][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:05:18,646][gdb0][DEBUG ] detect machine type
[2017-05-31 19:05:18,649][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:05:18,650][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:05:18,650][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 19:05:18,650][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:05:18,651][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:05:18,652][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:05:18,773][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:05:18,773][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 19:05:18,773][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:05:18,776][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 19:05:18,776][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 19:05:18,776][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:05:18,777][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:05:18,777][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 19:05:18,777][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:05:18,777][gdb0][WARNING]     args.func(args)
[2017-05-31 19:05:18,777][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:05:18,777][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 19:05:18,779][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:05:18,779][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 19:05:18,779][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 19:05:18,779][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 19:05:18,779][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 19:05:18,787][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:05:18,787][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:08:02,744][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:08:02,744][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb0
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa2089aefc8>
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  host                          : ['gdb0']
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fa2092c11b8>
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:08:02,745][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 19:08:02,745][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 19:08:02,745][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb0
[2017-05-31 19:08:02,745][ceph_deploy.install][DEBUG ] Detecting platform for host gdb0 ...
[2017-05-31 19:08:03,057][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:08:03,290][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:08:03,290][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:08:03,321][gdb0][DEBUG ] detect machine type
[2017-05-31 19:08:03,324][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:08:03,324][gdb0][INFO  ] Purging Ceph on gdb0
[2017-05-31 19:08:03,326][gdb0][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 19:08:03,647][gdb0][DEBUG ] Reading package lists...
[2017-05-31 19:08:03,761][gdb0][DEBUG ] Building dependency tree...
[2017-05-31 19:08:03,761][gdb0][DEBUG ] Reading state information...
[2017-05-31 19:08:03,825][gdb0][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 19:08:03,825][gdb0][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 19:08:03,825][gdb0][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 19:08:03,825][gdb0][DEBUG ]   ceph-fuse javascript-common libaio1 libcephfs2 libgoogle-perftools4
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   libibverbs1 libjs-jquery libleveldb1v5 liblttng-ust-ctl2 liblttng-ust0
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   libnspr4 libnss3 libnss3-nssdb libopts25 libpython2.7 librados2
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   libradosstriper1 librbd1 librgw2 libsnappy1v5 libtcmalloc-minimal4
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   libunwind8 liburcu4 linux-aws-headers-4.4.0-1013
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 19:08:03,826][gdb0][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 19:08:03,858][gdb0][DEBUG ] The following packages will be REMOVED:
[2017-05-31 19:08:03,858][gdb0][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 19:08:04,173][gdb0][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 19:08:04,173][gdb0][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 19:08:04,538][gdb0][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104462 files and directories currently installed.)
[2017-05-31 19:08:04,539][gdb0][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 19:08:04,753][gdb0][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 19:08:04,817][gdb0][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 19:08:04,881][gdb0][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 19:08:05,146][gdb0][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 19:08:05,260][gdb0][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 19:08:05,425][gdb0][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 19:08:05,539][gdb0][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 19:08:05,703][gdb0][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 19:08:05,818][gdb0][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 19:08:05,982][gdb0][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 19:08:05,998][gdb0][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[2017-05-31 19:08:05,998][gdb0][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-05-31 19:08:06,062][gdb0][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 19:08:06,176][gdb0][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 19:08:06,290][gdb0][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 19:08:06,291][gdb0][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 19:08:06,455][gdb0][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 19:08:07,522][gdb0][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 19:08:33,625][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:08:33,625][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb0
[2017-05-31 19:08:33,625][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:08:33,625][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:08:33,625][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff824f26710>
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  host                          : ['gdb0']
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7ff825833230>
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:08:33,626][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb0
[2017-05-31 19:08:33,866][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:08:34,058][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:08:34,058][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:08:34,074][gdb0][DEBUG ] detect machine type
[2017-05-31 19:08:34,077][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:08:34,302][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:08:34,529][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:08:34,530][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:08:34,545][gdb0][DEBUG ] detect machine type
[2017-05-31 19:08:34,549][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:08:34,549][gdb0][INFO  ] purging data on gdb0
[2017-05-31 19:08:34,550][gdb0][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 19:08:34,564][gdb0][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 19:09:11,389][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:09:11,389][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 19:09:11,389][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:09:11,389][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:09:11,389][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb55bdce518>
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fb55c6e5938>
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:09:11,390][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 19:09:11,626][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:09:11,853][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:09:11,854][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:09:11,869][gdb0][DEBUG ] detect machine type
[2017-05-31 19:09:11,873][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:09:32,698][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:09:32,698][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 19:09:32,698][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fadccfdd908>
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fadcd233aa0>
[2017-05-31 19:09:32,700][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:09:32,700][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:09:32,700][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:09:32,700][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:09:32,938][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:09:33,162][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:09:33,162][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:09:33,178][gdb0][DEBUG ] detect machine type
[2017-05-31 19:09:33,182][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:33,182][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:09:33,182][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 19:09:33,183][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:09:33,185][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 19:09:33,185][gdb0][DEBUG ] create a keyring file
[2017-05-31 19:09:33,187][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 19:09:33,187][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:33,189][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:09:33,410][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:09:33,417][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:09:33,433][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:09:33,449][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:09:33,456][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:09:33,472][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 19:09:33,488][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.3293.tmp
[2017-05-31 19:09:33,489][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.3293.tmp
[2017-05-31 19:09:33,492][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.3293.tmp
[2017-05-31 19:09:38,513][gdb0][INFO  ] checking OSD status...
[2017-05-31 19:09:38,513][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:38,516][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:09:38,882][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 19:09:44,982][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe13f609908>
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe13f85faa0>
[2017-05-31 19:09:44,983][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:09:44,983][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:09:44,983][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:09:44,983][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:09:45,226][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:09:45,453][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:09:45,454][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:09:45,470][gdb0][DEBUG ] detect machine type
[2017-05-31 19:09:45,473][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:45,474][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:09:45,474][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 19:09:45,474][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:09:45,474][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:45,477][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:09:45,597][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:09:45,597][gdb0][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:09:45,597][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:09:45,601][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 19:09:45,601][gdb0][WARNING] activate: OSD uuid is a397040a-60e7-423c-bb3e-a90280753962
[2017-05-31 19:09:45,601][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:09:45,601][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise a397040a-60e7-423c-bb3e-a90280753962
[2017-05-31 19:09:46,518][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.3415.tmp
[2017-05-31 19:09:46,518][gdb0][WARNING] activate: OSD id is 1
[2017-05-31 19:09:46,518][gdb0][WARNING] activate: Initializing OSD...
[2017-05-31 19:09:46,518][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 19:09:46,683][gdb0][WARNING] got monmap epoch 2
[2017-05-31 19:09:46,683][gdb0][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 1 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid a397040a-60e7-423c-bb3e-a90280753962 --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 19:09:46,698][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 19:09:46,699][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 19:09:46,699][gdb0][WARNING] activate: Authorizing OSD key...
[2017-05-31 19:09:46,699][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.1 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 19:09:46,863][gdb0][WARNING] added key for osd.1
[2017-05-31 19:09:46,863][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.3415.tmp
[2017-05-31 19:09:46,863][gdb0][WARNING] activate: ceph osd.1 data dir is ready at /mnt/memstore
[2017-05-31 19:09:46,863][gdb0][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-1 -> /mnt/memstore
[2017-05-31 19:09:46,863][gdb0][WARNING] start_daemon: Starting ceph osd.1...
[2017-05-31 19:09:46,863][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1
[2017-05-31 19:09:46,867][gdb0][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2017-05-31 19:09:46,931][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1 --runtime
[2017-05-31 19:09:46,995][gdb0][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@1
[2017-05-31 19:09:46,995][gdb0][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 19:09:47,027][gdb0][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@1
[2017-05-31 19:09:52,096][gdb0][INFO  ] checking OSD status...
[2017-05-31 19:09:52,096][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:52,099][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:09:52,266][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 00:09:24,868][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8dbf6f80e0>
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f8dc00051b8>
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:09:24,871][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:09:24,871][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-11 00:09:24,871][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-11 00:09:24,871][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-11 00:09:24,871][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-11 00:09:24,907][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:09:24,934][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:09:24,935][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:09:24,951][gdb3][DEBUG ] detect machine type
[2017-06-11 00:09:24,953][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:09:24,953][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-11 00:09:24,955][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-11 00:09:24,994][gdb3][DEBUG ] Reading package lists...
[2017-06-11 00:09:25,159][gdb3][DEBUG ] Building dependency tree...
[2017-06-11 00:09:25,159][gdb3][DEBUG ] Reading state information...
[2017-06-11 00:09:25,223][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-11 00:09:25,223][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-11 00:09:25,223][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-11 00:09:25,223][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-11 00:09:25,223][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-11 00:09:25,223][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws ntp python-blinker python-cephfs
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-06-11 00:09:25,224][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-11 00:09:25,288][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-11 00:09:25,288][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-11 00:09:25,553][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 40 not upgraded.
[2017-06-11 00:09:25,553][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-11 00:09:25,918][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 129562 files and directories currently installed.)
[2017-06-11 00:09:25,918][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-11 00:09:26,134][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-11 00:09:26,198][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-11 00:09:26,262][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-11 00:09:26,477][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-11 00:09:26,591][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-11 00:09:26,759][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-11 00:09:26,823][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-11 00:09:26,823][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-11 00:09:26,991][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-11 00:09:27,105][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-11 00:09:27,106][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-11 00:09:27,270][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-11 00:09:27,270][gdb3][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-06-11 00:09:27,278][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-11 00:09:27,445][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-11 00:09:27,476][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-11 00:09:27,492][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-11 00:09:27,656][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-06-11 00:09:28,473][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-11 00:09:28,641][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0cac286758>
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f0cacb93230>
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:09:28,643][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:09:28,643][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-11 00:09:28,669][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:09:28,683][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:09:28,683][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:09:28,700][gdb3][DEBUG ] detect machine type
[2017-06-11 00:09:28,702][gdb3][DEBUG ] find the location of an executable
[2017-06-11 00:09:28,718][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:09:28,731][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:09:28,732][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:09:28,748][gdb3][DEBUG ] detect machine type
[2017-06-11 00:09:28,751][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:09:28,751][gdb3][INFO  ] purging data on gdb3
[2017-06-11 00:09:28,752][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-11 00:09:28,765][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-11 00:09:28,934][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6dcc48ea28>
[2017-06-11 00:09:28,936][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:09:28,936][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f6dccd51848>
[2017-06-11 00:09:28,936][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:09:28,936][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:10:12,282][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:10:12,282][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-11 00:10:12,282][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:10:12,282][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:10:12,282][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:10:12,282][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3b19d035a8>
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f3b1a387758>
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-11 00:10:12,283][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-11 00:10:12,284][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-11 00:10:12,309][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:10:12,323][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:10:12,323][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:10:12,340][gdb3][DEBUG ] detect machine type
[2017-06-11 00:10:12,342][gdb3][DEBUG ] find the location of an executable
[2017-06-11 00:10:12,344][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-11 00:10:12,355][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-11 00:10:12,361][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-11 00:10:12,361][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-11 00:10:12,361][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-11 00:10:12,361][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-11 00:10:12,362][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-11 00:10:12,362][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-11 00:10:12,362][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-11 00:10:12,362][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-11 00:10:12,528][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:10:12,528][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-11 00:10:12,528][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:10:12,528][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f08add22ea8>
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f08adcf6b18>
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:10:12,530][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-11 00:10:12,530][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-11 00:10:12,556][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:10:12,570][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:10:12,571][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:10:12,587][gdb3][DEBUG ] detect machine type
[2017-06-11 00:10:12,590][gdb3][DEBUG ] find the location of an executable
[2017-06-11 00:10:12,590][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-11 00:10:12,591][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-11 00:10:12,591][gdb3][DEBUG ] get remote short hostname
[2017-06-11 00:10:12,591][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-11 00:10:12,591][gdb3][DEBUG ] get remote short hostname
[2017-06-11 00:10:12,591][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-11 00:10:12,592][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:10:12,593][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-11 00:10:12,594][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 00:10:12,594][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 00:10:12,594][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 00:10:12,594][gdb3][DEBUG ] create the monitor keyring file
[2017-06-11 00:10:12,596][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-11 00:10:12,666][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-11 00:10:12,666][gdb3][DEBUG ] ceph-mon: set fsid to f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:10:12,666][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-11 00:10:12,666][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 00:10:12,667][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-11 00:10:12,667][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-11 00:10:12,668][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 00:10:12,741][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-11 00:10:12,813][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-11 00:10:14,851][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 00:10:14,966][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 00:10:14,966][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-11 00:10:14,966][gdb3][DEBUG ] {
[2017-06-11 00:10:14,966][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-11 00:10:14,966][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]   "features": {
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-11 00:10:14,967][gdb3][DEBUG ]       "kraken", 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]       "luminous"
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     ], 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     "required_mon": [
[2017-06-11 00:10:14,967][gdb3][DEBUG ]       "kraken", 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]       "luminous"
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     ]
[2017-06-11 00:10:14,967][gdb3][DEBUG ]   }, 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]   "monmap": {
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     "created": "2017-06-11 00:10:12.642384", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     "features": {
[2017-06-11 00:10:14,968][gdb3][DEBUG ]       "optional": [], 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]       "persistent": [
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "kraken", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "luminous"
[2017-06-11 00:10:14,968][gdb3][DEBUG ]       ]
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     }, 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     "fsid": "f4f36e31-c61f-42db-97ec-96a4b3bd98b7", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     "modified": "2017-06-11 00:10:12.894445", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     "mons": [
[2017-06-11 00:10:14,968][gdb3][DEBUG ]       {
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "rank": 0
[2017-06-11 00:10:14,969][gdb3][DEBUG ]       }
[2017-06-11 00:10:14,969][gdb3][DEBUG ]     ]
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   }, 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "quorum": [
[2017-06-11 00:10:14,969][gdb3][DEBUG ]     0
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   ], 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "rank": 0, 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "state": "leader", 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "sync_provider": []
[2017-06-11 00:10:14,969][gdb3][DEBUG ] }
[2017-06-11 00:10:14,969][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 00:10:14,969][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-11 00:10:14,970][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 00:10:15,035][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-11 00:10:15,051][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:10:15,064][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:10:15,065][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:10:15,081][gdb3][DEBUG ] detect machine type
[2017-06-11 00:10:15,084][gdb3][DEBUG ] find the location of an executable
[2017-06-11 00:10:15,085][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 00:10:15,150][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-11 00:10:15,150][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-11 00:10:15,150][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-11 00:10:15,152][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmp2yNmqZ
[2017-06-11 00:10:15,167][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:10:15,181][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:10:15,181][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:10:15,197][gdb3][DEBUG ] detect machine type
[2017-06-11 00:10:15,200][gdb3][DEBUG ] get remote short hostname
[2017-06-11 00:10:15,200][gdb3][DEBUG ] fetch remote file
[2017-06-11 00:10:15,201][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 00:10:15,267][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-11 00:10:15,433][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-11 00:10:15,599][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-11 00:10:15,765][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-11 00:10:15,932][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-11 00:10:16,098][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-11 00:10:16,264][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-11 00:10:16,430][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-11 00:10:16,596][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-11 00:10:16,762][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-11 00:10:16,928][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-11 00:10:16,928][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-11 00:10:16,929][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170611001016'
[2017-06-11 00:10:16,929][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-11 00:10:16,929][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-11 00:10:16,929][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-11 00:10:16,929][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp2yNmqZ
[2017-06-11 00:10:17,112][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff2a565f560>
[2017-06-11 00:10:17,113][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:10:17,113][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-11 00:10:17,113][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7ff2a5f76938>
[2017-06-11 00:10:17,113][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:10:17,113][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:10:17,113][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-11 00:10:17,139][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:10:17,153][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:10:17,154][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:10:17,170][gdb3][DEBUG ] detect machine type
[2017-06-11 00:10:17,173][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:12:58,042][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fab03053560>
[2017-06-11 00:12:58,043][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:12:58,043][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:12:58,043][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fab0396a938>
[2017-06-11 00:12:58,043][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:12:58,043][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:12:58,043][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:12:58,293][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:12:58,514][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:12:58,514][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:12:58,532][gdb0][DEBUG ] detect machine type
[2017-06-11 00:12:58,536][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:12:58,707][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe64d0ea950>
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe64d340aa0>
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:12:58,709][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:12:58,950][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:12:59,177][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:12:59,178][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:12:59,194][gdb0][DEBUG ] detect machine type
[2017-06-11 00:12:59,198][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:12:59,199][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:12:59,199][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:12:59,199][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:12:59,202][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:12:59,202][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:12:59,204][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:12:59,325][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:12:59,332][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:12:59,348][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:12:59,356][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:12:59,371][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:12:59,379][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-11 00:13:04,400][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:13:04,400][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:13:04,403][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:13:04,618][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-11 00:13:04,785][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:13:04,785][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fece470a950>
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fece4960aa0>
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:13:04,787][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:13:05,030][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:13:05,262][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:13:05,263][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:13:05,278][gdb0][DEBUG ] detect machine type
[2017-06-11 00:13:05,282][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:13:05,283][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:13:05,283][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:13:05,283][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:13:05,283][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:13:05,285][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:13:05,406][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:13:05,406][gdb0][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-06-11 00:13:05,406][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:13:05,414][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:13:05,414][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:13:05,414][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:13:05,414][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:13:05,414][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:13:05,415][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:13:05,415][gdb0][WARNING]     args.func(args)
[2017-06-11 00:13:05,415][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-06-11 00:13:05,415][gdb0][WARNING]     init=args.mark_init,
[2017-06-11 00:13:05,415][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-11 00:13:05,415][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-06-11 00:13:05,415][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-06-11 00:13:05,415][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-06-11 00:13:05,415][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-06-11 00:13:05,424][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:13:05,424][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:14:15,105][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5b4b05c560>
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f5b4b973938>
[2017-06-11 00:14:15,107][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:15,107][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:15,107][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:14:15,342][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:15,538][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:15,539][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:15,554][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:15,558][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:14:15,730][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:15,730][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f61228b6950>
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:15,732][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:14:15,732][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6122b0caa0>
[2017-06-11 00:14:15,732][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:15,732][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:15,732][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:14:15,732][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:14:15,979][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:16,214][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:16,215][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:16,231][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:16,235][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:16,236][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:14:16,236][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:14:16,236][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:14:16,238][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-06-11 00:14:16,238][gdb0][DEBUG ] create a keyring file
[2017-06-11 00:14:16,240][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:14:16,240][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:16,242][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:14:16,362][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:14:16,370][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:14:16,386][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:14:16,401][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:14:16,409][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:14:16,425][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-11 00:14:21,433][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:14:21,434][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:21,436][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:14:21,602][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-11 00:14:21,767][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:21,767][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:14:21,767][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:21,767][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f87f2ff2950>
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f87f3248aa0>
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:14:21,768][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:14:22,011][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:22,242][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:22,242][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:22,258][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:22,262][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:22,263][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:14:22,264][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:14:22,264][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:14:22,264][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:22,267][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:14:22,387][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:14:22,387][gdb0][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-06-11 00:14:22,387][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:14:22,395][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:14:22,395][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:14:22,395][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:14:22,395][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:14:22,395][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:14:22,395][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:14:22,395][gdb0][WARNING]     args.func(args)
[2017-06-11 00:14:22,396][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-06-11 00:14:22,396][gdb0][WARNING]     init=args.mark_init,
[2017-06-11 00:14:22,396][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-11 00:14:22,396][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-06-11 00:14:22,396][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-06-11 00:14:22,396][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-06-11 00:14:22,396][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-06-11 00:14:22,404][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:14:22,404][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:14:32,489][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc337d4e560>
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fc338665938>
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:32,490][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:14:32,734][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:32,966][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:32,966][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:32,982][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:32,986][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:14:33,166][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efdceb02950>
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7efdced58aa0>
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:14:33,168][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:14:33,411][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:33,639][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:33,639][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:33,655][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:33,659][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:33,660][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:14:33,660][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:14:33,661][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:14:33,663][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:14:33,663][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:33,665][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:14:33,786][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:14:33,786][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:14:33,786][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:14:33,786][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:14:33,786][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:14:33,786][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:14:33,786][gdb0][WARNING]     args.func(args)
[2017-06-11 00:14:33,786][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-06-11 00:14:33,787][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-06-11 00:14:33,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-06-11 00:14:33,787][gdb0][WARNING]     return PrepareFilestore(args)
[2017-06-11 00:14:33,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-06-11 00:14:33,787][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-06-11 00:14:33,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-06-11 00:14:33,787][gdb0][WARNING]     self.set_type()
[2017-06-11 00:14:33,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-06-11 00:14:33,787][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-06-11 00:14:33,787][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-06-11 00:14:33,788][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:14:33,788][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:14:33,788][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-06-11 00:14:33,961][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:33,961][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:14:33,961][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f82706f4950>
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f827094aaa0>
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:14:33,963][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:14:34,203][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:34,433][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:34,434][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:34,450][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:34,454][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:34,455][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:14:34,455][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:14:34,455][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:14:34,455][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:34,457][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:14:34,577][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:14:34,578][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:14:34,578][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:14:34,578][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:14:34,578][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:14:34,578][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:14:34,578][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:14:34,578][gdb0][WARNING]     args.func(args)
[2017-06-11 00:14:34,578][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3684, in main_activate
[2017-06-11 00:14:34,578][gdb0][WARNING]     raise Error('%s does not exist' % args.path)
[2017-06-11 00:14:34,578][gdb0][WARNING] ceph_disk.main.Error: Error: /mnt/memstore does not exist
[2017-06-11 00:14:34,579][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:14:34,579][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:15:11,999][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:11,999][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:15:11,999][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f02dd89e560>
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f02de1b5938>
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:12,000][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:15:12,243][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:12,474][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:12,475][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:12,491][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:12,495][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:15:12,667][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa670dcb950>
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:12,669][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:15:12,669][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa671021aa0>
[2017-06-11 00:15:12,669][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:12,669][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:12,669][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:15:12,669][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:15:12,911][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:13,142][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:13,143][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:13,159][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:13,163][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:13,164][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:15:13,164][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:15:13,164][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:15:13,167][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:15:13,167][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:13,169][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:15:13,289][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:15:13,290][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:15:13,290][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:15:13,290][gdb0][WARNING]     args.func(args)
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-06-11 00:15:13,290][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-06-11 00:15:13,290][gdb0][WARNING]     return PrepareFilestore(args)
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-06-11 00:15:13,291][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-06-11 00:15:13,291][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-06-11 00:15:13,291][gdb0][WARNING]     self.set_type()
[2017-06-11 00:15:13,291][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-06-11 00:15:13,291][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-06-11 00:15:13,291][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-06-11 00:15:13,291][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:15:13,291][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:15:13,292][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-06-11 00:15:13,464][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:13,464][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:15:13,464][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:13,464][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8a8a923950>
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8a8ab79aa0>
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:15:13,465][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:15:13,711][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:13,942][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:13,943][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:13,959][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:13,963][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:13,963][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:15:13,964][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:15:13,964][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:15:13,964][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:13,966][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:15:14,086][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:15:14,087][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:15:14,087][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:15:14,087][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:15:14,087][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:15:14,087][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:15:14,087][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:15:14,087][gdb0][WARNING]     args.func(args)
[2017-06-11 00:15:14,087][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3684, in main_activate
[2017-06-11 00:15:14,087][gdb0][WARNING]     raise Error('%s does not exist' % args.path)
[2017-06-11 00:15:14,087][gdb0][WARNING] ceph_disk.main.Error: Error: /mnt/memstore does not exist
[2017-06-11 00:15:14,089][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:15:14,089][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:15:33,532][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0a4f92d560>
[2017-06-11 00:15:33,533][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:33,533][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:15:33,533][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f0a50244938>
[2017-06-11 00:15:33,533][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:33,533][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:33,533][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:15:33,775][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:34,002][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:34,003][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:34,019][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:34,022][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:15:34,194][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f030023f950>
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0300495aa0>
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:15:34,196][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:15:34,439][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:34,670][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:34,671][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:34,687][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:34,690][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:34,691][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:15:34,691][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:15:34,692][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:15:34,694][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-06-11 00:15:34,694][gdb0][DEBUG ] create a keyring file
[2017-06-11 00:15:34,696][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:15:34,696][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:34,698][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:15:34,818][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:15:34,818][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:15:34,818][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:15:34,818][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:15:34,819][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:15:34,819][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:15:34,819][gdb0][WARNING]     args.func(args)
[2017-06-11 00:15:34,819][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-06-11 00:15:34,819][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-06-11 00:15:34,819][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-06-11 00:15:34,819][gdb0][WARNING]     return PrepareFilestore(args)
[2017-06-11 00:15:34,819][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-06-11 00:15:34,819][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-06-11 00:15:34,819][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-06-11 00:15:34,820][gdb0][WARNING]     self.set_type()
[2017-06-11 00:15:34,820][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-06-11 00:15:34,820][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-06-11 00:15:34,820][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-06-11 00:15:34,820][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:15:34,820][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:15:34,820][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-06-11 00:15:34,993][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6aa5e11950>
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6aa6067aa0>
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:15:34,994][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:15:35,239][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:35,462][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:35,463][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:35,479][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:35,483][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:35,483][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:15:35,484][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:15:35,484][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:15:35,484][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:35,486][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:15:35,606][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:15:35,606][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:15:35,606][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:15:35,606][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:15:35,606][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:15:35,606][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:15:35,607][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:15:35,607][gdb0][WARNING]     args.func(args)
[2017-06-11 00:15:35,607][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3684, in main_activate
[2017-06-11 00:15:35,607][gdb0][WARNING]     raise Error('%s does not exist' % args.path)
[2017-06-11 00:15:35,607][gdb0][WARNING] ceph_disk.main.Error: Error: /mnt/memstore does not exist
[2017-06-11 00:15:35,607][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:15:35,607][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:16:29,599][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:16:29,599][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:16:29,599][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:16:29,599][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb9eadbd560>
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fb9eb6d4938>
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:16:29,600][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:16:29,839][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:16:30,071][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:16:30,071][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:16:30,088][gdb0][DEBUG ] detect machine type
[2017-06-11 00:16:30,092][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:16:30,265][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f68f0624950>
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f68f087aaa0>
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:16:30,267][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:16:30,507][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:16:30,734][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:16:30,735][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:16:30,751][gdb0][DEBUG ] detect machine type
[2017-06-11 00:16:30,755][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:16:30,756][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:16:30,756][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:16:30,756][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:16:30,758][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:16:30,759][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:16:30,761][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:16:30,881][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:16:30,889][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:16:30,904][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:16:30,912][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:16:30,928][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:16:30,943][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-06-11 00:16:30,943][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.5726.tmp
[2017-06-11 00:16:30,944][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.5726.tmp
[2017-06-11 00:16:30,951][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.5726.tmp
[2017-06-11 00:16:35,964][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:16:35,964][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:16:35,967][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:16:36,132][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-11 00:16:36,297][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:16:36,297][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:16:36,297][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:16:36,297][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:16:36,297][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:16:36,297][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fedbb592950>
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fedbb7e8aa0>
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:16:36,298][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:16:36,543][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:16:36,774][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:16:36,775][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:16:36,791][gdb0][DEBUG ] detect machine type
[2017-06-11 00:16:36,795][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:16:36,796][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:16:36,796][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:16:36,796][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:16:36,796][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:16:36,798][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:16:36,918][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:16:36,918][gdb0][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:16:36,919][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:16:36,926][gdb0][WARNING] activate: Cluster name is ceph
[2017-06-11 00:16:36,926][gdb0][WARNING] activate: OSD uuid is db0fe94b-dc3b-488f-b03f-a33e5706669c
[2017-06-11 00:16:36,926][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-11 00:16:36,926][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise db0fe94b-dc3b-488f-b03f-a33e5706669c
[2017-06-11 00:16:37,091][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.5843.tmp
[2017-06-11 00:16:37,091][gdb0][WARNING] activate: OSD id is 0
[2017-06-11 00:16:37,091][gdb0][WARNING] activate: Initializing OSD...
[2017-06-11 00:16:37,091][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-06-11 00:16:37,255][gdb0][WARNING] got monmap epoch 2
[2017-06-11 00:16:37,256][gdb0][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid db0fe94b-dc3b-488f-b03f-a33e5706669c --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-06-11 00:16:37,271][gdb0][WARNING] activate: Marking with init system systemd
[2017-06-11 00:16:37,271][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-06-11 00:16:37,273][gdb0][WARNING] activate: Authorizing OSD key...
[2017-06-11 00:16:37,273][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-06-11 00:16:37,437][gdb0][WARNING] added key for osd.0
[2017-06-11 00:16:37,438][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.5843.tmp
[2017-06-11 00:16:37,438][gdb0][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-06-11 00:16:37,438][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:16:37,438][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:16:37,438][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:16:37,438][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:16:37,438][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:16:37,438][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:16:37,438][gdb0][WARNING]     args.func(args)
[2017-06-11 00:16:37,438][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-06-11 00:16:37,438][gdb0][WARNING]     init=args.mark_init,
[2017-06-11 00:16:37,438][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3543, in activate_dir
[2017-06-11 00:16:37,439][gdb0][WARNING]     old = os.readlink(canonical)
[2017-06-11 00:16:37,439][gdb0][WARNING] OSError: [Errno 22] Invalid argument: '/var/lib/ceph/osd/ceph-0'
[2017-06-11 00:16:37,446][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:16:37,446][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:17:02,077][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:17:02,077][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:17:02,077][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:17:02,077][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:17:02,077][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:17:02,077][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faa27ff3560>
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7faa2890a938>
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:17:02,078][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:17:02,319][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:17:02,551][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:17:02,552][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:17:02,568][gdb0][DEBUG ] detect machine type
[2017-06-11 00:17:02,572][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:17:02,745][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:17:02,745][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:17:02,745][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:17:02,745][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:17:02,745][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9f7325f950>
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9f734b5aa0>
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:17:02,747][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:17:02,987][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:17:03,214][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:17:03,214][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:17:03,230][gdb0][DEBUG ] detect machine type
[2017-06-11 00:17:03,234][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:17:03,235][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:17:03,235][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:17:03,236][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:17:03,238][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-06-11 00:17:03,238][gdb0][DEBUG ] create a keyring file
[2017-06-11 00:17:03,240][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:17:03,240][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:17:03,242][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:17:03,363][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:17:03,370][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:17:03,386][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:17:03,402][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:17:03,409][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:17:03,425][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-11 00:17:08,432][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:17:08,432][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:17:08,435][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:17:08,600][gdb0][WARNING] there is 1 OSD down
[2017-06-11 00:17:08,600][gdb0][WARNING] there is 1 OSD out
[2017-06-11 00:17:08,600][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-11 00:17:08,766][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:17:08,766][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:17:08,766][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f48d6cca950>
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f48d6f20aa0>
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:17:08,768][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:17:09,007][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:17:09,238][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:17:09,239][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:17:09,255][gdb0][DEBUG ] detect machine type
[2017-06-11 00:17:09,259][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:17:09,260][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:17:09,260][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:17:09,260][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:17:09,260][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:17:09,262][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:17:09,382][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:17:09,383][gdb0][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:17:09,383][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:17:09,390][gdb0][WARNING] activate: Cluster name is ceph
[2017-06-11 00:17:09,391][gdb0][WARNING] activate: OSD uuid is db0fe94b-dc3b-488f-b03f-a33e5706669c
[2017-06-11 00:17:09,391][gdb0][WARNING] activate: OSD id is 0
[2017-06-11 00:17:09,391][gdb0][WARNING] activate: Marking with init system systemd
[2017-06-11 00:17:09,391][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-06-11 00:17:09,391][gdb0][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-06-11 00:17:09,392][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:17:09,392][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:17:09,392][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:17:09,392][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:17:09,392][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:17:09,392][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:17:09,392][gdb0][WARNING]     args.func(args)
[2017-06-11 00:17:09,393][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-06-11 00:17:09,393][gdb0][WARNING]     init=args.mark_init,
[2017-06-11 00:17:09,393][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3543, in activate_dir
[2017-06-11 00:17:09,393][gdb0][WARNING]     old = os.readlink(canonical)
[2017-06-11 00:17:09,393][gdb0][WARNING] OSError: [Errno 22] Invalid argument: '/var/lib/ceph/osd/ceph-0'
[2017-06-11 00:17:09,401][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:17:09,401][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:19:30,321][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:19:30,321][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:19:30,321][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5a94088560>
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f5a9499f938>
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:19:30,322][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:19:30,564][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:19:30,795][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:19:30,795][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:19:30,811][gdb0][DEBUG ] detect machine type
[2017-06-11 00:19:30,815][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:19:30,987][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:19:30,987][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb54454f950>
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:19:30,989][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb5447a5aa0>
[2017-06-11 00:19:30,989][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:19:30,989][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:19:30,989][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:19:30,989][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:19:31,227][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:19:31,458][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:19:31,458][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:19:31,474][gdb0][DEBUG ] detect machine type
[2017-06-11 00:19:31,478][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:31,479][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:19:31,479][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:19:31,479][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:19:31,482][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:19:31,482][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:31,485][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:19:31,605][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:19:31,613][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:19:31,628][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:19:31,636][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:19:31,651][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:19:31,667][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-11 00:19:36,676][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:19:36,676][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:36,679][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:19:36,844][gdb0][WARNING] there is 1 OSD down
[2017-06-11 00:19:36,844][gdb0][WARNING] there is 1 OSD out
[2017-06-11 00:19:36,844][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-11 00:19:37,009][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd7485dd950>
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd748833aa0>
[2017-06-11 00:19:37,011][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:19:37,011][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:19:37,011][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:19:37,011][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:19:37,255][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:19:37,482][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:19:37,482][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:19:37,499][gdb0][DEBUG ] detect machine type
[2017-06-11 00:19:37,502][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:37,503][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:19:37,503][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:19:37,504][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:19:37,504][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:37,506][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:19:37,626][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:19:37,626][gdb0][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:19:37,627][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:19:37,634][gdb0][WARNING] activate: Cluster name is ceph
[2017-06-11 00:19:37,634][gdb0][WARNING] activate: OSD uuid is db0fe94b-dc3b-488f-b03f-a33e5706669c
[2017-06-11 00:19:37,634][gdb0][WARNING] activate: OSD id is 0
[2017-06-11 00:19:37,634][gdb0][WARNING] activate: Marking with init system systemd
[2017-06-11 00:19:37,634][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-06-11 00:19:37,634][gdb0][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-06-11 00:19:37,634][gdb0][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-0 -> /mnt/memstore
[2017-06-11 00:19:37,635][gdb0][WARNING] start_daemon: Starting ceph osd.0...
[2017-06-11 00:19:37,635][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-06-11 00:19:37,638][gdb0][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-06-11 00:19:37,702][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-06-11 00:19:37,766][gdb0][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-06-11 00:19:37,766][gdb0][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-06-11 00:19:37,830][gdb0][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-06-11 00:19:42,900][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:19:42,900][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:42,903][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:19:43,070][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 00:21:23,142][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f36ae470560>
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f36aed87938>
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:21:23,143][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 00:21:23,444][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:21:23,679][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:21:23,679][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:21:23,697][gdb1][DEBUG ] detect machine type
[2017-06-11 00:21:23,700][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:21:23,871][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:21:23,871][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-06-11 00:21:23,871][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fabd4586950>
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:21:23,873][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fabd47dcaa0>
[2017-06-11 00:21:23,873][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:21:23,873][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:21:23,873][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:21:23,873][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-06-11 00:21:24,119][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:21:24,314][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:21:24,314][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:21:24,331][gdb1][DEBUG ] detect machine type
[2017-06-11 00:21:24,335][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:21:24,335][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:21:24,335][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-11 00:21:24,336][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:21:24,339][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-06-11 00:21:24,339][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:21:24,341][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:21:24,511][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:21:24,512][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:21:24,519][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:21:24,535][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:21:24,542][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:21:24,558][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-06-11 00:21:24,558][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.918.tmp
[2017-06-11 00:21:24,561][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.918.tmp
[2017-06-11 00:21:24,577][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.918.tmp
[2017-06-11 00:21:29,598][gdb1][INFO  ] checking OSD status...
[2017-06-11 00:21:29,598][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:21:29,601][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:21:29,917][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-11 00:21:30,082][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f74b7ac8950>
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f74b7d1eaa0>
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-11 00:21:30,084][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-06-11 00:21:30,327][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:21:30,554][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:21:30,555][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:21:30,571][gdb1][DEBUG ] detect machine type
[2017-06-11 00:21:30,575][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:21:30,576][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:21:30,576][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-06-11 00:21:30,576][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:21:30,576][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:21:30,578][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:21:30,749][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:21:30,749][gdb1][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:21:30,749][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:21:30,749][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-11 00:21:30,749][gdb1][WARNING] activate: OSD uuid is 83154121-02fd-4e57-a1d6-f080dd469712
[2017-06-11 00:21:30,749][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-11 00:21:30,749][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 83154121-02fd-4e57-a1d6-f080dd469712
[2017-06-11 00:21:30,781][gdb1][WARNING] Traceback (most recent call last):
[2017-06-11 00:21:30,781][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-11 00:21:30,781][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:21:30,781][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:21:30,782][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:21:30,782][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-06-11 00:21:30,782][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-11 00:21:30,782][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-06-11 00:21:30,782][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-06-11 00:21:30,782][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-06-11 00:21:30.770633 7f770c53c700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-06-11 00:21:30,782][gdb1][WARNING] error connecting to the cluster
[2017-06-11 00:21:30,782][gdb1][WARNING] 
[2017-06-11 00:21:30,790][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:21:30,790][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:21:58,875][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:21:58,875][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 00:21:58,875][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:21:58,875][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:21:58,875][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:21:58,875][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff9b6787560>
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7ff9b709e938>
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:21:58,876][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 00:21:59,119][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:21:59,347][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:21:59,347][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:21:59,363][gdb1][DEBUG ] detect machine type
[2017-06-11 00:21:59,367][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:21:59,542][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1bdb3b9950>
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1bdb60faa0>
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:21:59,544][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-06-11 00:21:59,787][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:22:00,023][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:22:00,023][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:22:00,040][gdb1][DEBUG ] detect machine type
[2017-06-11 00:22:00,044][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:00,045][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:22:00,045][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-11 00:22:00,045][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:22:00,047][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-11 00:22:00,048][gdb1][DEBUG ] create a keyring file
[2017-06-11 00:22:00,049][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-06-11 00:22:00,049][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:00,051][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:22:00,222][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:22:00,222][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:22:00,222][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:22:00,222][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:22:00,226][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:22:00,241][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-11 00:22:05,250][gdb1][INFO  ] checking OSD status...
[2017-06-11 00:22:05,250][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:05,253][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:22:05,418][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-11 00:22:05,583][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2dce406950>
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2dce65caa0>
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-11 00:22:05,585][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-06-11 00:22:05,823][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:22:06,051][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:22:06,051][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:22:06,068][gdb1][DEBUG ] detect machine type
[2017-06-11 00:22:06,072][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:06,073][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:22:06,073][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-06-11 00:22:06,073][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:22:06,073][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:06,075][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:22:06,245][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:22:06,246][gdb1][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:22:06,246][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:22:06,246][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-11 00:22:06,246][gdb1][WARNING] activate: OSD uuid is 83154121-02fd-4e57-a1d6-f080dd469712
[2017-06-11 00:22:06,246][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-11 00:22:06,246][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 83154121-02fd-4e57-a1d6-f080dd469712
[2017-06-11 00:22:06,511][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.1382.tmp
[2017-06-11 00:22:06,511][gdb1][WARNING] activate: OSD id is 1
[2017-06-11 00:22:06,511][gdb1][WARNING] activate: Initializing OSD...
[2017-06-11 00:22:06,511][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-06-11 00:22:06,625][gdb1][WARNING] got monmap epoch 2
[2017-06-11 00:22:06,625][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 1 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid 83154121-02fd-4e57-a1d6-f080dd469712 --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-06-11 00:22:06,657][gdb1][WARNING] activate: Marking with init system systemd
[2017-06-11 00:22:06,657][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-06-11 00:22:06,657][gdb1][WARNING] activate: Authorizing OSD key...
[2017-06-11 00:22:06,657][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.1 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-06-11 00:22:06,822][gdb1][WARNING] added key for osd.1
[2017-06-11 00:22:06,822][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.1382.tmp
[2017-06-11 00:22:06,822][gdb1][WARNING] activate: ceph osd.1 data dir is ready at /mnt/memstore
[2017-06-11 00:22:06,822][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-1 -> /mnt/memstore
[2017-06-11 00:22:06,822][gdb1][WARNING] start_daemon: Starting ceph osd.1...
[2017-06-11 00:22:06,822][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1
[2017-06-11 00:22:06,823][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2017-06-11 00:22:06,987][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1 --runtime
[2017-06-11 00:22:07,019][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@1
[2017-06-11 00:22:07,019][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /lib/systemd/system/ceph-osd@.service.
[2017-06-11 00:22:07,083][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@1
[2017-06-11 00:22:12,152][gdb1][INFO  ] checking OSD status...
[2017-06-11 00:22:12,152][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:12,155][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:22:12,322][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 20:16:32,118][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1a4165cfc8>
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f1a41f6f1b8>
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:16:32,119][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-11 20:16:32,119][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-11 20:16:32,119][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-11 20:16:32,119][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-11 20:16:32,146][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:16:32,160][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:16:32,161][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:16:32,177][gdb3][DEBUG ] detect machine type
[2017-06-11 20:16:32,180][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:16:32,180][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-11 20:16:32,181][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-11 20:16:32,219][gdb3][DEBUG ] Reading package lists...
[2017-06-11 20:16:32,383][gdb3][DEBUG ] Building dependency tree...
[2017-06-11 20:16:32,384][gdb3][DEBUG ] Reading state information...
[2017-06-11 20:16:32,448][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-11 20:16:32,448][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-11 20:16:32,448][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-11 20:16:32,448][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-11 20:16:32,448][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-11 20:16:32,448][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws ntp python-blinker python-cephfs
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-06-11 20:16:32,450][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-11 20:16:32,481][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-11 20:16:32,482][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-11 20:16:32,646][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 40 not upgraded.
[2017-06-11 20:16:32,646][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-11 20:16:32,654][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 129562 files and directories currently installed.)
[2017-06-11 20:16:32,657][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-11 20:16:32,822][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-11 20:16:32,936][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-11 20:16:32,968][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-11 20:16:33,182][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-11 20:16:33,296][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-11 20:16:33,461][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-11 20:16:33,524][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-11 20:16:33,532][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-11 20:16:33,696][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-11 20:16:33,761][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-11 20:16:33,777][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-11 20:16:33,943][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-11 20:16:34,007][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-11 20:16:34,121][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-11 20:16:34,235][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-11 20:16:34,235][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-11 20:16:34,299][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-06-11 20:16:35,166][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-11 20:16:35,332][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:16:35,332][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe142a53710>
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fe143360230>
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:16:35,333][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-11 20:16:35,359][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:16:35,373][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:16:35,374][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:16:35,390][gdb3][DEBUG ] detect machine type
[2017-06-11 20:16:35,393][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:16:35,408][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:16:35,423][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:16:35,423][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:16:35,440][gdb3][DEBUG ] detect machine type
[2017-06-11 20:16:35,442][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:16:35,442][gdb3][INFO  ] purging data on gdb3
[2017-06-11 20:16:35,443][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-11 20:16:35,456][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-11 20:16:35,627][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc0f7f579e0>
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fc0f881a848>
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:17:04,392][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feb54d2b560>
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7feb553af758>
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-11 20:17:04,393][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-11 20:17:04,393][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-11 20:17:04,420][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:17:04,434][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:17:04,435][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:17:04,451][gdb3][DEBUG ] detect machine type
[2017-06-11 20:17:04,454][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:17:04,455][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-11 20:17:04,466][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-11 20:17:04,473][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-11 20:17:04,474][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-11 20:17:04,639][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe6754a9e60>
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fe67547eb18>
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:17:04,641][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-11 20:17:04,641][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:17:04,641][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-11 20:17:04,641][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-11 20:17:04,668][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:17:04,682][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:17:04,683][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:17:04,700][gdb3][DEBUG ] detect machine type
[2017-06-11 20:17:04,702][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:17:04,702][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-11 20:17:04,703][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-11 20:17:04,703][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:17:04,703][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-11 20:17:04,703][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:17:04,703][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-11 20:17:04,704][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:17:04,705][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-11 20:17:04,706][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 20:17:04,706][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 20:17:04,706][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 20:17:04,707][gdb3][DEBUG ] create the monitor keyring file
[2017-06-11 20:17:04,708][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-11 20:17:04,746][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-11 20:17:04,746][gdb3][DEBUG ] ceph-mon: set fsid to 9ccb7614-4874-4746-9634-d8f0080c96da
[2017-06-11 20:17:04,749][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-11 20:17:04,751][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 20:17:04,751][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-11 20:17:04,752][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-11 20:17:04,753][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 20:17:04,824][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-11 20:17:04,897][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-11 20:17:06,935][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:17:07,000][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:17:07,000][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-11 20:17:07,000][gdb3][DEBUG ] {
[2017-06-11 20:17:07,000][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-11 20:17:07,000][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-11 20:17:07,000][gdb3][DEBUG ]   "features": {
[2017-06-11 20:17:07,000][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-11 20:17:07,000][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-11 20:17:07,000][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:17:07,000][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     ], 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     "required_mon": [
[2017-06-11 20:17:07,001][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     ]
[2017-06-11 20:17:07,001][gdb3][DEBUG ]   }, 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]   "monmap": {
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     "created": "2017-06-11 20:17:04.732069", 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     "features": {
[2017-06-11 20:17:07,001][gdb3][DEBUG ]       "optional": [], 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]       "persistent": [
[2017-06-11 20:17:07,001][gdb3][DEBUG ]         "kraken", 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]         "luminous"
[2017-06-11 20:17:07,001][gdb3][DEBUG ]       ]
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     }, 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]     "fsid": "9ccb7614-4874-4746-9634-d8f0080c96da", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]     "modified": "2017-06-11 20:17:04.974670", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]     "mons": [
[2017-06-11 20:17:07,002][gdb3][DEBUG ]       {
[2017-06-11 20:17:07,002][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]         "rank": 0
[2017-06-11 20:17:07,002][gdb3][DEBUG ]       }
[2017-06-11 20:17:07,002][gdb3][DEBUG ]     ]
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   }, 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   "quorum": [
[2017-06-11 20:17:07,002][gdb3][DEBUG ]     0
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   ], 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   "rank": 0, 
[2017-06-11 20:17:07,003][gdb3][DEBUG ]   "state": "leader", 
[2017-06-11 20:17:07,003][gdb3][DEBUG ]   "sync_provider": []
[2017-06-11 20:17:07,003][gdb3][DEBUG ] }
[2017-06-11 20:17:07,003][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:17:07,003][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-11 20:17:07,004][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:17:07,069][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-11 20:17:07,086][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:17:07,100][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:17:07,101][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:17:07,118][gdb3][DEBUG ] detect machine type
[2017-06-11 20:17:07,120][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:17:07,121][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:17:07,186][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-11 20:17:07,187][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-11 20:17:07,187][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-11 20:17:07,189][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpl8OhV4
[2017-06-11 20:17:07,204][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:17:07,218][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:17:07,219][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:17:07,235][gdb3][DEBUG ] detect machine type
[2017-06-11 20:17:07,237][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:17:07,238][gdb3][DEBUG ] fetch remote file
[2017-06-11 20:17:07,239][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:17:07,305][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-11 20:17:07,471][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-11 20:17:07,688][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-11 20:17:07,854][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-11 20:17:08,020][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-11 20:17:08,186][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-11 20:17:08,353][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-11 20:17:08,519][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-11 20:17:08,685][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-11 20:17:08,851][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-11 20:17:09,017][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-11 20:17:09,017][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-11 20:17:09,017][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170611201709'
[2017-06-11 20:17:09,018][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-11 20:17:09,018][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-11 20:17:09,018][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-11 20:17:09,018][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpl8OhV4
[2017-06-11 20:17:09,198][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2dac57c518>
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f2dace93938>
[2017-06-11 20:17:09,200][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:17:09,200][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:17:09,200][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-11 20:17:09,226][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:17:09,240][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:17:09,241][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:17:09,257][gdb3][DEBUG ] detect machine type
[2017-06-11 20:17:09,259][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:26:23,481][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:26:23,481][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-11 20:26:23,481][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:26:23,481][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:26:23,481][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcf6f01d560>
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fcf6f6a1758>
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:26:23,482][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-11 20:26:23,482][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-11 20:26:23,483][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-11 20:26:23,509][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:26:23,524][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:26:23,524][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:26:23,541][gdb3][DEBUG ] detect machine type
[2017-06-11 20:26:23,544][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:26:23,545][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-11 20:26:23,556][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-11 20:26:23,562][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-11 20:26:23,562][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-11 20:26:23,563][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-11 20:26:23,563][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-11 20:26:23,563][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-11 20:26:23,563][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-11 20:26:23,563][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-11 20:26:23,563][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-11 20:26:23,729][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:26:23,729][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-11 20:26:23,729][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:26:23,729][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:26:23,729][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:26:23,729][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:26:23,729][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-11 20:26:23,729][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:26:23,729][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7084e4ae60>
[2017-06-11 20:26:23,730][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:26:23,730][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f7084e1fb18>
[2017-06-11 20:26:23,730][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:26:23,730][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-11 20:26:23,730][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:26:23,730][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-11 20:26:23,731][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-11 20:26:23,757][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:26:23,771][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:26:23,772][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:26:23,788][gdb3][DEBUG ] detect machine type
[2017-06-11 20:26:23,791][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:26:23,791][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-11 20:26:23,791][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-11 20:26:23,791][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:26:23,792][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-11 20:26:23,792][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:26:23,792][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-11 20:26:23,793][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:26:23,794][ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-06-11 20:26:23,795][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2017-06-11 20:26:23,974][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:26:23,974][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-11 20:26:23,974][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:26:23,974][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:26:23,974][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:26:23,974][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:26:23,974][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:26:23,974][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6075860518>
[2017-06-11 20:26:23,974][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:26:23,975][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-11 20:26:23,975][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f6076177938>
[2017-06-11 20:26:23,975][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:26:23,975][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:26:23,975][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-11 20:26:24,001][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:26:24,015][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:26:24,016][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:26:24,032][gdb3][DEBUG ] detect machine type
[2017-06-11 20:26:24,035][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:26:24,037][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-06-11 20:26:24,037][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-06-11 20:27:06,596][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:27:06,596][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-11 20:27:06,596][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f477a38a560>
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f477aa0e758>
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:27:06,597][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-11 20:27:06,597][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-11 20:27:06,598][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-11 20:27:06,624][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:27:06,638][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:27:06,639][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:27:06,655][gdb3][DEBUG ] detect machine type
[2017-06-11 20:27:06,657][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:27:06,659][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-11 20:27:06,670][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-11 20:27:06,676][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-11 20:27:06,676][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-11 20:27:06,677][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-11 20:27:06,677][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-11 20:27:06,677][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-11 20:27:06,677][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-11 20:27:06,677][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-11 20:27:06,677][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-11 20:27:06,841][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2fd810ae60>
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f2fd80dfb18>
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-11 20:27:06,842][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:27:06,843][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-11 20:27:06,843][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-11 20:27:06,869][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:27:06,883][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:27:06,884][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:27:06,900][gdb3][DEBUG ] detect machine type
[2017-06-11 20:27:06,903][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:27:06,903][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-11 20:27:06,903][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-11 20:27:06,903][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:27:06,904][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-11 20:27:06,904][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:27:06,904][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-11 20:27:06,905][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:27:06,906][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-11 20:27:06,906][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 20:27:06,906][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-11 20:27:06,907][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-11 20:27:06,908][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 20:27:06,979][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-11 20:27:07,048][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-11 20:27:09,061][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:27:09,127][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:27:09,127][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-11 20:27:09,127][gdb3][DEBUG ] {
[2017-06-11 20:27:09,127][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-11 20:27:09,127][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-11 20:27:09,127][gdb3][DEBUG ]   "features": {
[2017-06-11 20:27:09,127][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-11 20:27:09,127][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-11 20:27:09,127][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:27:09,128][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:27:09,128][gdb3][DEBUG ]     ], 
[2017-06-11 20:27:09,128][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-11 20:27:09,128][gdb3][DEBUG ]     "required_mon": [
[2017-06-11 20:27:09,128][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:27:09,128][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:27:09,128][gdb3][DEBUG ]     ]
[2017-06-11 20:27:09,128][gdb3][DEBUG ]   }, 
[2017-06-11 20:27:09,128][gdb3][DEBUG ]   "monmap": {
[2017-06-11 20:27:09,128][gdb3][DEBUG ]     "created": "2017-06-11 20:17:04.732069", 
[2017-06-11 20:27:09,128][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-11 20:27:09,128][gdb3][DEBUG ]     "features": {
[2017-06-11 20:27:09,128][gdb3][DEBUG ]       "optional": [], 
[2017-06-11 20:27:09,128][gdb3][DEBUG ]       "persistent": [
[2017-06-11 20:27:09,128][gdb3][DEBUG ]         "kraken", 
[2017-06-11 20:27:09,128][gdb3][DEBUG ]         "luminous"
[2017-06-11 20:27:09,128][gdb3][DEBUG ]       ]
[2017-06-11 20:27:09,129][gdb3][DEBUG ]     }, 
[2017-06-11 20:27:09,129][gdb3][DEBUG ]     "fsid": "9ccb7614-4874-4746-9634-d8f0080c96da", 
[2017-06-11 20:27:09,129][gdb3][DEBUG ]     "modified": "2017-06-11 20:17:04.974670", 
[2017-06-11 20:27:09,129][gdb3][DEBUG ]     "mons": [
[2017-06-11 20:27:09,129][gdb3][DEBUG ]       {
[2017-06-11 20:27:09,129][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-11 20:27:09,129][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-11 20:27:09,129][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-11 20:27:09,129][gdb3][DEBUG ]         "rank": 0
[2017-06-11 20:27:09,129][gdb3][DEBUG ]       }
[2017-06-11 20:27:09,129][gdb3][DEBUG ]     ]
[2017-06-11 20:27:09,129][gdb3][DEBUG ]   }, 
[2017-06-11 20:27:09,129][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-11 20:27:09,129][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-11 20:27:09,129][gdb3][DEBUG ]   "quorum": [
[2017-06-11 20:27:09,129][gdb3][DEBUG ]     0
[2017-06-11 20:27:09,129][gdb3][DEBUG ]   ], 
[2017-06-11 20:27:09,130][gdb3][DEBUG ]   "rank": 0, 
[2017-06-11 20:27:09,130][gdb3][DEBUG ]   "state": "leader", 
[2017-06-11 20:27:09,130][gdb3][DEBUG ]   "sync_provider": []
[2017-06-11 20:27:09,130][gdb3][DEBUG ] }
[2017-06-11 20:27:09,130][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:27:09,130][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-11 20:27:09,131][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:27:09,196][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-11 20:27:09,211][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:27:09,225][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:27:09,226][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:27:09,242][gdb3][DEBUG ] detect machine type
[2017-06-11 20:27:09,245][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:27:09,246][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:27:09,311][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-11 20:27:09,311][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-11 20:27:09,311][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-11 20:27:09,313][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpxpQHXD
[2017-06-11 20:27:09,328][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:27:09,343][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:27:09,344][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:27:09,361][gdb3][DEBUG ] detect machine type
[2017-06-11 20:27:09,363][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:27:09,363][gdb3][DEBUG ] fetch remote file
[2017-06-11 20:27:09,365][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:27:09,430][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-11 20:27:09,597][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-11 20:27:09,763][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-11 20:27:09,929][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-11 20:27:10,096][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-11 20:27:10,261][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-06-11 20:27:10,261][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-06-11 20:27:10,262][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-06-11 20:27:10,262][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.mon.keyring' and backing up old key as 'ceph.mon.keyring-20170611202710'
[2017-06-11 20:27:10,262][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-06-11 20:27:10,262][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-06-11 20:27:10,262][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpxpQHXD
[2017-06-11 20:27:10,442][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:27:10,442][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-11 20:27:10,442][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:27:10,443][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:27:10,443][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:27:10,443][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:27:10,443][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:27:10,443][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4ca0655518>
[2017-06-11 20:27:10,443][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:27:10,443][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-11 20:27:10,443][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f4ca0f6c938>
[2017-06-11 20:27:10,443][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:27:10,443][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:27:10,443][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-11 20:27:10,470][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:27:10,484][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:27:10,484][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:27:10,501][gdb3][DEBUG ] detect machine type
[2017-06-11 20:27:10,503][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:28:05,019][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:28:05,019][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 20:28:05,019][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:28:05,019][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:28:05,019][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:28:05,019][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:28:05,020][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:28:05,020][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f334e562518>
[2017-06-11 20:28:05,020][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:28:05,020][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 20:28:05,020][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f334ee79938>
[2017-06-11 20:28:05,020][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:28:05,020][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:28:05,020][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 20:28:05,262][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:28:05,467][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:28:05,468][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:28:05,484][gdb1][DEBUG ] detect machine type
[2017-06-11 20:28:05,488][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:33:23,560][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:33:23,560][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 20:33:23,561][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:33:23,561][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:33:23,561][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:33:23,561][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:33:23,561][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:33:23,561][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f446dd22518>
[2017-06-11 20:33:23,561][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:33:23,561][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 20:33:23,561][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f446e639938>
[2017-06-11 20:33:23,561][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:33:23,561][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:33:23,561][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 20:33:23,802][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 20:33:24,034][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 20:33:24,035][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 20:33:24,051][gdb0][DEBUG ] detect machine type
[2017-06-11 20:33:24,054][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:33:26,532][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:33:26,533][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 20:33:26,533][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:33:26,533][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:33:26,533][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:33:26,533][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:33:26,533][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:33:26,533][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f98e57a7518>
[2017-06-11 20:33:26,533][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:33:26,533][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 20:33:26,533][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f98e60be938>
[2017-06-11 20:33:26,533][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:33:26,534][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:33:26,534][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 20:33:26,779][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:33:26,975][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:33:26,975][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:33:26,992][gdb1][DEBUG ] detect machine type
[2017-06-11 20:33:26,996][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:33:45,075][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:33:45,075][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-11 20:33:45,075][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:33:45,075][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f55ba716560>
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f55bad9a758>
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:33:45,076][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-11 20:33:45,076][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-11 20:33:45,077][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-11 20:33:45,103][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:33:45,117][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:33:45,117][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:33:45,134][gdb3][DEBUG ] detect machine type
[2017-06-11 20:33:45,136][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:33:45,137][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-11 20:33:45,149][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-11 20:33:45,155][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-11 20:33:45,155][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-11 20:33:45,155][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-11 20:33:45,155][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-11 20:33:45,155][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-11 20:33:45,156][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-11 20:33:45,156][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-11 20:33:45,156][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-11 20:33:45,321][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:33:45,321][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-06-11 20:33:45,321][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:33:45,321][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:33:45,321][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:33:45,321][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:33:45,322][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-11 20:33:45,322][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:33:45,322][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa1587bae60>
[2017-06-11 20:33:45,322][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:33:45,322][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fa15878fb18>
[2017-06-11 20:33:45,322][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:33:45,322][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-11 20:33:45,322][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:33:45,323][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-11 20:33:45,323][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-11 20:33:45,349][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:33:45,363][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:33:45,363][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:33:45,380][gdb3][DEBUG ] detect machine type
[2017-06-11 20:33:45,382][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:33:45,382][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-11 20:33:45,383][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-11 20:33:45,383][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:33:45,383][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-11 20:33:45,383][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:33:45,383][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-11 20:33:45,384][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:33:45,385][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-11 20:33:45,386][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 20:33:45,386][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-11 20:33:45,386][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-11 20:33:45,388][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 20:33:45,460][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-11 20:33:45,533][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-11 20:33:47,546][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:33:47,611][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:33:47,611][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-11 20:33:47,612][gdb3][DEBUG ] {
[2017-06-11 20:33:47,612][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-11 20:33:47,612][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-11 20:33:47,612][gdb3][DEBUG ]   "features": {
[2017-06-11 20:33:47,612][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-11 20:33:47,612][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-11 20:33:47,612][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:33:47,612][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:33:47,612][gdb3][DEBUG ]     ], 
[2017-06-11 20:33:47,612][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-11 20:33:47,612][gdb3][DEBUG ]     "required_mon": [
[2017-06-11 20:33:47,612][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:33:47,613][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:33:47,613][gdb3][DEBUG ]     ]
[2017-06-11 20:33:47,613][gdb3][DEBUG ]   }, 
[2017-06-11 20:33:47,613][gdb3][DEBUG ]   "monmap": {
[2017-06-11 20:33:47,613][gdb3][DEBUG ]     "created": "2017-06-11 20:17:04.732069", 
[2017-06-11 20:33:47,613][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-11 20:33:47,613][gdb3][DEBUG ]     "features": {
[2017-06-11 20:33:47,613][gdb3][DEBUG ]       "optional": [], 
[2017-06-11 20:33:47,613][gdb3][DEBUG ]       "persistent": [
[2017-06-11 20:33:47,613][gdb3][DEBUG ]         "kraken", 
[2017-06-11 20:33:47,613][gdb3][DEBUG ]         "luminous"
[2017-06-11 20:33:47,613][gdb3][DEBUG ]       ]
[2017-06-11 20:33:47,613][gdb3][DEBUG ]     }, 
[2017-06-11 20:33:47,613][gdb3][DEBUG ]     "fsid": "9ccb7614-4874-4746-9634-d8f0080c96da", 
[2017-06-11 20:33:47,613][gdb3][DEBUG ]     "modified": "2017-06-11 20:17:04.974670", 
[2017-06-11 20:33:47,613][gdb3][DEBUG ]     "mons": [
[2017-06-11 20:33:47,613][gdb3][DEBUG ]       {
[2017-06-11 20:33:47,614][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-11 20:33:47,614][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-11 20:33:47,614][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-11 20:33:47,614][gdb3][DEBUG ]         "rank": 0
[2017-06-11 20:33:47,614][gdb3][DEBUG ]       }
[2017-06-11 20:33:47,614][gdb3][DEBUG ]     ]
[2017-06-11 20:33:47,614][gdb3][DEBUG ]   }, 
[2017-06-11 20:33:47,614][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-11 20:33:47,614][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-11 20:33:47,614][gdb3][DEBUG ]   "quorum": [
[2017-06-11 20:33:47,614][gdb3][DEBUG ]     0
[2017-06-11 20:33:47,614][gdb3][DEBUG ]   ], 
[2017-06-11 20:33:47,614][gdb3][DEBUG ]   "rank": 0, 
[2017-06-11 20:33:47,614][gdb3][DEBUG ]   "state": "leader", 
[2017-06-11 20:33:47,614][gdb3][DEBUG ]   "sync_provider": []
[2017-06-11 20:33:47,614][gdb3][DEBUG ] }
[2017-06-11 20:33:47,614][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:33:47,615][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-11 20:33:47,616][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:33:47,681][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-11 20:33:47,697][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:33:47,711][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:33:47,712][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:33:47,729][gdb3][DEBUG ] detect machine type
[2017-06-11 20:33:47,731][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:33:47,733][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:33:47,798][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-11 20:33:47,798][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-11 20:33:47,798][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-11 20:33:47,800][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpzpwzxi
[2017-06-11 20:33:47,815][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:33:47,829][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:33:47,830][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:33:47,846][gdb3][DEBUG ] detect machine type
[2017-06-11 20:33:47,849][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:33:47,849][gdb3][DEBUG ] fetch remote file
[2017-06-11 20:33:47,850][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:33:47,916][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-11 20:33:48,082][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-11 20:33:48,249][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-11 20:33:48,415][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-11 20:33:48,581][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-11 20:33:48,747][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-06-11 20:33:48,747][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-06-11 20:33:48,747][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-06-11 20:33:48,747][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.mon.keyring' and backing up old key as 'ceph.mon.keyring-20170611203348'
[2017-06-11 20:33:48,747][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-06-11 20:33:48,748][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-06-11 20:33:48,748][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpzpwzxi
[2017-06-11 20:33:48,928][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:33:48,928][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-11 20:33:48,928][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:33:48,928][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:33:48,929][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:33:48,929][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:33:48,929][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:33:48,929][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7e044f7518>
[2017-06-11 20:33:48,929][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:33:48,929][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-11 20:33:48,929][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f7e04e0e938>
[2017-06-11 20:33:48,929][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:33:48,929][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:33:48,929][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-11 20:33:48,956][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:33:48,969][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:33:48,970][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:33:48,986][gdb3][DEBUG ] detect machine type
[2017-06-11 20:33:48,989][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:34:06,535][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:34:06,539][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 20:34:06,539][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:34:06,539][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:34:06,539][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:34:06,540][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:34:06,540][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:34:06,540][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f627abce518>
[2017-06-11 20:34:06,540][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:34:06,540][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 20:34:06,540][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f627b4e5938>
[2017-06-11 20:34:06,540][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:34:06,540][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:34:06,540][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 20:34:06,784][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:34:06,979][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:34:06,979][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:34:06,996][gdb1][DEBUG ] detect machine type
[2017-06-11 20:34:06,999][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:34:33,019][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:34:33,019][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-11 20:34:33,019][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:34:33,019][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:34:33,019][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:34:33,019][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:34:33,019][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:34:33,019][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb77e16dfc8>
[2017-06-11 20:34:33,019][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:34:33,019][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 20:34:33,020][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fb77ea801b8>
[2017-06-11 20:34:33,020][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:34:33,020][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:34:33,020][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-11 20:34:33,020][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-11 20:34:33,020][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-11 20:34:33,020][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-11 20:34:33,046][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:34:33,061][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:34:33,061][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:34:33,078][gdb3][DEBUG ] detect machine type
[2017-06-11 20:34:33,080][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:34:33,080][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-11 20:34:33,081][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-11 20:34:33,120][gdb3][DEBUG ] Reading package lists...
[2017-06-11 20:34:33,284][gdb3][DEBUG ] Building dependency tree...
[2017-06-11 20:34:33,284][gdb3][DEBUG ] Reading state information...
[2017-06-11 20:34:33,398][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-11 20:34:33,399][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-11 20:34:33,399][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-11 20:34:33,399][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-11 20:34:33,399][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-11 20:34:33,399][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-11 20:34:33,399][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
[2017-06-11 20:34:33,399][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws
[2017-06-11 20:34:33,399][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws ntp python-blinker python-cephfs
[2017-06-11 20:34:33,399][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-06-11 20:34:33,399][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-06-11 20:34:33,399][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-06-11 20:34:33,399][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-06-11 20:34:33,400][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-06-11 20:34:33,400][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-11 20:34:33,400][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-11 20:34:33,400][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-11 20:34:33,564][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 40 not upgraded.
[2017-06-11 20:34:33,564][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-11 20:34:33,572][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 129562 files and directories currently installed.)
[2017-06-11 20:34:33,580][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-11 20:34:33,745][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-11 20:34:33,859][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-11 20:34:33,867][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-11 20:34:34,081][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-11 20:34:34,196][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-11 20:34:34,360][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-11 20:34:34,392][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-11 20:34:34,424][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-11 20:34:34,593][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-11 20:34:34,660][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-11 20:34:34,667][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-11 20:34:34,832][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-11 20:34:34,864][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-11 20:34:35,028][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-11 20:34:35,092][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-11 20:34:35,093][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-11 20:34:35,207][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-06-11 20:34:36,023][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-11 20:34:36,190][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:34:36,190][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-11 20:34:36,190][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:34:36,190][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:34:36,190][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:34:36,190][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:34:36,190][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:34:36,190][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f339cb7b710>
[2017-06-11 20:34:36,190][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:34:36,190][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 20:34:36,191][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f339d488230>
[2017-06-11 20:34:36,191][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:34:36,191][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:34:36,191][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-11 20:34:36,217][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:34:36,231][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:34:36,232][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:34:36,248][gdb3][DEBUG ] detect machine type
[2017-06-11 20:34:36,251][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:34:36,267][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:34:36,282][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:34:36,283][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:34:36,299][gdb3][DEBUG ] detect machine type
[2017-06-11 20:34:36,302][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:34:36,302][gdb3][INFO  ] purging data on gdb3
[2017-06-11 20:34:36,303][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-11 20:34:36,316][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-11 20:34:36,486][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:34:36,486][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-11 20:34:36,486][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:34:36,486][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:34:36,486][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:34:36,486][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:34:36,486][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:34:36,486][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff8f727b9e0>
[2017-06-11 20:34:36,486][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:34:36,486][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7ff8f7b3e848>
[2017-06-11 20:34:36,487][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:34:36,487][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:34:53,224][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:34:53,224][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-11 20:34:53,224][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:34:53,224][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:34:53,224][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:34:53,224][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:34:53,224][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:34:53,225][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f865553a560>
[2017-06-11 20:34:53,225][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:34:53,225][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-11 20:34:53,225][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-11 20:34:53,225][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f8655bbe758>
[2017-06-11 20:34:53,225][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-11 20:34:53,225][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:34:53,225][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-11 20:34:53,225][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:34:53,225][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-11 20:34:53,225][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-11 20:34:53,225][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-11 20:34:53,252][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:34:53,265][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:34:53,266][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:34:53,282][gdb3][DEBUG ] detect machine type
[2017-06-11 20:34:53,285][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:34:53,286][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-11 20:34:53,297][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-11 20:34:53,304][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-11 20:34:53,304][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-11 20:34:53,304][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-11 20:34:53,304][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-11 20:34:53,304][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-11 20:34:53,304][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-11 20:34:53,304][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-11 20:34:53,305][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-11 20:34:53,470][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:34:53,470][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-06-11 20:34:53,470][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:34:53,470][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:34:53,470][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:34:53,470][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:34:53,470][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-11 20:34:53,470][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:34:53,471][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6e39641e60>
[2017-06-11 20:34:53,471][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:34:53,471][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f6e39616b18>
[2017-06-11 20:34:53,471][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:34:53,471][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-11 20:34:53,471][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:34:53,471][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-11 20:34:53,472][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-11 20:34:53,498][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:34:53,512][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:34:53,513][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:34:53,529][gdb3][DEBUG ] detect machine type
[2017-06-11 20:34:53,532][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:34:53,532][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-11 20:34:53,532][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-11 20:34:53,532][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:34:53,532][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-11 20:34:53,533][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:34:53,533][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-11 20:34:53,533][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:34:53,535][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-11 20:34:53,535][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 20:34:53,535][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 20:34:53,536][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 20:34:53,536][gdb3][DEBUG ] create the monitor keyring file
[2017-06-11 20:34:53,537][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-11 20:34:53,575][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-11 20:34:53,575][gdb3][DEBUG ] ceph-mon: set fsid to d6e4ca27-6f7c-47cd-b614-55a11d8b762b
[2017-06-11 20:34:53,577][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-11 20:34:53,580][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 20:34:53,581][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-11 20:34:53,581][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-11 20:34:53,582][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 20:34:53,653][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-11 20:34:53,720][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-11 20:34:55,789][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:34:55,854][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:34:55,854][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-11 20:34:55,854][gdb3][DEBUG ] {
[2017-06-11 20:34:55,854][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-11 20:34:55,854][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-11 20:34:55,854][gdb3][DEBUG ]   "features": {
[2017-06-11 20:34:55,854][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-11 20:34:55,854][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-11 20:34:55,855][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:34:55,855][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:34:55,855][gdb3][DEBUG ]     ], 
[2017-06-11 20:34:55,855][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-11 20:34:55,855][gdb3][DEBUG ]     "required_mon": [
[2017-06-11 20:34:55,855][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:34:55,855][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:34:55,855][gdb3][DEBUG ]     ]
[2017-06-11 20:34:55,855][gdb3][DEBUG ]   }, 
[2017-06-11 20:34:55,855][gdb3][DEBUG ]   "monmap": {
[2017-06-11 20:34:55,855][gdb3][DEBUG ]     "created": "2017-06-11 20:34:53.560722", 
[2017-06-11 20:34:55,855][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-11 20:34:55,855][gdb3][DEBUG ]     "features": {
[2017-06-11 20:34:55,855][gdb3][DEBUG ]       "optional": [], 
[2017-06-11 20:34:55,855][gdb3][DEBUG ]       "persistent": [
[2017-06-11 20:34:55,855][gdb3][DEBUG ]         "kraken", 
[2017-06-11 20:34:55,856][gdb3][DEBUG ]         "luminous"
[2017-06-11 20:34:55,856][gdb3][DEBUG ]       ]
[2017-06-11 20:34:55,856][gdb3][DEBUG ]     }, 
[2017-06-11 20:34:55,856][gdb3][DEBUG ]     "fsid": "d6e4ca27-6f7c-47cd-b614-55a11d8b762b", 
[2017-06-11 20:34:55,856][gdb3][DEBUG ]     "modified": "2017-06-11 20:34:53.799319", 
[2017-06-11 20:34:55,856][gdb3][DEBUG ]     "mons": [
[2017-06-11 20:34:55,856][gdb3][DEBUG ]       {
[2017-06-11 20:34:55,856][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-11 20:34:55,856][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-11 20:34:55,856][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-11 20:34:55,856][gdb3][DEBUG ]         "rank": 0
[2017-06-11 20:34:55,856][gdb3][DEBUG ]       }
[2017-06-11 20:34:55,856][gdb3][DEBUG ]     ]
[2017-06-11 20:34:55,856][gdb3][DEBUG ]   }, 
[2017-06-11 20:34:55,856][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-11 20:34:55,856][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-11 20:34:55,856][gdb3][DEBUG ]   "quorum": [
[2017-06-11 20:34:55,857][gdb3][DEBUG ]     0
[2017-06-11 20:34:55,857][gdb3][DEBUG ]   ], 
[2017-06-11 20:34:55,857][gdb3][DEBUG ]   "rank": 0, 
[2017-06-11 20:34:55,857][gdb3][DEBUG ]   "state": "leader", 
[2017-06-11 20:34:55,857][gdb3][DEBUG ]   "sync_provider": []
[2017-06-11 20:34:55,857][gdb3][DEBUG ] }
[2017-06-11 20:34:55,857][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:34:55,857][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-11 20:34:55,858][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:34:55,923][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-11 20:34:55,941][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:34:55,955][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:34:55,956][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:34:55,972][gdb3][DEBUG ] detect machine type
[2017-06-11 20:34:55,975][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:34:55,976][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:34:56,041][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-11 20:34:56,041][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-11 20:34:56,041][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-11 20:34:56,043][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmp1J5zXA
[2017-06-11 20:34:56,058][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:34:56,072][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:34:56,073][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:34:56,089][gdb3][DEBUG ] detect machine type
[2017-06-11 20:34:56,092][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:34:56,092][gdb3][DEBUG ] fetch remote file
[2017-06-11 20:34:56,093][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:34:56,159][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-11 20:34:56,325][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-11 20:34:56,491][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-11 20:34:56,657][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-11 20:34:56,824][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-11 20:34:56,990][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-11 20:34:57,156][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-11 20:34:57,322][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-11 20:34:57,489][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-11 20:34:57,655][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-11 20:34:57,820][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-11 20:34:57,820][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-11 20:34:57,820][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170611203457'
[2017-06-11 20:34:57,821][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-11 20:34:57,821][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-11 20:34:57,821][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-11 20:34:57,821][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp1J5zXA
[2017-06-11 20:34:58,005][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:34:58,005][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-11 20:34:58,006][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:34:58,006][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:34:58,006][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:34:58,006][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:34:58,006][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:34:58,006][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f803ce02518>
[2017-06-11 20:34:58,006][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:34:58,006][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-11 20:34:58,006][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f803d719938>
[2017-06-11 20:34:58,006][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:34:58,006][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:34:58,006][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-11 20:34:58,033][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:34:58,047][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:34:58,048][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:34:58,064][gdb3][DEBUG ] detect machine type
[2017-06-11 20:34:58,066][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:35:04,133][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:35:04,133][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 20:35:04,134][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:35:04,134][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:35:04,134][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:35:04,134][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:35:04,134][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:35:04,134][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f74a7ef3518>
[2017-06-11 20:35:04,134][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:35:04,134][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 20:35:04,134][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f74a880a938>
[2017-06-11 20:35:04,134][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:35:04,134][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:35:04,134][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 20:35:04,371][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:35:04,571][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:35:04,571][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:35:04,588][gdb1][DEBUG ] detect machine type
[2017-06-11 20:35:04,592][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:35:28,078][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:35:28,078][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 20:35:28,078][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:35:28,078][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:35:28,078][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:35:28,079][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:35:28,079][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:35:28,079][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd84eb2d518>
[2017-06-11 20:35:28,079][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:35:28,079][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 20:35:28,079][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fd84f444938>
[2017-06-11 20:35:28,079][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:35:28,079][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:35:28,079][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 20:35:28,323][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:35:28,554][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:35:28,555][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:35:28,571][gdb1][DEBUG ] detect machine type
[2017-06-11 20:35:28,575][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:36:04,273][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:36:04,273][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-06-11 20:36:04,273][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:36:04,273][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe354133908>
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe354389aa0>
[2017-06-11 20:36:04,274][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:36:04,275][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:36:04,275][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 20:36:04,275][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-11 20:36:04,516][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:36:04,742][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:36:04,742][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:36:04,759][gdb1][DEBUG ] detect machine type
[2017-06-11 20:36:04,762][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:36:04,763][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:36:04,763][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-11 20:36:04,763][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:36:04,766][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-11 20:36:04,766][gdb1][DEBUG ] create a keyring file
[2017-06-11 20:36:04,768][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-06-11 20:36:04,768][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:36:04,770][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-11 20:36:04,891][gdb1][WARNING] Traceback (most recent call last):
[2017-06-11 20:36:04,891][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-11 20:36:04,891][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 20:36:04,891][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-11 20:36:04,891][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-11 20:36:04,891][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2029, in main
[2017-06-11 20:36:04,891][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2025, in factory
[2017-06-11 20:36:04,891][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2037, in __init__
[2017-06-11 20:36:04,891][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2739, in __init__
[2017-06-11 20:36:04,891][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2747, in set_type
[2017-06-11 20:36:04,891][gdb1][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/buddystore'
[2017-06-11 20:36:04,899][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 20:36:04,899][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-11 20:36:04,899][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-06-11 20:39:24,147][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:39:24,147][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-06-11 20:39:24,147][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:39:24,147][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:39:24,147][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 20:39:24,147][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-11 20:39:24,147][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 20:39:24,147][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:39:24,147][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fde85746908>
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fde8599caa0>
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:39:24,148][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 20:39:24,149][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-11 20:39:24,398][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:39:24,597][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:39:24,598][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:39:24,616][gdb1][DEBUG ] detect machine type
[2017-06-11 20:39:24,620][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:39:24,621][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:39:24,621][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-11 20:39:24,621][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:39:24,624][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-06-11 20:39:24,624][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:39:24,626][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-11 20:39:24,796][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 20:39:24,797][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:39:24,797][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:39:24,797][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:39:24,813][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 20:39:24,828][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/buddystore
[2017-06-11 20:39:24,828][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/ceph_fsid.8543.tmp
[2017-06-11 20:39:24,836][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/fsid.8543.tmp
[2017-06-11 20:39:24,839][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/magic.8543.tmp
[2017-06-11 20:39:29,860][gdb1][INFO  ] checking OSD status...
[2017-06-11 20:39:29,860][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:39:29,863][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 20:39:30,028][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-11 20:40:44,041][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe9a611e908>
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe9a6374aa0>
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:40:44,042][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:40:44,043][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-11 20:40:44,043][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-11 20:40:44,279][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:40:44,507][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:40:44,508][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:40:44,524][gdb1][DEBUG ] detect machine type
[2017-06-11 20:40:44,528][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:40:44,529][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:40:44,529][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-06-11 20:40:44,529][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 20:40:44,529][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:40:44,531][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-06-11 20:40:44,702][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-06-11 20:40:44,702][gdb1][WARNING] activate: Cluster uuid is d6e4ca27-6f7c-47cd-b614-55a11d8b762b
[2017-06-11 20:40:44,702][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 20:40:44,702][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-11 20:40:44,702][gdb1][WARNING] activate: OSD uuid is 962e82c1-669b-4113-be62-467aa10847bb
[2017-06-11 20:40:44,702][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-11 20:40:44,703][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 962e82c1-669b-4113-be62-467aa10847bb
[2017-06-11 20:40:44,817][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/whoami.8662.tmp
[2017-06-11 20:40:44,817][gdb1][WARNING] activate: OSD id is 0
[2017-06-11 20:40:44,817][gdb1][WARNING] activate: Initializing OSD...
[2017-06-11 20:40:44,817][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/buddystore/activate.monmap
[2017-06-11 20:40:44,981][gdb1][WARNING] got monmap epoch 2
[2017-06-11 20:40:44,981][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/buddystore/activate.monmap --osd-data /mnt/buddystore --osd-journal /mnt/buddystore/journal --osd-uuid 962e82c1-669b-4113-be62-467aa10847bb --keyring /mnt/buddystore/keyring --setuser ceph --setgroup ceph
[2017-06-11 20:40:44,989][gdb1][WARNING] activate: Marking with init system systemd
[2017-06-11 20:40:44,989][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/systemd
[2017-06-11 20:40:44,989][gdb1][WARNING] activate: Authorizing OSD key...
[2017-06-11 20:40:44,989][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/buddystore/keyring osd allow * mon allow profile osd
[2017-06-11 20:40:45,154][gdb1][WARNING] added key for osd.0
[2017-06-11 20:40:45,154][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/active.8662.tmp
[2017-06-11 20:40:45,154][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/buddystore
[2017-06-11 20:40:45,154][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-0 -> /mnt/buddystore
[2017-06-11 20:40:45,154][gdb1][WARNING] start_daemon: Starting ceph osd.0...
[2017-06-11 20:40:45,154][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-06-11 20:40:45,154][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-06-11 20:40:45,218][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-06-11 20:40:45,250][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-06-11 20:40:45,253][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-06-11 20:40:45,317][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-06-11 20:40:50,437][gdb1][INFO  ] checking OSD status...
[2017-06-11 20:40:50,437][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:40:50,440][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 20:40:50,607][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 20:43:12,616][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f73d4091518>
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f73d49a8938>
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:43:12,617][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:43:12,618][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 20:43:12,859][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:43:13,090][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:43:13,091][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:43:13,108][gdb1][DEBUG ] detect machine type
[2017-06-11 20:43:13,111][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:43:13,283][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 20:43:13,284][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:43:13,285][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc376f90908>
[2017-06-11 20:43:13,285][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:43:13,285][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 20:43:13,285][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc3771e6aa0>
[2017-06-11 20:43:13,285][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:43:13,285][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:43:13,285][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 20:43:13,285][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-11 20:43:13,526][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:43:13,763][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:43:13,763][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:43:13,780][gdb1][DEBUG ] detect machine type
[2017-06-11 20:43:13,783][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:43:13,784][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:43:13,784][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-11 20:43:13,784][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:43:13,787][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-06-11 20:43:13,787][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:43:13,789][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-11 20:43:13,909][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 20:43:13,925][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:43:13,932][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:43:13,948][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:43:13,956][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 20:43:13,972][gdb1][WARNING] populate_data_path: Data dir /mnt/buddystore already exists
[2017-06-11 20:43:18,980][gdb1][INFO  ] checking OSD status...
[2017-06-11 20:43:18,980][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:43:18,983][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 20:43:19,148][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-11 20:43:19,312][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:43:19,313][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-06-11 20:43:19,313][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:43:19,313][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:43:19,313][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:43:19,313][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:43:19,313][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 20:43:19,313][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:43:19,313][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6683ca2908>
[2017-06-11 20:43:19,313][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:43:19,313][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6683ef8aa0>
[2017-06-11 20:43:19,314][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:43:19,314][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:43:19,314][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-11 20:43:19,314][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-11 20:43:19,555][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:43:19,751][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:43:19,751][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:43:19,768][gdb1][DEBUG ] detect machine type
[2017-06-11 20:43:19,772][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:43:19,772][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:43:19,772][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-06-11 20:43:19,773][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 20:43:19,773][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:43:19,775][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-06-11 20:43:19,895][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-06-11 20:43:19,895][gdb1][WARNING] activate: Cluster uuid is d6e4ca27-6f7c-47cd-b614-55a11d8b762b
[2017-06-11 20:43:19,895][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 20:43:19,911][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-11 20:43:19,911][gdb1][WARNING] activate: OSD uuid is 962e82c1-669b-4113-be62-467aa10847bb
[2017-06-11 20:43:19,911][gdb1][WARNING] activate: OSD id is 0
[2017-06-11 20:43:19,911][gdb1][WARNING] activate: Marking with init system systemd
[2017-06-11 20:43:19,911][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/systemd
[2017-06-11 20:43:19,911][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/buddystore
[2017-06-11 20:43:19,911][gdb1][WARNING] start_daemon: Starting ceph osd.0...
[2017-06-11 20:43:19,912][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-06-11 20:43:19,927][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-06-11 20:43:19,991][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-06-11 20:43:20,055][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-06-11 20:43:20,056][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-06-11 20:43:20,087][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-06-11 20:43:25,100][gdb1][INFO  ] checking OSD status...
[2017-06-11 20:43:25,100][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:43:25,103][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 20:43:25,270][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 20:52:14,034][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:52:14,034][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-11 20:52:14,034][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:52:14,034][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:52:14,034][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:52:14,035][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:52:14,035][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:52:14,035][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efcf3437fc8>
[2017-06-11 20:52:14,035][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:52:14,035][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 20:52:14,035][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7efcf3d4a1b8>
[2017-06-11 20:52:14,035][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:52:14,035][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:52:14,035][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-11 20:52:14,035][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-11 20:52:14,035][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-11 20:52:14,035][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-11 20:52:14,062][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:52:14,076][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:52:14,077][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:52:14,093][gdb3][DEBUG ] detect machine type
[2017-06-11 20:52:14,096][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:52:14,096][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-11 20:52:14,097][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-11 20:52:14,135][gdb3][DEBUG ] Reading package lists...
[2017-06-11 20:52:14,300][gdb3][DEBUG ] Building dependency tree...
[2017-06-11 20:52:14,300][gdb3][DEBUG ] Reading state information...
[2017-06-11 20:52:14,364][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-11 20:52:14,364][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-11 20:52:14,364][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-11 20:52:14,364][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-11 20:52:14,365][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-11 20:52:14,365][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-11 20:52:14,365][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
[2017-06-11 20:52:14,365][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws
[2017-06-11 20:52:14,365][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws ntp python-blinker python-cephfs
[2017-06-11 20:52:14,365][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-06-11 20:52:14,365][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-06-11 20:52:14,365][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-06-11 20:52:14,366][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-06-11 20:52:14,366][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-06-11 20:52:14,366][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-11 20:52:14,430][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-11 20:52:14,430][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-11 20:52:14,544][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 40 not upgraded.
[2017-06-11 20:52:14,545][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-11 20:52:14,609][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 129562 files and directories currently installed.)
[2017-06-11 20:52:14,610][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-11 20:52:14,774][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-11 20:52:14,838][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-11 20:52:14,870][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-11 20:52:15,084][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-11 20:52:15,199][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-11 20:52:15,366][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-11 20:52:15,430][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-11 20:52:15,494][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-11 20:52:15,660][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-11 20:52:15,724][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-11 20:52:15,724][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-11 20:52:15,889][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-11 20:52:15,921][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-11 20:52:16,085][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-11 20:52:16,150][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-11 20:52:16,150][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-11 20:52:16,264][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-06-11 20:52:17,131][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-11 20:52:17,298][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:52:17,298][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-11 20:52:17,298][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:52:17,299][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:52:17,299][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:52:17,299][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:52:17,299][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:52:17,299][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f72ee30e710>
[2017-06-11 20:52:17,299][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:52:17,299][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 20:52:17,299][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f72eec1b230>
[2017-06-11 20:52:17,299][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:52:17,299][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:52:17,299][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-11 20:52:17,326][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:52:17,340][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:52:17,341][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:52:17,357][gdb3][DEBUG ] detect machine type
[2017-06-11 20:52:17,359][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:52:17,375][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:52:17,390][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:52:17,390][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:52:17,407][gdb3][DEBUG ] detect machine type
[2017-06-11 20:52:17,410][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:52:17,410][gdb3][INFO  ] purging data on gdb3
[2017-06-11 20:52:17,411][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-11 20:52:17,424][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-11 20:52:17,596][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:52:17,596][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-11 20:52:17,596][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:52:17,597][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:52:17,597][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:52:17,597][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:52:17,597][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:52:17,597][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9726de79e0>
[2017-06-11 20:52:17,597][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:52:17,597][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f97276aa848>
[2017-06-11 20:52:17,597][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:52:17,597][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:52:43,193][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7746efc560>
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f7747580758>
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-11 20:52:43,194][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:52:43,195][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-11 20:52:43,195][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-11 20:52:43,195][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-11 20:52:43,221][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:52:43,235][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:52:43,235][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:52:43,252][gdb3][DEBUG ] detect machine type
[2017-06-11 20:52:43,254][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:52:43,255][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-11 20:52:43,267][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-11 20:52:43,273][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-11 20:52:43,273][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-11 20:52:43,273][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-11 20:52:43,273][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-11 20:52:43,274][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-11 20:52:43,274][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-11 20:52:43,274][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-11 20:52:43,274][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-11 20:52:43,440][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:52:43,440][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-06-11 20:52:43,440][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:52:43,440][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:52:43,441][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:52:43,441][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:52:43,441][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-11 20:52:43,441][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:52:43,441][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2485885e60>
[2017-06-11 20:52:43,441][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:52:43,441][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f248585ab18>
[2017-06-11 20:52:43,441][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:52:43,442][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-11 20:52:43,442][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:52:43,442][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-11 20:52:43,442][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-11 20:52:43,468][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:52:43,482][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:52:43,483][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:52:43,499][gdb3][DEBUG ] detect machine type
[2017-06-11 20:52:43,502][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:52:43,502][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-11 20:52:43,502][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-11 20:52:43,502][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:52:43,503][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-11 20:52:43,503][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:52:43,503][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-11 20:52:43,504][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:52:43,505][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-11 20:52:43,505][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 20:52:43,506][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 20:52:43,506][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 20:52:43,506][gdb3][DEBUG ] create the monitor keyring file
[2017-06-11 20:52:43,507][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-11 20:52:43,545][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-11 20:52:43,546][gdb3][DEBUG ] ceph-mon: set fsid to 45e038c0-5852-4ff0-82ba-f15493980e30
[2017-06-11 20:52:43,549][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-11 20:52:43,550][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 20:52:43,551][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-11 20:52:43,551][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-11 20:52:43,552][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 20:52:43,620][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-11 20:52:43,693][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-11 20:52:45,731][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:52:45,796][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:52:45,796][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-11 20:52:45,796][gdb3][DEBUG ] {
[2017-06-11 20:52:45,796][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-11 20:52:45,796][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-11 20:52:45,796][gdb3][DEBUG ]   "features": {
[2017-06-11 20:52:45,796][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-11 20:52:45,796][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-11 20:52:45,796][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:52:45,797][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:52:45,797][gdb3][DEBUG ]     ], 
[2017-06-11 20:52:45,797][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-11 20:52:45,797][gdb3][DEBUG ]     "required_mon": [
[2017-06-11 20:52:45,797][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:52:45,797][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:52:45,797][gdb3][DEBUG ]     ]
[2017-06-11 20:52:45,797][gdb3][DEBUG ]   }, 
[2017-06-11 20:52:45,797][gdb3][DEBUG ]   "monmap": {
[2017-06-11 20:52:45,797][gdb3][DEBUG ]     "created": "2017-06-11 20:52:43.531308", 
[2017-06-11 20:52:45,797][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-11 20:52:45,797][gdb3][DEBUG ]     "features": {
[2017-06-11 20:52:45,797][gdb3][DEBUG ]       "optional": [], 
[2017-06-11 20:52:45,797][gdb3][DEBUG ]       "persistent": [
[2017-06-11 20:52:45,797][gdb3][DEBUG ]         "kraken", 
[2017-06-11 20:52:45,797][gdb3][DEBUG ]         "luminous"
[2017-06-11 20:52:45,798][gdb3][DEBUG ]       ]
[2017-06-11 20:52:45,798][gdb3][DEBUG ]     }, 
[2017-06-11 20:52:45,798][gdb3][DEBUG ]     "fsid": "45e038c0-5852-4ff0-82ba-f15493980e30", 
[2017-06-11 20:52:45,798][gdb3][DEBUG ]     "modified": "2017-06-11 20:52:43.775650", 
[2017-06-11 20:52:45,798][gdb3][DEBUG ]     "mons": [
[2017-06-11 20:52:45,798][gdb3][DEBUG ]       {
[2017-06-11 20:52:45,798][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-11 20:52:45,798][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-11 20:52:45,798][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-11 20:52:45,798][gdb3][DEBUG ]         "rank": 0
[2017-06-11 20:52:45,798][gdb3][DEBUG ]       }
[2017-06-11 20:52:45,798][gdb3][DEBUG ]     ]
[2017-06-11 20:52:45,798][gdb3][DEBUG ]   }, 
[2017-06-11 20:52:45,798][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-11 20:52:45,798][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-11 20:52:45,798][gdb3][DEBUG ]   "quorum": [
[2017-06-11 20:52:45,798][gdb3][DEBUG ]     0
[2017-06-11 20:52:45,799][gdb3][DEBUG ]   ], 
[2017-06-11 20:52:45,799][gdb3][DEBUG ]   "rank": 0, 
[2017-06-11 20:52:45,799][gdb3][DEBUG ]   "state": "leader", 
[2017-06-11 20:52:45,799][gdb3][DEBUG ]   "sync_provider": []
[2017-06-11 20:52:45,799][gdb3][DEBUG ] }
[2017-06-11 20:52:45,799][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:52:45,799][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-11 20:52:45,800][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:52:45,865][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-11 20:52:45,884][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:52:45,898][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:52:45,898][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:52:45,915][gdb3][DEBUG ] detect machine type
[2017-06-11 20:52:45,918][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:52:45,919][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:52:45,984][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-11 20:52:45,984][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-11 20:52:45,984][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-11 20:52:45,986][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpNgvyxc
[2017-06-11 20:52:46,002][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:52:46,016][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:52:46,017][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:52:46,034][gdb3][DEBUG ] detect machine type
[2017-06-11 20:52:46,036][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:52:46,037][gdb3][DEBUG ] fetch remote file
[2017-06-11 20:52:46,038][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:52:46,104][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-11 20:52:46,270][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-11 20:52:46,436][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-11 20:52:46,603][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-11 20:52:46,819][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-11 20:52:46,986][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-11 20:52:47,153][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-11 20:52:47,319][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-11 20:52:47,485][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-11 20:52:47,651][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-11 20:52:47,817][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-11 20:52:47,817][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-11 20:52:47,817][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170611205247'
[2017-06-11 20:52:47,817][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-11 20:52:47,818][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-11 20:52:47,818][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-11 20:52:47,818][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpNgvyxc
[2017-06-11 20:52:47,997][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1217c86518>
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f121859d938>
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:52:47,998][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:52:47,998][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-11 20:52:48,025][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:52:48,038][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:52:48,039][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:52:48,055][gdb3][DEBUG ] detect machine type
[2017-06-11 20:52:48,058][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:53:09,594][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:53:09,594][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 20:53:09,594][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:53:09,594][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:53:09,594][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:53:09,594][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:53:09,594][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:53:09,594][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbfd12c4518>
[2017-06-11 20:53:09,595][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:53:09,595][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 20:53:09,595][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fbfd1bdb938>
[2017-06-11 20:53:09,595][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:53:09,595][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:53:09,595][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 20:53:09,853][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:53:10,083][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:53:10,083][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:53:10,100][gdb1][DEBUG ] detect machine type
[2017-06-11 20:53:10,104][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:53:10,276][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:53:10,276][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-06-11 20:53:10,276][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:53:10,276][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f42d48c2908>
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f42d4b18aa0>
[2017-06-11 20:53:10,277][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:53:10,278][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:53:10,278][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 20:53:10,278][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-11 20:53:10,519][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:53:10,747][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:53:10,748][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:53:10,764][gdb1][DEBUG ] detect machine type
[2017-06-11 20:53:10,768][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:53:10,769][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:53:10,769][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-11 20:53:10,769][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:53:10,771][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-06-11 20:53:10,772][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:53:10,773][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-11 20:53:10,944][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 20:53:10,944][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:53:10,944][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:53:10,944][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:53:10,952][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 20:53:10,968][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/buddystore
[2017-06-11 20:53:10,968][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/ceph_fsid.9822.tmp
[2017-06-11 20:53:10,968][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/fsid.9822.tmp
[2017-06-11 20:53:10,971][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/magic.9822.tmp
[2017-06-11 20:53:15,992][gdb1][INFO  ] checking OSD status...
[2017-06-11 20:53:15,992][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:53:15,995][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 20:53:16,160][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-11 20:53:16,325][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:53:16,325][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-06-11 20:53:16,325][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:53:16,326][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:53:16,326][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:53:16,326][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:53:16,326][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 20:53:16,326][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:53:16,326][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffb7a562908>
[2017-06-11 20:53:16,326][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:53:16,326][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ffb7a7b8aa0>
[2017-06-11 20:53:16,326][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:53:16,326][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:53:16,326][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-11 20:53:16,326][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-11 20:53:16,572][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:53:16,803][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:53:16,803][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:53:16,821][gdb1][DEBUG ] detect machine type
[2017-06-11 20:53:16,824][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:53:16,825][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:53:16,825][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-06-11 20:53:16,825][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 20:53:16,826][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:53:16,827][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-06-11 20:53:16,948][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-06-11 20:53:16,948][gdb1][WARNING] activate: Cluster uuid is 45e038c0-5852-4ff0-82ba-f15493980e30
[2017-06-11 20:53:16,948][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 20:53:16,964][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-11 20:53:16,964][gdb1][WARNING] activate: OSD uuid is 7cb38c52-c6fb-4d45-97c7-33dc62dd1d06
[2017-06-11 20:53:16,964][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-11 20:53:16,964][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 7cb38c52-c6fb-4d45-97c7-33dc62dd1d06
[2017-06-11 20:53:17,028][gdb1][WARNING] Traceback (most recent call last):
[2017-06-11 20:53:17,029][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-11 20:53:17,029][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 20:53:17,029][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-11 20:53:17,029][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-11 20:53:17,029][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-06-11 20:53:17,029][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-11 20:53:17,029][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-06-11 20:53:17,029][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-06-11 20:53:17,030][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-06-11 20:53:17.018260 7fa615ea9700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-06-11 20:53:17,030][gdb1][WARNING] error connecting to the cluster
[2017-06-11 20:53:17,030][gdb1][WARNING] 
[2017-06-11 20:53:17,037][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 20:53:17,037][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore

[2017-06-11 20:53:54,171][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f97d277c518>
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f97d3093938>
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:53:54,172][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:53:54,172][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 20:53:54,420][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:53:54,650][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:53:54,651][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:53:54,667][gdb1][DEBUG ] detect machine type
[2017-06-11 20:53:54,671][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:53:54,844][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:53:54,844][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-06-11 20:53:54,844][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:53:54,844][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:53:54,844][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 20:53:54,844][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-11 20:53:54,844][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 20:53:54,844][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:53:54,844][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 20:53:54,844][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 20:53:54,845][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:53:54,845][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 20:53:54,845][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 20:53:54,845][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:53:54,845][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f03798de908>
[2017-06-11 20:53:54,845][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:53:54,845][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 20:53:54,845][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0379b34aa0>
[2017-06-11 20:53:54,845][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:53:54,845][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:53:54,845][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 20:53:54,845][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-11 20:53:55,084][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:53:55,314][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:53:55,315][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:53:55,334][gdb1][DEBUG ] detect machine type
[2017-06-11 20:53:55,337][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:53:55,338][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:53:55,338][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-11 20:53:55,338][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:53:55,341][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-11 20:53:55,341][gdb1][DEBUG ] create a keyring file
[2017-06-11 20:53:55,342][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-06-11 20:53:55,343][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:53:55,345][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-11 20:53:55,515][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 20:53:55,515][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:53:55,515][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:53:55,515][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 20:53:55,517][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 20:53:55,532][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/buddystore
[2017-06-11 20:53:55,548][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/ceph_fsid.10139.tmp
[2017-06-11 20:53:55,548][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/fsid.10139.tmp
[2017-06-11 20:53:55,550][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/magic.10139.tmp
[2017-06-11 20:54:00,570][gdb1][INFO  ] checking OSD status...
[2017-06-11 20:54:00,571][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:54:00,573][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 20:54:00,738][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-11 20:54:00,902][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:54:00,903][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-06-11 20:54:00,903][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:54:00,903][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:54:00,903][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:54:00,903][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:54:00,903][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 20:54:00,903][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:54:00,903][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5defc47908>
[2017-06-11 20:54:00,903][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:54:00,903][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5defe9daa0>
[2017-06-11 20:54:00,903][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:54:00,904][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:54:00,904][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-11 20:54:00,904][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-11 20:54:01,147][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 20:54:01,375][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 20:54:01,376][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 20:54:01,392][gdb1][DEBUG ] detect machine type
[2017-06-11 20:54:01,396][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:54:01,397][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:54:01,397][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-06-11 20:54:01,397][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 20:54:01,397][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:54:01,399][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-06-11 20:54:01,570][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-06-11 20:54:01,570][gdb1][WARNING] activate: Cluster uuid is 45e038c0-5852-4ff0-82ba-f15493980e30
[2017-06-11 20:54:01,570][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 20:54:01,570][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-11 20:54:01,570][gdb1][WARNING] activate: OSD uuid is 36c02900-4a5d-4a03-8559-ab5300cc7c22
[2017-06-11 20:54:01,571][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-11 20:54:01,571][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 36c02900-4a5d-4a03-8559-ab5300cc7c22
[2017-06-11 20:54:01,735][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/whoami.10256.tmp
[2017-06-11 20:54:01,735][gdb1][WARNING] activate: OSD id is 0
[2017-06-11 20:54:01,735][gdb1][WARNING] activate: Initializing OSD...
[2017-06-11 20:54:01,735][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/buddystore/activate.monmap
[2017-06-11 20:54:01,849][gdb1][WARNING] got monmap epoch 2
[2017-06-11 20:54:01,850][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/buddystore/activate.monmap --osd-data /mnt/buddystore --osd-journal /mnt/buddystore/journal --osd-uuid 36c02900-4a5d-4a03-8559-ab5300cc7c22 --keyring /mnt/buddystore/keyring --setuser ceph --setgroup ceph
[2017-06-11 20:54:01,881][gdb1][WARNING] activate: Marking with init system systemd
[2017-06-11 20:54:01,882][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/systemd
[2017-06-11 20:54:01,882][gdb1][WARNING] activate: Authorizing OSD key...
[2017-06-11 20:54:01,882][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/buddystore/keyring osd allow * mon allow profile osd
[2017-06-11 20:54:02,046][gdb1][WARNING] added key for osd.0
[2017-06-11 20:54:02,046][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/active.10256.tmp
[2017-06-11 20:54:02,046][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/buddystore
[2017-06-11 20:54:02,046][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-0 -> /mnt/buddystore
[2017-06-11 20:54:02,046][gdb1][WARNING] start_daemon: Starting ceph osd.0...
[2017-06-11 20:54:02,047][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-06-11 20:54:02,054][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-06-11 20:54:02,118][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-06-11 20:54:02,150][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-06-11 20:54:02,150][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-06-11 20:54:02,214][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-06-11 20:54:07,333][gdb1][INFO  ] checking OSD status...
[2017-06-11 20:54:07,334][gdb1][DEBUG ] find the location of an executable
[2017-06-11 20:54:07,336][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 20:54:07,504][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-20 22:05:16,078][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:05:16,079][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-20 22:05:16,079][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:05:16,079][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:05:16,080][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:05:16,080][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:05:16,080][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:05:16,080][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f226d96e0e0>
[2017-06-20 22:05:16,080][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:05:16,080][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-20 22:05:16,080][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f226e27b1b8>
[2017-06-20 22:05:16,080][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:05:16,080][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:05:16,080][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-20 22:05:16,080][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-20 22:05:16,080][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-20 22:05:16,080][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-20 22:05:16,107][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:05:16,122][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:05:16,122][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:05:16,139][gdb3][DEBUG ] detect machine type
[2017-06-20 22:05:16,142][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:05:16,142][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-20 22:05:16,143][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-20 22:05:16,181][gdb3][DEBUG ] Reading package lists...
[2017-06-20 22:05:16,345][gdb3][DEBUG ] Building dependency tree...
[2017-06-20 22:05:16,346][gdb3][DEBUG ] Reading state information...
[2017-06-20 22:05:16,410][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-20 22:05:16,410][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-20 22:05:16,410][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-20 22:05:16,410][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-20 22:05:16,410][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-20 22:05:16,411][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-20 22:05:16,411][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-06-20 22:05:16,411][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-headers-4.4.0-1016-aws
[2017-06-20 22:05:16,411][gdb3][DEBUG ]   linux-headers-4.4.0-1017-aws linux-image-4.4.0-1013-aws
[2017-06-20 22:05:16,411][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws ntp python-blinker
[2017-06-20 22:05:16,411][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-06-20 22:05:16,411][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-06-20 22:05:16,411][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-06-20 22:05:16,411][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-06-20 22:05:16,412][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-06-20 22:05:16,412][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-20 22:05:16,476][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-20 22:05:16,476][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-20 22:05:16,590][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 45 not upgraded.
[2017-06-20 22:05:16,590][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-20 22:05:16,654][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 154602 files and directories currently installed.)
[2017-06-20 22:05:16,655][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-20 22:05:16,820][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-20 22:05:16,935][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-20 22:05:16,951][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-20 22:05:17,165][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-20 22:05:17,279][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-20 22:05:17,444][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-20 22:05:17,508][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-20 22:05:17,511][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-20 22:05:17,675][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-20 22:05:17,740][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-20 22:05:17,756][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-20 22:05:17,920][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-20 22:05:17,984][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-20 22:05:18,098][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-20 22:05:18,213][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-20 22:05:18,213][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-20 22:05:18,327][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-06-20 22:05:19,093][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-20 22:05:19,259][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc962044758>
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fc962951230>
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:05:19,260][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:05:19,260][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-20 22:05:19,287][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:05:19,300][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:05:19,301][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:05:19,318][gdb3][DEBUG ] detect machine type
[2017-06-20 22:05:19,320][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:05:19,336][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:05:19,350][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:05:19,350][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:05:19,367][gdb3][DEBUG ] detect machine type
[2017-06-20 22:05:19,370][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:05:19,370][gdb3][INFO  ] purging data on gdb3
[2017-06-20 22:05:19,371][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-20 22:05:19,384][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-20 22:05:19,553][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:05:19,554][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-20 22:05:19,554][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:05:19,554][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:05:19,554][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:05:19,554][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:05:19,554][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:05:19,554][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f91d5a25a28>
[2017-06-20 22:05:19,554][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:05:19,554][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f91d62e8848>
[2017-06-20 22:05:19,554][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:05:19,554][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:05:38,318][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:05:38,318][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-20 22:05:38,318][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:05:38,318][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:05:38,318][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:05:38,318][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:05:38,318][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:05:38,318][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f30243835a8>
[2017-06-20 22:05:38,319][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:05:38,319][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-20 22:05:38,319][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-20 22:05:38,319][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f3024a07758>
[2017-06-20 22:05:38,319][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-20 22:05:38,319][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:05:38,319][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-20 22:05:38,319][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:05:38,319][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-20 22:05:38,319][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-20 22:05:38,319][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-20 22:05:38,346][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:05:38,360][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:05:38,360][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:05:38,377][gdb3][DEBUG ] detect machine type
[2017-06-20 22:05:38,379][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:05:38,380][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-20 22:05:38,391][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-20 22:05:38,397][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-20 22:05:38,398][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-20 22:05:38,398][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-20 22:05:38,398][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-20 22:05:38,398][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-20 22:05:38,398][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-20 22:05:38,398][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-20 22:05:38,398][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-20 22:05:38,563][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5519c9fea8>
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f5519c73b18>
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-20 22:05:38,564][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:05:38,565][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-20 22:05:38,565][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-20 22:05:38,591][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:05:38,605][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:05:38,606][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:05:38,622][gdb3][DEBUG ] detect machine type
[2017-06-20 22:05:38,624][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:05:38,625][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-20 22:05:38,625][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-20 22:05:38,625][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:05:38,625][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-20 22:05:38,625][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:05:38,626][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-20 22:05:38,626][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:05:38,627][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-20 22:05:38,628][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-20 22:05:38,628][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-20 22:05:38,628][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-20 22:05:38,629][gdb3][DEBUG ] create the monitor keyring file
[2017-06-20 22:05:38,630][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-20 22:05:38,668][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-20 22:05:38,668][gdb3][DEBUG ] ceph-mon: set fsid to a69d8ca8-86b1-497a-bd30-832aff903e6c
[2017-06-20 22:05:38,671][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-20 22:05:38,672][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-20 22:05:38,673][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-20 22:05:38,673][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-20 22:05:38,674][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-20 22:05:38,742][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-20 22:05:38,809][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-20 22:05:40,879][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:05:40,945][gdb3][DEBUG ] ********************************************************************************
[2017-06-20 22:05:40,945][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-20 22:05:40,945][gdb3][DEBUG ] {
[2017-06-20 22:05:40,945][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-20 22:05:40,945][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-20 22:05:40,945][gdb3][DEBUG ]   "features": {
[2017-06-20 22:05:40,945][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-20 22:05:40,945][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-20 22:05:40,945][gdb3][DEBUG ]       "kraken", 
[2017-06-20 22:05:40,945][gdb3][DEBUG ]       "luminous"
[2017-06-20 22:05:40,945][gdb3][DEBUG ]     ], 
[2017-06-20 22:05:40,946][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-20 22:05:40,946][gdb3][DEBUG ]     "required_mon": [
[2017-06-20 22:05:40,946][gdb3][DEBUG ]       "kraken", 
[2017-06-20 22:05:40,946][gdb3][DEBUG ]       "luminous"
[2017-06-20 22:05:40,946][gdb3][DEBUG ]     ]
[2017-06-20 22:05:40,946][gdb3][DEBUG ]   }, 
[2017-06-20 22:05:40,946][gdb3][DEBUG ]   "monmap": {
[2017-06-20 22:05:40,946][gdb3][DEBUG ]     "created": "2017-06-20 22:05:38.653272", 
[2017-06-20 22:05:40,946][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-20 22:05:40,946][gdb3][DEBUG ]     "features": {
[2017-06-20 22:05:40,946][gdb3][DEBUG ]       "optional": [], 
[2017-06-20 22:05:40,946][gdb3][DEBUG ]       "persistent": [
[2017-06-20 22:05:40,946][gdb3][DEBUG ]         "kraken", 
[2017-06-20 22:05:40,946][gdb3][DEBUG ]         "luminous"
[2017-06-20 22:05:40,946][gdb3][DEBUG ]       ]
[2017-06-20 22:05:40,946][gdb3][DEBUG ]     }, 
[2017-06-20 22:05:40,946][gdb3][DEBUG ]     "fsid": "a69d8ca8-86b1-497a-bd30-832aff903e6c", 
[2017-06-20 22:05:40,947][gdb3][DEBUG ]     "modified": "2017-06-20 22:05:38.899886", 
[2017-06-20 22:05:40,947][gdb3][DEBUG ]     "mons": [
[2017-06-20 22:05:40,947][gdb3][DEBUG ]       {
[2017-06-20 22:05:40,947][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-20 22:05:40,947][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-20 22:05:40,947][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-20 22:05:40,947][gdb3][DEBUG ]         "rank": 0
[2017-06-20 22:05:40,947][gdb3][DEBUG ]       }
[2017-06-20 22:05:40,947][gdb3][DEBUG ]     ]
[2017-06-20 22:05:40,947][gdb3][DEBUG ]   }, 
[2017-06-20 22:05:40,947][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-20 22:05:40,947][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-20 22:05:40,947][gdb3][DEBUG ]   "quorum": [
[2017-06-20 22:05:40,947][gdb3][DEBUG ]     0
[2017-06-20 22:05:40,947][gdb3][DEBUG ]   ], 
[2017-06-20 22:05:40,947][gdb3][DEBUG ]   "rank": 0, 
[2017-06-20 22:05:40,947][gdb3][DEBUG ]   "state": "leader", 
[2017-06-20 22:05:40,948][gdb3][DEBUG ]   "sync_provider": []
[2017-06-20 22:05:40,948][gdb3][DEBUG ] }
[2017-06-20 22:05:40,948][gdb3][DEBUG ] ********************************************************************************
[2017-06-20 22:05:40,948][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-20 22:05:40,949][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:05:41,014][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-20 22:05:41,029][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:05:41,042][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:05:41,043][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:05:41,059][gdb3][DEBUG ] detect machine type
[2017-06-20 22:05:41,062][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:05:41,063][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:05:41,128][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-20 22:05:41,128][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-20 22:05:41,128][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-20 22:05:41,130][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpZcY6hf
[2017-06-20 22:05:41,146][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:05:41,159][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:05:41,160][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:05:41,176][gdb3][DEBUG ] detect machine type
[2017-06-20 22:05:41,179][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:05:41,179][gdb3][DEBUG ] fetch remote file
[2017-06-20 22:05:41,180][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:05:41,246][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-20 22:05:41,412][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-20 22:05:41,578][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-20 22:05:41,744][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-20 22:05:41,910][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-20 22:05:42,076][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-20 22:05:42,242][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-20 22:05:42,408][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-20 22:05:42,575][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-20 22:05:42,741][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-20 22:05:42,906][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-20 22:05:42,906][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-20 22:05:42,907][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170620220542'
[2017-06-20 22:05:42,907][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-20 22:05:42,908][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-20 22:05:42,908][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-20 22:05:42,908][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpZcY6hf
[2017-06-20 22:05:43,094][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:05:43,094][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-20 22:05:43,095][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:05:43,095][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:05:43,095][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:05:43,095][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:05:43,095][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:05:43,095][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f258e821560>
[2017-06-20 22:05:43,095][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:05:43,095][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-20 22:05:43,096][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f258f138938>
[2017-06-20 22:05:43,096][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:05:43,096][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:05:43,096][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-20 22:05:43,123][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:05:43,137][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:05:43,137][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:05:43,154][gdb3][DEBUG ] detect machine type
[2017-06-20 22:05:43,156][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:06:03,024][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:06:03,024][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-20 22:06:03,024][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:06:03,024][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:06:03,024][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:06:03,024][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:06:03,024][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:06:03,024][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2d0ac47560>
[2017-06-20 22:06:03,025][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:06:03,025][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-20 22:06:03,025][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f2d0b55e938>
[2017-06-20 22:06:03,025][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:06:03,025][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:06:03,025][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-20 22:06:03,273][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:06:03,500][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:06:03,500][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:06:03,517][gdb1][DEBUG ] detect machine type
[2017-06-20 22:06:03,520][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:06:03,691][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-20 22:06:03,692][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:06:03,693][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2ea12b1950>
[2017-06-20 22:06:03,693][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:06:03,693][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-20 22:06:03,693][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2ea1507aa0>
[2017-06-20 22:06:03,693][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:06:03,693][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:06:03,693][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-20 22:06:03,693][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-20 22:06:03,933][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:06:04,124][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:06:04,124][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:06:04,140][gdb1][DEBUG ] detect machine type
[2017-06-20 22:06:04,145][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:06:04,146][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:06:04,146][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-20 22:06:04,146][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:06:04,149][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-06-20 22:06:04,149][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:06:04,151][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-20 22:06:04,272][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:06:04,287][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:06:04,303][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:06:04,311][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:06:04,326][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-20 22:06:04,330][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/buddystore
[2017-06-20 22:06:04,333][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/ceph_fsid.28089.tmp
[2017-06-20 22:06:04,341][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/fsid.28089.tmp
[2017-06-20 22:06:04,344][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/magic.28089.tmp
[2017-06-20 22:06:09,365][gdb1][INFO  ] checking OSD status...
[2017-06-20 22:06:09,366][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:06:09,368][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-20 22:06:09,534][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-20 22:06:09,697][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:06:09,697][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-06-20 22:06:09,697][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:06:09,697][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:06:09,697][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:06:09,698][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:06:09,698][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-20 22:06:09,698][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:06:09,698][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4cfffec950>
[2017-06-20 22:06:09,698][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:06:09,698][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f4d00242aa0>
[2017-06-20 22:06:09,698][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:06:09,698][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:06:09,698][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-20 22:06:09,698][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-20 22:06:09,940][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:06:10,171][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:06:10,172][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:06:10,189][gdb1][DEBUG ] detect machine type
[2017-06-20 22:06:10,193][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:06:10,193][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:06:10,194][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-06-20 22:06:10,194][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-20 22:06:10,194][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:06:10,196][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-06-20 22:06:10,367][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-06-20 22:06:10,367][gdb1][WARNING] activate: Cluster uuid is a69d8ca8-86b1-497a-bd30-832aff903e6c
[2017-06-20 22:06:10,367][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:06:10,367][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-20 22:06:10,367][gdb1][WARNING] activate: OSD uuid is bed1d9db-db25-4d5a-9b4f-99294b238086
[2017-06-20 22:06:10,367][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-20 22:06:10,367][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise bed1d9db-db25-4d5a-9b4f-99294b238086
[2017-06-20 22:06:10,399][gdb1][WARNING] Traceback (most recent call last):
[2017-06-20 22:06:10,399][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-20 22:06:10,399][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-20 22:06:10,399][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-20 22:06:10,399][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-20 22:06:10,399][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-06-20 22:06:10,400][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-20 22:06:10,400][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-06-20 22:06:10,400][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-06-20 22:06:10,400][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-06-20 22:06:10.390722 7f465a3a3700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-06-20 22:06:10,400][gdb1][WARNING] error connecting to the cluster
[2017-06-20 22:06:10,400][gdb1][WARNING] 
[2017-06-20 22:06:10,408][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-20 22:06:10,408][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore

[2017-06-20 22:07:18,323][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:07:18,323][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-20 22:07:18,323][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:07:18,323][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:07:18,323][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:07:18,323][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:07:18,324][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:07:18,324][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0d6a7c0560>
[2017-06-20 22:07:18,324][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:07:18,324][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-20 22:07:18,324][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f0d6b0d7938>
[2017-06-20 22:07:18,324][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:07:18,324][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:07:18,324][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-20 22:07:18,556][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:07:18,784][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:07:18,785][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:07:18,801][gdb1][DEBUG ] detect machine type
[2017-06-20 22:07:18,804][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:07:18,975][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:07:18,975][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:07:18,976][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efcea07d950>
[2017-06-20 22:07:18,977][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:07:18,977][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-20 22:07:18,977][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7efcea2d3aa0>
[2017-06-20 22:07:18,977][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:07:18,977][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:07:18,977][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-20 22:07:18,977][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-20 22:07:19,217][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:07:19,448][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:07:19,448][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:07:19,465][gdb1][DEBUG ] detect machine type
[2017-06-20 22:07:19,468][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:07:19,469][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:07:19,469][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-20 22:07:19,469][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:07:19,472][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-20 22:07:19,472][gdb1][DEBUG ] create a keyring file
[2017-06-20 22:07:19,473][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-06-20 22:07:19,474][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:07:19,476][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-20 22:07:19,596][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:07:19,612][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:07:19,628][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:07:19,635][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:07:19,651][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-20 22:07:19,658][gdb1][WARNING] populate_data_path: Data dir /mnt/buddystore already exists
[2017-06-20 22:07:24,671][gdb1][INFO  ] checking OSD status...
[2017-06-20 22:07:24,671][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:07:24,674][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-20 22:07:24,839][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-20 22:07:25,003][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd052539950>
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd05278faa0>
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:07:25,004][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:07:25,005][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-20 22:07:25,005][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-20 22:07:25,236][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:07:25,467][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:07:25,468][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:07:25,484][gdb1][DEBUG ] detect machine type
[2017-06-20 22:07:25,488][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:07:25,489][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:07:25,489][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-06-20 22:07:25,489][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-20 22:07:25,489][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:07:25,491][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-06-20 22:07:25,612][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-06-20 22:07:25,612][gdb1][WARNING] activate: Cluster uuid is a69d8ca8-86b1-497a-bd30-832aff903e6c
[2017-06-20 22:07:25,612][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:07:25,628][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-20 22:07:25,628][gdb1][WARNING] activate: OSD uuid is bed1d9db-db25-4d5a-9b4f-99294b238086
[2017-06-20 22:07:25,628][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-20 22:07:25,628][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise bed1d9db-db25-4d5a-9b4f-99294b238086
[2017-06-20 22:07:25,792][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/whoami.28534.tmp
[2017-06-20 22:07:25,793][gdb1][WARNING] activate: OSD id is 0
[2017-06-20 22:07:25,793][gdb1][WARNING] activate: Initializing OSD...
[2017-06-20 22:07:25,793][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/buddystore/activate.monmap
[2017-06-20 22:07:25,957][gdb1][WARNING] got monmap epoch 2
[2017-06-20 22:07:25,957][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/buddystore/activate.monmap --osd-data /mnt/buddystore --osd-journal /mnt/buddystore/journal --osd-uuid bed1d9db-db25-4d5a-9b4f-99294b238086 --keyring /mnt/buddystore/keyring --setuser ceph --setgroup ceph
[2017-06-20 22:07:28,830][gdb1][WARNING] Traceback (most recent call last):
[2017-06-20 22:07:28,830][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-20 22:07:28,830][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-20 22:07:28,830][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-20 22:07:28,831][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-20 22:07:28,831][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-06-20 22:07:28,831][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-20 22:07:28,831][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3638, in activate
[2017-06-20 22:07:28,831][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3076, in mkfs
[2017-06-20 22:07:28,831][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3023, in ceph_osd_mkfs
[2017-06-20 22:07:28,831][gdb1][WARNING] ceph_disk.main.Error: Error: ['ceph-osd', '--cluster', 'ceph', '--mkfs', '--mkkey', '-i', u'0', '--monmap', '/mnt/buddystore/activate.monmap', '--osd-data', '/mnt/buddystore', '--osd-journal', '/mnt/buddystore/journal', '--osd-uuid', u'bed1d9db-db25-4d5a-9b4f-99294b238086', '--keyring', '/mnt/buddystore/keyring', '--setuser', 'ceph', '--setgroup', 'ceph'] failed : /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: In function 'virtual void BuddyStore::set_fsid(uuid_d)' thread 7fdc41521cc0 time 2017-06-20 22:07:25.946696
[2017-06-20 22:07:28,831][gdb1][WARNING] /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: 194: FAILED assert(r >= 0)
[2017-06-20 22:07:28,831][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:07:28,831][gdb1][WARNING]  1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x102) [0x55f94bd46da2]
[2017-06-20 22:07:28,831][gdb1][WARNING]  2: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55f94bb150f8]
[2017-06-20 22:07:28,831][gdb1][WARNING]  3: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55f94b73aa5f]
[2017-06-20 22:07:28,831][gdb1][WARNING]  4: (main()+0x1041) [0x55f94b68ac11]
[2017-06-20 22:07:28,831][gdb1][WARNING]  5: (__libc_start_main()+0xf0) [0x7fdc3e98f830]
[2017-06-20 22:07:28,832][gdb1][WARNING]  6: (_start()+0x29) [0x55f94b712e79]
[2017-06-20 22:07:28,832][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:07:28,832][gdb1][WARNING] 2017-06-20 22:07:25.957509 7fdc41521cc0 -1 /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: In function 'virtual void BuddyStore::set_fsid(uuid_d)' thread 7fdc41521cc0 time 2017-06-20 22:07:25.946696
[2017-06-20 22:07:28,832][gdb1][WARNING] /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: 194: FAILED assert(r >= 0)
[2017-06-20 22:07:28,832][gdb1][WARNING] 
[2017-06-20 22:07:28,832][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:07:28,832][gdb1][WARNING]  1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x102) [0x55f94bd46da2]
[2017-06-20 22:07:28,832][gdb1][WARNING]  2: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55f94bb150f8]
[2017-06-20 22:07:28,832][gdb1][WARNING]  3: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55f94b73aa5f]
[2017-06-20 22:07:28,832][gdb1][WARNING]  4: (main()+0x1041) [0x55f94b68ac11]
[2017-06-20 22:07:28,832][gdb1][WARNING]  5: (__libc_start_main()+0xf0) [0x7fdc3e98f830]
[2017-06-20 22:07:28,832][gdb1][WARNING]  6: (_start()+0x29) [0x55f94b712e79]
[2017-06-20 22:07:28,832][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:07:28,832][gdb1][WARNING] 
[2017-06-20 22:07:28,833][gdb1][WARNING]      0> 2017-06-20 22:07:25.957509 7fdc41521cc0 -1 /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: In function 'virtual void BuddyStore::set_fsid(uuid_d)' thread 7fdc41521cc0 time 2017-06-20 22:07:25.946696
[2017-06-20 22:07:28,833][gdb1][WARNING] /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: 194: FAILED assert(r >= 0)
[2017-06-20 22:07:28,833][gdb1][WARNING] 
[2017-06-20 22:07:28,833][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:07:28,833][gdb1][WARNING]  1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x102) [0x55f94bd46da2]
[2017-06-20 22:07:28,833][gdb1][WARNING]  2: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55f94bb150f8]
[2017-06-20 22:07:28,833][gdb1][WARNING]  3: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55f94b73aa5f]
[2017-06-20 22:07:28,833][gdb1][WARNING]  4: (main()+0x1041) [0x55f94b68ac11]
[2017-06-20 22:07:28,833][gdb1][WARNING]  5: (__libc_start_main()+0xf0) [0x7fdc3e98f830]
[2017-06-20 22:07:28,833][gdb1][WARNING]  6: (_start()+0x29) [0x55f94b712e79]
[2017-06-20 22:07:28,833][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:07:28,833][gdb1][WARNING] 
[2017-06-20 22:07:28,833][gdb1][WARNING] *** Caught signal (Aborted) **
[2017-06-20 22:07:28,833][gdb1][WARNING]  in thread 7fdc41521cc0 thread_name:ceph-osd
[2017-06-20 22:07:28,834][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:07:28,834][gdb1][WARNING]  1: (()+0x9ecb22) [0x55f94bce4b22]
[2017-06-20 22:07:28,834][gdb1][WARNING]  2: (()+0x11390) [0x7fdc3fa09390]
[2017-06-20 22:07:28,834][gdb1][WARNING]  3: (gsignal()+0x38) [0x7fdc3e9a4428]
[2017-06-20 22:07:28,834][gdb1][WARNING]  4: (abort()+0x16a) [0x7fdc3e9a602a]
[2017-06-20 22:07:28,834][gdb1][WARNING]  5: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x28e) [0x55f94bd46f2e]
[2017-06-20 22:07:28,834][gdb1][WARNING]  6: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55f94bb150f8]
[2017-06-20 22:07:28,834][gdb1][WARNING]  7: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55f94b73aa5f]
[2017-06-20 22:07:28,834][gdb1][WARNING]  8: (main()+0x1041) [0x55f94b68ac11]
[2017-06-20 22:07:28,834][gdb1][WARNING]  9: (__libc_start_main()+0xf0) [0x7fdc3e98f830]
[2017-06-20 22:07:28,834][gdb1][WARNING]  10: (_start()+0x29) [0x55f94b712e79]
[2017-06-20 22:07:28,834][gdb1][WARNING] 2017-06-20 22:07:25.959333 7fdc41521cc0 -1 *** Caught signal (Aborted) **
[2017-06-20 22:07:28,834][gdb1][WARNING]  in thread 7fdc41521cc0 thread_name:ceph-osd
[2017-06-20 22:07:28,834][gdb1][WARNING] 
[2017-06-20 22:07:28,835][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:07:28,835][gdb1][WARNING]  1: (()+0x9ecb22) [0x55f94bce4b22]
[2017-06-20 22:07:28,835][gdb1][WARNING]  2: (()+0x11390) [0x7fdc3fa09390]
[2017-06-20 22:07:28,835][gdb1][WARNING]  3: (gsignal()+0x38) [0x7fdc3e9a4428]
[2017-06-20 22:07:28,835][gdb1][WARNING]  4: (abort()+0x16a) [0x7fdc3e9a602a]
[2017-06-20 22:07:28,835][gdb1][WARNING]  5: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x28e) [0x55f94bd46f2e]
[2017-06-20 22:07:28,835][gdb1][WARNING]  6: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55f94bb150f8]
[2017-06-20 22:07:28,835][gdb1][WARNING]  7: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55f94b73aa5f]
[2017-06-20 22:07:28,835][gdb1][WARNING]  8: (main()+0x1041) [0x55f94b68ac11]
[2017-06-20 22:07:28,835][gdb1][WARNING]  9: (__libc_start_main()+0xf0) [0x7fdc3e98f830]
[2017-06-20 22:07:28,835][gdb1][WARNING]  10: (_start()+0x29) [0x55f94b712e79]
[2017-06-20 22:07:28,835][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:07:28,835][gdb1][WARNING] 
[2017-06-20 22:07:28,835][gdb1][WARNING]      0> 2017-06-20 22:07:25.959333 7fdc41521cc0 -1 *** Caught signal (Aborted) **
[2017-06-20 22:07:28,835][gdb1][WARNING]  in thread 7fdc41521cc0 thread_name:ceph-osd
[2017-06-20 22:07:28,836][gdb1][WARNING] 
[2017-06-20 22:07:28,836][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:07:28,836][gdb1][WARNING]  1: (()+0x9ecb22) [0x55f94bce4b22]
[2017-06-20 22:07:28,836][gdb1][WARNING]  2: (()+0x11390) [0x7fdc3fa09390]
[2017-06-20 22:07:28,836][gdb1][WARNING]  3: (gsignal()+0x38) [0x7fdc3e9a4428]
[2017-06-20 22:07:28,836][gdb1][WARNING]  4: (abort()+0x16a) [0x7fdc3e9a602a]
[2017-06-20 22:07:28,836][gdb1][WARNING]  5: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x28e) [0x55f94bd46f2e]
[2017-06-20 22:07:28,836][gdb1][WARNING]  6: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55f94bb150f8]
[2017-06-20 22:07:28,836][gdb1][WARNING]  7: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55f94b73aa5f]
[2017-06-20 22:07:28,836][gdb1][WARNING]  8: (main()+0x1041) [0x55f94b68ac11]
[2017-06-20 22:07:28,836][gdb1][WARNING]  9: (__libc_start_main()+0xf0) [0x7fdc3e98f830]
[2017-06-20 22:07:28,836][gdb1][WARNING]  10: (_start()+0x29) [0x55f94b712e79]
[2017-06-20 22:07:28,836][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:07:28,836][gdb1][WARNING] 
[2017-06-20 22:07:28,837][gdb1][WARNING] /usr/bin/timeout: the monitored command dumped core
[2017-06-20 22:07:28,837][gdb1][WARNING] 
[2017-06-20 22:07:28,837][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-20 22:07:28,837][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore

[2017-06-20 22:09:13,595][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:09:13,595][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-20 22:09:13,595][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:09:13,595][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:09:13,595][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:09:13,595][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:09:13,595][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:09:13,595][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feae68965a8>
[2017-06-20 22:09:13,595][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:09:13,595][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-20 22:09:13,596][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-20 22:09:13,596][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7feae6f1a758>
[2017-06-20 22:09:13,596][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-20 22:09:13,596][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:09:13,596][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-20 22:09:13,596][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:09:13,596][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-20 22:09:13,596][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-20 22:09:13,596][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-20 22:09:13,622][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:09:13,636][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:09:13,637][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:09:13,653][gdb3][DEBUG ] detect machine type
[2017-06-20 22:09:13,656][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:09:13,657][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-20 22:09:13,668][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-20 22:09:13,674][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-20 22:09:13,674][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-20 22:09:13,674][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-20 22:09:13,675][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-20 22:09:13,675][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-20 22:09:13,675][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-20 22:09:13,675][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-20 22:09:13,675][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-20 22:09:13,842][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:09:13,842][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-06-20 22:09:13,842][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:09:13,843][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:09:13,843][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:09:13,843][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:09:13,843][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-20 22:09:13,843][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:09:13,843][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6e56751ea8>
[2017-06-20 22:09:13,843][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:09:13,843][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f6e56725b18>
[2017-06-20 22:09:13,843][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:09:13,843][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-20 22:09:13,843][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:09:13,844][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-20 22:09:13,844][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-20 22:09:13,870][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:09:13,884][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:09:13,885][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:09:13,902][gdb3][DEBUG ] detect machine type
[2017-06-20 22:09:13,904][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:09:13,904][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-20 22:09:13,905][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-20 22:09:13,905][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:09:13,905][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-20 22:09:13,905][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:09:13,905][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-20 22:09:13,906][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:09:13,907][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-20 22:09:13,908][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-20 22:09:13,908][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-20 22:09:13,908][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-20 22:09:13,910][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-20 22:09:13,981][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-20 22:09:14,054][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-20 22:09:16,068][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:09:16,133][gdb3][DEBUG ] ********************************************************************************
[2017-06-20 22:09:16,133][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-20 22:09:16,133][gdb3][DEBUG ] {
[2017-06-20 22:09:16,133][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-20 22:09:16,133][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-20 22:09:16,133][gdb3][DEBUG ]   "features": {
[2017-06-20 22:09:16,133][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-20 22:09:16,133][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-20 22:09:16,134][gdb3][DEBUG ]       "kraken", 
[2017-06-20 22:09:16,134][gdb3][DEBUG ]       "luminous"
[2017-06-20 22:09:16,134][gdb3][DEBUG ]     ], 
[2017-06-20 22:09:16,134][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-20 22:09:16,134][gdb3][DEBUG ]     "required_mon": [
[2017-06-20 22:09:16,134][gdb3][DEBUG ]       "kraken", 
[2017-06-20 22:09:16,134][gdb3][DEBUG ]       "luminous"
[2017-06-20 22:09:16,134][gdb3][DEBUG ]     ]
[2017-06-20 22:09:16,134][gdb3][DEBUG ]   }, 
[2017-06-20 22:09:16,134][gdb3][DEBUG ]   "monmap": {
[2017-06-20 22:09:16,134][gdb3][DEBUG ]     "created": "2017-06-20 22:05:38.653272", 
[2017-06-20 22:09:16,134][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-20 22:09:16,134][gdb3][DEBUG ]     "features": {
[2017-06-20 22:09:16,134][gdb3][DEBUG ]       "optional": [], 
[2017-06-20 22:09:16,134][gdb3][DEBUG ]       "persistent": [
[2017-06-20 22:09:16,134][gdb3][DEBUG ]         "kraken", 
[2017-06-20 22:09:16,134][gdb3][DEBUG ]         "luminous"
[2017-06-20 22:09:16,135][gdb3][DEBUG ]       ]
[2017-06-20 22:09:16,135][gdb3][DEBUG ]     }, 
[2017-06-20 22:09:16,135][gdb3][DEBUG ]     "fsid": "a69d8ca8-86b1-497a-bd30-832aff903e6c", 
[2017-06-20 22:09:16,135][gdb3][DEBUG ]     "modified": "2017-06-20 22:05:38.899886", 
[2017-06-20 22:09:16,135][gdb3][DEBUG ]     "mons": [
[2017-06-20 22:09:16,135][gdb3][DEBUG ]       {
[2017-06-20 22:09:16,135][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-20 22:09:16,135][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-20 22:09:16,135][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-20 22:09:16,135][gdb3][DEBUG ]         "rank": 0
[2017-06-20 22:09:16,135][gdb3][DEBUG ]       }
[2017-06-20 22:09:16,135][gdb3][DEBUG ]     ]
[2017-06-20 22:09:16,135][gdb3][DEBUG ]   }, 
[2017-06-20 22:09:16,135][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-20 22:09:16,135][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-20 22:09:16,135][gdb3][DEBUG ]   "quorum": [
[2017-06-20 22:09:16,135][gdb3][DEBUG ]     0
[2017-06-20 22:09:16,136][gdb3][DEBUG ]   ], 
[2017-06-20 22:09:16,136][gdb3][DEBUG ]   "rank": 0, 
[2017-06-20 22:09:16,136][gdb3][DEBUG ]   "state": "leader", 
[2017-06-20 22:09:16,136][gdb3][DEBUG ]   "sync_provider": []
[2017-06-20 22:09:16,136][gdb3][DEBUG ] }
[2017-06-20 22:09:16,136][gdb3][DEBUG ] ********************************************************************************
[2017-06-20 22:09:16,136][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-20 22:09:16,137][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:09:16,202][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-20 22:09:16,216][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:09:16,231][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:09:16,231][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:09:16,248][gdb3][DEBUG ] detect machine type
[2017-06-20 22:09:16,250][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:09:16,251][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:09:16,316][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-20 22:09:16,317][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-20 22:09:16,317][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-20 22:09:16,318][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmp7sh5Nq
[2017-06-20 22:09:16,334][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:09:16,347][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:09:16,348][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:09:16,365][gdb3][DEBUG ] detect machine type
[2017-06-20 22:09:16,367][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:09:16,368][gdb3][DEBUG ] fetch remote file
[2017-06-20 22:09:16,369][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:09:16,435][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-20 22:09:16,601][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-20 22:09:16,767][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-20 22:09:16,933][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-20 22:09:17,100][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-20 22:09:17,265][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-06-20 22:09:17,265][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-06-20 22:09:17,265][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-06-20 22:09:17,266][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.mon.keyring' and backing up old key as 'ceph.mon.keyring-20170620220917'
[2017-06-20 22:09:17,266][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-06-20 22:09:17,266][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-06-20 22:09:17,266][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp7sh5Nq
[2017-06-20 22:09:17,447][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:09:17,447][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-20 22:09:17,447][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:09:17,448][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:09:17,448][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:09:17,448][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:09:17,448][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:09:17,448][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe93dc32560>
[2017-06-20 22:09:17,448][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:09:17,448][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-20 22:09:17,448][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fe93e549938>
[2017-06-20 22:09:17,448][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:09:17,448][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:09:17,448][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-20 22:09:17,474][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:09:17,489][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:09:17,489][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:09:17,506][gdb3][DEBUG ] detect machine type
[2017-06-20 22:09:17,508][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:09:33,534][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:09:33,534][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-20 22:09:33,535][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:09:33,535][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:09:33,535][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:09:33,535][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:09:33,535][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:09:33,535][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f55dbfe9560>
[2017-06-20 22:09:33,535][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:09:33,535][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-20 22:09:33,535][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f55dc900938>
[2017-06-20 22:09:33,535][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:09:33,535][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:09:33,535][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-20 22:09:33,777][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:09:34,004][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:09:34,004][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:09:34,021][gdb1][DEBUG ] detect machine type
[2017-06-20 22:09:34,025][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:09:34,197][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:09:34,197][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-06-20 22:09:34,197][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:09:34,197][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:09:34,197][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-20 22:09:34,197][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2442412950>
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2442668aa0>
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:09:34,198][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-20 22:09:34,199][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-20 22:09:34,445][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:09:34,676][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:09:34,676][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:09:34,693][gdb1][DEBUG ] detect machine type
[2017-06-20 22:09:34,697][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:09:34,698][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:09:34,698][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-20 22:09:34,698][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:09:34,701][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-20 22:09:34,701][gdb1][DEBUG ] create a keyring file
[2017-06-20 22:09:34,703][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-06-20 22:09:34,703][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:09:34,706][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-20 22:09:34,876][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:09:34,876][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:09:34,876][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:09:34,876][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:09:34,876][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-20 22:09:34,892][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/buddystore
[2017-06-20 22:09:34,892][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/ceph_fsid.28772.tmp
[2017-06-20 22:09:34,896][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/fsid.28772.tmp
[2017-06-20 22:09:34,899][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/magic.28772.tmp
[2017-06-20 22:09:39,920][gdb1][INFO  ] checking OSD status...
[2017-06-20 22:09:39,920][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:09:39,923][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-20 22:09:40,088][gdb1][WARNING] there is 1 OSD down
[2017-06-20 22:09:40,088][gdb1][WARNING] there is 1 OSD out
[2017-06-20 22:09:40,089][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-20 22:09:40,253][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:09:40,253][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f32ad0d8950>
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f32ad32eaa0>
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:09:40,254][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-20 22:09:40,255][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-20 22:09:40,500][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:09:40,731][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:09:40,732][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:09:40,748][gdb1][DEBUG ] detect machine type
[2017-06-20 22:09:40,753][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:09:40,753][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:09:40,754][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-06-20 22:09:40,754][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-20 22:09:40,754][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:09:40,756][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-06-20 22:09:40,926][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-06-20 22:09:40,926][gdb1][WARNING] activate: Cluster uuid is 3ce7f1c5-c1e4-4eaf-8fed-4f207357f5a7
[2017-06-20 22:09:40,926][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:09:40,926][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-20 22:09:40,927][gdb1][WARNING] activate: OSD uuid is ad212492-3b37-485c-9723-977d779c342b
[2017-06-20 22:09:40,927][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-20 22:09:40,927][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise ad212492-3b37-485c-9723-977d779c342b
[2017-06-20 22:09:41,242][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/whoami.28889.tmp
[2017-06-20 22:09:41,242][gdb1][WARNING] activate: OSD id is 1
[2017-06-20 22:09:41,242][gdb1][WARNING] activate: Initializing OSD...
[2017-06-20 22:09:41,242][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/buddystore/activate.monmap
[2017-06-20 22:09:41,356][gdb1][WARNING] got monmap epoch 2
[2017-06-20 22:09:41,356][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 1 --monmap /mnt/buddystore/activate.monmap --osd-data /mnt/buddystore --osd-journal /mnt/buddystore/journal --osd-uuid ad212492-3b37-485c-9723-977d779c342b --keyring /mnt/buddystore/keyring --setuser ceph --setgroup ceph
[2017-06-20 22:09:41,521][gdb1][WARNING] Traceback (most recent call last):
[2017-06-20 22:09:41,521][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-20 22:09:41,521][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-20 22:09:41,521][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-20 22:09:41,521][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-20 22:09:41,521][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-06-20 22:09:41,521][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-20 22:09:41,521][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3638, in activate
[2017-06-20 22:09:41,521][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3076, in mkfs
[2017-06-20 22:09:41,521][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3023, in ceph_osd_mkfs
[2017-06-20 22:09:41,521][gdb1][WARNING] ceph_disk.main.Error: Error: ['ceph-osd', '--cluster', 'ceph', '--mkfs', '--mkkey', '-i', u'1', '--monmap', '/mnt/buddystore/activate.monmap', '--osd-data', '/mnt/buddystore', '--osd-journal', '/mnt/buddystore/journal', '--osd-uuid', u'ad212492-3b37-485c-9723-977d779c342b', '--keyring', '/mnt/buddystore/keyring', '--setuser', 'ceph', '--setgroup', 'ceph'] failed : /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: In function 'virtual void BuddyStore::set_fsid(uuid_d)' thread 7f2270abccc0 time 2017-06-20 22:09:41.359673
[2017-06-20 22:09:41,521][gdb1][WARNING] /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: 194: FAILED assert(r >= 0)
[2017-06-20 22:09:41,522][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:09:41,522][gdb1][WARNING]  1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x102) [0x55c76d98fda2]
[2017-06-20 22:09:41,522][gdb1][WARNING]  2: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55c76d75e0f8]
[2017-06-20 22:09:41,522][gdb1][WARNING]  3: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55c76d383a5f]
[2017-06-20 22:09:41,522][gdb1][WARNING]  4: (main()+0x1041) [0x55c76d2d3c11]
[2017-06-20 22:09:41,522][gdb1][WARNING]  5: (__libc_start_main()+0xf0) [0x7f226df2a830]
[2017-06-20 22:09:41,522][gdb1][WARNING]  6: (_start()+0x29) [0x55c76d35be79]
[2017-06-20 22:09:41,522][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:09:41,522][gdb1][WARNING] 2017-06-20 22:09:41.361300 7f2270abccc0 -1 /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: In function 'virtual void BuddyStore::set_fsid(uuid_d)' thread 7f2270abccc0 time 2017-06-20 22:09:41.359673
[2017-06-20 22:09:41,522][gdb1][WARNING] /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: 194: FAILED assert(r >= 0)
[2017-06-20 22:09:41,522][gdb1][WARNING] 
[2017-06-20 22:09:41,522][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:09:41,522][gdb1][WARNING]  1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x102) [0x55c76d98fda2]
[2017-06-20 22:09:41,523][gdb1][WARNING]  2: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55c76d75e0f8]
[2017-06-20 22:09:41,523][gdb1][WARNING]  3: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55c76d383a5f]
[2017-06-20 22:09:41,523][gdb1][WARNING]  4: (main()+0x1041) [0x55c76d2d3c11]
[2017-06-20 22:09:41,523][gdb1][WARNING]  5: (__libc_start_main()+0xf0) [0x7f226df2a830]
[2017-06-20 22:09:41,523][gdb1][WARNING]  6: (_start()+0x29) [0x55c76d35be79]
[2017-06-20 22:09:41,523][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:09:41,523][gdb1][WARNING] 
[2017-06-20 22:09:41,523][gdb1][WARNING]      0> 2017-06-20 22:09:41.361300 7f2270abccc0 -1 /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: In function 'virtual void BuddyStore::set_fsid(uuid_d)' thread 7f2270abccc0 time 2017-06-20 22:09:41.359673
[2017-06-20 22:09:41,523][gdb1][WARNING] /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: 194: FAILED assert(r >= 0)
[2017-06-20 22:09:41,523][gdb1][WARNING] 
[2017-06-20 22:09:41,523][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:09:41,523][gdb1][WARNING]  1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x102) [0x55c76d98fda2]
[2017-06-20 22:09:41,523][gdb1][WARNING]  2: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55c76d75e0f8]
[2017-06-20 22:09:41,523][gdb1][WARNING]  3: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55c76d383a5f]
[2017-06-20 22:09:41,523][gdb1][WARNING]  4: (main()+0x1041) [0x55c76d2d3c11]
[2017-06-20 22:09:41,524][gdb1][WARNING]  5: (__libc_start_main()+0xf0) [0x7f226df2a830]
[2017-06-20 22:09:41,524][gdb1][WARNING]  6: (_start()+0x29) [0x55c76d35be79]
[2017-06-20 22:09:41,524][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:09:41,524][gdb1][WARNING] 
[2017-06-20 22:09:41,524][gdb1][WARNING] *** Caught signal (Aborted) **
[2017-06-20 22:09:41,524][gdb1][WARNING]  in thread 7f2270abccc0 thread_name:ceph-osd
[2017-06-20 22:09:41,524][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:09:41,524][gdb1][WARNING]  1: (()+0x9ecb22) [0x55c76d92db22]
[2017-06-20 22:09:41,524][gdb1][WARNING]  2: (()+0x11390) [0x7f226efa4390]
[2017-06-20 22:09:41,524][gdb1][WARNING]  3: (gsignal()+0x38) [0x7f226df3f428]
[2017-06-20 22:09:41,524][gdb1][WARNING]  4: (abort()+0x16a) [0x7f226df4102a]
[2017-06-20 22:09:41,524][gdb1][WARNING]  5: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x28e) [0x55c76d98ff2e]
[2017-06-20 22:09:41,524][gdb1][WARNING]  6: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55c76d75e0f8]
[2017-06-20 22:09:41,524][gdb1][WARNING]  7: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55c76d383a5f]
[2017-06-20 22:09:41,525][gdb1][WARNING]  8: (main()+0x1041) [0x55c76d2d3c11]
[2017-06-20 22:09:41,525][gdb1][WARNING]  9: (__libc_start_main()+0xf0) [0x7f226df2a830]
[2017-06-20 22:09:41,525][gdb1][WARNING]  10: (_start()+0x29) [0x55c76d35be79]
[2017-06-20 22:09:41,525][gdb1][WARNING] 2017-06-20 22:09:41.363101 7f2270abccc0 -1 *** Caught signal (Aborted) **
[2017-06-20 22:09:41,525][gdb1][WARNING]  in thread 7f2270abccc0 thread_name:ceph-osd
[2017-06-20 22:09:41,525][gdb1][WARNING] 
[2017-06-20 22:09:41,525][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:09:41,525][gdb1][WARNING]  1: (()+0x9ecb22) [0x55c76d92db22]
[2017-06-20 22:09:41,525][gdb1][WARNING]  2: (()+0x11390) [0x7f226efa4390]
[2017-06-20 22:09:41,525][gdb1][WARNING]  3: (gsignal()+0x38) [0x7f226df3f428]
[2017-06-20 22:09:41,525][gdb1][WARNING]  4: (abort()+0x16a) [0x7f226df4102a]
[2017-06-20 22:09:41,525][gdb1][WARNING]  5: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x28e) [0x55c76d98ff2e]
[2017-06-20 22:09:41,525][gdb1][WARNING]  6: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55c76d75e0f8]
[2017-06-20 22:09:41,525][gdb1][WARNING]  7: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55c76d383a5f]
[2017-06-20 22:09:41,526][gdb1][WARNING]  8: (main()+0x1041) [0x55c76d2d3c11]
[2017-06-20 22:09:41,526][gdb1][WARNING]  9: (__libc_start_main()+0xf0) [0x7f226df2a830]
[2017-06-20 22:09:41,526][gdb1][WARNING]  10: (_start()+0x29) [0x55c76d35be79]
[2017-06-20 22:09:41,526][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:09:41,526][gdb1][WARNING] 
[2017-06-20 22:09:41,526][gdb1][WARNING]      0> 2017-06-20 22:09:41.363101 7f2270abccc0 -1 *** Caught signal (Aborted) **
[2017-06-20 22:09:41,526][gdb1][WARNING]  in thread 7f2270abccc0 thread_name:ceph-osd
[2017-06-20 22:09:41,526][gdb1][WARNING] 
[2017-06-20 22:09:41,526][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:09:41,526][gdb1][WARNING]  1: (()+0x9ecb22) [0x55c76d92db22]
[2017-06-20 22:09:41,526][gdb1][WARNING]  2: (()+0x11390) [0x7f226efa4390]
[2017-06-20 22:09:41,526][gdb1][WARNING]  3: (gsignal()+0x38) [0x7f226df3f428]
[2017-06-20 22:09:41,526][gdb1][WARNING]  4: (abort()+0x16a) [0x7f226df4102a]
[2017-06-20 22:09:41,526][gdb1][WARNING]  5: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x28e) [0x55c76d98ff2e]
[2017-06-20 22:09:41,526][gdb1][WARNING]  6: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x55c76d75e0f8]
[2017-06-20 22:09:41,527][gdb1][WARNING]  7: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x55c76d383a5f]
[2017-06-20 22:09:41,527][gdb1][WARNING]  8: (main()+0x1041) [0x55c76d2d3c11]
[2017-06-20 22:09:41,527][gdb1][WARNING]  9: (__libc_start_main()+0xf0) [0x7f226df2a830]
[2017-06-20 22:09:41,527][gdb1][WARNING]  10: (_start()+0x29) [0x55c76d35be79]
[2017-06-20 22:09:41,527][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:09:41,527][gdb1][WARNING] 
[2017-06-20 22:09:41,527][gdb1][WARNING] /usr/bin/timeout: the monitored command dumped core
[2017-06-20 22:09:41,527][gdb1][WARNING] 
[2017-06-20 22:09:41,527][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-20 22:09:41,527][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore

[2017-06-20 22:11:17,870][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:11:17,870][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-06-20 22:11:17,870][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:11:17,870][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f56d2347950>
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f56d259daa0>
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:11:17,871][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:11:17,872][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-20 22:11:17,872][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-20 22:11:18,116][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:11:18,344][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:11:18,344][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:11:18,361][gdb1][DEBUG ] detect machine type
[2017-06-20 22:11:18,365][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:11:18,366][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:11:18,366][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-20 22:11:18,366][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:11:18,369][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-06-20 22:11:18,369][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:11:18,371][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-20 22:11:18,542][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:11:18,542][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:11:18,542][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:11:18,542][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:11:18,542][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-20 22:11:18,558][gdb1][WARNING] populate_data_path: Data dir /mnt/buddystore already exists
[2017-06-20 22:11:23,564][gdb1][INFO  ] checking OSD status...
[2017-06-20 22:11:23,565][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:11:23,567][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-20 22:11:23,733][gdb1][WARNING] there are 2 OSDs down
[2017-06-20 22:11:23,733][gdb1][WARNING] there are 2 OSDs out
[2017-06-20 22:11:23,733][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-20 22:12:05,068][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:12:05,068][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9fefdc9950>
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9ff001faa0>
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:12:05,069][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-20 22:12:05,070][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-20 22:12:05,309][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:12:05,536][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:12:05,537][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:12:05,553][gdb1][DEBUG ] detect machine type
[2017-06-20 22:12:05,557][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:12:05,558][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:12:05,558][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-06-20 22:12:05,558][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-20 22:12:05,558][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:12:05,560][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-06-20 22:12:05,731][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-06-20 22:12:05,731][gdb1][WARNING] activate: Cluster uuid is 3ce7f1c5-c1e4-4eaf-8fed-4f207357f5a7
[2017-06-20 22:12:05,731][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:12:05,731][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-20 22:12:05,732][gdb1][WARNING] activate: OSD uuid is ad212492-3b37-485c-9723-977d779c342b
[2017-06-20 22:12:05,732][gdb1][WARNING] activate: OSD id is 1
[2017-06-20 22:12:05,732][gdb1][WARNING] activate: Initializing OSD...
[2017-06-20 22:12:05,732][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/buddystore/activate.monmap
[2017-06-20 22:12:05,846][gdb1][WARNING] got monmap epoch 2
[2017-06-20 22:12:05,846][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 1 --monmap /mnt/buddystore/activate.monmap --osd-data /mnt/buddystore --osd-journal /mnt/buddystore/journal --osd-uuid ad212492-3b37-485c-9723-977d779c342b --keyring /mnt/buddystore/keyring --setuser ceph --setgroup ceph
[2017-06-20 22:12:06,010][gdb1][WARNING] Traceback (most recent call last):
[2017-06-20 22:12:06,010][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-20 22:12:06,011][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-20 22:12:06,011][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-20 22:12:06,011][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-20 22:12:06,011][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-06-20 22:12:06,011][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-20 22:12:06,011][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3638, in activate
[2017-06-20 22:12:06,011][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3076, in mkfs
[2017-06-20 22:12:06,011][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3023, in ceph_osd_mkfs
[2017-06-20 22:12:06,011][gdb1][WARNING] ceph_disk.main.Error: Error: ['ceph-osd', '--cluster', 'ceph', '--mkfs', '--mkkey', '-i', u'1', '--monmap', '/mnt/buddystore/activate.monmap', '--osd-data', '/mnt/buddystore', '--osd-journal', '/mnt/buddystore/journal', '--osd-uuid', u'ad212492-3b37-485c-9723-977d779c342b', '--keyring', '/mnt/buddystore/keyring', '--setuser', 'ceph', '--setgroup', 'ceph'] failed : /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: In function 'virtual void BuddyStore::set_fsid(uuid_d)' thread 7f4c3f3bfcc0 time 2017-06-20 22:12:05.854546
[2017-06-20 22:12:06,011][gdb1][WARNING] /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: 194: FAILED assert(r >= 0)
[2017-06-20 22:12:06,011][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:12:06,011][gdb1][WARNING]  1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x102) [0x5566bc053da2]
[2017-06-20 22:12:06,012][gdb1][WARNING]  2: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x5566bbe220f8]
[2017-06-20 22:12:06,012][gdb1][WARNING]  3: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x5566bba47a5f]
[2017-06-20 22:12:06,012][gdb1][WARNING]  4: (main()+0x1041) [0x5566bb997c11]
[2017-06-20 22:12:06,012][gdb1][WARNING]  5: (__libc_start_main()+0xf0) [0x7f4c3c82d830]
[2017-06-20 22:12:06,012][gdb1][WARNING]  6: (_start()+0x29) [0x5566bba1fe79]
[2017-06-20 22:12:06,012][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:12:06,012][gdb1][WARNING] 2017-06-20 22:12:05.856211 7f4c3f3bfcc0 -1 /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: In function 'virtual void BuddyStore::set_fsid(uuid_d)' thread 7f4c3f3bfcc0 time 2017-06-20 22:12:05.854546
[2017-06-20 22:12:06,012][gdb1][WARNING] /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: 194: FAILED assert(r >= 0)
[2017-06-20 22:12:06,012][gdb1][WARNING] 
[2017-06-20 22:12:06,012][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:12:06,012][gdb1][WARNING]  1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x102) [0x5566bc053da2]
[2017-06-20 22:12:06,012][gdb1][WARNING]  2: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x5566bbe220f8]
[2017-06-20 22:12:06,012][gdb1][WARNING]  3: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x5566bba47a5f]
[2017-06-20 22:12:06,013][gdb1][WARNING]  4: (main()+0x1041) [0x5566bb997c11]
[2017-06-20 22:12:06,013][gdb1][WARNING]  5: (__libc_start_main()+0xf0) [0x7f4c3c82d830]
[2017-06-20 22:12:06,013][gdb1][WARNING]  6: (_start()+0x29) [0x5566bba1fe79]
[2017-06-20 22:12:06,013][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:12:06,013][gdb1][WARNING] 
[2017-06-20 22:12:06,013][gdb1][WARNING]      0> 2017-06-20 22:12:05.856211 7f4c3f3bfcc0 -1 /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: In function 'virtual void BuddyStore::set_fsid(uuid_d)' thread 7f4c3f3bfcc0 time 2017-06-20 22:12:05.854546
[2017-06-20 22:12:06,013][gdb1][WARNING] /home/ubuntu/ceph_bak/src/os/buddystore/BuddyStore.cc: 194: FAILED assert(r >= 0)
[2017-06-20 22:12:06,013][gdb1][WARNING] 
[2017-06-20 22:12:06,013][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:12:06,013][gdb1][WARNING]  1: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x102) [0x5566bc053da2]
[2017-06-20 22:12:06,013][gdb1][WARNING]  2: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x5566bbe220f8]
[2017-06-20 22:12:06,013][gdb1][WARNING]  3: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x5566bba47a5f]
[2017-06-20 22:12:06,013][gdb1][WARNING]  4: (main()+0x1041) [0x5566bb997c11]
[2017-06-20 22:12:06,014][gdb1][WARNING]  5: (__libc_start_main()+0xf0) [0x7f4c3c82d830]
[2017-06-20 22:12:06,014][gdb1][WARNING]  6: (_start()+0x29) [0x5566bba1fe79]
[2017-06-20 22:12:06,014][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:12:06,014][gdb1][WARNING] 
[2017-06-20 22:12:06,014][gdb1][WARNING] *** Caught signal (Aborted) **
[2017-06-20 22:12:06,014][gdb1][WARNING]  in thread 7f4c3f3bfcc0 thread_name:ceph-osd
[2017-06-20 22:12:06,014][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:12:06,014][gdb1][WARNING]  1: (()+0x9ecb22) [0x5566bbff1b22]
[2017-06-20 22:12:06,014][gdb1][WARNING]  2: (()+0x11390) [0x7f4c3d8a7390]
[2017-06-20 22:12:06,014][gdb1][WARNING]  3: (gsignal()+0x38) [0x7f4c3c842428]
[2017-06-20 22:12:06,014][gdb1][WARNING]  4: (abort()+0x16a) [0x7f4c3c84402a]
[2017-06-20 22:12:06,014][gdb1][WARNING]  5: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x28e) [0x5566bc053f2e]
[2017-06-20 22:12:06,014][gdb1][WARNING]  6: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x5566bbe220f8]
[2017-06-20 22:12:06,015][gdb1][WARNING]  7: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x5566bba47a5f]
[2017-06-20 22:12:06,015][gdb1][WARNING]  8: (main()+0x1041) [0x5566bb997c11]
[2017-06-20 22:12:06,015][gdb1][WARNING]  9: (__libc_start_main()+0xf0) [0x7f4c3c82d830]
[2017-06-20 22:12:06,015][gdb1][WARNING]  10: (_start()+0x29) [0x5566bba1fe79]
[2017-06-20 22:12:06,015][gdb1][WARNING] 2017-06-20 22:12:05.857998 7f4c3f3bfcc0 -1 *** Caught signal (Aborted) **
[2017-06-20 22:12:06,015][gdb1][WARNING]  in thread 7f4c3f3bfcc0 thread_name:ceph-osd
[2017-06-20 22:12:06,015][gdb1][WARNING] 
[2017-06-20 22:12:06,015][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:12:06,015][gdb1][WARNING]  1: (()+0x9ecb22) [0x5566bbff1b22]
[2017-06-20 22:12:06,015][gdb1][WARNING]  2: (()+0x11390) [0x7f4c3d8a7390]
[2017-06-20 22:12:06,015][gdb1][WARNING]  3: (gsignal()+0x38) [0x7f4c3c842428]
[2017-06-20 22:12:06,015][gdb1][WARNING]  4: (abort()+0x16a) [0x7f4c3c84402a]
[2017-06-20 22:12:06,015][gdb1][WARNING]  5: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x28e) [0x5566bc053f2e]
[2017-06-20 22:12:06,015][gdb1][WARNING]  6: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x5566bbe220f8]
[2017-06-20 22:12:06,016][gdb1][WARNING]  7: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x5566bba47a5f]
[2017-06-20 22:12:06,016][gdb1][WARNING]  8: (main()+0x1041) [0x5566bb997c11]
[2017-06-20 22:12:06,016][gdb1][WARNING]  9: (__libc_start_main()+0xf0) [0x7f4c3c82d830]
[2017-06-20 22:12:06,016][gdb1][WARNING]  10: (_start()+0x29) [0x5566bba1fe79]
[2017-06-20 22:12:06,016][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:12:06,016][gdb1][WARNING] 
[2017-06-20 22:12:06,016][gdb1][WARNING]      0> 2017-06-20 22:12:05.857998 7f4c3f3bfcc0 -1 *** Caught signal (Aborted) **
[2017-06-20 22:12:06,016][gdb1][WARNING]  in thread 7f4c3f3bfcc0 thread_name:ceph-osd
[2017-06-20 22:12:06,016][gdb1][WARNING] 
[2017-06-20 22:12:06,016][gdb1][WARNING]  ceph version 12.0.1-1343-g3243588 (32435883f95c43da5782dcd45dc2bf9fc43838ee)
[2017-06-20 22:12:06,016][gdb1][WARNING]  1: (()+0x9ecb22) [0x5566bbff1b22]
[2017-06-20 22:12:06,016][gdb1][WARNING]  2: (()+0x11390) [0x7f4c3d8a7390]
[2017-06-20 22:12:06,016][gdb1][WARNING]  3: (gsignal()+0x38) [0x7f4c3c842428]
[2017-06-20 22:12:06,016][gdb1][WARNING]  4: (abort()+0x16a) [0x7f4c3c84402a]
[2017-06-20 22:12:06,017][gdb1][WARNING]  5: (ceph::__ceph_assert_fail(char const*, char const*, int, char const*)+0x28e) [0x5566bc053f2e]
[2017-06-20 22:12:06,017][gdb1][WARNING]  6: (BuddyStore::set_fsid(uuid_d)+0xc8) [0x5566bbe220f8]
[2017-06-20 22:12:06,017][gdb1][WARNING]  7: (OSD::mkfs(CephContext*, ObjectStore*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, uuid_d, int)+0x13f) [0x5566bba47a5f]
[2017-06-20 22:12:06,017][gdb1][WARNING]  8: (main()+0x1041) [0x5566bb997c11]
[2017-06-20 22:12:06,017][gdb1][WARNING]  9: (__libc_start_main()+0xf0) [0x7f4c3c82d830]
[2017-06-20 22:12:06,017][gdb1][WARNING]  10: (_start()+0x29) [0x5566bba1fe79]
[2017-06-20 22:12:06,017][gdb1][WARNING]  NOTE: a copy of the executable, or `objdump -rdS <executable>` is needed to interpret this.
[2017-06-20 22:12:06,017][gdb1][WARNING] 
[2017-06-20 22:12:06,017][gdb1][WARNING] /usr/bin/timeout: the monitored command dumped core
[2017-06-20 22:12:06,017][gdb1][WARNING] 
[2017-06-20 22:12:06,017][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-20 22:12:06,018][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore

[2017-06-20 22:38:33,342][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:38:33,342][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-20 22:38:33,342][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:38:33,342][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:38:33,343][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:38:33,343][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:38:33,343][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:38:33,343][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5fafd930e0>
[2017-06-20 22:38:33,343][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:38:33,343][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-20 22:38:33,343][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f5fb06a01b8>
[2017-06-20 22:38:33,343][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:38:33,343][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:38:33,343][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-20 22:38:33,343][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-20 22:38:33,343][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-20 22:38:33,343][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-20 22:38:33,369][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:38:33,383][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:38:33,383][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:38:33,400][gdb3][DEBUG ] detect machine type
[2017-06-20 22:38:33,402][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:38:33,402][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-20 22:38:33,403][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-20 22:38:33,441][gdb3][DEBUG ] Reading package lists...
[2017-06-20 22:38:33,605][gdb3][DEBUG ] Building dependency tree...
[2017-06-20 22:38:33,605][gdb3][DEBUG ] Reading state information...
[2017-06-20 22:38:33,669][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-20 22:38:33,670][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-20 22:38:33,670][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-20 22:38:33,670][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-20 22:38:33,670][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-20 22:38:33,670][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-20 22:38:33,671][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-06-20 22:38:33,671][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-headers-4.4.0-1016-aws
[2017-06-20 22:38:33,671][gdb3][DEBUG ]   linux-headers-4.4.0-1017-aws linux-image-4.4.0-1013-aws
[2017-06-20 22:38:33,671][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws ntp python-blinker
[2017-06-20 22:38:33,671][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-06-20 22:38:33,671][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-06-20 22:38:33,671][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-06-20 22:38:33,671][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-06-20 22:38:33,671][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-06-20 22:38:33,672][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-20 22:38:33,703][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-20 22:38:33,704][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-20 22:38:33,818][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 45 not upgraded.
[2017-06-20 22:38:33,818][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-20 22:38:33,882][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 154602 files and directories currently installed.)
[2017-06-20 22:38:33,882][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-20 22:38:34,052][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-20 22:38:34,166][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-20 22:38:34,198][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-20 22:38:34,413][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-20 22:38:34,527][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-20 22:38:34,692][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-20 22:38:34,723][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-20 22:38:34,755][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-20 22:38:34,920][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-20 22:38:35,036][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-20 22:38:35,036][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-20 22:38:35,200][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-20 22:38:35,201][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-20 22:38:35,365][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-20 22:38:35,429][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-20 22:38:35,429][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-20 22:38:35,543][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-06-20 22:38:36,360][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-20 22:38:36,524][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:38:36,524][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-20 22:38:36,524][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:38:36,524][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:38:36,524][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:38:36,524][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:38:36,524][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:38:36,524][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6765a81758>
[2017-06-20 22:38:36,525][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:38:36,525][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-20 22:38:36,525][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f676638e230>
[2017-06-20 22:38:36,525][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:38:36,525][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:38:36,525][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-20 22:38:36,551][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:38:36,565][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:38:36,565][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:38:36,581][gdb3][DEBUG ] detect machine type
[2017-06-20 22:38:36,584][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:38:36,599][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:38:36,614][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:38:36,615][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:38:36,631][gdb3][DEBUG ] detect machine type
[2017-06-20 22:38:36,634][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:38:36,634][gdb3][INFO  ] purging data on gdb3
[2017-06-20 22:38:36,635][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-20 22:38:36,647][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-20 22:38:36,819][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:38:36,820][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-20 22:38:36,820][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:38:36,820][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:38:36,820][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:38:36,820][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:38:36,820][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:38:36,820][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa7b63aea28>
[2017-06-20 22:38:36,820][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:38:36,820][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fa7b6c71848>
[2017-06-20 22:38:36,820][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:38:36,820][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:38:42,943][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:38:42,944][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb2
[2017-06-20 22:38:42,944][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:38:42,944][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:38:42,944][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:38:42,944][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:38:42,944][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:38:42,944][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f93a907f5a8>
[2017-06-20 22:38:42,944][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:38:42,944][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-20 22:38:42,944][ceph_deploy.cli][INFO  ]  mon                           : ['gdb2']
[2017-06-20 22:38:42,945][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f93a9703758>
[2017-06-20 22:38:42,945][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-20 22:38:42,945][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:38:42,945][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-20 22:38:42,945][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:38:42,945][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-20 22:38:42,945][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-20 22:38:42,945][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-20 22:38:42,967][gdb2][DEBUG ] connected to host: gdb3 
[2017-06-20 22:38:42,971][gdb2][INFO  ] Running command: ssh -CT -o BatchMode=yes gdb2
[2017-06-20 22:38:48,977][ceph_deploy][ERROR ] RuntimeError: connecting to host: gdb2 resulted in errors: HostNotFound gdb2

[2017-06-20 22:38:49,146][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:38:49,146][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-06-20 22:38:49,146][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:38:49,146][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:38:49,146][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:38:49,146][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:38:49,146][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-20 22:38:49,146][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:38:49,146][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7facfd722ea8>
[2017-06-20 22:38:49,147][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:38:49,147][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7facfd6f6b18>
[2017-06-20 22:38:49,147][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:38:49,147][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-20 22:38:49,147][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:38:49,147][ceph_deploy.mon][WARNING] keyring (ceph.mon.keyring) not found, creating a new one
[2017-06-20 22:38:49,147][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-20 22:38:49,148][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-20 22:38:49,148][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-20 22:38:49,148][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-20 22:38:49,175][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:38:49,189][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:38:49,189][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:38:49,206][gdb3][DEBUG ] detect machine type
[2017-06-20 22:38:49,208][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:38:49,209][ceph_deploy.mon][ERROR ] ceph needs to be installed in remote host: gdb3
[2017-06-20 22:38:49,209][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2017-06-20 22:38:49,390][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:38:49,390][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb2
[2017-06-20 22:38:49,390][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:38:49,390][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:38:49,391][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:38:49,391][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:38:49,391][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:38:49,391][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1e88545560>
[2017-06-20 22:38:49,391][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:38:49,391][ceph_deploy.cli][INFO  ]  client                        : ['gdb2']
[2017-06-20 22:38:49,391][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f1e88e5c938>
[2017-06-20 22:38:49,391][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:38:49,391][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:38:49,391][ceph_deploy][ERROR ] RuntimeError: ceph.client.admin.keyring not found

[2017-06-20 22:38:54,427][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:38:54,427][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-20 22:38:54,427][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:38:54,427][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:38:54,427][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:38:54,427][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:38:54,428][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:38:54,428][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9c742690e0>
[2017-06-20 22:38:54,428][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:38:54,428][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-20 22:38:54,428][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f9c74b761b8>
[2017-06-20 22:38:54,428][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:38:54,428][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:38:54,428][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-20 22:38:54,428][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-20 22:38:54,428][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-20 22:38:54,428][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-20 22:38:54,454][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:38:54,468][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:38:54,469][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:38:54,485][gdb3][DEBUG ] detect machine type
[2017-06-20 22:38:54,487][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:38:54,487][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-20 22:38:54,488][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-20 22:38:54,526][gdb3][DEBUG ] Reading package lists...
[2017-06-20 22:38:54,691][gdb3][DEBUG ] Building dependency tree...
[2017-06-20 22:38:54,691][gdb3][DEBUG ] Reading state information...
[2017-06-20 22:38:54,755][gdb3][DEBUG ] Package 'ceph' is not installed, so not removed
[2017-06-20 22:38:54,755][gdb3][DEBUG ] Package 'ceph-common' is not installed, so not removed
[2017-06-20 22:38:54,755][gdb3][DEBUG ] Package 'ceph-mds' is not installed, so not removed
[2017-06-20 22:38:54,755][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-20 22:38:54,755][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-20 22:38:54,756][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-20 22:38:54,756][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-20 22:38:54,756][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-20 22:38:54,756][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-20 22:38:54,756][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-06-20 22:38:54,756][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-headers-4.4.0-1016-aws
[2017-06-20 22:38:54,756][gdb3][DEBUG ]   linux-headers-4.4.0-1017-aws linux-image-4.4.0-1013-aws
[2017-06-20 22:38:54,756][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws ntp python-blinker
[2017-06-20 22:38:54,756][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-06-20 22:38:54,756][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-06-20 22:38:54,756][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-06-20 22:38:54,756][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-06-20 22:38:54,757][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-06-20 22:38:54,757][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-20 22:38:54,757][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.
[2017-06-20 22:38:54,923][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:38:54,923][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-20 22:38:54,923][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:38:54,924][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:38:54,924][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:38:54,924][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:38:54,924][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:38:54,924][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc5ffcd3758>
[2017-06-20 22:38:54,924][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:38:54,924][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-20 22:38:54,924][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fc6005e0230>
[2017-06-20 22:38:54,924][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:38:54,924][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:38:54,924][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-20 22:38:54,950][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:38:54,965][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:38:54,965][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:38:54,982][gdb3][DEBUG ] detect machine type
[2017-06-20 22:38:54,985][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:38:55,000][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:38:55,014][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:38:55,014][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:38:55,031][gdb3][DEBUG ] detect machine type
[2017-06-20 22:38:55,033][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:38:55,033][gdb3][INFO  ] purging data on gdb3
[2017-06-20 22:38:55,034][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-20 22:38:55,047][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-20 22:38:55,220][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:38:55,220][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-20 22:38:55,220][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:38:55,220][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:38:55,220][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:38:55,220][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:38:55,220][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:38:55,220][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5ca521da28>
[2017-06-20 22:38:55,221][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:38:55,221][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f5ca5ae0848>
[2017-06-20 22:38:55,221][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:38:55,221][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:39:10,366][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:39:10,367][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-20 22:39:10,367][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:39:10,367][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:39:10,367][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:39:10,367][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:39:10,368][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:39:10,368][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fafd73f35a8>
[2017-06-20 22:39:10,368][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:39:10,368][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-20 22:39:10,368][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-20 22:39:10,368][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fafd7a77758>
[2017-06-20 22:39:10,368][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-20 22:39:10,368][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:39:10,368][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-20 22:39:10,368][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:39:10,369][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-20 22:39:10,369][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-20 22:39:10,369][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-20 22:39:10,395][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:39:10,409][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:39:10,410][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:39:10,426][gdb3][DEBUG ] detect machine type
[2017-06-20 22:39:10,429][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:39:10,430][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-20 22:39:10,441][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-20 22:39:10,448][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-20 22:39:10,448][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-20 22:39:10,448][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-20 22:39:10,448][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-20 22:39:10,448][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-20 22:39:10,448][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-20 22:39:10,449][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-20 22:39:10,449][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-20 22:39:10,616][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe7aede3ea8>
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fe7aedb7b18>
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:39:10,617][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-20 22:39:10,618][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:39:10,618][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-20 22:39:10,618][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-20 22:39:10,644][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:39:10,658][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:39:10,658][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:39:10,675][gdb3][DEBUG ] detect machine type
[2017-06-20 22:39:10,678][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:39:10,678][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-20 22:39:10,678][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-20 22:39:10,678][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:39:10,678][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-20 22:39:10,678][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:39:10,679][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-20 22:39:10,679][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:39:10,681][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-20 22:39:10,681][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-20 22:39:10,681][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-20 22:39:10,682][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-20 22:39:10,682][gdb3][DEBUG ] create the monitor keyring file
[2017-06-20 22:39:10,683][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-20 22:39:10,721][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-20 22:39:10,721][gdb3][DEBUG ] ceph-mon: set fsid to 1772ec35-97bc-4b31-b6f5-7e4a2d604e9e
[2017-06-20 22:39:10,724][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-20 22:39:10,728][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-20 22:39:10,728][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-20 22:39:10,729][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-20 22:39:10,730][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-20 22:39:10,797][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-20 22:39:10,870][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-20 22:39:12,940][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:39:13,005][gdb3][DEBUG ] ********************************************************************************
[2017-06-20 22:39:13,005][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-20 22:39:13,005][gdb3][DEBUG ] {
[2017-06-20 22:39:13,005][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-20 22:39:13,005][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-20 22:39:13,005][gdb3][DEBUG ]   "features": {
[2017-06-20 22:39:13,005][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-20 22:39:13,005][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-20 22:39:13,006][gdb3][DEBUG ]       "kraken", 
[2017-06-20 22:39:13,006][gdb3][DEBUG ]       "luminous"
[2017-06-20 22:39:13,006][gdb3][DEBUG ]     ], 
[2017-06-20 22:39:13,006][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-20 22:39:13,006][gdb3][DEBUG ]     "required_mon": [
[2017-06-20 22:39:13,006][gdb3][DEBUG ]       "kraken", 
[2017-06-20 22:39:13,006][gdb3][DEBUG ]       "luminous"
[2017-06-20 22:39:13,006][gdb3][DEBUG ]     ]
[2017-06-20 22:39:13,006][gdb3][DEBUG ]   }, 
[2017-06-20 22:39:13,006][gdb3][DEBUG ]   "monmap": {
[2017-06-20 22:39:13,006][gdb3][DEBUG ]     "created": "2017-06-20 22:39:10.706217", 
[2017-06-20 22:39:13,006][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-20 22:39:13,006][gdb3][DEBUG ]     "features": {
[2017-06-20 22:39:13,006][gdb3][DEBUG ]       "optional": [], 
[2017-06-20 22:39:13,006][gdb3][DEBUG ]       "persistent": [
[2017-06-20 22:39:13,006][gdb3][DEBUG ]         "kraken", 
[2017-06-20 22:39:13,006][gdb3][DEBUG ]         "luminous"
[2017-06-20 22:39:13,007][gdb3][DEBUG ]       ]
[2017-06-20 22:39:13,007][gdb3][DEBUG ]     }, 
[2017-06-20 22:39:13,007][gdb3][DEBUG ]     "fsid": "1772ec35-97bc-4b31-b6f5-7e4a2d604e9e", 
[2017-06-20 22:39:13,007][gdb3][DEBUG ]     "modified": "2017-06-20 22:39:10.955676", 
[2017-06-20 22:39:13,007][gdb3][DEBUG ]     "mons": [
[2017-06-20 22:39:13,007][gdb3][DEBUG ]       {
[2017-06-20 22:39:13,007][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-20 22:39:13,007][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-20 22:39:13,007][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-20 22:39:13,007][gdb3][DEBUG ]         "rank": 0
[2017-06-20 22:39:13,008][gdb3][DEBUG ]       }
[2017-06-20 22:39:13,008][gdb3][DEBUG ]     ]
[2017-06-20 22:39:13,008][gdb3][DEBUG ]   }, 
[2017-06-20 22:39:13,008][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-20 22:39:13,008][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-20 22:39:13,009][gdb3][DEBUG ]   "quorum": [
[2017-06-20 22:39:13,009][gdb3][DEBUG ]     0
[2017-06-20 22:39:13,009][gdb3][DEBUG ]   ], 
[2017-06-20 22:39:13,009][gdb3][DEBUG ]   "rank": 0, 
[2017-06-20 22:39:13,009][gdb3][DEBUG ]   "state": "leader", 
[2017-06-20 22:39:13,009][gdb3][DEBUG ]   "sync_provider": []
[2017-06-20 22:39:13,009][gdb3][DEBUG ] }
[2017-06-20 22:39:13,009][gdb3][DEBUG ] ********************************************************************************
[2017-06-20 22:39:13,009][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-20 22:39:13,010][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:39:13,076][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-20 22:39:13,091][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:39:13,104][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:39:13,105][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:39:13,122][gdb3][DEBUG ] detect machine type
[2017-06-20 22:39:13,124][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:39:13,126][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:39:13,191][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-20 22:39:13,191][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-20 22:39:13,191][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-20 22:39:13,193][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpB9n4Ut
[2017-06-20 22:39:13,208][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:39:13,221][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:39:13,222][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:39:13,238][gdb3][DEBUG ] detect machine type
[2017-06-20 22:39:13,240][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:39:13,241][gdb3][DEBUG ] fetch remote file
[2017-06-20 22:39:13,242][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:39:13,308][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-20 22:39:13,474][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-20 22:39:13,640][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-20 22:39:13,806][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-20 22:39:13,972][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-20 22:39:14,138][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-20 22:39:14,304][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-20 22:39:14,471][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-20 22:39:14,637][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-20 22:39:14,803][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-20 22:39:14,968][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-20 22:39:14,968][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-20 22:39:14,968][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170620223914'
[2017-06-20 22:39:14,969][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-20 22:39:14,969][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-20 22:39:14,969][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-20 22:39:14,969][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpB9n4Ut
[2017-06-20 22:39:15,150][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:39:15,150][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-20 22:39:15,150][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:39:15,150][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:39:15,150][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:39:15,150][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:39:15,150][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:39:15,150][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe014e64560>
[2017-06-20 22:39:15,151][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:39:15,151][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-20 22:39:15,151][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fe01577b938>
[2017-06-20 22:39:15,151][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:39:15,151][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:39:15,151][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-20 22:39:15,177][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:39:15,190][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:39:15,191][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:39:15,208][gdb3][DEBUG ] detect machine type
[2017-06-20 22:39:15,211][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:40:03,136][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f37a6e0d560>
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f37a7724938>
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:40:03,137][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:40:03,138][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-20 22:40:03,401][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:40:03,609][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:40:03,609][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:40:03,626][gdb1][DEBUG ] detect machine type
[2017-06-20 22:40:03,630][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:40:03,802][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:40:03,802][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-06-20 22:40:03,802][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:40:03,802][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:40:03,802][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb33e205950>
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb33e45baa0>
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:40:03,803][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:40:03,804][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-20 22:40:03,804][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-20 22:40:04,053][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:40:04,292][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:40:04,293][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:40:04,309][gdb1][DEBUG ] detect machine type
[2017-06-20 22:40:04,314][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:40:04,315][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:40:04,315][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-20 22:40:04,315][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:40:04,318][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-20 22:40:04,318][gdb1][DEBUG ] create a keyring file
[2017-06-20 22:40:04,320][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-06-20 22:40:04,320][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:40:04,322][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-20 22:40:04,492][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:40:04,493][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:40:04,493][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:40:04,493][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:40:04,500][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-20 22:40:04,516][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/buddystore
[2017-06-20 22:40:04,516][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/ceph_fsid.4186.tmp
[2017-06-20 22:40:04,516][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/fsid.4186.tmp
[2017-06-20 22:40:04,520][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/magic.4186.tmp
[2017-06-20 22:40:09,557][gdb1][INFO  ] checking OSD status...
[2017-06-20 22:40:09,557][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:40:09,559][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-20 22:40:09,775][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-20 22:40:09,940][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feb48064950>
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7feb482baaa0>
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:40:09,941][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-20 22:40:09,942][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-20 22:40:10,193][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:40:10,437][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:40:10,438][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:40:10,454][gdb1][DEBUG ] detect machine type
[2017-06-20 22:40:10,458][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:40:10,459][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:40:10,459][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-06-20 22:40:10,459][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-20 22:40:10,459][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:40:10,461][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-06-20 22:40:10,632][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-06-20 22:40:10,632][gdb1][WARNING] activate: Cluster uuid is 1772ec35-97bc-4b31-b6f5-7e4a2d604e9e
[2017-06-20 22:40:10,632][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:40:10,632][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-20 22:40:10,632][gdb1][WARNING] activate: OSD uuid is 157a0abe-b47e-4d5a-883e-c15e951b335f
[2017-06-20 22:40:10,633][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-20 22:40:10,633][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 157a0abe-b47e-4d5a-883e-c15e951b335f
[2017-06-20 22:40:10,797][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/whoami.4303.tmp
[2017-06-20 22:40:10,797][gdb1][WARNING] activate: OSD id is 0
[2017-06-20 22:40:10,797][gdb1][WARNING] activate: Initializing OSD...
[2017-06-20 22:40:10,797][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/buddystore/activate.monmap
[2017-06-20 22:40:10,962][gdb1][WARNING] got monmap epoch 2
[2017-06-20 22:40:10,962][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/buddystore/activate.monmap --osd-data /mnt/buddystore --osd-journal /mnt/buddystore/journal --osd-uuid 157a0abe-b47e-4d5a-883e-c15e951b335f --keyring /mnt/buddystore/keyring --setuser ceph --setgroup ceph
[2017-06-20 22:40:10,977][gdb1][WARNING] activate: Marking with init system systemd
[2017-06-20 22:40:10,978][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/systemd
[2017-06-20 22:40:10,978][gdb1][WARNING] activate: Authorizing OSD key...
[2017-06-20 22:40:10,978][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/buddystore/keyring osd allow * mon allow profile osd
[2017-06-20 22:40:11,142][gdb1][WARNING] added key for osd.0
[2017-06-20 22:40:11,142][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/active.4303.tmp
[2017-06-20 22:40:11,142][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/buddystore
[2017-06-20 22:40:11,142][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-0 -> /mnt/buddystore
[2017-06-20 22:40:11,142][gdb1][WARNING] start_daemon: Starting ceph osd.0...
[2017-06-20 22:40:11,143][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-06-20 22:40:11,143][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-06-20 22:40:11,257][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-06-20 22:40:11,321][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-06-20 22:40:11,321][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-06-20 22:40:11,435][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-06-20 22:40:16,501][gdb1][INFO  ] checking OSD status...
[2017-06-20 22:40:16,501][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:40:16,504][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-20 22:40:16,671][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-21 01:58:20,946][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f247131bfc8>
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f2471c2e1b8>
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-21 01:58:20,947][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-21 01:58:20,947][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-21 01:58:20,947][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-21 01:58:20,948][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-21 01:58:20,948][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-21 01:58:20,974][gdb3][DEBUG ] connection detected need for sudo
[2017-06-21 01:58:20,988][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-21 01:58:20,988][gdb3][DEBUG ] detect platform information from remote host
[2017-06-21 01:58:21,005][gdb3][DEBUG ] detect machine type
[2017-06-21 01:58:21,008][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-21 01:58:21,008][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-21 01:58:21,009][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-21 01:58:21,047][gdb3][DEBUG ] Reading package lists...
[2017-06-21 01:58:21,211][gdb3][DEBUG ] Building dependency tree...
[2017-06-21 01:58:21,211][gdb3][DEBUG ] Reading state information...
[2017-06-21 01:58:21,275][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-21 01:58:21,276][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-21 01:58:21,276][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-21 01:58:21,276][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-21 01:58:21,276][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-21 01:58:21,276][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-21 01:58:21,277][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-06-21 01:58:21,277][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-headers-4.4.0-1016-aws
[2017-06-21 01:58:21,277][gdb3][DEBUG ]   linux-headers-4.4.0-1017-aws linux-image-4.4.0-1013-aws
[2017-06-21 01:58:21,277][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws ntp python-blinker
[2017-06-21 01:58:21,277][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-06-21 01:58:21,277][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-06-21 01:58:21,277][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-06-21 01:58:21,277][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-06-21 01:58:21,278][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-06-21 01:58:21,278][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-21 01:58:21,342][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-21 01:58:21,342][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-21 01:58:21,456][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 45 not upgraded.
[2017-06-21 01:58:21,456][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-21 01:58:21,520][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 154602 files and directories currently installed.)
[2017-06-21 01:58:21,521][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-21 01:58:21,692][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-21 01:58:21,806][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-21 01:58:21,807][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-21 01:58:22,022][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-21 01:58:22,136][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-21 01:58:22,301][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-21 01:58:22,333][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-21 01:58:22,365][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-21 01:58:22,529][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-21 01:58:22,643][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-21 01:58:22,643][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-21 01:58:22,810][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-21 01:58:22,826][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-21 01:58:22,990][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-21 01:58:23,054][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-21 01:58:23,054][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-21 01:58:23,168][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-06-21 01:58:23,934][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-21 01:58:24,100][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-21 01:58:24,101][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-21 01:58:24,101][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-21 01:58:24,101][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-21 01:58:24,101][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-21 01:58:24,101][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-21 01:58:24,101][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-21 01:58:24,101][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f24d7540710>
[2017-06-21 01:58:24,101][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-21 01:58:24,101][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-21 01:58:24,101][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f24d7e4d230>
[2017-06-21 01:58:24,102][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-21 01:58:24,102][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-21 01:58:24,102][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-21 01:58:24,128][gdb3][DEBUG ] connection detected need for sudo
[2017-06-21 01:58:24,142][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-21 01:58:24,142][gdb3][DEBUG ] detect platform information from remote host
[2017-06-21 01:58:24,160][gdb3][DEBUG ] detect machine type
[2017-06-21 01:58:24,162][gdb3][DEBUG ] find the location of an executable
[2017-06-21 01:58:24,177][gdb3][DEBUG ] connection detected need for sudo
[2017-06-21 01:58:24,191][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-21 01:58:24,192][gdb3][DEBUG ] detect platform information from remote host
[2017-06-21 01:58:24,208][gdb3][DEBUG ] detect machine type
[2017-06-21 01:58:24,210][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-21 01:58:24,210][gdb3][INFO  ] purging data on gdb3
[2017-06-21 01:58:24,211][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-21 01:58:24,224][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-21 01:58:24,393][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-21 01:58:24,394][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-21 01:58:24,394][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-21 01:58:24,394][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-21 01:58:24,394][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-21 01:58:24,394][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-21 01:58:24,394][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-21 01:58:24,394][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f272b74a9e0>
[2017-06-21 01:58:24,394][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-21 01:58:24,394][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f272c00d848>
[2017-06-21 01:58:24,394][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-21 01:58:24,394][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-21 01:58:38,910][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-21 01:58:38,911][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-21 01:58:38,911][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-21 01:58:38,911][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-21 01:58:38,911][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-21 01:58:38,911][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-21 01:58:38,911][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-21 01:58:38,911][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6482310560>
[2017-06-21 01:58:38,911][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-21 01:58:38,911][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-21 01:58:38,911][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-21 01:58:38,911][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f6482994758>
[2017-06-21 01:58:38,912][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-21 01:58:38,912][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-21 01:58:38,912][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-21 01:58:38,912][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-21 01:58:38,912][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-21 01:58:38,912][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-21 01:58:38,912][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-21 01:58:38,938][gdb3][DEBUG ] connection detected need for sudo
[2017-06-21 01:58:38,951][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-21 01:58:38,952][gdb3][DEBUG ] detect platform information from remote host
[2017-06-21 01:58:38,968][gdb3][DEBUG ] detect machine type
[2017-06-21 01:58:38,971][gdb3][DEBUG ] find the location of an executable
[2017-06-21 01:58:38,972][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-21 01:58:38,983][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-21 01:58:38,989][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-21 01:58:38,989][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-21 01:58:38,989][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-21 01:58:38,989][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-21 01:58:38,990][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-21 01:58:38,990][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-21 01:58:38,990][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-21 01:58:38,990][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-21 01:58:39,154][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-21 01:58:39,154][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-06-21 01:58:39,154][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-21 01:58:39,154][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-21 01:58:39,154][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-21 01:58:39,154][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-21 01:58:39,154][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-21 01:58:39,154][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-21 01:58:39,154][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdf8c5ace60>
[2017-06-21 01:58:39,155][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-21 01:58:39,155][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fdf8c581b18>
[2017-06-21 01:58:39,155][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-21 01:58:39,155][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-21 01:58:39,155][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-21 01:58:39,155][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-21 01:58:39,155][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-21 01:58:39,182][gdb3][DEBUG ] connection detected need for sudo
[2017-06-21 01:58:39,195][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-21 01:58:39,196][gdb3][DEBUG ] detect platform information from remote host
[2017-06-21 01:58:39,212][gdb3][DEBUG ] detect machine type
[2017-06-21 01:58:39,215][gdb3][DEBUG ] find the location of an executable
[2017-06-21 01:58:39,215][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-21 01:58:39,215][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-21 01:58:39,215][gdb3][DEBUG ] get remote short hostname
[2017-06-21 01:58:39,215][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-21 01:58:39,216][gdb3][DEBUG ] get remote short hostname
[2017-06-21 01:58:39,216][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-21 01:58:39,216][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-21 01:58:39,218][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-21 01:58:39,218][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-21 01:58:39,218][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-21 01:58:39,219][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-21 01:58:39,219][gdb3][DEBUG ] create the monitor keyring file
[2017-06-21 01:58:39,220][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-21 01:58:39,258][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-21 01:58:39,258][gdb3][DEBUG ] ceph-mon: set fsid to f73dedd9-ed04-4a8d-bee2-55162010357b
[2017-06-21 01:58:39,273][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-21 01:58:39,274][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-21 01:58:39,274][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-21 01:58:39,275][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-21 01:58:39,276][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-21 01:58:39,343][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-21 01:58:39,411][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-21 01:58:41,483][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-21 01:58:41,548][gdb3][DEBUG ] ********************************************************************************
[2017-06-21 01:58:41,548][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-21 01:58:41,548][gdb3][DEBUG ] {
[2017-06-21 01:58:41,548][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-21 01:58:41,548][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-21 01:58:41,548][gdb3][DEBUG ]   "features": {
[2017-06-21 01:58:41,548][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-21 01:58:41,549][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-21 01:58:41,549][gdb3][DEBUG ]       "kraken", 
[2017-06-21 01:58:41,549][gdb3][DEBUG ]       "luminous"
[2017-06-21 01:58:41,549][gdb3][DEBUG ]     ], 
[2017-06-21 01:58:41,549][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-21 01:58:41,549][gdb3][DEBUG ]     "required_mon": [
[2017-06-21 01:58:41,549][gdb3][DEBUG ]       "kraken", 
[2017-06-21 01:58:41,549][gdb3][DEBUG ]       "luminous"
[2017-06-21 01:58:41,549][gdb3][DEBUG ]     ]
[2017-06-21 01:58:41,549][gdb3][DEBUG ]   }, 
[2017-06-21 01:58:41,549][gdb3][DEBUG ]   "monmap": {
[2017-06-21 01:58:41,549][gdb3][DEBUG ]     "created": "2017-06-21 01:58:39.243629", 
[2017-06-21 01:58:41,549][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-21 01:58:41,549][gdb3][DEBUG ]     "features": {
[2017-06-21 01:58:41,549][gdb3][DEBUG ]       "optional": [], 
[2017-06-21 01:58:41,549][gdb3][DEBUG ]       "persistent": [
[2017-06-21 01:58:41,549][gdb3][DEBUG ]         "kraken", 
[2017-06-21 01:58:41,550][gdb3][DEBUG ]         "luminous"
[2017-06-21 01:58:41,550][gdb3][DEBUG ]       ]
[2017-06-21 01:58:41,550][gdb3][DEBUG ]     }, 
[2017-06-21 01:58:41,550][gdb3][DEBUG ]     "fsid": "f73dedd9-ed04-4a8d-bee2-55162010357b", 
[2017-06-21 01:58:41,550][gdb3][DEBUG ]     "modified": "2017-06-21 01:58:39.496769", 
[2017-06-21 01:58:41,550][gdb3][DEBUG ]     "mons": [
[2017-06-21 01:58:41,550][gdb3][DEBUG ]       {
[2017-06-21 01:58:41,550][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-21 01:58:41,550][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-21 01:58:41,550][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-21 01:58:41,550][gdb3][DEBUG ]         "rank": 0
[2017-06-21 01:58:41,550][gdb3][DEBUG ]       }
[2017-06-21 01:58:41,550][gdb3][DEBUG ]     ]
[2017-06-21 01:58:41,550][gdb3][DEBUG ]   }, 
[2017-06-21 01:58:41,550][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-21 01:58:41,550][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-21 01:58:41,551][gdb3][DEBUG ]   "quorum": [
[2017-06-21 01:58:41,551][gdb3][DEBUG ]     0
[2017-06-21 01:58:41,551][gdb3][DEBUG ]   ], 
[2017-06-21 01:58:41,551][gdb3][DEBUG ]   "rank": 0, 
[2017-06-21 01:58:41,551][gdb3][DEBUG ]   "state": "leader", 
[2017-06-21 01:58:41,551][gdb3][DEBUG ]   "sync_provider": []
[2017-06-21 01:58:41,551][gdb3][DEBUG ] }
[2017-06-21 01:58:41,551][gdb3][DEBUG ] ********************************************************************************
[2017-06-21 01:58:41,551][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-21 01:58:41,552][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-21 01:58:41,617][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-21 01:58:41,632][gdb3][DEBUG ] connection detected need for sudo
[2017-06-21 01:58:41,646][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-21 01:58:41,646][gdb3][DEBUG ] detect platform information from remote host
[2017-06-21 01:58:41,663][gdb3][DEBUG ] detect machine type
[2017-06-21 01:58:41,665][gdb3][DEBUG ] find the location of an executable
[2017-06-21 01:58:41,666][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-21 01:58:41,731][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-21 01:58:41,732][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-21 01:58:41,732][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-21 01:58:41,733][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpYNYKfz
[2017-06-21 01:58:41,749][gdb3][DEBUG ] connection detected need for sudo
[2017-06-21 01:58:41,763][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-21 01:58:41,764][gdb3][DEBUG ] detect platform information from remote host
[2017-06-21 01:58:41,781][gdb3][DEBUG ] detect machine type
[2017-06-21 01:58:41,783][gdb3][DEBUG ] get remote short hostname
[2017-06-21 01:58:41,783][gdb3][DEBUG ] fetch remote file
[2017-06-21 01:58:41,784][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-21 01:58:41,850][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-21 01:58:42,017][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-21 01:58:42,233][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-21 01:58:42,399][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-21 01:58:42,565][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-21 01:58:42,731][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-21 01:58:42,898][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-21 01:58:43,064][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-21 01:58:43,230][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-21 01:58:43,396][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-21 01:58:43,562][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-21 01:58:43,562][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-21 01:58:43,562][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170621015843'
[2017-06-21 01:58:43,562][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-21 01:58:43,562][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-21 01:58:43,562][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-21 01:58:43,563][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpYNYKfz
[2017-06-21 01:58:43,746][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-21 01:58:43,746][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-21 01:58:43,746][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-21 01:58:43,746][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-21 01:58:43,746][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-21 01:58:43,746][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-21 01:58:43,746][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-21 01:58:43,746][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f20a53ca518>
[2017-06-21 01:58:43,747][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-21 01:58:43,747][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-21 01:58:43,747][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f20a5ce1938>
[2017-06-21 01:58:43,747][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-21 01:58:43,747][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-21 01:58:43,747][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-21 01:58:43,773][gdb3][DEBUG ] connection detected need for sudo
[2017-06-21 01:58:43,787][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-21 01:58:43,788][gdb3][DEBUG ] detect platform information from remote host
[2017-06-21 01:58:43,804][gdb3][DEBUG ] detect machine type
[2017-06-21 01:58:43,807][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-21 01:58:50,220][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcb9eebc518>
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fcb9f7d3938>
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-21 01:58:50,221][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-21 01:58:50,222][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-21 01:58:50,611][gdb1][DEBUG ] connection detected need for sudo
[2017-06-21 01:58:50,844][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-21 01:58:50,844][gdb1][DEBUG ] detect platform information from remote host
[2017-06-21 01:58:50,861][gdb1][DEBUG ] detect machine type
[2017-06-21 01:58:50,865][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-21 01:58:51,036][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-21 01:58:51,037][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-06-21 01:58:51,037][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-21 01:58:51,037][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-21 01:58:51,037][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-21 01:58:51,037][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-21 01:58:51,037][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-21 01:58:51,037][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-21 01:58:51,037][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-21 01:58:51,037][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-21 01:58:51,037][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-21 01:58:51,038][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-21 01:58:51,038][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-21 01:58:51,038][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-21 01:58:51,038][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f668e42c908>
[2017-06-21 01:58:51,038][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-21 01:58:51,038][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-21 01:58:51,038][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f668e682aa0>
[2017-06-21 01:58:51,038][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-21 01:58:51,038][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-21 01:58:51,038][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-21 01:58:51,038][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-21 01:58:51,280][gdb1][DEBUG ] connection detected need for sudo
[2017-06-21 01:58:51,515][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-21 01:58:51,516][gdb1][DEBUG ] detect platform information from remote host
[2017-06-21 01:58:51,532][gdb1][DEBUG ] detect machine type
[2017-06-21 01:58:51,535][gdb1][DEBUG ] find the location of an executable
[2017-06-21 01:58:51,536][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-21 01:58:51,536][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-21 01:58:51,536][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-21 01:58:51,539][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-21 01:58:51,539][gdb1][DEBUG ] create a keyring file
[2017-06-21 01:58:51,540][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-06-21 01:58:51,540][gdb1][DEBUG ] find the location of an executable
[2017-06-21 01:58:51,542][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-06-21 01:58:51,663][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-21 01:58:51,679][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-21 01:58:51,686][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-21 01:58:51,702][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-21 01:58:51,710][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-21 01:58:51,726][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/buddystore
[2017-06-21 01:58:51,726][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/ceph_fsid.10578.tmp
[2017-06-21 01:58:51,726][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/fsid.10578.tmp
[2017-06-21 01:58:51,729][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/magic.10578.tmp
[2017-06-21 01:58:56,750][gdb1][INFO  ] checking OSD status...
[2017-06-21 01:58:56,750][gdb1][DEBUG ] find the location of an executable
[2017-06-21 01:58:56,753][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-21 01:58:56,969][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-21 01:58:57,134][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-21 01:58:57,134][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-06-21 01:58:57,134][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-21 01:58:57,134][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-21 01:58:57,134][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-21 01:58:57,134][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-21 01:58:57,135][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-21 01:58:57,135][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-21 01:58:57,135][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2e5bcef908>
[2017-06-21 01:58:57,135][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-21 01:58:57,135][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2e5bf45aa0>
[2017-06-21 01:58:57,135][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-21 01:58:57,135][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-21 01:58:57,135][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-06-21 01:58:57,135][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-06-21 01:58:57,385][gdb1][DEBUG ] connection detected need for sudo
[2017-06-21 01:58:57,612][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-21 01:58:57,612][gdb1][DEBUG ] detect platform information from remote host
[2017-06-21 01:58:57,629][gdb1][DEBUG ] detect machine type
[2017-06-21 01:58:57,632][gdb1][DEBUG ] find the location of an executable
[2017-06-21 01:58:57,633][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-21 01:58:57,633][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-06-21 01:58:57,634][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-21 01:58:57,634][gdb1][DEBUG ] find the location of an executable
[2017-06-21 01:58:57,636][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-06-21 01:58:57,806][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-06-21 01:58:57,806][gdb1][WARNING] activate: Cluster uuid is f73dedd9-ed04-4a8d-bee2-55162010357b
[2017-06-21 01:58:57,806][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-21 01:58:57,806][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-21 01:58:57,807][gdb1][WARNING] activate: OSD uuid is 85522641-c10b-4539-b146-ec31afda9641
[2017-06-21 01:58:57,807][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-21 01:58:57,807][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 85522641-c10b-4539-b146-ec31afda9641
[2017-06-21 01:58:57,971][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/whoami.10695.tmp
[2017-06-21 01:58:57,971][gdb1][WARNING] activate: OSD id is 0
[2017-06-21 01:58:57,971][gdb1][WARNING] activate: Initializing OSD...
[2017-06-21 01:58:57,971][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/buddystore/activate.monmap
[2017-06-21 01:58:58,085][gdb1][WARNING] got monmap epoch 2
[2017-06-21 01:58:58,085][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/buddystore/activate.monmap --osd-data /mnt/buddystore --osd-journal /mnt/buddystore/journal --osd-uuid 85522641-c10b-4539-b146-ec31afda9641 --keyring /mnt/buddystore/keyring --setuser ceph --setgroup ceph
[2017-06-21 01:58:58,101][gdb1][WARNING] activate: Marking with init system systemd
[2017-06-21 01:58:58,101][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/systemd
[2017-06-21 01:58:58,101][gdb1][WARNING] activate: Authorizing OSD key...
[2017-06-21 01:58:58,101][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/buddystore/keyring osd allow * mon allow profile osd
[2017-06-21 01:58:58,266][gdb1][WARNING] added key for osd.0
[2017-06-21 01:58:58,266][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/active.10695.tmp
[2017-06-21 01:58:58,266][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/buddystore
[2017-06-21 01:58:58,266][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-0 -> /mnt/buddystore
[2017-06-21 01:58:58,266][gdb1][WARNING] start_daemon: Starting ceph osd.0...
[2017-06-21 01:58:58,266][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-06-21 01:58:58,266][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-06-21 01:58:58,430][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-06-21 01:58:58,495][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-06-21 01:58:58,495][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-06-21 01:58:58,527][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-06-21 01:59:03,646][gdb1][INFO  ] checking OSD status...
[2017-06-21 01:59:03,646][gdb1][DEBUG ] find the location of an executable
[2017-06-21 01:59:03,649][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-21 01:59:03,816][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 02:24:14,873][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:24:14,874][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-07-06 02:24:14,874][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:24:14,874][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:24:14,874][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:24:14,874][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:24:14,875][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:24:14,875][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa253a25fc8>
[2017-07-06 02:24:14,875][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:24:14,875][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 02:24:14,875][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fa2543381b8>
[2017-07-06 02:24:14,875][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:24:14,875][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:24:14,875][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-07-06 02:24:14,875][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-07-06 02:24:14,875][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-07-06 02:24:14,875][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-07-06 02:24:14,901][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:24:14,915][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:24:14,916][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:24:14,932][gdb3][DEBUG ] detect machine type
[2017-07-06 02:24:14,934][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 02:24:14,935][gdb3][INFO  ] Purging Ceph on gdb3
[2017-07-06 02:24:14,935][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-07-06 02:24:14,973][gdb3][DEBUG ] Reading package lists...
[2017-07-06 02:24:15,138][gdb3][DEBUG ] Building dependency tree...
[2017-07-06 02:24:15,138][gdb3][DEBUG ] Reading state information...
[2017-07-06 02:24:15,252][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-07-06 02:24:15,252][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-07-06 02:24:15,252][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1018 linux-headers-4.4.0-1013-aws
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-headers-4.4.0-1017-aws
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   linux-headers-4.4.0-1018-aws linux-image-4.4.0-1013-aws
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   linux-image-4.4.0-1018-aws ntp python-blinker python-cephfs
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-07-06 02:24:15,253][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-07-06 02:24:15,254][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-07-06 02:24:15,254][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-07-06 02:24:15,254][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-07-06 02:24:15,254][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-07-06 02:24:15,254][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-07-06 02:24:15,368][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 60 not upgraded.
[2017-07-06 02:24:15,368][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-07-06 02:24:15,482][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 179644 files and directories currently installed.)
[2017-07-06 02:24:15,482][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-07-06 02:24:15,647][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-07-06 02:24:15,711][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-07-06 02:24:15,742][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-07-06 02:24:15,958][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-07-06 02:24:16,072][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-07-06 02:24:16,237][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-07-06 02:24:16,305][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-07-06 02:24:16,337][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-07-06 02:24:16,505][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-07-06 02:24:16,569][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-07-06 02:24:16,584][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-07-06 02:24:16,749][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-07-06 02:24:16,813][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-07-06 02:24:16,927][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-07-06 02:24:17,042][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-07-06 02:24:17,042][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-07-06 02:24:17,156][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-07-06 02:24:17,973][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-07-06 02:24:18,135][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:24:18,135][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-07-06 02:24:18,135][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:24:18,135][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:24:18,135][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:24:18,135][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:24:18,135][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:24:18,135][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6f30416710>
[2017-07-06 02:24:18,136][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:24:18,136][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 02:24:18,136][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f6f30d23230>
[2017-07-06 02:24:18,136][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:24:18,136][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:24:18,136][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-07-06 02:24:18,162][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:24:18,175][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:24:18,176][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:24:18,192][gdb3][DEBUG ] detect machine type
[2017-07-06 02:24:18,195][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:24:18,210][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:24:18,224][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:24:18,225][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:24:18,241][gdb3][DEBUG ] detect machine type
[2017-07-06 02:24:18,244][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 02:24:18,244][gdb3][INFO  ] purging data on gdb3
[2017-07-06 02:24:18,245][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-07-06 02:24:18,257][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-07-06 02:24:18,427][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:24:18,427][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-07-06 02:24:18,427][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:24:18,427][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:24:18,427][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:24:18,427][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:24:18,428][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:24:18,428][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd5dedca9e0>
[2017-07-06 02:24:18,428][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:24:18,428][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fd5df68d848>
[2017-07-06 02:24:18,428][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:24:18,428][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:24:34,960][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:24:34,960][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efdb7f58560>
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7efdb85dc758>
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-07-06 02:24:34,961][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:24:34,962][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-07-06 02:24:34,962][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-07-06 02:24:34,962][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-07-06 02:24:34,988][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:24:35,002][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:24:35,002][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:24:35,018][gdb3][DEBUG ] detect machine type
[2017-07-06 02:24:35,021][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:24:35,022][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-07-06 02:24:35,033][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-07-06 02:24:35,039][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-07-06 02:24:35,039][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-07-06 02:24:35,039][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-07-06 02:24:35,040][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-07-06 02:24:35,040][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-07-06 02:24:35,040][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-07-06 02:24:35,040][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-07-06 02:24:35,040][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-07-06 02:24:35,206][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:24:35,206][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-07-06 02:24:35,207][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:24:35,207][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:24:35,207][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:24:35,207][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 02:24:35,207][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-07-06 02:24:35,207][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:24:35,207][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7eff446bee60>
[2017-07-06 02:24:35,207][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:24:35,207][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7eff44693b18>
[2017-07-06 02:24:35,208][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:24:35,208][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-07-06 02:24:35,208][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:24:35,208][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-07-06 02:24:35,209][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-07-06 02:24:35,235][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:24:35,248][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:24:35,249][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:24:35,265][gdb3][DEBUG ] detect machine type
[2017-07-06 02:24:35,267][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:24:35,268][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-07-06 02:24:35,268][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-07-06 02:24:35,268][gdb3][DEBUG ] get remote short hostname
[2017-07-06 02:24:35,268][gdb3][DEBUG ] deploying mon to gdb3
[2017-07-06 02:24:35,268][gdb3][DEBUG ] get remote short hostname
[2017-07-06 02:24:35,269][gdb3][DEBUG ] remote hostname: gdb3
[2017-07-06 02:24:35,269][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 02:24:35,271][gdb3][DEBUG ] create the mon path if it does not exist
[2017-07-06 02:24:35,271][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 02:24:35,271][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 02:24:35,272][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 02:24:35,272][gdb3][DEBUG ] create the monitor keyring file
[2017-07-06 02:24:35,273][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-07-06 02:24:35,311][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-07-06 02:24:35,311][gdb3][DEBUG ] ceph-mon: set fsid to 0185a83f-6906-48e3-b8d8-52a016316c98
[2017-07-06 02:24:35,314][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-07-06 02:24:35,316][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 02:24:35,316][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-07-06 02:24:35,317][gdb3][DEBUG ] create the init path if it does not exist
[2017-07-06 02:24:35,318][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 02:24:35,385][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-07-06 02:24:35,453][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-07-06 02:24:37,493][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:24:37,558][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 02:24:37,558][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-07-06 02:24:37,558][gdb3][DEBUG ] {
[2017-07-06 02:24:37,558][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-07-06 02:24:37,558][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-07-06 02:24:37,558][gdb3][DEBUG ]   "features": {
[2017-07-06 02:24:37,558][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-07-06 02:24:37,558][gdb3][DEBUG ]     "quorum_mon": [
[2017-07-06 02:24:37,559][gdb3][DEBUG ]       "kraken", 
[2017-07-06 02:24:37,559][gdb3][DEBUG ]       "luminous"
[2017-07-06 02:24:37,559][gdb3][DEBUG ]     ], 
[2017-07-06 02:24:37,559][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-07-06 02:24:37,559][gdb3][DEBUG ]     "required_mon": [
[2017-07-06 02:24:37,559][gdb3][DEBUG ]       "kraken", 
[2017-07-06 02:24:37,559][gdb3][DEBUG ]       "luminous"
[2017-07-06 02:24:37,559][gdb3][DEBUG ]     ]
[2017-07-06 02:24:37,559][gdb3][DEBUG ]   }, 
[2017-07-06 02:24:37,559][gdb3][DEBUG ]   "monmap": {
[2017-07-06 02:24:37,559][gdb3][DEBUG ]     "created": "2017-07-06 02:24:35.296678", 
[2017-07-06 02:24:37,559][gdb3][DEBUG ]     "epoch": 2, 
[2017-07-06 02:24:37,559][gdb3][DEBUG ]     "features": {
[2017-07-06 02:24:37,559][gdb3][DEBUG ]       "optional": [], 
[2017-07-06 02:24:37,559][gdb3][DEBUG ]       "persistent": [
[2017-07-06 02:24:37,559][gdb3][DEBUG ]         "kraken", 
[2017-07-06 02:24:37,559][gdb3][DEBUG ]         "luminous"
[2017-07-06 02:24:37,559][gdb3][DEBUG ]       ]
[2017-07-06 02:24:37,560][gdb3][DEBUG ]     }, 
[2017-07-06 02:24:37,560][gdb3][DEBUG ]     "fsid": "0185a83f-6906-48e3-b8d8-52a016316c98", 
[2017-07-06 02:24:37,560][gdb3][DEBUG ]     "modified": "2017-07-06 02:24:35.530456", 
[2017-07-06 02:24:37,560][gdb3][DEBUG ]     "mons": [
[2017-07-06 02:24:37,560][gdb3][DEBUG ]       {
[2017-07-06 02:24:37,560][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-07-06 02:24:37,560][gdb3][DEBUG ]         "name": "gdb3", 
[2017-07-06 02:24:37,560][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-07-06 02:24:37,560][gdb3][DEBUG ]         "rank": 0
[2017-07-06 02:24:37,560][gdb3][DEBUG ]       }
[2017-07-06 02:24:37,560][gdb3][DEBUG ]     ]
[2017-07-06 02:24:37,560][gdb3][DEBUG ]   }, 
[2017-07-06 02:24:37,560][gdb3][DEBUG ]   "name": "gdb3", 
[2017-07-06 02:24:37,560][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-07-06 02:24:37,560][gdb3][DEBUG ]   "quorum": [
[2017-07-06 02:24:37,560][gdb3][DEBUG ]     0
[2017-07-06 02:24:37,560][gdb3][DEBUG ]   ], 
[2017-07-06 02:24:37,561][gdb3][DEBUG ]   "rank": 0, 
[2017-07-06 02:24:37,561][gdb3][DEBUG ]   "state": "leader", 
[2017-07-06 02:24:37,561][gdb3][DEBUG ]   "sync_provider": []
[2017-07-06 02:24:37,561][gdb3][DEBUG ] }
[2017-07-06 02:24:37,561][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 02:24:37,561][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-07-06 02:24:37,562][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:24:37,627][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-07-06 02:24:37,644][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:24:37,658][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:24:37,659][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:24:37,675][gdb3][DEBUG ] detect machine type
[2017-07-06 02:24:37,678][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:24:37,679][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:24:37,744][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-07-06 02:24:37,744][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-07-06 02:24:37,744][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-07-06 02:24:37,746][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmplqgORL
[2017-07-06 02:24:37,762][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:24:37,777][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:24:37,777][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:24:37,794][gdb3][DEBUG ] detect machine type
[2017-07-06 02:24:37,796][gdb3][DEBUG ] get remote short hostname
[2017-07-06 02:24:37,797][gdb3][DEBUG ] fetch remote file
[2017-07-06 02:24:37,798][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:24:37,864][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-07-06 02:24:38,030][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-07-06 02:24:38,196][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-07-06 02:24:38,362][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-07-06 02:24:38,528][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-07-06 02:24:38,695][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-07-06 02:24:38,861][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-07-06 02:24:39,027][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-07-06 02:24:39,193][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-07-06 02:24:39,359][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-07-06 02:24:39,525][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-07-06 02:24:39,525][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-07-06 02:24:39,525][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170706022439'
[2017-07-06 02:24:39,526][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-07-06 02:24:39,526][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-07-06 02:24:39,526][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-07-06 02:24:39,526][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmplqgORL
[2017-07-06 02:24:39,704][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f168d768518>
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f168e07f938>
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:24:39,705][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:24:39,706][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 02:24:39,732][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:24:39,746][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:24:39,746][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:24:39,763][gdb3][DEBUG ] detect machine type
[2017-07-06 02:24:39,765][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 02:25:40,222][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:25:40,222][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-07-06 02:25:40,222][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:25:40,223][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:25:40,223][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:25:40,223][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 02:25:40,223][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:25:40,223][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcac1853518>
[2017-07-06 02:25:40,223][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:25:40,223][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-07-06 02:25:40,223][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fcac216a938>
[2017-07-06 02:25:40,223][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:25:40,223][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:25:40,223][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-07-06 02:25:40,465][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 02:25:40,692][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 02:25:40,693][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 02:25:40,709][gdb1][DEBUG ] detect machine type
[2017-07-06 02:25:40,713][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 02:25:40,884][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:25:40,884][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-07-06 02:25:40,884][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:25:40,884][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:25:40,884][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff3e6ecf908>
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff3e7125aa0>
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:25:40,885][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:25:40,886][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-07-06 02:25:40,886][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 02:25:41,122][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 02:25:41,349][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 02:25:41,349][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 02:25:41,366][gdb1][DEBUG ] detect machine type
[2017-07-06 02:25:41,370][gdb1][DEBUG ] find the location of an executable
[2017-07-06 02:25:41,371][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 02:25:41,371][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-07-06 02:25:41,371][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 02:25:41,374][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-07-06 02:25:41,374][gdb1][DEBUG ] create a keyring file
[2017-07-06 02:25:41,376][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-07-06 02:25:41,376][gdb1][DEBUG ] find the location of an executable
[2017-07-06 02:25:41,378][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-07-06 02:25:41,549][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 02:25:41,549][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 02:25:41,549][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 02:25:41,549][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 02:25:41,549][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 02:25:41,565][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/buddystore
[2017-07-06 02:25:41,580][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/ceph_fsid.4167.tmp
[2017-07-06 02:25:41,581][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/fsid.4167.tmp
[2017-07-06 02:25:41,581][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/magic.4167.tmp
[2017-07-06 02:25:46,601][gdb1][INFO  ] checking OSD status...
[2017-07-06 02:25:46,602][gdb1][DEBUG ] find the location of an executable
[2017-07-06 02:25:46,604][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 02:25:46,769][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-07-06 02:25:46,935][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f32a8abd908>
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f32a8d13aa0>
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:25:46,936][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 02:25:46,937][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 02:25:47,181][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 02:25:47,413][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 02:25:47,414][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 02:25:47,430][gdb1][DEBUG ] detect machine type
[2017-07-06 02:25:47,434][gdb1][DEBUG ] find the location of an executable
[2017-07-06 02:25:47,435][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 02:25:47,435][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-07-06 02:25:47,435][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-07-06 02:25:47,435][gdb1][DEBUG ] find the location of an executable
[2017-07-06 02:25:47,437][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-07-06 02:25:47,558][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-07-06 02:25:47,558][gdb1][WARNING] activate: Cluster uuid is 0185a83f-6906-48e3-b8d8-52a016316c98
[2017-07-06 02:25:47,558][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 02:25:47,574][gdb1][WARNING] activate: Cluster name is ceph
[2017-07-06 02:25:47,574][gdb1][WARNING] activate: OSD uuid is 6ce3796a-a1a7-4959-9d8b-ce3b4a16cd04
[2017-07-06 02:25:47,574][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-07-06 02:25:47,574][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 6ce3796a-a1a7-4959-9d8b-ce3b4a16cd04
[2017-07-06 02:25:47,738][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/whoami.4284.tmp
[2017-07-06 02:25:47,738][gdb1][WARNING] activate: OSD id is 0
[2017-07-06 02:25:47,739][gdb1][WARNING] activate: Initializing OSD...
[2017-07-06 02:25:47,739][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/buddystore/activate.monmap
[2017-07-06 02:25:47,903][gdb1][WARNING] got monmap epoch 2
[2017-07-06 02:25:47,903][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/buddystore/activate.monmap --osd-data /mnt/buddystore --osd-journal /mnt/buddystore/journal --osd-uuid 6ce3796a-a1a7-4959-9d8b-ce3b4a16cd04 --keyring /mnt/buddystore/keyring --setuser ceph --setgroup ceph
[2017-07-06 02:25:47,919][gdb1][WARNING] activate: Marking with init system systemd
[2017-07-06 02:25:47,919][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/systemd
[2017-07-06 02:25:47,922][gdb1][WARNING] activate: Authorizing OSD key...
[2017-07-06 02:25:47,922][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/buddystore/keyring osd allow * mon allow profile osd
[2017-07-06 02:25:48,087][gdb1][WARNING] added key for osd.0
[2017-07-06 02:25:48,087][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/active.4284.tmp
[2017-07-06 02:25:48,087][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/buddystore
[2017-07-06 02:25:48,087][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-0 -> /mnt/buddystore
[2017-07-06 02:25:48,087][gdb1][WARNING] start_daemon: Starting ceph osd.0...
[2017-07-06 02:25:48,087][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-07-06 02:25:48,087][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-07-06 02:25:48,151][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-07-06 02:25:48,215][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-07-06 02:25:48,215][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-07-06 02:25:48,279][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-07-06 02:25:53,349][gdb1][INFO  ] checking OSD status...
[2017-07-06 02:25:53,349][gdb1][DEBUG ] find the location of an executable
[2017-07-06 02:25:53,351][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 02:25:53,519][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 03:22:41,728][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f284293e128>
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f284324b1b8>
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:22:41,729][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:22:41,729][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-07-06 03:22:41,729][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-07-06 03:22:41,729][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-07-06 03:22:41,730][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-07-06 03:22:41,755][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:22:41,769][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:22:41,769][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:22:41,785][gdb3][DEBUG ] detect machine type
[2017-07-06 03:22:41,788][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:22:41,788][gdb3][INFO  ] Purging Ceph on gdb3
[2017-07-06 03:22:41,789][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-07-06 03:22:41,826][gdb3][DEBUG ] Reading package lists...
[2017-07-06 03:22:41,991][gdb3][DEBUG ] Building dependency tree...
[2017-07-06 03:22:41,991][gdb3][DEBUG ] Reading state information...
[2017-07-06 03:22:42,055][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-07-06 03:22:42,055][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-07-06 03:22:42,056][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-07-06 03:22:42,056][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-07-06 03:22:42,056][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-07-06 03:22:42,056][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-07-06 03:22:42,056][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-07-06 03:22:42,056][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1018 linux-headers-4.4.0-1013-aws
[2017-07-06 03:22:42,056][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-headers-4.4.0-1017-aws
[2017-07-06 03:22:42,057][gdb3][DEBUG ]   linux-headers-4.4.0-1018-aws linux-image-4.4.0-1013-aws
[2017-07-06 03:22:42,057][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws
[2017-07-06 03:22:42,057][gdb3][DEBUG ]   linux-image-4.4.0-1018-aws ntp python-blinker python-cephfs
[2017-07-06 03:22:42,057][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-07-06 03:22:42,057][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-07-06 03:22:42,057][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-07-06 03:22:42,057][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-07-06 03:22:42,057][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-07-06 03:22:42,057][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-07-06 03:22:42,073][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-07-06 03:22:42,073][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-07-06 03:22:42,187][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 60 not upgraded.
[2017-07-06 03:22:42,188][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-07-06 03:22:42,302][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 179644 files and directories currently installed.)
[2017-07-06 03:22:42,302][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-07-06 03:22:42,466][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-07-06 03:22:42,580][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-07-06 03:22:42,580][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-07-06 03:22:42,797][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-07-06 03:22:42,912][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-07-06 03:22:43,077][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-07-06 03:22:43,144][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-07-06 03:22:43,176][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-07-06 03:22:43,340][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-07-06 03:22:43,455][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-07-06 03:22:43,457][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-07-06 03:22:43,624][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-07-06 03:22:43,656][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-07-06 03:22:43,820][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-07-06 03:22:43,884][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-07-06 03:22:43,884][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-07-06 03:22:43,998][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-07-06 03:22:44,815][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-07-06 03:22:44,977][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:22:44,977][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-07-06 03:22:44,977][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:22:44,977][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:22:44,977][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:22:44,977][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:22:44,977][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:22:44,977][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f16b76db7a0>
[2017-07-06 03:22:44,977][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:22:44,977][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 03:22:44,978][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f16b7fe8230>
[2017-07-06 03:22:44,978][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:22:44,978][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:22:44,978][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-07-06 03:22:45,003][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:22:45,017][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:22:45,018][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:22:45,034][gdb3][DEBUG ] detect machine type
[2017-07-06 03:22:45,036][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:22:45,052][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:22:45,066][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:22:45,066][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:22:45,083][gdb3][DEBUG ] detect machine type
[2017-07-06 03:22:45,085][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:22:45,085][gdb3][INFO  ] purging data on gdb3
[2017-07-06 03:22:45,086][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-07-06 03:22:45,099][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-07-06 03:22:45,267][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:22:45,268][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-07-06 03:22:45,268][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:22:45,268][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:22:45,268][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:22:45,268][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:22:45,268][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:22:45,268][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f373e892a70>
[2017-07-06 03:22:45,268][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:22:45,268][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f373f155848>
[2017-07-06 03:22:45,268][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:22:45,268][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:23:03,739][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:23:03,739][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-07-06 03:23:03,739][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f52eeefd5f0>
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f52ef581758>
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:23:03,740][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-07-06 03:23:03,740][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-07-06 03:23:03,741][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-07-06 03:23:03,766][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:23:03,781][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:23:03,781][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:23:03,798][gdb3][DEBUG ] detect machine type
[2017-07-06 03:23:03,800][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:23:03,801][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-07-06 03:23:03,812][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-07-06 03:23:03,818][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-07-06 03:23:03,819][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-07-06 03:23:03,819][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-07-06 03:23:03,819][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-07-06 03:23:03,819][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-07-06 03:23:03,819][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-07-06 03:23:03,819][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-07-06 03:23:03,819][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-07-06 03:23:03,986][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:23:03,986][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-07-06 03:23:03,986][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:23:03,986][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:23:03,986][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:23:03,986][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 03:23:03,986][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-07-06 03:23:03,986][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:23:03,986][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff327537ef0>
[2017-07-06 03:23:03,987][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:23:03,987][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7ff32750bb18>
[2017-07-06 03:23:03,987][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:23:03,987][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-07-06 03:23:03,987][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:23:03,987][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-07-06 03:23:03,987][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-07-06 03:23:04,014][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:23:04,027][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:23:04,028][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:23:04,044][gdb3][DEBUG ] detect machine type
[2017-07-06 03:23:04,047][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:23:04,047][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-07-06 03:23:04,047][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-07-06 03:23:04,047][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:23:04,048][gdb3][DEBUG ] deploying mon to gdb3
[2017-07-06 03:23:04,048][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:23:04,048][gdb3][DEBUG ] remote hostname: gdb3
[2017-07-06 03:23:04,049][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:23:04,050][gdb3][DEBUG ] create the mon path if it does not exist
[2017-07-06 03:23:04,050][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 03:23:04,051][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 03:23:04,051][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 03:23:04,051][gdb3][DEBUG ] create the monitor keyring file
[2017-07-06 03:23:04,052][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-07-06 03:23:04,090][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-07-06 03:23:04,090][gdb3][DEBUG ] ceph-mon: set fsid to 0b8c0e26-e476-4cda-b7d0-56db088c2dc8
[2017-07-06 03:23:04,092][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-07-06 03:23:04,093][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 03:23:04,093][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-07-06 03:23:04,094][gdb3][DEBUG ] create the init path if it does not exist
[2017-07-06 03:23:04,095][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 03:23:04,166][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-07-06 03:23:04,234][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-07-06 03:23:06,304][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:23:06,369][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 03:23:06,369][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-07-06 03:23:06,369][gdb3][DEBUG ] {
[2017-07-06 03:23:06,370][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-07-06 03:23:06,370][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-07-06 03:23:06,370][gdb3][DEBUG ]   "features": {
[2017-07-06 03:23:06,370][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-07-06 03:23:06,370][gdb3][DEBUG ]     "quorum_mon": [
[2017-07-06 03:23:06,370][gdb3][DEBUG ]       "kraken", 
[2017-07-06 03:23:06,370][gdb3][DEBUG ]       "luminous"
[2017-07-06 03:23:06,370][gdb3][DEBUG ]     ], 
[2017-07-06 03:23:06,370][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-07-06 03:23:06,370][gdb3][DEBUG ]     "required_mon": [
[2017-07-06 03:23:06,370][gdb3][DEBUG ]       "kraken", 
[2017-07-06 03:23:06,370][gdb3][DEBUG ]       "luminous"
[2017-07-06 03:23:06,370][gdb3][DEBUG ]     ]
[2017-07-06 03:23:06,370][gdb3][DEBUG ]   }, 
[2017-07-06 03:23:06,370][gdb3][DEBUG ]   "monmap": {
[2017-07-06 03:23:06,371][gdb3][DEBUG ]     "created": "2017-07-06 03:23:04.075648", 
[2017-07-06 03:23:06,371][gdb3][DEBUG ]     "epoch": 2, 
[2017-07-06 03:23:06,371][gdb3][DEBUG ]     "features": {
[2017-07-06 03:23:06,371][gdb3][DEBUG ]       "optional": [], 
[2017-07-06 03:23:06,371][gdb3][DEBUG ]       "persistent": [
[2017-07-06 03:23:06,371][gdb3][DEBUG ]         "kraken", 
[2017-07-06 03:23:06,371][gdb3][DEBUG ]         "luminous"
[2017-07-06 03:23:06,371][gdb3][DEBUG ]       ]
[2017-07-06 03:23:06,371][gdb3][DEBUG ]     }, 
[2017-07-06 03:23:06,371][gdb3][DEBUG ]     "fsid": "0b8c0e26-e476-4cda-b7d0-56db088c2dc8", 
[2017-07-06 03:23:06,371][gdb3][DEBUG ]     "modified": "2017-07-06 03:23:04.328311", 
[2017-07-06 03:23:06,371][gdb3][DEBUG ]     "mons": [
[2017-07-06 03:23:06,371][gdb3][DEBUG ]       {
[2017-07-06 03:23:06,371][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-07-06 03:23:06,371][gdb3][DEBUG ]         "name": "gdb3", 
[2017-07-06 03:23:06,371][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-07-06 03:23:06,371][gdb3][DEBUG ]         "rank": 0
[2017-07-06 03:23:06,372][gdb3][DEBUG ]       }
[2017-07-06 03:23:06,372][gdb3][DEBUG ]     ]
[2017-07-06 03:23:06,372][gdb3][DEBUG ]   }, 
[2017-07-06 03:23:06,372][gdb3][DEBUG ]   "name": "gdb3", 
[2017-07-06 03:23:06,372][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-07-06 03:23:06,372][gdb3][DEBUG ]   "quorum": [
[2017-07-06 03:23:06,372][gdb3][DEBUG ]     0
[2017-07-06 03:23:06,372][gdb3][DEBUG ]   ], 
[2017-07-06 03:23:06,372][gdb3][DEBUG ]   "rank": 0, 
[2017-07-06 03:23:06,372][gdb3][DEBUG ]   "state": "leader", 
[2017-07-06 03:23:06,372][gdb3][DEBUG ]   "sync_provider": []
[2017-07-06 03:23:06,372][gdb3][DEBUG ] }
[2017-07-06 03:23:06,372][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 03:23:06,372][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-07-06 03:23:06,373][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:23:06,438][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-07-06 03:23:06,453][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:23:06,467][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:23:06,468][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:23:06,484][gdb3][DEBUG ] detect machine type
[2017-07-06 03:23:06,487][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:23:06,488][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:23:06,553][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-07-06 03:23:06,553][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-07-06 03:23:06,553][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-07-06 03:23:06,555][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpPFYiOd
[2017-07-06 03:23:06,570][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:23:06,584][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:23:06,584][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:23:06,601][gdb3][DEBUG ] detect machine type
[2017-07-06 03:23:06,603][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:23:06,604][gdb3][DEBUG ] fetch remote file
[2017-07-06 03:23:06,605][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:23:06,671][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-07-06 03:23:06,837][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-07-06 03:23:07,003][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-07-06 03:23:07,169][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-07-06 03:23:07,336][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-07-06 03:23:07,502][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-07-06 03:23:07,668][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-07-06 03:23:07,834][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-07-06 03:23:08,000][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-07-06 03:23:08,166][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-07-06 03:23:08,332][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-07-06 03:23:08,332][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-07-06 03:23:08,332][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170706032308'
[2017-07-06 03:23:08,332][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-07-06 03:23:08,332][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-07-06 03:23:08,333][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-07-06 03:23:08,333][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpPFYiOd
[2017-07-06 03:23:08,513][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:23:08,513][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-07-06 03:23:08,513][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:23:08,513][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:23:08,513][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:23:08,514][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:23:08,514][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:23:08,514][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb1c47265a8>
[2017-07-06 03:23:08,514][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:23:08,514][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-07-06 03:23:08,514][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fb1c503d938>
[2017-07-06 03:23:08,514][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:23:08,514][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:23:08,514][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 03:23:08,540][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:23:08,554][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:23:08,554][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:23:08,572][gdb3][DEBUG ] detect machine type
[2017-07-06 03:23:08,574][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:23:46,349][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:23:46,349][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-07-06 03:23:46,349][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:23:46,349][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:23:46,349][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:23:46,349][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 03:23:46,349][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:23:46,350][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe2f9f815a8>
[2017-07-06 03:23:46,350][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:23:46,350][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-07-06 03:23:46,350][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fe2fa898938>
[2017-07-06 03:23:46,350][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:23:46,350][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:23:46,350][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-07-06 03:23:46,806][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:23:47,036][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:23:47,037][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:23:47,069][gdb1][DEBUG ] detect machine type
[2017-07-06 03:23:47,073][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:23:47,245][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:23:47,245][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f78c866e998>
[2017-07-06 03:23:47,246][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:23:47,247][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 03:23:47,247][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f78c88c4aa0>
[2017-07-06 03:23:47,247][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:23:47,247][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:23:47,247][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-07-06 03:23:47,247][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 03:23:47,493][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:23:47,721][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:23:47,721][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:23:47,739][gdb1][DEBUG ] detect machine type
[2017-07-06 03:23:47,742][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:23:47,743][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:23:47,743][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-07-06 03:23:47,744][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:23:47,746][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-07-06 03:23:47,746][gdb1][DEBUG ] create a keyring file
[2017-07-06 03:23:47,749][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-07-06 03:23:47,749][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:23:47,751][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-07-06 03:23:47,972][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:23:47,975][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:23:47,991][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:23:48,007][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:23:48,022][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 03:23:48,030][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/buddystore
[2017-07-06 03:23:48,033][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/ceph_fsid.8373.tmp
[2017-07-06 03:23:48,037][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/fsid.8373.tmp
[2017-07-06 03:23:48,040][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/magic.8373.tmp
[2017-07-06 03:23:53,061][gdb1][INFO  ] checking OSD status...
[2017-07-06 03:23:53,061][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:23:53,064][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 03:23:53,229][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-07-06 03:23:53,393][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:23:53,393][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-07-06 03:23:53,393][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:23:53,393][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:23:53,393][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:23:53,394][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:23:53,394][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-07-06 03:23:53,394][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:23:53,394][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fad70372998>
[2017-07-06 03:23:53,394][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:23:53,394][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fad705c8aa0>
[2017-07-06 03:23:53,394][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:23:53,394][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:23:53,394][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 03:23:53,394][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 03:23:53,638][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:23:53,873][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:23:53,874][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:23:53,890][gdb1][DEBUG ] detect machine type
[2017-07-06 03:23:53,894][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:23:53,895][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:23:53,895][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-07-06 03:23:53,895][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-07-06 03:23:53,895][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:23:53,897][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-07-06 03:23:54,067][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-07-06 03:23:54,068][gdb1][WARNING] activate: Cluster uuid is 0b8c0e26-e476-4cda-b7d0-56db088c2dc8
[2017-07-06 03:23:54,068][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:23:54,068][gdb1][WARNING] activate: Cluster name is ceph
[2017-07-06 03:23:54,068][gdb1][WARNING] activate: OSD uuid is 63b6759c-9990-4858-9e9c-09e83957a9a2
[2017-07-06 03:23:54,068][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-07-06 03:23:54,068][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 63b6759c-9990-4858-9e9c-09e83957a9a2
[2017-07-06 03:23:54,232][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/whoami.8490.tmp
[2017-07-06 03:23:54,232][gdb1][WARNING] activate: OSD id is 0
[2017-07-06 03:23:54,233][gdb1][WARNING] activate: Initializing OSD...
[2017-07-06 03:23:54,233][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/buddystore/activate.monmap
[2017-07-06 03:23:54,347][gdb1][WARNING] got monmap epoch 2
[2017-07-06 03:23:54,347][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/buddystore/activate.monmap --osd-data /mnt/buddystore --osd-journal /mnt/buddystore/journal --osd-uuid 63b6759c-9990-4858-9e9c-09e83957a9a2 --keyring /mnt/buddystore/keyring --setuser ceph --setgroup ceph
[2017-07-06 03:23:54,411][gdb1][WARNING] activate: Marking with init system systemd
[2017-07-06 03:23:54,411][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/systemd
[2017-07-06 03:23:54,411][gdb1][WARNING] activate: Authorizing OSD key...
[2017-07-06 03:23:54,411][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/buddystore/keyring osd allow * mon allow profile osd
[2017-07-06 03:23:54,575][gdb1][WARNING] added key for osd.0
[2017-07-06 03:23:54,576][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/active.8490.tmp
[2017-07-06 03:23:54,576][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/buddystore
[2017-07-06 03:23:54,576][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-0 -> /mnt/buddystore
[2017-07-06 03:23:54,576][gdb1][WARNING] start_daemon: Starting ceph osd.0...
[2017-07-06 03:23:54,576][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-07-06 03:23:54,576][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-07-06 03:23:54,640][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-07-06 03:23:54,704][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-07-06 03:23:54,704][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-07-06 03:23:54,768][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-07-06 03:23:59,837][gdb1][INFO  ] checking OSD status...
[2017-07-06 03:23:59,837][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:23:59,840][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 03:24:00,008][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 03:45:12,480][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:45:12,480][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-07-06 03:45:12,480][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:45:12,480][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:45:12,480][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:45:12,480][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:45:12,480][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:45:12,480][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6922c21128>
[2017-07-06 03:45:12,480][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:45:12,480][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 03:45:12,481][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f692352e1b8>
[2017-07-06 03:45:12,481][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:45:12,481][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:45:12,481][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-07-06 03:45:12,481][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-07-06 03:45:12,481][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-07-06 03:45:12,481][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-07-06 03:45:12,507][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:45:12,521][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:45:12,521][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:45:12,537][gdb3][DEBUG ] detect machine type
[2017-07-06 03:45:12,540][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:45:12,540][gdb3][INFO  ] Purging Ceph on gdb3
[2017-07-06 03:45:12,541][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-07-06 03:45:12,579][gdb3][DEBUG ] Reading package lists...
[2017-07-06 03:45:12,743][gdb3][DEBUG ] Building dependency tree...
[2017-07-06 03:45:12,743][gdb3][DEBUG ] Reading state information...
[2017-07-06 03:45:12,807][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-07-06 03:45:12,807][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-07-06 03:45:12,807][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1018 linux-headers-4.4.0-1013-aws
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-headers-4.4.0-1017-aws
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   linux-headers-4.4.0-1018-aws linux-image-4.4.0-1013-aws
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   linux-image-4.4.0-1018-aws ntp python-blinker python-cephfs
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-07-06 03:45:12,808][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-07-06 03:45:12,809][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-07-06 03:45:12,873][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-07-06 03:45:12,873][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-07-06 03:45:12,987][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 60 not upgraded.
[2017-07-06 03:45:12,987][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-07-06 03:45:13,051][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 179644 files and directories currently installed.)
[2017-07-06 03:45:13,053][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-07-06 03:45:13,218][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-07-06 03:45:13,332][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-07-06 03:45:13,364][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-07-06 03:45:13,578][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-07-06 03:45:13,693][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-07-06 03:45:13,861][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-07-06 03:45:13,893][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-07-06 03:45:13,925][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-07-06 03:45:14,095][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-07-06 03:45:14,161][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-07-06 03:45:14,176][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-07-06 03:45:14,342][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-07-06 03:45:14,406][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-07-06 03:45:14,520][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-07-06 03:45:14,634][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-07-06 03:45:14,634][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-07-06 03:45:14,698][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-07-06 03:45:15,564][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-07-06 03:45:15,729][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:45:15,729][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-07-06 03:45:15,729][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:45:15,729][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:45:15,729][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:45:15,729][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:45:15,729][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:45:15,729][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feace9df7a0>
[2017-07-06 03:45:15,730][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:45:15,730][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 03:45:15,730][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7feacf2ec230>
[2017-07-06 03:45:15,730][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:45:15,730][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:45:15,730][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-07-06 03:45:15,755][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:45:15,770][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:45:15,770][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:45:15,787][gdb3][DEBUG ] detect machine type
[2017-07-06 03:45:15,789][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:45:15,805][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:45:15,818][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:45:15,819][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:45:15,835][gdb3][DEBUG ] detect machine type
[2017-07-06 03:45:15,838][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:45:15,838][gdb3][INFO  ] purging data on gdb3
[2017-07-06 03:45:15,839][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-07-06 03:45:15,852][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-07-06 03:45:16,022][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:45:16,022][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-07-06 03:45:16,022][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:45:16,023][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:45:16,023][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:45:16,023][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:45:16,023][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:45:16,023][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd34c1a6a70>
[2017-07-06 03:45:16,023][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:45:16,023][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fd34ca69848>
[2017-07-06 03:45:16,023][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:45:16,023][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:45:30,628][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:45:30,628][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-07-06 03:45:30,628][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fde7c92c5f0>
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fde7cfb0758>
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-07-06 03:45:30,629][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:45:30,630][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-07-06 03:45:30,630][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-07-06 03:45:30,630][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-07-06 03:45:30,656][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:45:30,670][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:45:30,670][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:45:30,687][gdb3][DEBUG ] detect machine type
[2017-07-06 03:45:30,689][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:45:30,690][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-07-06 03:45:30,702][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-07-06 03:45:30,708][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-07-06 03:45:30,708][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-07-06 03:45:30,708][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-07-06 03:45:30,708][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-07-06 03:45:30,708][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-07-06 03:45:30,709][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-07-06 03:45:30,709][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-07-06 03:45:30,709][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-07-06 03:45:30,873][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:45:30,873][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-07-06 03:45:30,873][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:45:30,873][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:45:30,874][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:45:30,874][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 03:45:30,874][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-07-06 03:45:30,874][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:45:30,874][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff040fc9ef0>
[2017-07-06 03:45:30,874][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:45:30,874][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7ff040f9db18>
[2017-07-06 03:45:30,874][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:45:30,874][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-07-06 03:45:30,874][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:45:30,875][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-07-06 03:45:30,875][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-07-06 03:45:30,901][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:45:30,914][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:45:30,915][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:45:30,931][gdb3][DEBUG ] detect machine type
[2017-07-06 03:45:30,933][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:45:30,934][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-07-06 03:45:30,934][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-07-06 03:45:30,934][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:45:30,934][gdb3][DEBUG ] deploying mon to gdb3
[2017-07-06 03:45:30,934][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:45:30,935][gdb3][DEBUG ] remote hostname: gdb3
[2017-07-06 03:45:30,935][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:45:30,937][gdb3][DEBUG ] create the mon path if it does not exist
[2017-07-06 03:45:30,937][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 03:45:30,937][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 03:45:30,938][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 03:45:30,938][gdb3][DEBUG ] create the monitor keyring file
[2017-07-06 03:45:30,939][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-07-06 03:45:30,977][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-07-06 03:45:30,977][gdb3][DEBUG ] ceph-mon: set fsid to 01f0a223-ad61-4739-bd33-ecc05bf51507
[2017-07-06 03:45:30,979][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-07-06 03:45:30,982][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 03:45:30,983][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-07-06 03:45:30,983][gdb3][DEBUG ] create the init path if it does not exist
[2017-07-06 03:45:30,984][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 03:45:31,052][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-07-06 03:45:31,119][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-07-06 03:45:33,160][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:45:33,225][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 03:45:33,225][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-07-06 03:45:33,225][gdb3][DEBUG ] {
[2017-07-06 03:45:33,225][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-07-06 03:45:33,225][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-07-06 03:45:33,225][gdb3][DEBUG ]   "features": {
[2017-07-06 03:45:33,225][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-07-06 03:45:33,225][gdb3][DEBUG ]     "quorum_mon": [
[2017-07-06 03:45:33,225][gdb3][DEBUG ]       "kraken", 
[2017-07-06 03:45:33,225][gdb3][DEBUG ]       "luminous"
[2017-07-06 03:45:33,226][gdb3][DEBUG ]     ], 
[2017-07-06 03:45:33,226][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-07-06 03:45:33,226][gdb3][DEBUG ]     "required_mon": [
[2017-07-06 03:45:33,226][gdb3][DEBUG ]       "kraken", 
[2017-07-06 03:45:33,226][gdb3][DEBUG ]       "luminous"
[2017-07-06 03:45:33,226][gdb3][DEBUG ]     ]
[2017-07-06 03:45:33,226][gdb3][DEBUG ]   }, 
[2017-07-06 03:45:33,226][gdb3][DEBUG ]   "monmap": {
[2017-07-06 03:45:33,226][gdb3][DEBUG ]     "created": "2017-07-06 03:45:30.961658", 
[2017-07-06 03:45:33,226][gdb3][DEBUG ]     "epoch": 2, 
[2017-07-06 03:45:33,226][gdb3][DEBUG ]     "features": {
[2017-07-06 03:45:33,226][gdb3][DEBUG ]       "optional": [], 
[2017-07-06 03:45:33,226][gdb3][DEBUG ]       "persistent": [
[2017-07-06 03:45:33,226][gdb3][DEBUG ]         "kraken", 
[2017-07-06 03:45:33,226][gdb3][DEBUG ]         "luminous"
[2017-07-06 03:45:33,226][gdb3][DEBUG ]       ]
[2017-07-06 03:45:33,226][gdb3][DEBUG ]     }, 
[2017-07-06 03:45:33,227][gdb3][DEBUG ]     "fsid": "01f0a223-ad61-4739-bd33-ecc05bf51507", 
[2017-07-06 03:45:33,227][gdb3][DEBUG ]     "modified": "2017-07-06 03:45:31.197830", 
[2017-07-06 03:45:33,227][gdb3][DEBUG ]     "mons": [
[2017-07-06 03:45:33,227][gdb3][DEBUG ]       {
[2017-07-06 03:45:33,227][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-07-06 03:45:33,227][gdb3][DEBUG ]         "name": "gdb3", 
[2017-07-06 03:45:33,227][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-07-06 03:45:33,227][gdb3][DEBUG ]         "rank": 0
[2017-07-06 03:45:33,227][gdb3][DEBUG ]       }
[2017-07-06 03:45:33,227][gdb3][DEBUG ]     ]
[2017-07-06 03:45:33,227][gdb3][DEBUG ]   }, 
[2017-07-06 03:45:33,227][gdb3][DEBUG ]   "name": "gdb3", 
[2017-07-06 03:45:33,227][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-07-06 03:45:33,227][gdb3][DEBUG ]   "quorum": [
[2017-07-06 03:45:33,227][gdb3][DEBUG ]     0
[2017-07-06 03:45:33,227][gdb3][DEBUG ]   ], 
[2017-07-06 03:45:33,227][gdb3][DEBUG ]   "rank": 0, 
[2017-07-06 03:45:33,228][gdb3][DEBUG ]   "state": "leader", 
[2017-07-06 03:45:33,228][gdb3][DEBUG ]   "sync_provider": []
[2017-07-06 03:45:33,228][gdb3][DEBUG ] }
[2017-07-06 03:45:33,228][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 03:45:33,228][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-07-06 03:45:33,229][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:45:33,294][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-07-06 03:45:33,309][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:45:33,322][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:45:33,323][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:45:33,339][gdb3][DEBUG ] detect machine type
[2017-07-06 03:45:33,341][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:45:33,343][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:45:33,408][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-07-06 03:45:33,408][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-07-06 03:45:33,408][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-07-06 03:45:33,410][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpGypRCf
[2017-07-06 03:45:33,425][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:45:33,439][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:45:33,439][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:45:33,456][gdb3][DEBUG ] detect machine type
[2017-07-06 03:45:33,458][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:45:33,459][gdb3][DEBUG ] fetch remote file
[2017-07-06 03:45:33,460][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:45:33,526][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-07-06 03:45:33,692][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-07-06 03:45:33,858][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-07-06 03:45:34,024][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-07-06 03:45:34,190][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-07-06 03:45:34,356][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-07-06 03:45:34,523][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-07-06 03:45:34,689][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-07-06 03:45:34,855][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-07-06 03:45:35,021][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-07-06 03:45:35,186][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-07-06 03:45:35,186][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-07-06 03:45:35,186][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170706034535'
[2017-07-06 03:45:35,187][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-07-06 03:45:35,187][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-07-06 03:45:35,187][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-07-06 03:45:35,187][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpGypRCf
[2017-07-06 03:45:35,366][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:45:35,367][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-07-06 03:45:35,367][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:45:35,367][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:45:35,367][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:45:35,367][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:45:35,367][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:45:35,367][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f915d12b5a8>
[2017-07-06 03:45:35,367][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:45:35,367][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-07-06 03:45:35,367][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f915da42938>
[2017-07-06 03:45:35,367][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:45:35,368][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:45:35,368][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 03:45:35,394][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:45:35,408][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:45:35,408][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:45:35,425][gdb3][DEBUG ] detect machine type
[2017-07-06 03:45:35,427][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:45:40,658][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:45:40,658][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-07-06 03:45:40,658][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:45:40,658][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:45:40,659][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:45:40,659][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 03:45:40,659][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:45:40,659][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5407d5c5a8>
[2017-07-06 03:45:40,659][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:45:40,659][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-07-06 03:45:40,659][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f5408673938>
[2017-07-06 03:45:40,659][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:45:40,659][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:45:40,659][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-07-06 03:45:40,897][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:45:41,130][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:45:41,130][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:45:41,147][gdb1][DEBUG ] detect machine type
[2017-07-06 03:45:41,151][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:45:41,322][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:45:41,323][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3e2c592998>
[2017-07-06 03:45:41,324][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:45:41,324][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 03:45:41,324][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3e2c7e8aa0>
[2017-07-06 03:45:41,324][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:45:41,324][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:45:41,324][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-07-06 03:45:41,324][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 03:45:41,562][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:45:41,797][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:45:41,797][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:45:41,814][gdb1][DEBUG ] detect machine type
[2017-07-06 03:45:41,817][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:45:41,818][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:45:41,818][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-07-06 03:45:41,819][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:45:41,821][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-07-06 03:45:41,821][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:45:41,824][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-07-06 03:45:41,994][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:45:41,994][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:45:41,994][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:45:41,994][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:45:41,996][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 03:45:42,011][gdb1][WARNING] populate_data_path: Data dir /mnt/buddystore already exists
[2017-07-06 03:45:47,024][gdb1][INFO  ] checking OSD status...
[2017-07-06 03:45:47,024][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:45:47,027][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 03:45:47,192][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-07-06 03:45:47,357][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:45:47,358][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-07-06 03:45:47,358][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:45:47,358][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:45:47,358][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:45:47,358][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:45:47,358][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-07-06 03:45:47,358][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:45:47,358][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff02dd02998>
[2017-07-06 03:45:47,358][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:45:47,358][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff02df58aa0>
[2017-07-06 03:45:47,358][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:45:47,359][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:45:47,359][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 03:45:47,359][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 03:45:47,606][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:45:47,833][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:45:47,834][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:45:47,851][gdb1][DEBUG ] detect machine type
[2017-07-06 03:45:47,855][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:45:47,855][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:45:47,856][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-07-06 03:45:47,856][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-07-06 03:45:47,856][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:45:47,858][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-07-06 03:45:48,028][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-07-06 03:45:48,029][gdb1][WARNING] activate: Cluster uuid is 0b8c0e26-e476-4cda-b7d0-56db088c2dc8
[2017-07-06 03:45:48,029][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:45:48,029][gdb1][WARNING] Traceback (most recent call last):
[2017-07-06 03:45:48,029][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-07-06 03:45:48,029][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-07-06 03:45:48,029][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-07-06 03:45:48,029][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-07-06 03:45:48,029][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-07-06 03:45:48,029][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-07-06 03:45:48,029][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3609, in activate
[2017-07-06 03:45:48,029][gdb1][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid 0b8c0e26-e476-4cda-b7d0-56db088c2dc8
[2017-07-06 03:45:48,030][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-07-06 03:45:48,030][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore

[2017-07-06 03:56:02,776][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f669b9a2128>
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f669c2af1b8>
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:56:02,777][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:56:02,777][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-07-06 03:56:02,778][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-07-06 03:56:02,778][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-07-06 03:56:02,778][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-07-06 03:56:02,804][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:56:02,818][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:56:02,819][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:56:02,835][gdb3][DEBUG ] detect machine type
[2017-07-06 03:56:02,838][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:56:02,838][gdb3][INFO  ] Purging Ceph on gdb3
[2017-07-06 03:56:02,839][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-07-06 03:56:02,877][gdb3][DEBUG ] Reading package lists...
[2017-07-06 03:56:03,041][gdb3][DEBUG ] Building dependency tree...
[2017-07-06 03:56:03,042][gdb3][DEBUG ] Reading state information...
[2017-07-06 03:56:03,106][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-07-06 03:56:03,106][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-07-06 03:56:03,106][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-07-06 03:56:03,106][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-07-06 03:56:03,106][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-07-06 03:56:03,106][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-07-06 03:56:03,106][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-07-06 03:56:03,106][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1018 linux-headers-4.4.0-1013-aws
[2017-07-06 03:56:03,106][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-headers-4.4.0-1017-aws
[2017-07-06 03:56:03,106][gdb3][DEBUG ]   linux-headers-4.4.0-1018-aws linux-image-4.4.0-1013-aws
[2017-07-06 03:56:03,107][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws
[2017-07-06 03:56:03,107][gdb3][DEBUG ]   linux-image-4.4.0-1018-aws ntp python-blinker python-cephfs
[2017-07-06 03:56:03,107][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-07-06 03:56:03,107][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-07-06 03:56:03,107][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-07-06 03:56:03,107][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-07-06 03:56:03,107][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-07-06 03:56:03,107][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-07-06 03:56:03,171][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-07-06 03:56:03,171][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-07-06 03:56:03,285][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 60 not upgraded.
[2017-07-06 03:56:03,286][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-07-06 03:56:03,350][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 179644 files and directories currently installed.)
[2017-07-06 03:56:03,350][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-07-06 03:56:03,568][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-07-06 03:56:03,632][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-07-06 03:56:03,664][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-07-06 03:56:03,879][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-07-06 03:56:03,995][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-07-06 03:56:04,162][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-07-06 03:56:04,194][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-07-06 03:56:04,226][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-07-06 03:56:04,391][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-07-06 03:56:04,455][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-07-06 03:56:04,487][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-07-06 03:56:04,651][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-07-06 03:56:04,683][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-07-06 03:56:04,850][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-07-06 03:56:04,914][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-07-06 03:56:04,914][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-07-06 03:56:05,028][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-07-06 03:56:05,845][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-07-06 03:56:06,010][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:56:06,010][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-07-06 03:56:06,010][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:56:06,010][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:56:06,011][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:56:06,011][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:56:06,011][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:56:06,011][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff02ceff7a0>
[2017-07-06 03:56:06,011][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:56:06,011][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 03:56:06,011][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7ff02d80c230>
[2017-07-06 03:56:06,011][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:56:06,011][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:56:06,011][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-07-06 03:56:06,038][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:56:06,052][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:56:06,052][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:56:06,068][gdb3][DEBUG ] detect machine type
[2017-07-06 03:56:06,071][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:56:06,086][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:56:06,100][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:56:06,100][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:56:06,117][gdb3][DEBUG ] detect machine type
[2017-07-06 03:56:06,119][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:56:06,119][gdb3][INFO  ] purging data on gdb3
[2017-07-06 03:56:06,120][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-07-06 03:56:06,133][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-07-06 03:56:06,306][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:56:06,306][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-07-06 03:56:06,306][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:56:06,306][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:56:06,306][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:56:06,306][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:56:06,307][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:56:06,307][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb02679ea70>
[2017-07-06 03:56:06,307][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:56:06,307][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fb027061848>
[2017-07-06 03:56:06,307][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:56:06,307][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:56:19,441][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:56:19,441][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f503405b5f0>
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f50346df758>
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:56:19,442][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-07-06 03:56:19,443][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-07-06 03:56:19,443][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-07-06 03:56:19,469][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:56:19,483][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:56:19,484][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:56:19,500][gdb3][DEBUG ] detect machine type
[2017-07-06 03:56:19,502][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:56:19,503][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-07-06 03:56:19,514][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-07-06 03:56:19,521][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-07-06 03:56:19,521][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-07-06 03:56:19,521][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-07-06 03:56:19,521][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-07-06 03:56:19,521][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-07-06 03:56:19,521][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-07-06 03:56:19,521][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-07-06 03:56:19,521][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-07-06 03:56:19,685][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:56:19,685][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-07-06 03:56:19,685][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:56:19,685][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:56:19,685][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:56:19,686][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 03:56:19,686][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-07-06 03:56:19,686][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:56:19,686][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9288c65ef0>
[2017-07-06 03:56:19,686][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:56:19,686][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f9288c39b18>
[2017-07-06 03:56:19,686][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:56:19,686][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-07-06 03:56:19,686][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:56:19,687][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-07-06 03:56:19,687][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-07-06 03:56:19,713][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:56:19,727][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:56:19,727][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:56:19,744][gdb3][DEBUG ] detect machine type
[2017-07-06 03:56:19,746][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:56:19,746][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-07-06 03:56:19,747][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-07-06 03:56:19,747][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:56:19,747][gdb3][DEBUG ] deploying mon to gdb3
[2017-07-06 03:56:19,747][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:56:19,747][gdb3][DEBUG ] remote hostname: gdb3
[2017-07-06 03:56:19,748][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:56:19,749][gdb3][DEBUG ] create the mon path if it does not exist
[2017-07-06 03:56:19,750][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 03:56:19,750][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 03:56:19,750][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 03:56:19,750][gdb3][DEBUG ] create the monitor keyring file
[2017-07-06 03:56:19,751][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-07-06 03:56:19,789][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-07-06 03:56:19,790][gdb3][DEBUG ] ceph-mon: set fsid to 8113d479-744e-47dc-977e-d630deb1cc30
[2017-07-06 03:56:19,793][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-07-06 03:56:19,794][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 03:56:19,795][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-07-06 03:56:19,795][gdb3][DEBUG ] create the init path if it does not exist
[2017-07-06 03:56:19,796][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 03:56:19,866][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-07-06 03:56:19,934][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-07-06 03:56:22,004][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:56:22,069][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 03:56:22,069][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-07-06 03:56:22,069][gdb3][DEBUG ] {
[2017-07-06 03:56:22,069][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-07-06 03:56:22,069][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-07-06 03:56:22,069][gdb3][DEBUG ]   "features": {
[2017-07-06 03:56:22,070][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-07-06 03:56:22,070][gdb3][DEBUG ]     "quorum_mon": [
[2017-07-06 03:56:22,070][gdb3][DEBUG ]       "kraken", 
[2017-07-06 03:56:22,070][gdb3][DEBUG ]       "luminous"
[2017-07-06 03:56:22,070][gdb3][DEBUG ]     ], 
[2017-07-06 03:56:22,070][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-07-06 03:56:22,070][gdb3][DEBUG ]     "required_mon": [
[2017-07-06 03:56:22,070][gdb3][DEBUG ]       "kraken", 
[2017-07-06 03:56:22,070][gdb3][DEBUG ]       "luminous"
[2017-07-06 03:56:22,070][gdb3][DEBUG ]     ]
[2017-07-06 03:56:22,070][gdb3][DEBUG ]   }, 
[2017-07-06 03:56:22,070][gdb3][DEBUG ]   "monmap": {
[2017-07-06 03:56:22,070][gdb3][DEBUG ]     "created": "2017-07-06 03:56:19.775103", 
[2017-07-06 03:56:22,070][gdb3][DEBUG ]     "epoch": 2, 
[2017-07-06 03:56:22,070][gdb3][DEBUG ]     "features": {
[2017-07-06 03:56:22,070][gdb3][DEBUG ]       "optional": [], 
[2017-07-06 03:56:22,071][gdb3][DEBUG ]       "persistent": [
[2017-07-06 03:56:22,071][gdb3][DEBUG ]         "kraken", 
[2017-07-06 03:56:22,071][gdb3][DEBUG ]         "luminous"
[2017-07-06 03:56:22,071][gdb3][DEBUG ]       ]
[2017-07-06 03:56:22,071][gdb3][DEBUG ]     }, 
[2017-07-06 03:56:22,071][gdb3][DEBUG ]     "fsid": "8113d479-744e-47dc-977e-d630deb1cc30", 
[2017-07-06 03:56:22,071][gdb3][DEBUG ]     "modified": "2017-07-06 03:56:20.022860", 
[2017-07-06 03:56:22,071][gdb3][DEBUG ]     "mons": [
[2017-07-06 03:56:22,071][gdb3][DEBUG ]       {
[2017-07-06 03:56:22,071][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-07-06 03:56:22,071][gdb3][DEBUG ]         "name": "gdb3", 
[2017-07-06 03:56:22,071][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-07-06 03:56:22,071][gdb3][DEBUG ]         "rank": 0
[2017-07-06 03:56:22,071][gdb3][DEBUG ]       }
[2017-07-06 03:56:22,071][gdb3][DEBUG ]     ]
[2017-07-06 03:56:22,071][gdb3][DEBUG ]   }, 
[2017-07-06 03:56:22,072][gdb3][DEBUG ]   "name": "gdb3", 
[2017-07-06 03:56:22,072][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-07-06 03:56:22,072][gdb3][DEBUG ]   "quorum": [
[2017-07-06 03:56:22,072][gdb3][DEBUG ]     0
[2017-07-06 03:56:22,072][gdb3][DEBUG ]   ], 
[2017-07-06 03:56:22,072][gdb3][DEBUG ]   "rank": 0, 
[2017-07-06 03:56:22,072][gdb3][DEBUG ]   "state": "leader", 
[2017-07-06 03:56:22,072][gdb3][DEBUG ]   "sync_provider": []
[2017-07-06 03:56:22,072][gdb3][DEBUG ] }
[2017-07-06 03:56:22,072][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 03:56:22,072][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-07-06 03:56:22,073][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:56:22,138][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-07-06 03:56:22,157][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:56:22,171][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:56:22,171][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:56:22,188][gdb3][DEBUG ] detect machine type
[2017-07-06 03:56:22,190][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:56:22,191][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:56:22,256][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-07-06 03:56:22,257][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-07-06 03:56:22,257][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-07-06 03:56:22,259][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmp1LOi32
[2017-07-06 03:56:22,274][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:56:22,287][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:56:22,288][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:56:22,304][gdb3][DEBUG ] detect machine type
[2017-07-06 03:56:22,306][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:56:22,307][gdb3][DEBUG ] fetch remote file
[2017-07-06 03:56:22,308][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:56:22,374][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-07-06 03:56:22,540][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-07-06 03:56:22,706][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-07-06 03:56:22,872][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-07-06 03:56:23,039][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-07-06 03:56:23,205][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-07-06 03:56:23,371][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-07-06 03:56:23,538][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-07-06 03:56:23,704][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-07-06 03:56:23,870][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-07-06 03:56:24,085][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-07-06 03:56:24,085][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-07-06 03:56:24,086][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170706035624'
[2017-07-06 03:56:24,086][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-07-06 03:56:24,086][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-07-06 03:56:24,086][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-07-06 03:56:24,086][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp1LOi32
[2017-07-06 03:56:24,265][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff23c2675a8>
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7ff23cb7e938>
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:56:24,266][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:56:24,266][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 03:56:24,292][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:56:24,306][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:56:24,307][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:56:24,323][gdb3][DEBUG ] detect machine type
[2017-07-06 03:56:24,325][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:56:35,415][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:56:35,415][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-07-06 03:56:35,415][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:56:35,415][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:56:35,415][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:56:35,415][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 03:56:35,415][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:56:35,415][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9737ccf5a8>
[2017-07-06 03:56:35,415][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:56:35,415][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-07-06 03:56:35,416][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f97385e6938>
[2017-07-06 03:56:35,416][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:56:35,416][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:56:35,416][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-07-06 03:56:35,654][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:56:35,885][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:56:35,885][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:56:35,902][gdb1][DEBUG ] detect machine type
[2017-07-06 03:56:35,906][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:56:36,075][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:56:36,075][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-07-06 03:56:36,075][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:56:36,075][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:56:36,075][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 03:56:36,075][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fee5e74d998>
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fee5e9a3aa0>
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:56:36,076][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-07-06 03:56:36,077][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 03:56:36,314][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:56:36,549][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:56:36,549][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:56:36,566][gdb1][DEBUG ] detect machine type
[2017-07-06 03:56:36,570][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:56:36,571][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:56:36,571][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-07-06 03:56:36,571][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:56:36,574][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-07-06 03:56:36,574][gdb1][DEBUG ] create a keyring file
[2017-07-06 03:56:36,576][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-07-06 03:56:36,576][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:56:36,578][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-07-06 03:56:36,749][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:56:36,749][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:56:36,749][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:56:36,749][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:56:36,757][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 03:56:36,772][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/buddystore
[2017-07-06 03:56:36,788][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/ceph_fsid.11639.tmp
[2017-07-06 03:56:36,788][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/fsid.11639.tmp
[2017-07-06 03:56:36,791][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/magic.11639.tmp
[2017-07-06 03:56:41,812][gdb1][INFO  ] checking OSD status...
[2017-07-06 03:56:41,812][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:56:41,815][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 03:56:41,980][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-07-06 03:56:42,145][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:56:42,145][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff4355c9998>
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff43581faa0>
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:56:42,146][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 03:56:42,147][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 03:56:42,385][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:56:42,613][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:56:42,613][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:56:42,630][gdb1][DEBUG ] detect machine type
[2017-07-06 03:56:42,634][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:56:42,635][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:56:42,635][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-07-06 03:56:42,635][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-07-06 03:56:42,635][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:56:42,637][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-07-06 03:56:42,808][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-07-06 03:56:42,808][gdb1][WARNING] activate: Cluster uuid is 8113d479-744e-47dc-977e-d630deb1cc30
[2017-07-06 03:56:42,808][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:56:42,808][gdb1][WARNING] activate: Cluster name is ceph
[2017-07-06 03:56:42,808][gdb1][WARNING] activate: OSD uuid is d07a7cf0-1f80-439f-bcbb-519ae7608bea
[2017-07-06 03:56:42,808][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-07-06 03:56:42,808][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise d07a7cf0-1f80-439f-bcbb-519ae7608bea
[2017-07-06 03:56:42,973][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/whoami.11756.tmp
[2017-07-06 03:56:42,973][gdb1][WARNING] activate: OSD id is 0
[2017-07-06 03:56:42,973][gdb1][WARNING] activate: Initializing OSD...
[2017-07-06 03:56:42,973][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/buddystore/activate.monmap
[2017-07-06 03:56:43,087][gdb1][WARNING] got monmap epoch 2
[2017-07-06 03:56:43,091][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/buddystore/activate.monmap --osd-data /mnt/buddystore --osd-journal /mnt/buddystore/journal --osd-uuid d07a7cf0-1f80-439f-bcbb-519ae7608bea --keyring /mnt/buddystore/keyring --setuser ceph --setgroup ceph
[2017-07-06 03:56:43,122][gdb1][WARNING] activate: Marking with init system systemd
[2017-07-06 03:56:43,122][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/systemd
[2017-07-06 03:56:43,126][gdb1][WARNING] activate: Authorizing OSD key...
[2017-07-06 03:56:43,126][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/buddystore/keyring osd allow * mon allow profile osd
[2017-07-06 03:56:43,290][gdb1][WARNING] added key for osd.0
[2017-07-06 03:56:43,290][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/active.11756.tmp
[2017-07-06 03:56:43,290][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/buddystore
[2017-07-06 03:56:43,290][gdb1][WARNING] Traceback (most recent call last):
[2017-07-06 03:56:43,290][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-07-06 03:56:43,291][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-07-06 03:56:43,291][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-07-06 03:56:43,291][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-07-06 03:56:43,291][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-07-06 03:56:43,291][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3543, in activate_dir
[2017-07-06 03:56:43,291][gdb1][WARNING] OSError: [Errno 22] Invalid argument: '/var/lib/ceph/osd/ceph-0'
[2017-07-06 03:56:43,291][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-07-06 03:56:43,291][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore

[2017-07-06 03:57:37,244][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:57:37,244][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-07-06 03:57:37,244][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:57:37,244][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:57:37,244][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:57:37,244][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 03:57:37,244][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:57:37,244][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fad0d7525a8>
[2017-07-06 03:57:37,244][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:57:37,244][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-07-06 03:57:37,244][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fad0e069938>
[2017-07-06 03:57:37,245][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:57:37,245][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:57:37,245][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-07-06 03:57:37,486][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:57:37,721][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:57:37,721][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:57:37,738][gdb1][DEBUG ] detect machine type
[2017-07-06 03:57:37,742][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:57:37,912][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:57:37,912][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-07-06 03:57:37,912][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:57:37,912][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f70d10a6998>
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:57:37,913][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 03:57:37,914][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f70d12fcaa0>
[2017-07-06 03:57:37,914][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:57:37,914][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:57:37,914][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-07-06 03:57:37,914][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 03:57:38,162][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:57:38,393][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:57:38,393][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:57:38,410][gdb1][DEBUG ] detect machine type
[2017-07-06 03:57:38,414][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:57:38,415][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:57:38,415][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-07-06 03:57:38,416][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:57:38,418][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-07-06 03:57:38,418][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:57:38,420][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-07-06 03:57:38,591][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:57:38,591][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:57:38,591][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:57:38,591][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:57:38,595][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 03:57:38,610][gdb1][WARNING] populate_data_path: Data dir /mnt/buddystore already exists
[2017-07-06 03:57:43,619][gdb1][INFO  ] checking OSD status...
[2017-07-06 03:57:43,619][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:57:43,622][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 03:57:43,837][gdb1][WARNING] there is 1 OSD down
[2017-07-06 03:57:43,838][gdb1][WARNING] there is 1 OSD out
[2017-07-06 03:57:43,838][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-07-06 03:57:44,004][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:57:44,005][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-07-06 03:57:44,005][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:57:44,005][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:57:44,005][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:57:44,005][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:57:44,005][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-07-06 03:57:44,005][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:57:44,005][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc766602998>
[2017-07-06 03:57:44,005][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:57:44,005][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc766858aa0>
[2017-07-06 03:57:44,006][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:57:44,006][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:57:44,006][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 03:57:44,006][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 03:57:44,246][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:57:44,481][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:57:44,481][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:57:44,498][gdb1][DEBUG ] detect machine type
[2017-07-06 03:57:44,501][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:57:44,502][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:57:44,502][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-07-06 03:57:44,502][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-07-06 03:57:44,503][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:57:44,504][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-07-06 03:57:44,675][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-07-06 03:57:44,675][gdb1][WARNING] activate: Cluster uuid is 8113d479-744e-47dc-977e-d630deb1cc30
[2017-07-06 03:57:44,675][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:57:44,675][gdb1][WARNING] activate: Cluster name is ceph
[2017-07-06 03:57:44,675][gdb1][WARNING] activate: OSD uuid is d07a7cf0-1f80-439f-bcbb-519ae7608bea
[2017-07-06 03:57:44,676][gdb1][WARNING] activate: OSD id is 0
[2017-07-06 03:57:44,676][gdb1][WARNING] activate: Marking with init system systemd
[2017-07-06 03:57:44,676][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/systemd
[2017-07-06 03:57:44,676][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/buddystore
[2017-07-06 03:57:44,676][gdb1][WARNING] Traceback (most recent call last):
[2017-07-06 03:57:44,676][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-07-06 03:57:44,676][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-07-06 03:57:44,676][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-07-06 03:57:44,676][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-07-06 03:57:44,676][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-07-06 03:57:44,676][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3543, in activate_dir
[2017-07-06 03:57:44,676][gdb1][WARNING] OSError: [Errno 22] Invalid argument: '/var/lib/ceph/osd/ceph-0'
[2017-07-06 03:57:44,677][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-07-06 03:57:44,677][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore

[2017-07-06 03:59:47,288][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:59:47,288][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-07-06 03:59:47,288][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:59:47,288][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:59:47,289][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:59:47,289][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 03:59:47,289][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:59:47,289][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff98b61b5a8>
[2017-07-06 03:59:47,289][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:59:47,289][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-07-06 03:59:47,289][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7ff98bf32938>
[2017-07-06 03:59:47,289][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:59:47,289][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:59:47,289][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-07-06 03:59:47,529][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:59:47,757][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:59:47,758][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:59:47,774][gdb1][DEBUG ] detect machine type
[2017-07-06 03:59:47,778][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:59:47,948][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/buddystore
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 03:59:47,949][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:59:47,950][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe950f7f998>
[2017-07-06 03:59:47,950][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:59:47,950][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 03:59:47,950][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe9511d5aa0>
[2017-07-06 03:59:47,950][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:59:47,950][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:59:47,950][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-07-06 03:59:47,950][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 03:59:48,181][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:59:48,413][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:59:48,413][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:59:48,430][gdb1][DEBUG ] detect machine type
[2017-07-06 03:59:48,434][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:59:48,434][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:59:48,435][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-07-06 03:59:48,435][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:59:48,437][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-07-06 03:59:48,437][gdb1][DEBUG ] create a keyring file
[2017-07-06 03:59:48,439][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/buddystore journal None activate False
[2017-07-06 03:59:48,439][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:59:48,441][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/buddystore
[2017-07-06 03:59:48,612][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:59:48,612][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:59:48,612][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:59:48,612][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:59:48,612][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 03:59:48,628][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/buddystore
[2017-07-06 03:59:48,628][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/ceph_fsid.12562.tmp
[2017-07-06 03:59:48,628][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/fsid.12562.tmp
[2017-07-06 03:59:48,636][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/magic.12562.tmp
[2017-07-06 03:59:53,656][gdb1][INFO  ] checking OSD status...
[2017-07-06 03:59:53,657][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:59:53,659][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 03:59:53,874][gdb1][WARNING] there is 1 OSD down
[2017-07-06 03:59:53,875][gdb1][WARNING] there is 1 OSD out
[2017-07-06 03:59:53,875][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-07-06 03:59:54,038][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:59:54,038][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/buddystore
[2017-07-06 03:59:54,038][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:59:54,038][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:59:54,039][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:59:54,039][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:59:54,039][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-07-06 03:59:54,039][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:59:54,039][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8a73196998>
[2017-07-06 03:59:54,039][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:59:54,039][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8a733ecaa0>
[2017-07-06 03:59:54,039][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:59:54,039][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:59:54,039][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/buddystore', None)]
[2017-07-06 03:59:54,039][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/buddystore:
[2017-07-06 03:59:54,286][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:59:54,521][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:59:54,522][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:59:54,539][gdb1][DEBUG ] detect machine type
[2017-07-06 03:59:54,543][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:59:54,543][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:59:54,543][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/buddystore
[2017-07-06 03:59:54,544][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-07-06 03:59:54,544][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:59:54,546][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/buddystore
[2017-07-06 03:59:54,716][gdb1][WARNING] main_activate: path = /mnt/buddystore
[2017-07-06 03:59:54,716][gdb1][WARNING] activate: Cluster uuid is 8113d479-744e-47dc-977e-d630deb1cc30
[2017-07-06 03:59:54,716][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:59:54,717][gdb1][WARNING] activate: Cluster name is ceph
[2017-07-06 03:59:54,717][gdb1][WARNING] activate: OSD uuid is 5f419c89-3e77-48e4-97d2-5be91d944f55
[2017-07-06 03:59:54,717][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-07-06 03:59:54,717][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 5f419c89-3e77-48e4-97d2-5be91d944f55
[2017-07-06 03:59:54,981][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/whoami.12679.tmp
[2017-07-06 03:59:54,981][gdb1][WARNING] activate: OSD id is 1
[2017-07-06 03:59:54,982][gdb1][WARNING] activate: Initializing OSD...
[2017-07-06 03:59:54,982][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/buddystore/activate.monmap
[2017-07-06 03:59:55,096][gdb1][WARNING] got monmap epoch 2
[2017-07-06 03:59:55,096][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 1 --monmap /mnt/buddystore/activate.monmap --osd-data /mnt/buddystore --osd-journal /mnt/buddystore/journal --osd-uuid 5f419c89-3e77-48e4-97d2-5be91d944f55 --keyring /mnt/buddystore/keyring --setuser ceph --setgroup ceph
[2017-07-06 03:59:55,128][gdb1][WARNING] activate: Marking with init system systemd
[2017-07-06 03:59:55,128][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/systemd
[2017-07-06 03:59:55,128][gdb1][WARNING] activate: Authorizing OSD key...
[2017-07-06 03:59:55,128][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.1 -i /mnt/buddystore/keyring osd allow * mon allow profile osd
[2017-07-06 03:59:55,292][gdb1][WARNING] added key for osd.1
[2017-07-06 03:59:55,292][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/buddystore/active.12679.tmp
[2017-07-06 03:59:55,293][gdb1][WARNING] activate: ceph osd.1 data dir is ready at /mnt/buddystore
[2017-07-06 03:59:55,293][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-1 -> /mnt/buddystore
[2017-07-06 03:59:55,293][gdb1][WARNING] start_daemon: Starting ceph osd.1...
[2017-07-06 03:59:55,293][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1
[2017-07-06 03:59:55,293][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2017-07-06 03:59:55,357][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1 --runtime
[2017-07-06 03:59:55,421][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@1
[2017-07-06 03:59:55,421][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /lib/systemd/system/ceph-osd@.service.
[2017-07-06 03:59:55,452][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@1
[2017-07-06 04:00:00,522][gdb1][INFO  ] checking OSD status...
[2017-07-06 04:00:00,522][gdb1][DEBUG ] find the location of an executable
[2017-07-06 04:00:00,524][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 04:00:00,689][gdb1][WARNING] there is 1 OSD down
[2017-07-06 04:00:00,690][gdb1][WARNING] there is 1 OSD out
[2017-07-06 04:00:00,692][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 04:00:45,185][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 04:00:45,185][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy rgw create gdb1
[2017-07-06 04:00:45,185][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 04:00:45,185][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 04:00:45,185][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 04:00:45,185][ceph_deploy.cli][INFO  ]  rgw                           : [('gdb1', 'rgw.gdb1')]
[2017-07-06 04:00:45,185][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 04:00:45,185][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-07-06 04:00:45,185][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 04:00:45,186][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f52d2028830>
[2017-07-06 04:00:45,186][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 04:00:45,186][ceph_deploy.cli][INFO  ]  func                          : <function rgw at 0x7f52d26e9758>
[2017-07-06 04:00:45,186][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 04:00:45,186][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 04:00:45,186][ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts gdb1:rgw.gdb1
[2017-07-06 04:00:45,434][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 04:00:45,629][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 04:00:45,629][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 04:00:45,645][gdb1][DEBUG ] detect machine type
[2017-07-06 04:00:45,649][ceph_deploy.rgw][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 04:00:45,649][ceph_deploy.rgw][DEBUG ] remote host will use systemd
[2017-07-06 04:00:45,650][ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to gdb1
[2017-07-06 04:00:45,650][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 04:00:45,653][gdb1][WARNING] rgw keyring does not exist yet, creating one
[2017-07-06 04:00:45,653][gdb1][DEBUG ] create a keyring file
[2017-07-06 04:00:45,655][gdb1][DEBUG ] create path recursively if it doesn't exist
[2017-07-06 04:00:45,657][gdb1][INFO  ] Running command: sudo ceph --cluster ceph --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create client.rgw.gdb1 osd allow rwx mon allow rw -o /var/lib/ceph/radosgw/ceph-rgw.gdb1/keyring
[2017-07-06 04:00:45,828][gdb1][INFO  ] Running command: sudo systemctl enable ceph-radosgw@rgw.gdb1
[2017-07-06 04:00:45,839][gdb1][WARNING] Failed to execute operation: No such file or directory
[2017-07-06 04:00:45,840][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-07-06 04:00:45,840][ceph_deploy.rgw][ERROR ] Failed to execute command: systemctl enable ceph-radosgw@rgw.gdb1
[2017-07-06 04:00:45,840][ceph_deploy][ERROR ] GenericError: Failed to create 1 RGWs

