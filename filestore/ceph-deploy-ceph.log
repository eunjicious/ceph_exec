[2017-04-26 20:27:01,422][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:27:01,422][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f47c8ae1560>
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f47c9165758>
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-04-26 20:27:01,423][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:27:01,424][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-04-26 20:27:01,424][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-04-26 20:27:01,424][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-04-26 20:27:01,462][gdb3][DEBUG ] connection detected need for sudo
[2017-04-26 20:27:01,476][gdb3][DEBUG ] connected to host: gdb3 
[2017-04-26 20:27:01,476][gdb3][DEBUG ] detect platform information from remote host
[2017-04-26 20:27:01,493][gdb3][DEBUG ] detect machine type
[2017-04-26 20:27:01,495][gdb3][DEBUG ] find the location of an executable
[2017-04-26 20:27:01,496][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-04-26 20:27:01,509][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-04-26 20:27:01,513][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-04-26 20:27:01,514][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-04-26 20:27:01,514][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-04-26 20:27:01,514][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-04-26 20:27:01,514][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-04-26 20:27:01,514][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-04-26 20:27:01,514][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-04-26 20:27:01,514][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-04-26 20:27:25,941][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:27:25,941][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-04-26 20:27:25,941][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:27:25,941][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:27:25,941][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:27:25,941][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-04-26 20:27:25,941][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-04-26 20:27:25,942][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:27:25,942][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f855e6e9e60>
[2017-04-26 20:27:25,942][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:27:25,942][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f855e6beb18>
[2017-04-26 20:27:25,942][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:27:25,942][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-04-26 20:27:25,942][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:27:25,943][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-04-26 20:27:25,943][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-04-26 20:27:25,969][gdb3][DEBUG ] connection detected need for sudo
[2017-04-26 20:27:25,982][gdb3][DEBUG ] connected to host: gdb3 
[2017-04-26 20:27:25,983][gdb3][DEBUG ] detect platform information from remote host
[2017-04-26 20:27:25,999][gdb3][DEBUG ] detect machine type
[2017-04-26 20:27:26,002][gdb3][DEBUG ] find the location of an executable
[2017-04-26 20:27:26,002][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-04-26 20:27:26,002][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-04-26 20:27:26,002][gdb3][DEBUG ] get remote short hostname
[2017-04-26 20:27:26,003][gdb3][DEBUG ] deploying mon to gdb3
[2017-04-26 20:27:26,003][gdb3][DEBUG ] get remote short hostname
[2017-04-26 20:27:26,003][gdb3][DEBUG ] remote hostname: gdb3
[2017-04-26 20:27:26,004][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:27:26,005][gdb3][DEBUG ] create the mon path if it does not exist
[2017-04-26 20:27:26,005][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-04-26 20:27:26,006][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-04-26 20:27:26,006][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-04-26 20:27:26,006][gdb3][DEBUG ] create the monitor keyring file
[2017-04-26 20:27:26,007][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-04-26 20:27:26,045][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-04-26 20:27:26,045][gdb3][DEBUG ] ceph-mon: set fsid to 5f73f0a8-714e-4c1f-8410-722fc431c29c
[2017-04-26 20:27:26,048][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-04-26 20:27:26,052][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-04-26 20:27:26,053][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-04-26 20:27:26,053][gdb3][DEBUG ] create the init path if it does not exist
[2017-04-26 20:27:26,054][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-04-26 20:27:26,072][gdb3][WARNING] Created symlink from /etc/systemd/system/multi-user.target.wants/ceph.target to /lib/systemd/system/ceph.target.
[2017-04-26 20:27:26,141][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-04-26 20:27:26,151][gdb3][WARNING] Created symlink from /etc/systemd/system/ceph-mon.target.wants/ceph-mon@gdb3.service to /lib/systemd/system/ceph-mon@.service.
[2017-04-26 20:27:26,216][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-04-26 20:27:28,254][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-04-26 20:27:28,319][gdb3][DEBUG ] ********************************************************************************
[2017-04-26 20:27:28,319][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-04-26 20:27:28,319][gdb3][DEBUG ] {
[2017-04-26 20:27:28,320][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-04-26 20:27:28,320][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-04-26 20:27:28,320][gdb3][DEBUG ]   "features": {
[2017-04-26 20:27:28,320][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-04-26 20:27:28,320][gdb3][DEBUG ]     "quorum_mon": [
[2017-04-26 20:27:28,320][gdb3][DEBUG ]       "kraken", 
[2017-04-26 20:27:28,320][gdb3][DEBUG ]       "luminous"
[2017-04-26 20:27:28,320][gdb3][DEBUG ]     ], 
[2017-04-26 20:27:28,320][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-04-26 20:27:28,320][gdb3][DEBUG ]     "required_mon": [
[2017-04-26 20:27:28,320][gdb3][DEBUG ]       "kraken", 
[2017-04-26 20:27:28,320][gdb3][DEBUG ]       "luminous"
[2017-04-26 20:27:28,320][gdb3][DEBUG ]     ]
[2017-04-26 20:27:28,320][gdb3][DEBUG ]   }, 
[2017-04-26 20:27:28,321][gdb3][DEBUG ]   "monmap": {
[2017-04-26 20:27:28,321][gdb3][DEBUG ]     "created": "2017-04-26 20:27:26.031008", 
[2017-04-26 20:27:28,321][gdb3][DEBUG ]     "epoch": 2, 
[2017-04-26 20:27:28,321][gdb3][DEBUG ]     "features": {
[2017-04-26 20:27:28,321][gdb3][DEBUG ]       "optional": [], 
[2017-04-26 20:27:28,321][gdb3][DEBUG ]       "persistent": [
[2017-04-26 20:27:28,321][gdb3][DEBUG ]         "kraken", 
[2017-04-26 20:27:28,321][gdb3][DEBUG ]         "luminous"
[2017-04-26 20:27:28,321][gdb3][DEBUG ]       ]
[2017-04-26 20:27:28,321][gdb3][DEBUG ]     }, 
[2017-04-26 20:27:28,321][gdb3][DEBUG ]     "fsid": "5f73f0a8-714e-4c1f-8410-722fc431c29c", 
[2017-04-26 20:27:28,321][gdb3][DEBUG ]     "modified": "2017-04-26 20:27:26.318310", 
[2017-04-26 20:27:28,321][gdb3][DEBUG ]     "mons": [
[2017-04-26 20:27:28,321][gdb3][DEBUG ]       {
[2017-04-26 20:27:28,321][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-04-26 20:27:28,321][gdb3][DEBUG ]         "name": "gdb3", 
[2017-04-26 20:27:28,321][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-04-26 20:27:28,322][gdb3][DEBUG ]         "rank": 0
[2017-04-26 20:27:28,322][gdb3][DEBUG ]       }
[2017-04-26 20:27:28,322][gdb3][DEBUG ]     ]
[2017-04-26 20:27:28,322][gdb3][DEBUG ]   }, 
[2017-04-26 20:27:28,322][gdb3][DEBUG ]   "name": "gdb3", 
[2017-04-26 20:27:28,322][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-04-26 20:27:28,322][gdb3][DEBUG ]   "quorum": [
[2017-04-26 20:27:28,322][gdb3][DEBUG ]     0
[2017-04-26 20:27:28,322][gdb3][DEBUG ]   ], 
[2017-04-26 20:27:28,322][gdb3][DEBUG ]   "rank": 0, 
[2017-04-26 20:27:28,322][gdb3][DEBUG ]   "state": "leader", 
[2017-04-26 20:27:28,322][gdb3][DEBUG ]   "sync_provider": []
[2017-04-26 20:27:28,322][gdb3][DEBUG ] }
[2017-04-26 20:27:28,322][gdb3][DEBUG ] ********************************************************************************
[2017-04-26 20:27:28,322][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-04-26 20:27:28,323][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-04-26 20:27:28,388][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-04-26 20:27:28,403][gdb3][DEBUG ] connection detected need for sudo
[2017-04-26 20:27:28,417][gdb3][DEBUG ] connected to host: gdb3 
[2017-04-26 20:27:28,418][gdb3][DEBUG ] detect platform information from remote host
[2017-04-26 20:27:28,435][gdb3][DEBUG ] detect machine type
[2017-04-26 20:27:28,437][gdb3][DEBUG ] find the location of an executable
[2017-04-26 20:27:28,438][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-04-26 20:27:28,503][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-04-26 20:27:28,504][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-04-26 20:27:28,504][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-04-26 20:27:28,506][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmp3RI7CF
[2017-04-26 20:27:28,521][gdb3][DEBUG ] connection detected need for sudo
[2017-04-26 20:27:28,535][gdb3][DEBUG ] connected to host: gdb3 
[2017-04-26 20:27:28,535][gdb3][DEBUG ] detect platform information from remote host
[2017-04-26 20:27:28,552][gdb3][DEBUG ] detect machine type
[2017-04-26 20:27:28,554][gdb3][DEBUG ] get remote short hostname
[2017-04-26 20:27:28,554][gdb3][DEBUG ] fetch remote file
[2017-04-26 20:27:28,556][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-04-26 20:27:28,622][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-04-26 20:27:28,788][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-04-26 20:27:28,954][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-04-26 20:27:29,120][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-04-26 20:27:29,286][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-04-26 20:27:29,452][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-04-26 20:27:29,618][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-04-26 20:27:29,785][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-04-26 20:27:29,951][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-04-26 20:27:30,117][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-04-26 20:27:30,282][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-04-26 20:27:30,283][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-04-26 20:27:30,283][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring
[2017-04-26 20:27:30,283][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-04-26 20:27:30,283][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-04-26 20:27:30,283][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-04-26 20:27:30,283][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp3RI7CF
[2017-04-26 20:28:10,254][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:28:10,254][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd create --zap-disk gdb0:/dev/xvdb
[2017-04-26 20:28:10,254][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:28:10,254][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:28:10,254][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:28:10,254][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fada6de4908>
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fada703aaa0>
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:28:10,255][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:28:10,256][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-04-26 20:28:11,905][gdb0][DEBUG ] connection detected need for sudo
[2017-04-26 20:28:12,129][gdb0][DEBUG ] connected to host: gdb0 
[2017-04-26 20:28:12,129][gdb0][DEBUG ] detect platform information from remote host
[2017-04-26 20:28:12,146][gdb0][DEBUG ] detect machine type
[2017-04-26 20:28:12,149][gdb0][DEBUG ] find the location of an executable
[2017-04-26 20:28:12,150][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:28:12,150][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-04-26 20:28:12,151][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:28:12,152][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-04-26 20:28:12,153][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-04-26 20:28:24,615][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:28:24,615][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-04-26 20:28:24,615][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:28:24,615][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:28:24,615][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9c21767908>
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9c219bdaa0>
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:28:24,616][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:28:24,617][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-04-26 20:28:24,853][gdb0][DEBUG ] connection detected need for sudo
[2017-04-26 20:28:25,077][gdb0][DEBUG ] connected to host: gdb0 
[2017-04-26 20:28:25,077][gdb0][DEBUG ] detect platform information from remote host
[2017-04-26 20:28:25,093][gdb0][DEBUG ] detect machine type
[2017-04-26 20:28:25,097][gdb0][DEBUG ] find the location of an executable
[2017-04-26 20:28:25,098][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:28:25,098][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-04-26 20:28:25,098][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:28:25,101][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-04-26 20:28:25,101][gdb0][DEBUG ] create a keyring file
[2017-04-26 20:28:25,103][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-04-26 20:28:25,103][gdb0][DEBUG ] find the location of an executable
[2017-04-26 20:28:25,105][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:28:25,226][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:28:25,233][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:28:25,249][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:28:25,265][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:28:25,272][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:25,273][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:28:25,273][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:28:25,288][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:25,289][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:25,289][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:25,289][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-04-26 20:28:25,292][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-04-26 20:28:25,300][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-04-26 20:28:25,307][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-04-26 20:28:25,323][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:25,323][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-04-26 20:28:25,323][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:25,324][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-04-26 20:28:25,324][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-04-26 20:28:26,341][gdb0][DEBUG ] Creating new GPT entries.
[2017-04-26 20:28:26,341][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-04-26 20:28:26,341][gdb0][DEBUG ] other utilities.
[2017-04-26 20:28:26,342][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-04-26 20:28:27,359][gdb0][DEBUG ] Creating new GPT entries.
[2017-04-26 20:28:27,360][gdb0][DEBUG ] The operation has completed successfully.
[2017-04-26 20:28:27,360][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-04-26 20:28:27,360][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:28:27,367][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:28:27,399][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:28:27,407][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:27,407][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:27,407][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-04-26 20:28:27,407][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:27,407][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-04-26 20:28:27,407][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:a6ec59e8-9251-40ad-98b2-d52a052847e7 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-04-26 20:28:28,424][gdb0][DEBUG ] Setting name!
[2017-04-26 20:28:28,424][gdb0][DEBUG ] partNum is 1
[2017-04-26 20:28:28,425][gdb0][DEBUG ] REALLY setting name!
[2017-04-26 20:28:28,425][gdb0][DEBUG ] The operation has completed successfully.
[2017-04-26 20:28:28,425][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:28:28,425][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:28:28,639][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:28:28,754][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:28:28,968][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:28,968][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:28,969][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:28:28,969][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/a6ec59e8-9251-40ad-98b2-d52a052847e7
[2017-04-26 20:28:28,969][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-04-26 20:28:29,986][gdb0][DEBUG ] The operation has completed successfully.
[2017-04-26 20:28:29,986][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:28:29,986][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:28:30,150][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:28:30,315][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:28:30,529][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/a6ec59e8-9251-40ad-98b2-d52a052847e7
[2017-04-26 20:28:30,530][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:30,530][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-04-26 20:28:30,530][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:30,530][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-04-26 20:28:30,530][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:30,530][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-04-26 20:28:30,530][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:66650c9e-bf9e-42e8-b88c-c02eba037968 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-04-26 20:28:31,547][gdb0][DEBUG ] Setting name!
[2017-04-26 20:28:31,548][gdb0][DEBUG ] partNum is 0
[2017-04-26 20:28:31,548][gdb0][DEBUG ] REALLY setting name!
[2017-04-26 20:28:31,548][gdb0][DEBUG ] The operation has completed successfully.
[2017-04-26 20:28:31,548][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:28:31,548][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:28:31,712][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:28:31,927][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:28:32,141][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:32,142][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:32,142][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:28:32,142][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-04-26 20:28:32,142][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-04-26 20:28:32,808][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-04-26 20:28:32,808][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-04-26 20:28:32,808][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-04-26 20:28:32,808][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-04-26 20:28:32,808][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-04-26 20:28:32,808][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-04-26 20:28:32,808][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-04-26 20:28:32,809][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-04-26 20:28:32,809][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-04-26 20:28:32,809][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.II7btw with options noatime,inode64
[2017-04-26 20:28:32,809][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.II7btw
[2017-04-26 20:28:32,810][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.II7btw
[2017-04-26 20:28:32,813][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.II7btw/ceph_fsid.8001.tmp
[2017-04-26 20:28:32,815][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.II7btw/fsid.8001.tmp
[2017-04-26 20:28:32,818][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.II7btw/magic.8001.tmp
[2017-04-26 20:28:32,822][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.II7btw/journal_uuid.8001.tmp
[2017-04-26 20:28:32,822][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.II7btw/journal -> /dev/disk/by-partuuid/a6ec59e8-9251-40ad-98b2-d52a052847e7
[2017-04-26 20:28:32,822][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.II7btw
[2017-04-26 20:28:32,825][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.II7btw
[2017-04-26 20:28:32,825][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.II7btw
[2017-04-26 20:28:32,857][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:28:32,857][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-04-26 20:28:33,874][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-04-26 20:28:33,874][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-04-26 20:28:33,874][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-04-26 20:28:33,875][gdb0][DEBUG ] The operation has completed successfully.
[2017-04-26 20:28:33,875][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:28:33,875][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:28:33,875][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:28:34,039][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:28:34,071][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-04-26 20:28:34,105][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-04-26 20:28:39,227][gdb0][INFO  ] checking OSD status...
[2017-04-26 20:28:39,227][gdb0][DEBUG ] find the location of an executable
[2017-04-26 20:28:39,230][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-04-26 20:28:39,345][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-04-26 20:29:11,206][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:29:11,206][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-04-26 20:29:11,206][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:29:11,206][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:29:11,206][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:29:11,206][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-04-26 20:29:11,206][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:29:11,206][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:29:11,206][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe1ba173908>
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe1ba3c9aa0>
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:29:11,207][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:29:11,207][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-04-26 20:29:12,640][ceph_deploy.osd][ERROR ] connecting to host: gdb1 resulted in errors: HostNotFound gdb1
[2017-04-26 20:29:12,640][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-04-26 20:29:36,935][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:29:36,935][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb2:/dev/xvdb/
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  disk                          : [('gdb2', '/dev/xvdb/', None)]
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f772e54b908>
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:29:36,936][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f772e7a1aa0>
[2017-04-26 20:29:36,937][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:29:36,937][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:29:36,937][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:29:36,937][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb2:/dev/xvdb/:
[2017-04-26 20:29:40,089][gdb2][DEBUG ] connection detected need for sudo
[2017-04-26 20:29:40,317][gdb2][DEBUG ] connected to host: gdb2 
[2017-04-26 20:29:40,318][gdb2][DEBUG ] detect platform information from remote host
[2017-04-26 20:29:40,334][gdb2][DEBUG ] detect machine type
[2017-04-26 20:29:40,338][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:29:40,339][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:29:40,339][ceph_deploy.osd][DEBUG ] Deploying osd to gdb2
[2017-04-26 20:29:40,339][gdb2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:29:40,342][ceph_deploy.osd][DEBUG ] Preparing host gdb2 disk /dev/xvdb/ journal None activate True
[2017-04-26 20:29:40,342][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:29:40,344][gdb2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb/
[2017-04-26 20:29:40,464][gdb2][WARNING] Traceback (most recent call last):
[2017-04-26 20:29:40,464][gdb2][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-04-26 20:29:40,465][gdb2][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-04-26 20:29:40,465][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-04-26 20:29:40,465][gdb2][WARNING]     main(sys.argv[1:])
[2017-04-26 20:29:40,465][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-04-26 20:29:40,465][gdb2][WARNING]     args.func(args)
[2017-04-26 20:29:40,465][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-04-26 20:29:40,465][gdb2][WARNING]     Prepare.factory(args).prepare()
[2017-04-26 20:29:40,465][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-04-26 20:29:40,465][gdb2][WARNING]     return PrepareFilestore(args)
[2017-04-26 20:29:40,465][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-04-26 20:29:40,465][gdb2][WARNING]     self.data = PrepareFilestoreData(args)
[2017-04-26 20:29:40,465][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-04-26 20:29:40,465][gdb2][WARNING]     self.set_type()
[2017-04-26 20:29:40,466][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-04-26 20:29:40,466][gdb2][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-04-26 20:29:40,466][gdb2][WARNING] OSError: [Errno 20] Not a directory: '/dev/xvdb/'
[2017-04-26 20:29:40,466][gdb2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-04-26 20:29:40,466][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb/
[2017-04-26 20:29:40,466][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-04-26 20:30:02,195][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:30:02,195][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb2:/dev/xvdb/
[2017-04-26 20:30:02,195][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:30:02,195][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:30:02,195][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:30:02,195][ceph_deploy.cli][INFO  ]  disk                          : [('gdb2', '/dev/xvdb/', None)]
[2017-04-26 20:30:02,195][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:30:02,195][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:30:02,195][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6145df9908>
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f614604faa0>
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:30:02,196][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:30:02,196][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb2:/dev/xvdb/:
[2017-04-26 20:30:02,438][gdb2][DEBUG ] connection detected need for sudo
[2017-04-26 20:30:02,665][gdb2][DEBUG ] connected to host: gdb2 
[2017-04-26 20:30:02,665][gdb2][DEBUG ] detect platform information from remote host
[2017-04-26 20:30:02,681][gdb2][DEBUG ] detect machine type
[2017-04-26 20:30:02,685][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:30:02,686][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:30:02,686][ceph_deploy.osd][DEBUG ] Deploying osd to gdb2
[2017-04-26 20:30:02,686][gdb2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:30:02,690][ceph_deploy.osd][DEBUG ] Preparing host gdb2 disk /dev/xvdb/ journal None activate True
[2017-04-26 20:30:02,690][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:30:02,692][gdb2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb/
[2017-04-26 20:30:02,812][gdb2][WARNING] Traceback (most recent call last):
[2017-04-26 20:30:02,812][gdb2][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-04-26 20:30:02,812][gdb2][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-04-26 20:30:02,812][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-04-26 20:30:02,813][gdb2][WARNING]     main(sys.argv[1:])
[2017-04-26 20:30:02,813][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-04-26 20:30:02,813][gdb2][WARNING]     args.func(args)
[2017-04-26 20:30:02,813][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-04-26 20:30:02,813][gdb2][WARNING]     Prepare.factory(args).prepare()
[2017-04-26 20:30:02,813][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-04-26 20:30:02,813][gdb2][WARNING]     return PrepareFilestore(args)
[2017-04-26 20:30:02,813][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-04-26 20:30:02,813][gdb2][WARNING]     self.data = PrepareFilestoreData(args)
[2017-04-26 20:30:02,813][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-04-26 20:30:02,813][gdb2][WARNING]     self.set_type()
[2017-04-26 20:30:02,813][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-04-26 20:30:02,813][gdb2][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-04-26 20:30:02,814][gdb2][WARNING] OSError: [Errno 20] Not a directory: '/dev/xvdb/'
[2017-04-26 20:30:02,814][gdb2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-04-26 20:30:02,814][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb/
[2017-04-26 20:30:02,814][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-04-26 20:30:08,636][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb2:/dev/xvdb
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ]  disk                          : [('gdb2', '/dev/xvdb', None)]
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:30:08,637][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:30:08,638][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4a39ddc908>
[2017-04-26 20:30:08,638][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:30:08,638][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:30:08,638][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f4a3a032aa0>
[2017-04-26 20:30:08,638][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:30:08,638][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:30:08,638][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:30:08,638][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb2:/dev/xvdb:
[2017-04-26 20:30:08,877][gdb2][DEBUG ] connection detected need for sudo
[2017-04-26 20:30:09,109][gdb2][DEBUG ] connected to host: gdb2 
[2017-04-26 20:30:09,110][gdb2][DEBUG ] detect platform information from remote host
[2017-04-26 20:30:09,126][gdb2][DEBUG ] detect machine type
[2017-04-26 20:30:09,130][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:30:09,130][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:30:09,130][ceph_deploy.osd][DEBUG ] Deploying osd to gdb2
[2017-04-26 20:30:09,131][gdb2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:30:09,133][ceph_deploy.osd][DEBUG ] Preparing host gdb2 disk /dev/xvdb journal None activate True
[2017-04-26 20:30:09,133][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:30:09,136][gdb2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:30:09,256][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:30:09,260][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:30:09,275][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:30:09,291][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:30:09,298][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:09,299][gdb2][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:30:09,299][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:30:09,314][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:09,314][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:09,314][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:09,315][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-04-26 20:30:09,322][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-04-26 20:30:09,330][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-04-26 20:30:09,337][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-04-26 20:30:09,345][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:09,345][gdb2][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-04-26 20:30:09,345][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:09,345][gdb2][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-04-26 20:30:09,346][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-04-26 20:30:09,350][gdb2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-04-26 20:30:09,350][gdb2][WARNING] backup header from main header.
[2017-04-26 20:30:09,350][gdb2][WARNING] 
[2017-04-26 20:30:10,417][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:30:10,417][gdb2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-04-26 20:30:10,417][gdb2][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-04-26 20:30:10,417][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:30:10,417][gdb2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-04-26 20:30:10,418][gdb2][DEBUG ] other utilities.
[2017-04-26 20:30:10,418][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-04-26 20:30:11,435][gdb2][DEBUG ] Creating new GPT entries.
[2017-04-26 20:30:11,435][gdb2][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-04-26 20:30:11,435][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:30:11,435][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:30:11,435][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:30:11,435][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:30:11,443][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:11,443][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:11,443][gdb2][WARNING] ptype_tobe_for_name: name = journal
[2017-04-26 20:30:11,443][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:11,444][gdb2][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-04-26 20:30:11,444][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:4c99e31b-6dbe-4bea-a699-a36608d7740d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-04-26 20:30:12,511][gdb2][DEBUG ] Setting name!
[2017-04-26 20:30:12,511][gdb2][DEBUG ] partNum is 1
[2017-04-26 20:30:12,511][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:30:12,511][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:30:12,511][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:30:12,511][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:30:12,726][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:30:12,890][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:30:13,055][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:13,055][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:13,055][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:30:13,055][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/4c99e31b-6dbe-4bea-a699-a36608d7740d
[2017-04-26 20:30:13,055][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-04-26 20:30:14,072][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:30:14,073][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:30:14,073][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:30:14,287][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:30:14,401][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:30:14,465][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/4c99e31b-6dbe-4bea-a699-a36608d7740d
[2017-04-26 20:30:14,466][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:14,466][gdb2][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-04-26 20:30:14,466][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:14,466][gdb2][WARNING] ptype_tobe_for_name: name = data
[2017-04-26 20:30:14,466][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:14,466][gdb2][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-04-26 20:30:14,466][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:0e394a7a-11bd-45b7-80f4-34b566df618b --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-04-26 20:30:15,483][gdb2][DEBUG ] Setting name!
[2017-04-26 20:30:15,484][gdb2][DEBUG ] partNum is 0
[2017-04-26 20:30:15,484][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:30:15,484][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:30:15,484][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:30:15,484][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:30:15,698][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:30:15,813][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:30:15,877][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:15,877][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:15,877][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:30:15,877][gdb2][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-04-26 20:30:15,877][gdb2][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-04-26 20:30:16,543][gdb2][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-04-26 20:30:16,543][gdb2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-04-26 20:30:16,543][gdb2][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-04-26 20:30:16,543][gdb2][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-04-26 20:30:16,543][gdb2][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-04-26 20:30:16,544][gdb2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-04-26 20:30:16,544][gdb2][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-04-26 20:30:16,544][gdb2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-04-26 20:30:16,544][gdb2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-04-26 20:30:16,544][gdb2][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.3YDO2o with options noatime,inode64
[2017-04-26 20:30:16,544][gdb2][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.3YDO2o
[2017-04-26 20:30:16,576][gdb2][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.3YDO2o
[2017-04-26 20:30:16,576][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.3YDO2o/ceph_fsid.14451.tmp
[2017-04-26 20:30:16,576][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.3YDO2o/fsid.14451.tmp
[2017-04-26 20:30:16,576][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.3YDO2o/magic.14451.tmp
[2017-04-26 20:30:16,576][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.3YDO2o/journal_uuid.14451.tmp
[2017-04-26 20:30:16,576][gdb2][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.3YDO2o/journal -> /dev/disk/by-partuuid/4c99e31b-6dbe-4bea-a699-a36608d7740d
[2017-04-26 20:30:16,576][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.3YDO2o
[2017-04-26 20:30:16,576][gdb2][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.3YDO2o
[2017-04-26 20:30:16,577][gdb2][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.3YDO2o
[2017-04-26 20:30:16,608][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:30:16,608][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-04-26 20:30:17,625][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:30:17,626][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:30:17,626][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:30:17,840][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:30:18,055][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:30:18,269][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-04-26 20:30:18,303][gdb2][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-04-26 20:30:23,425][gdb2][INFO  ] checking OSD status...
[2017-04-26 20:30:23,426][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:30:23,429][gdb2][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-04-26 20:30:23,544][ceph_deploy.osd][DEBUG ] Host gdb2 is now ready for osd use.
[2017-04-26 20:31:05,840][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:31:05,840][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3 gdb0 gdb2
[2017-04-26 20:31:05,840][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:31:05,840][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:31:05,840][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:31:05,840][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-04-26 20:31:05,840][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:31:05,840][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feb0047d518>
[2017-04-26 20:31:05,840][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:31:05,840][ceph_deploy.cli][INFO  ]  client                        : ['gdb3', 'gdb0', 'gdb2']
[2017-04-26 20:31:05,840][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7feb00d94938>
[2017-04-26 20:31:05,841][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:31:05,841][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:31:05,841][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-04-26 20:31:05,866][gdb3][DEBUG ] connection detected need for sudo
[2017-04-26 20:31:05,880][gdb3][DEBUG ] connected to host: gdb3 
[2017-04-26 20:31:05,881][gdb3][DEBUG ] detect platform information from remote host
[2017-04-26 20:31:05,898][gdb3][DEBUG ] detect machine type
[2017-04-26 20:31:05,900][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:31:05,901][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-04-26 20:31:06,137][gdb0][DEBUG ] connection detected need for sudo
[2017-04-26 20:31:06,337][gdb0][DEBUG ] connected to host: gdb0 
[2017-04-26 20:31:06,338][gdb0][DEBUG ] detect platform information from remote host
[2017-04-26 20:31:06,353][gdb0][DEBUG ] detect machine type
[2017-04-26 20:31:06,357][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:31:06,360][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb2
[2017-04-26 20:31:06,586][gdb2][DEBUG ] connection detected need for sudo
[2017-04-26 20:31:06,781][gdb2][DEBUG ] connected to host: gdb2 
[2017-04-26 20:31:06,782][gdb2][DEBUG ] detect platform information from remote host
[2017-04-26 20:31:06,798][gdb2][DEBUG ] detect machine type
[2017-04-26 20:31:06,802][gdb2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:31:39,847][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:31:39,847][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb2:/dev/xvdb/
[2017-04-26 20:31:39,847][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:31:39,847][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:31:39,847][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:31:39,847][ceph_deploy.cli][INFO  ]  disk                          : [('gdb2', '/dev/xvdb/', None)]
[2017-04-26 20:31:39,847][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa1d4ac1908>
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa1d4d17aa0>
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:31:39,848][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:31:39,849][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb2:/dev/xvdb/:
[2017-04-26 20:31:40,090][gdb2][DEBUG ] connection detected need for sudo
[2017-04-26 20:31:40,281][gdb2][DEBUG ] connected to host: gdb2 
[2017-04-26 20:31:40,282][gdb2][DEBUG ] detect platform information from remote host
[2017-04-26 20:31:40,298][gdb2][DEBUG ] detect machine type
[2017-04-26 20:31:40,301][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:31:40,302][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:31:40,302][ceph_deploy.osd][DEBUG ] Deploying osd to gdb2
[2017-04-26 20:31:40,302][gdb2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:31:40,305][ceph_deploy.osd][DEBUG ] Preparing host gdb2 disk /dev/xvdb/ journal None activate True
[2017-04-26 20:31:40,305][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:31:40,307][gdb2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb/
[2017-04-26 20:31:40,427][gdb2][WARNING] Traceback (most recent call last):
[2017-04-26 20:31:40,428][gdb2][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-04-26 20:31:40,428][gdb2][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-04-26 20:31:40,428][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-04-26 20:31:40,428][gdb2][WARNING]     main(sys.argv[1:])
[2017-04-26 20:31:40,428][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-04-26 20:31:40,428][gdb2][WARNING]     args.func(args)
[2017-04-26 20:31:40,428][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-04-26 20:31:40,429][gdb2][WARNING]     Prepare.factory(args).prepare()
[2017-04-26 20:31:40,429][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-04-26 20:31:40,429][gdb2][WARNING]     return PrepareFilestore(args)
[2017-04-26 20:31:40,429][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-04-26 20:31:40,429][gdb2][WARNING]     self.data = PrepareFilestoreData(args)
[2017-04-26 20:31:40,429][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-04-26 20:31:40,429][gdb2][WARNING]     self.set_type()
[2017-04-26 20:31:40,429][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-04-26 20:31:40,429][gdb2][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-04-26 20:31:40,429][gdb2][WARNING] OSError: [Errno 20] Not a directory: '/dev/xvdb/'
[2017-04-26 20:31:40,429][gdb2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-04-26 20:31:40,430][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb/
[2017-04-26 20:31:40,430][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-04-26 20:31:42,723][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:31:42,723][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb2:/dev/xvdb
[2017-04-26 20:31:42,723][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  disk                          : [('gdb2', '/dev/xvdb', None)]
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1cc03b9908>
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:31:42,724][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1cc060faa0>
[2017-04-26 20:31:42,725][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:31:42,725][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:31:42,725][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:31:42,725][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb2:/dev/xvdb:
[2017-04-26 20:31:42,962][gdb2][DEBUG ] connection detected need for sudo
[2017-04-26 20:31:43,153][gdb2][DEBUG ] connected to host: gdb2 
[2017-04-26 20:31:43,153][gdb2][DEBUG ] detect platform information from remote host
[2017-04-26 20:31:43,169][gdb2][DEBUG ] detect machine type
[2017-04-26 20:31:43,173][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:31:43,174][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:31:43,174][ceph_deploy.osd][DEBUG ] Deploying osd to gdb2
[2017-04-26 20:31:43,174][gdb2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:31:43,176][ceph_deploy.osd][DEBUG ] Preparing host gdb2 disk /dev/xvdb journal None activate True
[2017-04-26 20:31:43,177][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:31:43,179][gdb2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:31:43,300][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:31:43,303][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:31:43,319][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:31:43,335][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:31:43,342][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:43,344][gdb2][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:31:43,344][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:31:43,359][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:43,360][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:43,360][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:43,360][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:31:43,360][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:31:43,360][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-04-26 20:31:43,363][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-04-26 20:31:43,371][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-04-26 20:31:43,379][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-04-26 20:31:43,394][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:43,395][gdb2][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-04-26 20:31:43,395][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:43,395][gdb2][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-04-26 20:31:43,411][gdb2][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-04-26 20:31:43,411][gdb2][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-04-26 20:31:43,426][gdb2][WARNING] 10+0 records in
[2017-04-26 20:31:43,427][gdb2][WARNING] 10+0 records out
[2017-04-26 20:31:43,427][gdb2][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00718685 s, 1.5 GB/s
[2017-04-26 20:31:43,427][gdb2][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-04-26 20:31:43,541][gdb2][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-04-26 20:31:43,573][gdb2][WARNING] 10+0 records in
[2017-04-26 20:31:43,573][gdb2][WARNING] 10+0 records out
[2017-04-26 20:31:43,573][gdb2][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.0415194 s, 253 MB/s
[2017-04-26 20:31:43,573][gdb2][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-04-26 20:31:43,573][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-04-26 20:31:43,575][gdb2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-04-26 20:31:43,575][gdb2][WARNING] backup header from main header.
[2017-04-26 20:31:43,575][gdb2][WARNING] 
[2017-04-26 20:31:43,575][gdb2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-04-26 20:31:43,575][gdb2][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-04-26 20:31:43,575][gdb2][WARNING] 
[2017-04-26 20:31:43,575][gdb2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-04-26 20:31:43,575][gdb2][WARNING] 
[2017-04-26 20:31:44,642][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:31:44,643][gdb2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-04-26 20:31:44,643][gdb2][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-04-26 20:31:44,643][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:31:44,643][gdb2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-04-26 20:31:44,643][gdb2][DEBUG ] other utilities.
[2017-04-26 20:31:44,643][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-04-26 20:31:45,660][gdb2][DEBUG ] Creating new GPT entries.
[2017-04-26 20:31:45,660][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:31:45,660][gdb2][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-04-26 20:31:45,661][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:31:45,661][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:31:45,661][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:31:45,668][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:45,669][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:45,669][gdb2][WARNING] ptype_tobe_for_name: name = journal
[2017-04-26 20:31:45,669][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:45,669][gdb2][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-04-26 20:31:45,669][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:b51ebf30-2075-4654-ab71-8ee7ac390c24 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-04-26 20:31:46,686][gdb2][DEBUG ] Setting name!
[2017-04-26 20:31:46,686][gdb2][DEBUG ] partNum is 1
[2017-04-26 20:31:46,686][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:31:46,687][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:31:46,687][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:31:46,687][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:31:46,901][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:31:47,015][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:31:47,080][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:47,080][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:47,080][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:31:47,080][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/b51ebf30-2075-4654-ab71-8ee7ac390c24
[2017-04-26 20:31:47,080][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-04-26 20:31:48,097][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:31:48,097][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:31:48,097][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:31:48,262][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:31:48,476][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:31:48,492][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/b51ebf30-2075-4654-ab71-8ee7ac390c24
[2017-04-26 20:31:48,492][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:48,492][gdb2][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-04-26 20:31:48,492][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:48,492][gdb2][WARNING] ptype_tobe_for_name: name = data
[2017-04-26 20:31:48,492][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:48,493][gdb2][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-04-26 20:31:48,493][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e692f1d0-0acd-451e-9390-86c81a5ceb52 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-04-26 20:31:49,510][gdb2][DEBUG ] Setting name!
[2017-04-26 20:31:49,510][gdb2][DEBUG ] partNum is 0
[2017-04-26 20:31:49,510][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:31:49,510][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:31:49,510][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:31:49,510][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:31:49,725][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:31:49,939][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:31:50,154][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:50,154][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:50,154][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:31:50,154][gdb2][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-04-26 20:31:50,154][gdb2][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-04-26 20:31:50,720][gdb2][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-04-26 20:31:50,720][gdb2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-04-26 20:31:50,720][gdb2][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-04-26 20:31:50,720][gdb2][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-04-26 20:31:50,720][gdb2][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-04-26 20:31:50,720][gdb2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-04-26 20:31:50,721][gdb2][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-04-26 20:31:50,721][gdb2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-04-26 20:31:50,721][gdb2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-04-26 20:31:50,721][gdb2][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.5m8sK6 with options noatime,inode64
[2017-04-26 20:31:50,721][gdb2][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.5m8sK6
[2017-04-26 20:31:50,737][gdb2][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.5m8sK6
[2017-04-26 20:31:50,737][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.5m8sK6/ceph_fsid.15560.tmp
[2017-04-26 20:31:50,740][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.5m8sK6/fsid.15560.tmp
[2017-04-26 20:31:50,743][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.5m8sK6/magic.15560.tmp
[2017-04-26 20:31:50,745][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.5m8sK6/journal_uuid.15560.tmp
[2017-04-26 20:31:50,748][gdb2][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.5m8sK6/journal -> /dev/disk/by-partuuid/b51ebf30-2075-4654-ab71-8ee7ac390c24
[2017-04-26 20:31:50,748][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.5m8sK6
[2017-04-26 20:31:50,748][gdb2][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.5m8sK6
[2017-04-26 20:31:50,749][gdb2][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.5m8sK6
[2017-04-26 20:31:50,780][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:31:50,780][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-04-26 20:31:51,798][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:31:51,798][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:31:51,798][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:31:52,012][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:31:52,227][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:31:52,259][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-04-26 20:31:52,293][gdb2][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-04-26 20:31:57,415][gdb2][INFO  ] checking OSD status...
[2017-04-26 20:31:57,416][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:31:57,418][gdb2][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-04-26 20:31:57,584][ceph_deploy.osd][DEBUG ] Host gdb2 is now ready for osd use.
[2017-04-26 20:33:26,698][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb2:/dev/xvdb1
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  disk                          : [('gdb2', '/dev/xvdb1', None)]
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe4f88a5908>
[2017-04-26 20:33:26,699][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:33:26,700][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:33:26,700][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe4f8afbaa0>
[2017-04-26 20:33:26,700][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:33:26,700][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:33:26,700][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:33:26,700][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb2:/dev/xvdb1:
[2017-04-26 20:33:26,941][gdb2][DEBUG ] connection detected need for sudo
[2017-04-26 20:33:27,165][gdb2][DEBUG ] connected to host: gdb2 
[2017-04-26 20:33:27,166][gdb2][DEBUG ] detect platform information from remote host
[2017-04-26 20:33:27,181][gdb2][DEBUG ] detect machine type
[2017-04-26 20:33:27,185][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:33:27,186][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:33:27,186][ceph_deploy.osd][DEBUG ] Deploying osd to gdb2
[2017-04-26 20:33:27,187][gdb2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:33:27,189][ceph_deploy.osd][DEBUG ] Preparing host gdb2 disk /dev/xvdb1 journal None activate True
[2017-04-26 20:33:27,189][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:33:27,191][gdb2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb1
[2017-04-26 20:33:27,312][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:33:27,315][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:33:27,331][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:33:27,346][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:33:27,354][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:33:27,354][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:33:27,370][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:33:27,370][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:33:27,370][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-04-26 20:33:27,378][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-04-26 20:33:27,385][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-04-26 20:33:27,393][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-04-26 20:33:27,400][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:33:27,401][gdb2][WARNING] Traceback (most recent call last):
[2017-04-26 20:33:27,401][gdb2][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-04-26 20:33:27,401][gdb2][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-04-26 20:33:27,401][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-04-26 20:33:27,402][gdb2][WARNING]     main(sys.argv[1:])
[2017-04-26 20:33:27,402][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-04-26 20:33:27,402][gdb2][WARNING]     args.func(args)
[2017-04-26 20:33:27,402][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-04-26 20:33:27,402][gdb2][WARNING]     Prepare.factory(args).prepare()
[2017-04-26 20:33:27,402][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2018, in prepare
[2017-04-26 20:33:27,402][gdb2][WARNING]     self.prepare_locked()
[2017-04-26 20:33:27,402][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2049, in prepare_locked
[2017-04-26 20:33:27,403][gdb2][WARNING]     self.data.prepare(self.journal)
[2017-04-26 20:33:27,403][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2816, in prepare
[2017-04-26 20:33:27,403][gdb2][WARNING]     self.prepare_device(*to_prepare_list)
[2017-04-26 20:33:27,403][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2971, in prepare_device
[2017-04-26 20:33:27,403][gdb2][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2017-04-26 20:33:27,403][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2881, in prepare_device
[2017-04-26 20:33:27,403][gdb2][WARNING]     zap(self.args.data)
[2017-04-26 20:33:27,403][gdb2][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1580, in zap
[2017-04-26 20:33:27,403][gdb2][WARNING]     raise Error('not full block device; cannot zap', dev)
[2017-04-26 20:33:27,403][gdb2][WARNING] ceph_disk.main.Error: Error: not full block device; cannot zap: /dev/xvdb1
[2017-04-26 20:33:27,411][gdb2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-04-26 20:33:27,411][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb1
[2017-04-26 20:33:27,411][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-04-26 20:34:20,135][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:34:20,135][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb2:/dev/xvdb
[2017-04-26 20:34:20,135][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:34:20,135][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:34:20,135][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:34:20,135][ceph_deploy.cli][INFO  ]  disk                          : [('gdb2', '/dev/xvdb', None)]
[2017-04-26 20:34:20,135][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:34:20,135][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:34:20,135][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f11439d1908>
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1143c27aa0>
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:34:20,136][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:34:20,137][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb2:/dev/xvdb:
[2017-04-26 20:34:20,378][gdb2][DEBUG ] connection detected need for sudo
[2017-04-26 20:34:20,609][gdb2][DEBUG ] connected to host: gdb2 
[2017-04-26 20:34:20,609][gdb2][DEBUG ] detect platform information from remote host
[2017-04-26 20:34:20,626][gdb2][DEBUG ] detect machine type
[2017-04-26 20:34:20,629][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:34:20,630][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:34:20,630][ceph_deploy.osd][DEBUG ] Deploying osd to gdb2
[2017-04-26 20:34:20,631][gdb2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:34:20,633][ceph_deploy.osd][DEBUG ] Preparing host gdb2 disk /dev/xvdb journal None activate True
[2017-04-26 20:34:20,633][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:34:20,636][gdb2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:34:20,756][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:34:20,760][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:34:20,776][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:34:20,791][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:34:20,799][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:20,799][gdb2][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:34:20,799][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:34:20,815][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:20,815][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:20,815][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:20,815][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:34:20,815][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:34:20,815][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-04-26 20:34:20,819][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-04-26 20:34:20,834][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-04-26 20:34:20,836][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-04-26 20:34:20,843][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:20,843][gdb2][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-04-26 20:34:20,844][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:20,844][gdb2][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-04-26 20:34:20,875][gdb2][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-04-26 20:34:20,875][gdb2][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-04-26 20:34:20,883][gdb2][WARNING] 10+0 records in
[2017-04-26 20:34:20,883][gdb2][WARNING] 10+0 records out
[2017-04-26 20:34:20,883][gdb2][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00708251 s, 1.5 GB/s
[2017-04-26 20:34:20,884][gdb2][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-04-26 20:34:20,998][gdb2][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-04-26 20:34:20,998][gdb2][WARNING] 10+0 records in
[2017-04-26 20:34:20,998][gdb2][WARNING] 10+0 records out
[2017-04-26 20:34:20,998][gdb2][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00506573 s, 2.1 GB/s
[2017-04-26 20:34:20,998][gdb2][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-04-26 20:34:20,998][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-04-26 20:34:20,998][gdb2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-04-26 20:34:20,999][gdb2][WARNING] backup header from main header.
[2017-04-26 20:34:20,999][gdb2][WARNING] 
[2017-04-26 20:34:20,999][gdb2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-04-26 20:34:20,999][gdb2][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-04-26 20:34:20,999][gdb2][WARNING] 
[2017-04-26 20:34:20,999][gdb2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-04-26 20:34:20,999][gdb2][WARNING] 
[2017-04-26 20:34:22,066][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:34:22,066][gdb2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-04-26 20:34:22,067][gdb2][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-04-26 20:34:22,067][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:34:22,067][gdb2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-04-26 20:34:22,067][gdb2][DEBUG ] other utilities.
[2017-04-26 20:34:22,067][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-04-26 20:34:23,084][gdb2][DEBUG ] Creating new GPT entries.
[2017-04-26 20:34:23,084][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:34:23,084][gdb2][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-04-26 20:34:23,084][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:34:23,084][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:34:23,085][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:34:23,088][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:23,088][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:23,088][gdb2][WARNING] ptype_tobe_for_name: name = journal
[2017-04-26 20:34:23,088][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:23,088][gdb2][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-04-26 20:34:23,088][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:8d3a675d-8c4a-4cfd-98a6-170fdf30f128 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-04-26 20:34:24,155][gdb2][DEBUG ] Setting name!
[2017-04-26 20:34:24,156][gdb2][DEBUG ] partNum is 1
[2017-04-26 20:34:24,156][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:34:24,156][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:34:24,156][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:34:24,156][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:34:24,320][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:34:24,485][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:34:24,500][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:24,501][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:24,501][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:34:24,501][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/8d3a675d-8c4a-4cfd-98a6-170fdf30f128
[2017-04-26 20:34:24,501][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-04-26 20:34:25,518][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:34:25,518][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:34:25,518][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:34:25,733][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:34:25,847][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:34:25,879][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/8d3a675d-8c4a-4cfd-98a6-170fdf30f128
[2017-04-26 20:34:25,880][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:25,880][gdb2][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-04-26 20:34:25,880][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:25,880][gdb2][WARNING] ptype_tobe_for_name: name = data
[2017-04-26 20:34:25,880][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:25,880][gdb2][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-04-26 20:34:25,880][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:4aab05aa-a301-4239-b456-2ecac25ae1ad --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-04-26 20:34:26,897][gdb2][DEBUG ] Setting name!
[2017-04-26 20:34:26,897][gdb2][DEBUG ] partNum is 0
[2017-04-26 20:34:26,898][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:34:26,898][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:34:26,898][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:34:26,898][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:34:27,112][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:34:27,327][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:34:27,391][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:27,391][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:27,391][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:34:27,391][gdb2][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-04-26 20:34:27,391][gdb2][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-04-26 20:34:28,057][gdb2][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-04-26 20:34:28,057][gdb2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-04-26 20:34:28,057][gdb2][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-04-26 20:34:28,058][gdb2][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-04-26 20:34:28,058][gdb2][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-04-26 20:34:28,058][gdb2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-04-26 20:34:28,058][gdb2][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-04-26 20:34:28,058][gdb2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-04-26 20:34:28,058][gdb2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-04-26 20:34:28,058][gdb2][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.HB0vcy with options noatime,inode64
[2017-04-26 20:34:28,058][gdb2][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.HB0vcy
[2017-04-26 20:34:28,058][gdb2][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.HB0vcy
[2017-04-26 20:34:28,058][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.HB0vcy/ceph_fsid.16610.tmp
[2017-04-26 20:34:28,062][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.HB0vcy/fsid.16610.tmp
[2017-04-26 20:34:28,063][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.HB0vcy/magic.16610.tmp
[2017-04-26 20:34:28,095][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.HB0vcy/journal_uuid.16610.tmp
[2017-04-26 20:34:28,095][gdb2][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.HB0vcy/journal -> /dev/disk/by-partuuid/8d3a675d-8c4a-4cfd-98a6-170fdf30f128
[2017-04-26 20:34:28,095][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.HB0vcy
[2017-04-26 20:34:28,095][gdb2][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.HB0vcy
[2017-04-26 20:34:28,095][gdb2][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.HB0vcy
[2017-04-26 20:34:28,111][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:34:28,111][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-04-26 20:34:29,128][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:34:29,128][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:34:29,128][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:34:29,343][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:34:29,557][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:34:29,589][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-04-26 20:34:29,599][gdb2][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-04-26 20:34:34,721][gdb2][INFO  ] checking OSD status...
[2017-04-26 20:34:34,722][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:34:34,724][gdb2][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-04-26 20:34:34,889][ceph_deploy.osd][DEBUG ] Host gdb2 is now ready for osd use.
[2017-04-26 20:37:24,301][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb3:/dev/xvdb
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ]  disk                          : [('gdb3', '/dev/xvdb', None)]
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:37:24,302][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:37:24,303][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fda6ffa3908>
[2017-04-26 20:37:24,303][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:37:24,303][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:37:24,303][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fda701f9aa0>
[2017-04-26 20:37:24,303][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:37:24,303][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:37:24,303][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:37:24,303][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb3:/dev/xvdb:
[2017-04-26 20:37:24,329][gdb3][DEBUG ] connection detected need for sudo
[2017-04-26 20:37:24,342][gdb3][DEBUG ] connected to host: gdb3 
[2017-04-26 20:37:24,343][gdb3][DEBUG ] detect platform information from remote host
[2017-04-26 20:37:24,359][gdb3][DEBUG ] detect machine type
[2017-04-26 20:37:24,361][gdb3][DEBUG ] find the location of an executable
[2017-04-26 20:37:24,362][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:37:24,362][ceph_deploy.osd][DEBUG ] Deploying osd to gdb3
[2017-04-26 20:37:24,362][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:37:24,364][gdb3][WARNING] osd keyring does not exist yet, creating one
[2017-04-26 20:37:24,364][gdb3][DEBUG ] create a keyring file
[2017-04-26 20:37:24,365][ceph_deploy.osd][DEBUG ] Preparing host gdb3 disk /dev/xvdb journal None activate True
[2017-04-26 20:37:24,365][gdb3][DEBUG ] find the location of an executable
[2017-04-26 20:37:24,366][gdb3][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:37:24,486][gdb3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:37:24,494][gdb3][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:37:24,510][gdb3][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:37:24,517][gdb3][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:37:24,533][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:24,533][gdb3][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:37:24,533][gdb3][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:37:24,541][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:24,541][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:24,541][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:24,541][gdb3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-04-26 20:37:24,557][gdb3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-04-26 20:37:24,558][gdb3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-04-26 20:37:24,566][gdb3][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-04-26 20:37:24,581][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:24,582][gdb3][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-04-26 20:37:24,582][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:24,582][gdb3][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-04-26 20:37:24,582][gdb3][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-04-26 20:37:25,649][gdb3][DEBUG ] Creating new GPT entries.
[2017-04-26 20:37:25,649][gdb3][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-04-26 20:37:25,650][gdb3][DEBUG ] other utilities.
[2017-04-26 20:37:25,650][gdb3][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-04-26 20:37:26,666][gdb3][DEBUG ] Creating new GPT entries.
[2017-04-26 20:37:26,667][gdb3][DEBUG ] The operation has completed successfully.
[2017-04-26 20:37:26,667][gdb3][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-04-26 20:37:26,667][gdb3][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:37:26,667][gdb3][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:37:26,667][gdb3][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:37:26,683][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:26,683][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:26,683][gdb3][WARNING] ptype_tobe_for_name: name = journal
[2017-04-26 20:37:26,683][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:26,683][gdb3][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-04-26 20:37:26,683][gdb3][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:def4448b-a91f-4cff-b7f6-01f65b2a3869 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-04-26 20:37:27,700][gdb3][DEBUG ] Setting name!
[2017-04-26 20:37:27,700][gdb3][DEBUG ] partNum is 1
[2017-04-26 20:37:27,700][gdb3][DEBUG ] REALLY setting name!
[2017-04-26 20:37:27,700][gdb3][DEBUG ] The operation has completed successfully.
[2017-04-26 20:37:27,700][gdb3][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:37:27,700][gdb3][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:37:27,915][gdb3][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:37:28,029][gdb3][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:37:28,093][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:28,093][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:28,093][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:37:28,093][gdb3][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/def4448b-a91f-4cff-b7f6-01f65b2a3869
[2017-04-26 20:37:28,093][gdb3][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-04-26 20:37:29,110][gdb3][DEBUG ] The operation has completed successfully.
[2017-04-26 20:37:29,110][gdb3][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:37:29,111][gdb3][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:37:29,275][gdb3][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:37:29,439][gdb3][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:37:29,471][gdb3][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/def4448b-a91f-4cff-b7f6-01f65b2a3869
[2017-04-26 20:37:29,471][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:29,471][gdb3][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-04-26 20:37:29,471][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:29,471][gdb3][WARNING] ptype_tobe_for_name: name = data
[2017-04-26 20:37:29,471][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:29,471][gdb3][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-04-26 20:37:29,472][gdb3][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:7f3ae0eb-f104-4173-97e5-bc35b15a766e --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-04-26 20:37:30,488][gdb3][DEBUG ] Setting name!
[2017-04-26 20:37:30,488][gdb3][DEBUG ] partNum is 0
[2017-04-26 20:37:30,489][gdb3][DEBUG ] REALLY setting name!
[2017-04-26 20:37:30,489][gdb3][DEBUG ] The operation has completed successfully.
[2017-04-26 20:37:30,489][gdb3][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:37:30,489][gdb3][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:37:30,703][gdb3][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:37:30,918][gdb3][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:37:30,950][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:30,950][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:30,950][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:37:30,950][gdb3][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-04-26 20:37:30,950][gdb3][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-04-26 20:37:31,716][gdb3][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-04-26 20:37:31,716][gdb3][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-04-26 20:37:31,716][gdb3][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-04-26 20:37:31,716][gdb3][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-04-26 20:37:31,716][gdb3][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-04-26 20:37:31,717][gdb3][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-04-26 20:37:31,717][gdb3][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-04-26 20:37:31,717][gdb3][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-04-26 20:37:31,717][gdb3][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-04-26 20:37:31,717][gdb3][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.GZUKhk with options noatime,inode64
[2017-04-26 20:37:31,717][gdb3][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.GZUKhk
[2017-04-26 20:37:31,733][gdb3][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.GZUKhk
[2017-04-26 20:37:31,733][gdb3][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.GZUKhk/ceph_fsid.5768.tmp
[2017-04-26 20:37:31,734][gdb3][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.GZUKhk/fsid.5768.tmp
[2017-04-26 20:37:31,737][gdb3][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.GZUKhk/magic.5768.tmp
[2017-04-26 20:37:31,753][gdb3][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.GZUKhk/journal_uuid.5768.tmp
[2017-04-26 20:37:31,753][gdb3][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.GZUKhk/journal -> /dev/disk/by-partuuid/def4448b-a91f-4cff-b7f6-01f65b2a3869
[2017-04-26 20:37:31,753][gdb3][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.GZUKhk
[2017-04-26 20:37:31,753][gdb3][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.GZUKhk
[2017-04-26 20:37:31,753][gdb3][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.GZUKhk
[2017-04-26 20:37:31,785][gdb3][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:37:31,786][gdb3][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-04-26 20:37:32,803][gdb3][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-04-26 20:37:32,803][gdb3][DEBUG ] The new table will be used at the next reboot or after you
[2017-04-26 20:37:32,803][gdb3][DEBUG ] run partprobe(8) or kpartx(8)
[2017-04-26 20:37:32,803][gdb3][DEBUG ] The operation has completed successfully.
[2017-04-26 20:37:32,803][gdb3][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:37:32,803][gdb3][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:37:32,803][gdb3][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:37:33,018][gdb3][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:37:33,037][gdb3][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-04-26 20:37:33,056][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-04-26 20:37:38,184][gdb3][INFO  ] checking OSD status...
[2017-04-26 20:37:38,184][gdb3][DEBUG ] find the location of an executable
[2017-04-26 20:37:38,186][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-04-26 20:37:38,351][ceph_deploy.osd][DEBUG ] Host gdb3 is now ready for osd use.
[2017-04-26 20:38:41,456][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:38:41,456][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb2:/dev/xvdb
[2017-04-26 20:38:41,456][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:38:41,456][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  disk                          : [('gdb2', '/dev/xvdb', None)]
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9b56125908>
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9b5637baa0>
[2017-04-26 20:38:41,457][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:38:41,458][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:38:41,458][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:38:41,458][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb2:/dev/xvdb:
[2017-04-26 20:38:41,698][gdb2][DEBUG ] connection detected need for sudo
[2017-04-26 20:38:41,925][gdb2][DEBUG ] connected to host: gdb2 
[2017-04-26 20:38:41,925][gdb2][DEBUG ] detect platform information from remote host
[2017-04-26 20:38:41,941][gdb2][DEBUG ] detect machine type
[2017-04-26 20:38:41,945][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:38:41,946][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:38:41,946][ceph_deploy.osd][DEBUG ] Deploying osd to gdb2
[2017-04-26 20:38:41,947][gdb2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:38:41,949][ceph_deploy.osd][DEBUG ] Preparing host gdb2 disk /dev/xvdb journal None activate True
[2017-04-26 20:38:41,949][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:38:41,952][gdb2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:38:42,072][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:38:42,076][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:38:42,091][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:38:42,107][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:38:42,115][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:42,115][gdb2][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:38:42,115][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:38:42,130][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:42,131][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:42,131][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:42,131][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:38:42,131][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:38:42,131][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-04-26 20:38:42,138][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-04-26 20:38:42,146][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-04-26 20:38:42,154][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-04-26 20:38:42,169][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:42,169][gdb2][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-04-26 20:38:42,169][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:42,169][gdb2][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-04-26 20:38:42,185][gdb2][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-04-26 20:38:42,185][gdb2][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-04-26 20:38:42,299][gdb2][WARNING] 10+0 records in
[2017-04-26 20:38:42,300][gdb2][WARNING] 10+0 records out
[2017-04-26 20:38:42,300][gdb2][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.112547 s, 93.2 MB/s
[2017-04-26 20:38:42,300][gdb2][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-04-26 20:38:42,316][gdb2][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-04-26 20:38:42,331][gdb2][WARNING] 10+0 records in
[2017-04-26 20:38:42,331][gdb2][WARNING] 10+0 records out
[2017-04-26 20:38:42,331][gdb2][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.0125217 s, 837 MB/s
[2017-04-26 20:38:42,332][gdb2][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-04-26 20:38:42,332][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-04-26 20:38:42,332][gdb2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-04-26 20:38:42,332][gdb2][WARNING] backup header from main header.
[2017-04-26 20:38:42,332][gdb2][WARNING] 
[2017-04-26 20:38:42,332][gdb2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-04-26 20:38:42,332][gdb2][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-04-26 20:38:42,332][gdb2][WARNING] 
[2017-04-26 20:38:42,332][gdb2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-04-26 20:38:42,332][gdb2][WARNING] 
[2017-04-26 20:38:43,399][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:38:43,400][gdb2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-04-26 20:38:43,400][gdb2][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-04-26 20:38:43,400][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:38:43,400][gdb2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-04-26 20:38:43,400][gdb2][DEBUG ] other utilities.
[2017-04-26 20:38:43,400][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-04-26 20:38:44,417][gdb2][DEBUG ] Creating new GPT entries.
[2017-04-26 20:38:44,417][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:38:44,417][gdb2][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-04-26 20:38:44,417][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:38:44,418][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:38:44,421][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:38:44,437][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:44,437][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:44,437][gdb2][WARNING] ptype_tobe_for_name: name = journal
[2017-04-26 20:38:44,437][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:44,437][gdb2][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-04-26 20:38:44,437][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:bfb512db-b96b-444a-ac0b-38c881cfacd7 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-04-26 20:38:45,454][gdb2][DEBUG ] Setting name!
[2017-04-26 20:38:45,454][gdb2][DEBUG ] partNum is 1
[2017-04-26 20:38:45,454][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:38:45,454][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:38:45,455][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:38:45,455][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:38:45,669][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:38:45,783][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:38:45,998][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:45,998][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:45,998][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:38:45,998][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/bfb512db-b96b-444a-ac0b-38c881cfacd7
[2017-04-26 20:38:45,998][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-04-26 20:38:47,015][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:38:47,016][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:38:47,016][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:38:47,230][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:38:47,345][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:38:47,376][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/bfb512db-b96b-444a-ac0b-38c881cfacd7
[2017-04-26 20:38:47,377][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:47,377][gdb2][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-04-26 20:38:47,377][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:47,377][gdb2][WARNING] ptype_tobe_for_name: name = data
[2017-04-26 20:38:47,377][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:47,377][gdb2][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-04-26 20:38:47,377][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e83d789e-b57d-4e48-b170-561f6c62cf18 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-04-26 20:38:48,394][gdb2][DEBUG ] Setting name!
[2017-04-26 20:38:48,394][gdb2][DEBUG ] partNum is 0
[2017-04-26 20:38:48,394][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:38:48,394][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:38:48,394][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:38:48,395][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:38:48,609][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:38:48,774][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:38:48,988][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:48,988][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:48,989][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:38:48,989][gdb2][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-04-26 20:38:48,989][gdb2][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-04-26 20:38:49,604][gdb2][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-04-26 20:38:49,605][gdb2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-04-26 20:38:49,605][gdb2][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-04-26 20:38:49,605][gdb2][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-04-26 20:38:49,605][gdb2][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-04-26 20:38:49,605][gdb2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-04-26 20:38:49,605][gdb2][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-04-26 20:38:49,605][gdb2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-04-26 20:38:49,605][gdb2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-04-26 20:38:49,605][gdb2][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.biC_0S with options noatime,inode64
[2017-04-26 20:38:49,605][gdb2][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.biC_0S
[2017-04-26 20:38:49,637][gdb2][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.biC_0S
[2017-04-26 20:38:49,637][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.biC_0S/ceph_fsid.17564.tmp
[2017-04-26 20:38:49,637][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.biC_0S/fsid.17564.tmp
[2017-04-26 20:38:49,641][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.biC_0S/magic.17564.tmp
[2017-04-26 20:38:49,644][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.biC_0S/journal_uuid.17564.tmp
[2017-04-26 20:38:49,646][gdb2][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.biC_0S/journal -> /dev/disk/by-partuuid/bfb512db-b96b-444a-ac0b-38c881cfacd7
[2017-04-26 20:38:49,647][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.biC_0S
[2017-04-26 20:38:49,648][gdb2][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.biC_0S
[2017-04-26 20:38:49,648][gdb2][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.biC_0S
[2017-04-26 20:38:49,680][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:38:49,680][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-04-26 20:38:50,697][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:38:50,697][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:38:50,698][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:38:50,912][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:38:51,177][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:38:51,209][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-04-26 20:38:51,227][gdb2][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-04-26 20:38:56,349][gdb2][INFO  ] checking OSD status...
[2017-04-26 20:38:56,349][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:38:56,352][gdb2][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-04-26 20:38:56,517][ceph_deploy.osd][DEBUG ] Host gdb2 is now ready for osd use.
[2017-04-26 20:41:22,786][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:41:22,786][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb2:/dev/xvdb
[2017-04-26 20:41:22,786][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  disk                          : [('gdb2', '/dev/xvdb', None)]
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0478326908>
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:41:22,787][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:41:22,788][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f047857caa0>
[2017-04-26 20:41:22,788][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:41:22,788][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:41:22,788][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:41:22,788][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb2:/dev/xvdb:
[2017-04-26 20:41:23,025][gdb2][DEBUG ] connection detected need for sudo
[2017-04-26 20:41:23,218][gdb2][DEBUG ] connected to host: gdb2 
[2017-04-26 20:41:23,218][gdb2][DEBUG ] detect platform information from remote host
[2017-04-26 20:41:23,235][gdb2][DEBUG ] detect machine type
[2017-04-26 20:41:23,239][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:41:23,240][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:41:23,240][ceph_deploy.osd][DEBUG ] Deploying osd to gdb2
[2017-04-26 20:41:23,240][gdb2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:41:23,243][ceph_deploy.osd][DEBUG ] Preparing host gdb2 disk /dev/xvdb journal None activate True
[2017-04-26 20:41:23,243][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:41:23,245][gdb2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:41:23,366][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:41:23,369][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:41:23,385][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:41:23,401][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:41:23,417][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:23,417][gdb2][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:41:23,417][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:41:23,425][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:23,425][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:23,425][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:23,425][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:41:23,425][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:41:23,425][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-04-26 20:41:23,433][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-04-26 20:41:23,440][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-04-26 20:41:23,448][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-04-26 20:41:23,455][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:23,455][gdb2][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-04-26 20:41:23,456][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:23,456][gdb2][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-04-26 20:41:23,471][gdb2][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-04-26 20:41:23,471][gdb2][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-04-26 20:41:23,586][gdb2][WARNING] 10+0 records in
[2017-04-26 20:41:23,586][gdb2][WARNING] 10+0 records out
[2017-04-26 20:41:23,586][gdb2][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.10425 s, 101 MB/s
[2017-04-26 20:41:23,586][gdb2][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-04-26 20:41:23,593][gdb2][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-04-26 20:41:23,625][gdb2][WARNING] 10+0 records in
[2017-04-26 20:41:23,625][gdb2][WARNING] 10+0 records out
[2017-04-26 20:41:23,625][gdb2][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.0289232 s, 363 MB/s
[2017-04-26 20:41:23,626][gdb2][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-04-26 20:41:23,626][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-04-26 20:41:23,626][gdb2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-04-26 20:41:23,626][gdb2][WARNING] backup header from main header.
[2017-04-26 20:41:23,626][gdb2][WARNING] 
[2017-04-26 20:41:23,626][gdb2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-04-26 20:41:23,626][gdb2][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-04-26 20:41:23,627][gdb2][WARNING] 
[2017-04-26 20:41:23,627][gdb2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-04-26 20:41:23,627][gdb2][WARNING] 
[2017-04-26 20:41:24,694][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:41:24,694][gdb2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-04-26 20:41:24,694][gdb2][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-04-26 20:41:24,694][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:41:24,694][gdb2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-04-26 20:41:24,694][gdb2][DEBUG ] other utilities.
[2017-04-26 20:41:24,695][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-04-26 20:41:25,661][gdb2][DEBUG ] Creating new GPT entries.
[2017-04-26 20:41:25,662][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:41:25,662][gdb2][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-04-26 20:41:25,662][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:41:25,678][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:41:25,709][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:41:25,713][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:25,713][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:25,713][gdb2][WARNING] ptype_tobe_for_name: name = journal
[2017-04-26 20:41:25,713][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:25,713][gdb2][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-04-26 20:41:25,713][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:248cd1be-cf9e-4d0d-9e1f-bb8b813e6728 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-04-26 20:41:26,730][gdb2][DEBUG ] Setting name!
[2017-04-26 20:41:26,730][gdb2][DEBUG ] partNum is 1
[2017-04-26 20:41:26,731][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:41:26,731][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:41:26,731][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:41:26,731][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:41:26,945][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:41:27,110][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:41:27,142][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:27,142][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:27,142][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:41:27,142][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/248cd1be-cf9e-4d0d-9e1f-bb8b813e6728
[2017-04-26 20:41:27,142][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-04-26 20:41:28,159][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:41:28,159][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:41:28,159][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:41:28,374][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:41:28,538][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:41:28,554][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/248cd1be-cf9e-4d0d-9e1f-bb8b813e6728
[2017-04-26 20:41:28,554][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:28,555][gdb2][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-04-26 20:41:28,555][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:28,555][gdb2][WARNING] ptype_tobe_for_name: name = data
[2017-04-26 20:41:28,555][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:28,555][gdb2][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-04-26 20:41:28,555][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e893e429-a60c-41e1-ae9c-ff5e7b9a4f4c --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-04-26 20:41:29,572][gdb2][DEBUG ] Setting name!
[2017-04-26 20:41:29,572][gdb2][DEBUG ] partNum is 0
[2017-04-26 20:41:29,572][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:41:29,572][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:41:29,572][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:41:29,572][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:41:29,787][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:41:29,951][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:41:30,166][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:30,166][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:30,166][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:41:30,166][gdb2][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-04-26 20:41:30,166][gdb2][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-04-26 20:41:30,832][gdb2][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-04-26 20:41:30,832][gdb2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-04-26 20:41:30,833][gdb2][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-04-26 20:41:30,833][gdb2][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-04-26 20:41:30,833][gdb2][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-04-26 20:41:30,833][gdb2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-04-26 20:41:30,833][gdb2][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-04-26 20:41:30,833][gdb2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-04-26 20:41:30,833][gdb2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-04-26 20:41:30,833][gdb2][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.D3sTsk with options noatime,inode64
[2017-04-26 20:41:30,833][gdb2][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.D3sTsk
[2017-04-26 20:41:30,833][gdb2][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.D3sTsk
[2017-04-26 20:41:30,833][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.D3sTsk/ceph_fsid.18495.tmp
[2017-04-26 20:41:30,833][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.D3sTsk/fsid.18495.tmp
[2017-04-26 20:41:30,849][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.D3sTsk/magic.18495.tmp
[2017-04-26 20:41:30,849][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.D3sTsk/journal_uuid.18495.tmp
[2017-04-26 20:41:30,850][gdb2][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.D3sTsk/journal -> /dev/disk/by-partuuid/248cd1be-cf9e-4d0d-9e1f-bb8b813e6728
[2017-04-26 20:41:30,851][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.D3sTsk
[2017-04-26 20:41:30,854][gdb2][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.D3sTsk
[2017-04-26 20:41:30,854][gdb2][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.D3sTsk
[2017-04-26 20:41:30,870][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:41:30,870][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-04-26 20:41:31,937][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:41:31,937][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:41:31,937][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:41:32,102][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:41:32,266][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:41:32,330][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-04-26 20:41:32,333][gdb2][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-04-26 20:41:37,455][gdb2][INFO  ] checking OSD status...
[2017-04-26 20:41:37,455][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:41:37,458][gdb2][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-04-26 20:41:37,523][ceph_deploy.osd][DEBUG ] Host gdb2 is now ready for osd use.
[2017-04-26 20:43:00,175][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:43:00,175][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb2:/dev/xvdb
[2017-04-26 20:43:00,175][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:43:00,175][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:43:00,175][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:43:00,175][ceph_deploy.cli][INFO  ]  disk                          : [('gdb2', '/dev/xvdb', None)]
[2017-04-26 20:43:00,175][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:43:00,175][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3551bf9908>
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3551e4faa0>
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:43:00,176][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:43:00,177][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb2:/dev/xvdb:
[2017-04-26 20:43:00,413][gdb2][DEBUG ] connection detected need for sudo
[2017-04-26 20:43:00,641][gdb2][DEBUG ] connected to host: gdb2 
[2017-04-26 20:43:00,641][gdb2][DEBUG ] detect platform information from remote host
[2017-04-26 20:43:00,657][gdb2][DEBUG ] detect machine type
[2017-04-26 20:43:00,661][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:43:00,662][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:43:00,663][ceph_deploy.osd][DEBUG ] Deploying osd to gdb2
[2017-04-26 20:43:00,663][gdb2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:43:00,665][gdb2][WARNING] osd keyring does not exist yet, creating one
[2017-04-26 20:43:00,665][gdb2][DEBUG ] create a keyring file
[2017-04-26 20:43:00,667][ceph_deploy.osd][DEBUG ] Preparing host gdb2 disk /dev/xvdb journal None activate True
[2017-04-26 20:43:00,667][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:43:00,669][gdb2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:43:00,789][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:43:00,797][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:43:00,813][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:43:00,828][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:43:00,836][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:00,836][gdb2][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:43:00,836][gdb2][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:43:00,852][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:00,852][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:00,852][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:00,852][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:43:00,852][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:43:00,852][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-04-26 20:43:00,860][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-04-26 20:43:00,867][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-04-26 20:43:00,875][gdb2][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-04-26 20:43:00,882][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:00,883][gdb2][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-04-26 20:43:00,883][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:00,883][gdb2][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-04-26 20:43:00,915][gdb2][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-04-26 20:43:00,915][gdb2][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-04-26 20:43:00,915][gdb2][WARNING] 10+0 records in
[2017-04-26 20:43:00,915][gdb2][WARNING] 10+0 records out
[2017-04-26 20:43:00,915][gdb2][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00720679 s, 1.5 GB/s
[2017-04-26 20:43:00,915][gdb2][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-04-26 20:43:01,029][gdb2][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-04-26 20:43:01,061][gdb2][WARNING] 10+0 records in
[2017-04-26 20:43:01,061][gdb2][WARNING] 10+0 records out
[2017-04-26 20:43:01,061][gdb2][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.0405467 s, 259 MB/s
[2017-04-26 20:43:01,061][gdb2][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-04-26 20:43:01,063][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-04-26 20:43:01,066][gdb2][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-04-26 20:43:01,066][gdb2][WARNING] backup header from main header.
[2017-04-26 20:43:01,066][gdb2][WARNING] 
[2017-04-26 20:43:01,066][gdb2][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-04-26 20:43:01,066][gdb2][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-04-26 20:43:01,067][gdb2][WARNING] 
[2017-04-26 20:43:01,068][gdb2][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-04-26 20:43:01,068][gdb2][WARNING] 
[2017-04-26 20:43:02,185][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:43:02,186][gdb2][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-04-26 20:43:02,186][gdb2][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-04-26 20:43:02,186][gdb2][DEBUG ] ****************************************************************************
[2017-04-26 20:43:02,186][gdb2][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-04-26 20:43:02,186][gdb2][DEBUG ] other utilities.
[2017-04-26 20:43:02,186][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-04-26 20:43:03,203][gdb2][DEBUG ] Creating new GPT entries.
[2017-04-26 20:43:03,204][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:43:03,204][gdb2][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-04-26 20:43:03,204][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:43:03,204][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:43:03,207][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:43:03,223][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:03,223][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:03,223][gdb2][WARNING] ptype_tobe_for_name: name = journal
[2017-04-26 20:43:03,223][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:03,223][gdb2][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-04-26 20:43:03,223][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:2114c530-d373-4c14-b137-b69e64a62765 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-04-26 20:43:04,241][gdb2][DEBUG ] Setting name!
[2017-04-26 20:43:04,241][gdb2][DEBUG ] partNum is 1
[2017-04-26 20:43:04,241][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:43:04,241][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:43:04,241][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:43:04,241][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:43:04,506][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:43:04,620][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:43:04,652][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:04,652][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:04,652][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:43:04,652][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/2114c530-d373-4c14-b137-b69e64a62765
[2017-04-26 20:43:04,652][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-04-26 20:43:05,670][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:43:05,670][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:43:05,670][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:43:05,884][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:43:05,999][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:43:06,213][gdb2][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/2114c530-d373-4c14-b137-b69e64a62765
[2017-04-26 20:43:06,214][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:06,214][gdb2][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-04-26 20:43:06,214][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:06,214][gdb2][WARNING] ptype_tobe_for_name: name = data
[2017-04-26 20:43:06,214][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:06,214][gdb2][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-04-26 20:43:06,214][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:84592275-103b-41be-9a39-9ba48def95cc --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-04-26 20:43:07,232][gdb2][DEBUG ] Setting name!
[2017-04-26 20:43:07,232][gdb2][DEBUG ] partNum is 0
[2017-04-26 20:43:07,232][gdb2][DEBUG ] REALLY setting name!
[2017-04-26 20:43:07,232][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:43:07,232][gdb2][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:43:07,232][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:43:07,447][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:43:07,611][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:43:07,826][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:07,826][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:07,826][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:43:07,826][gdb2][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-04-26 20:43:07,826][gdb2][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-04-26 20:43:08,442][gdb2][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-04-26 20:43:08,442][gdb2][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-04-26 20:43:08,442][gdb2][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-04-26 20:43:08,442][gdb2][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-04-26 20:43:08,442][gdb2][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-04-26 20:43:08,442][gdb2][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-04-26 20:43:08,442][gdb2][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-04-26 20:43:08,443][gdb2][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-04-26 20:43:08,443][gdb2][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-04-26 20:43:08,443][gdb2][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.AvCinM with options noatime,inode64
[2017-04-26 20:43:08,443][gdb2][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.AvCinM
[2017-04-26 20:43:08,444][gdb2][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.AvCinM
[2017-04-26 20:43:08,445][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.AvCinM/ceph_fsid.19485.tmp
[2017-04-26 20:43:08,449][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.AvCinM/fsid.19485.tmp
[2017-04-26 20:43:08,452][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.AvCinM/magic.19485.tmp
[2017-04-26 20:43:08,453][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.AvCinM/journal_uuid.19485.tmp
[2017-04-26 20:43:08,457][gdb2][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.AvCinM/journal -> /dev/disk/by-partuuid/2114c530-d373-4c14-b137-b69e64a62765
[2017-04-26 20:43:08,457][gdb2][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.AvCinM
[2017-04-26 20:43:08,457][gdb2][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.AvCinM
[2017-04-26 20:43:08,457][gdb2][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.AvCinM
[2017-04-26 20:43:08,489][gdb2][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:43:08,489][gdb2][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-04-26 20:43:09,506][gdb2][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-04-26 20:43:09,506][gdb2][DEBUG ] The new table will be used at the next reboot or after you
[2017-04-26 20:43:09,506][gdb2][DEBUG ] run partprobe(8) or kpartx(8)
[2017-04-26 20:43:09,506][gdb2][DEBUG ] The operation has completed successfully.
[2017-04-26 20:43:09,507][gdb2][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:43:09,507][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:43:09,507][gdb2][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:43:09,671][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:43:09,703][gdb2][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-04-26 20:43:09,721][gdb2][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-04-26 20:43:14,893][gdb2][INFO  ] checking OSD status...
[2017-04-26 20:43:14,893][gdb2][DEBUG ] find the location of an executable
[2017-04-26 20:43:14,896][gdb2][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-04-26 20:43:14,961][ceph_deploy.osd][DEBUG ] Host gdb2 is now ready for osd use.
[2017-04-26 20:50:19,070][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:50:19,070][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-04-26 20:50:19,070][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:50:19,070][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:50:19,070][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:50:19,070][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-04-26 20:50:19,070][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:50:19,070][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f14dbfbf908>
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f14dc215aa0>
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:50:19,071][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:50:19,072][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-04-26 20:50:19,177][ceph_deploy.osd][ERROR ] connecting to host: gdb1 resulted in errors: HostNotFound gdb1
[2017-04-26 20:50:19,177][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-04-26 20:51:04,682][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:51:04,682][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-04-26 20:51:04,682][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:51:04,682][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc16d44b908>
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc16d6a1aa0>
[2017-04-26 20:51:04,683][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:51:04,684][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:51:04,684][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:51:04,684][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-04-26 20:51:04,937][gdb1][DEBUG ] connection detected need for sudo
[2017-04-26 20:51:05,173][gdb1][DEBUG ] connected to host: gdb1 
[2017-04-26 20:51:05,173][gdb1][DEBUG ] detect platform information from remote host
[2017-04-26 20:51:05,189][gdb1][DEBUG ] detect machine type
[2017-04-26 20:51:05,193][gdb1][DEBUG ] find the location of an executable
[2017-04-26 20:51:05,194][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:51:05,194][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-04-26 20:51:05,194][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:51:05,196][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-04-26 20:51:05,197][gdb1][DEBUG ] create a keyring file
[2017-04-26 20:51:05,198][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-04-26 20:51:05,198][gdb1][DEBUG ] find the location of an executable
[2017-04-26 20:51:05,200][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:51:05,321][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:51:05,336][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:51:05,344][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:51:05,360][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:51:05,367][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:51:05,367][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:51:05,367][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:51:05,383][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:51:05,383][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:51:05,383][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:51:05,383][gdb1][WARNING] Traceback (most recent call last):
[2017-04-26 20:51:05,383][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-04-26 20:51:05,384][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-04-26 20:51:05,384][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-04-26 20:51:05,384][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-04-26 20:51:05,384][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2029, in main
[2017-04-26 20:51:05,384][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2018, in prepare
[2017-04-26 20:51:05,384][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2049, in prepare_locked
[2017-04-26 20:51:05,384][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2816, in prepare
[2017-04-26 20:51:05,384][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2971, in prepare_device
[2017-04-26 20:51:05,384][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2878, in prepare_device
[2017-04-26 20:51:05,384][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2841, in sanity_checks
[2017-04-26 20:51:05,384][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-04-26 20:51:05,384][gdb1][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-04-26 20:51:05,388][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-04-26 20:51:05,388][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:51:05,388][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-04-26 20:51:56,292][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:51:56,292][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-04-26 20:51:56,292][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:51:56,292][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:51:56,292][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:51:56,292][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-04-26 20:51:56,292][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:51:56,292][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6752adc908>
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6752d32aa0>
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:51:56,293][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:51:56,294][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-04-26 20:51:56,545][gdb1][DEBUG ] connection detected need for sudo
[2017-04-26 20:51:56,781][gdb1][DEBUG ] connected to host: gdb1 
[2017-04-26 20:51:56,782][gdb1][DEBUG ] detect platform information from remote host
[2017-04-26 20:51:56,798][gdb1][DEBUG ] detect machine type
[2017-04-26 20:51:56,801][gdb1][DEBUG ] find the location of an executable
[2017-04-26 20:51:56,802][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:51:56,802][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-04-26 20:51:56,803][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:51:56,805][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-04-26 20:51:56,805][gdb1][DEBUG ] find the location of an executable
[2017-04-26 20:51:56,807][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:51:56,927][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:51:56,943][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:51:56,950][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:51:56,966][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:51:56,974][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:51:56,974][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:51:56,974][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:51:56,989][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:51:56,990][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:51:56,990][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:51:56,990][gdb1][WARNING] Traceback (most recent call last):
[2017-04-26 20:51:56,990][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-04-26 20:51:56,990][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-04-26 20:51:56,990][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-04-26 20:51:56,990][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-04-26 20:51:56,990][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2029, in main
[2017-04-26 20:51:56,990][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2018, in prepare
[2017-04-26 20:51:56,990][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2049, in prepare_locked
[2017-04-26 20:51:56,990][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2816, in prepare
[2017-04-26 20:51:56,990][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2971, in prepare_device
[2017-04-26 20:51:56,990][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2878, in prepare_device
[2017-04-26 20:51:56,991][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2841, in sanity_checks
[2017-04-26 20:51:56,991][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-04-26 20:51:56,991][gdb1][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-04-26 20:51:56,994][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-04-26 20:51:56,994][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:51:56,994][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-04-26 20:52:05,605][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:52:05,606][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6bd3013908>
[2017-04-26 20:52:05,607][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:52:05,607][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:52:05,607][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6bd3269aa0>
[2017-04-26 20:52:05,607][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:52:05,607][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:52:05,607][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:52:05,607][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-04-26 20:52:05,845][gdb1][DEBUG ] connection detected need for sudo
[2017-04-26 20:52:06,081][gdb1][DEBUG ] connected to host: gdb1 
[2017-04-26 20:52:06,082][gdb1][DEBUG ] detect platform information from remote host
[2017-04-26 20:52:06,098][gdb1][DEBUG ] detect machine type
[2017-04-26 20:52:06,102][gdb1][DEBUG ] find the location of an executable
[2017-04-26 20:52:06,102][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:52:06,102][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-04-26 20:52:06,103][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:52:06,105][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-04-26 20:52:06,105][gdb1][DEBUG ] find the location of an executable
[2017-04-26 20:52:06,107][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:52:06,227][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:52:06,243][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:52:06,251][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:52:06,267][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:52:06,282][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:06,282][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:52:06,283][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:52:06,286][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:06,286][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:06,286][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:06,287][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:52:06,287][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:52:06,287][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-04-26 20:52:06,303][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-04-26 20:52:06,310][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-04-26 20:52:06,318][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-04-26 20:52:06,334][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:06,334][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-04-26 20:52:06,334][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:06,334][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-04-26 20:52:06,350][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-04-26 20:52:06,350][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-04-26 20:52:06,351][gdb1][WARNING] 10+0 records in
[2017-04-26 20:52:06,352][gdb1][WARNING] 10+0 records out
[2017-04-26 20:52:06,352][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00724707 s, 1.4 GB/s
[2017-04-26 20:52:06,352][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-04-26 20:52:06,466][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-04-26 20:52:06,466][gdb1][WARNING] 10+0 records in
[2017-04-26 20:52:06,466][gdb1][WARNING] 10+0 records out
[2017-04-26 20:52:06,466][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00513938 s, 2.0 GB/s
[2017-04-26 20:52:06,466][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-04-26 20:52:06,466][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-04-26 20:52:06,466][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-04-26 20:52:06,466][gdb1][WARNING] backup header from main header.
[2017-04-26 20:52:06,467][gdb1][WARNING] 
[2017-04-26 20:52:06,467][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-04-26 20:52:06,467][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-04-26 20:52:06,467][gdb1][WARNING] 
[2017-04-26 20:52:06,467][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-04-26 20:52:06,467][gdb1][WARNING] 
[2017-04-26 20:52:07,534][gdb1][DEBUG ] ****************************************************************************
[2017-04-26 20:52:07,534][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-04-26 20:52:07,534][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-04-26 20:52:07,535][gdb1][DEBUG ] ****************************************************************************
[2017-04-26 20:52:07,535][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-04-26 20:52:07,535][gdb1][DEBUG ] other utilities.
[2017-04-26 20:52:07,535][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-04-26 20:52:08,552][gdb1][DEBUG ] Creating new GPT entries.
[2017-04-26 20:52:08,552][gdb1][DEBUG ] The operation has completed successfully.
[2017-04-26 20:52:08,552][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-04-26 20:52:08,552][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:52:08,552][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:52:08,568][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:52:08,600][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:08,600][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:08,600][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-04-26 20:52:08,600][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:08,600][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-04-26 20:52:08,600][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:58d57f45-595b-41e7-b0e9-09ed172a7aec --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-04-26 20:52:09,617][gdb1][DEBUG ] Setting name!
[2017-04-26 20:52:09,617][gdb1][DEBUG ] partNum is 1
[2017-04-26 20:52:09,618][gdb1][DEBUG ] REALLY setting name!
[2017-04-26 20:52:09,618][gdb1][DEBUG ] The operation has completed successfully.
[2017-04-26 20:52:09,618][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:52:09,618][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:52:09,832][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:52:09,997][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:52:10,029][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:10,029][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:10,029][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:52:10,029][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/58d57f45-595b-41e7-b0e9-09ed172a7aec
[2017-04-26 20:52:10,029][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-04-26 20:52:11,046][gdb1][DEBUG ] The operation has completed successfully.
[2017-04-26 20:52:11,046][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:52:11,047][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:52:11,261][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:52:11,476][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:52:11,508][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/58d57f45-595b-41e7-b0e9-09ed172a7aec
[2017-04-26 20:52:11,508][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:11,508][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-04-26 20:52:11,508][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:11,508][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-04-26 20:52:11,508][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:11,508][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-04-26 20:52:11,508][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:7849b5cf-8184-4855-985d-df2d6c7e4f97 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-04-26 20:52:12,525][gdb1][DEBUG ] Setting name!
[2017-04-26 20:52:12,525][gdb1][DEBUG ] partNum is 0
[2017-04-26 20:52:12,525][gdb1][DEBUG ] REALLY setting name!
[2017-04-26 20:52:12,525][gdb1][DEBUG ] The operation has completed successfully.
[2017-04-26 20:52:12,525][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:52:12,526][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:52:12,740][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:52:12,904][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:52:12,969][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:12,969][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:12,969][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:52:12,969][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-04-26 20:52:12,969][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-04-26 20:52:13,585][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-04-26 20:52:13,585][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-04-26 20:52:13,585][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-04-26 20:52:13,585][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-04-26 20:52:13,585][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-04-26 20:52:13,585][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-04-26 20:52:13,585][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-04-26 20:52:13,586][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-04-26 20:52:13,586][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-04-26 20:52:13,586][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.HQY9mP with options noatime,inode64
[2017-04-26 20:52:13,586][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.HQY9mP
[2017-04-26 20:52:13,618][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.HQY9mP
[2017-04-26 20:52:13,618][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.HQY9mP/ceph_fsid.7642.tmp
[2017-04-26 20:52:13,618][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.HQY9mP/fsid.7642.tmp
[2017-04-26 20:52:13,618][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.HQY9mP/magic.7642.tmp
[2017-04-26 20:52:13,619][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.HQY9mP/journal_uuid.7642.tmp
[2017-04-26 20:52:13,622][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.HQY9mP/journal -> /dev/disk/by-partuuid/58d57f45-595b-41e7-b0e9-09ed172a7aec
[2017-04-26 20:52:13,623][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.HQY9mP
[2017-04-26 20:52:13,624][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.HQY9mP
[2017-04-26 20:52:13,624][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.HQY9mP
[2017-04-26 20:52:13,656][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:52:13,656][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-04-26 20:52:14,673][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-04-26 20:52:14,674][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-04-26 20:52:14,674][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-04-26 20:52:14,674][gdb1][DEBUG ] The operation has completed successfully.
[2017-04-26 20:52:14,674][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:52:14,674][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:52:14,674][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:52:14,838][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:52:14,854][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-04-26 20:52:14,864][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-04-26 20:52:19,936][gdb1][INFO  ] checking OSD status...
[2017-04-26 20:52:19,936][gdb1][DEBUG ] find the location of an executable
[2017-04-26 20:52:19,938][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-04-26 20:52:20,053][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-04-26 20:54:37,963][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:54:37,963][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-04-26 20:54:37,963][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:54:37,963][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:54:37,963][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:54:37,963][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-04-26 20:54:37,963][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f416caf6908>
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f416cd4caa0>
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:54:37,964][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:54:37,965][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-04-26 20:54:38,201][gdb0][DEBUG ] connection detected need for sudo
[2017-04-26 20:54:38,429][gdb0][DEBUG ] connected to host: gdb0 
[2017-04-26 20:54:38,429][gdb0][DEBUG ] detect platform information from remote host
[2017-04-26 20:54:38,445][gdb0][DEBUG ] detect machine type
[2017-04-26 20:54:38,449][gdb0][DEBUG ] find the location of an executable
[2017-04-26 20:54:38,450][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:54:38,450][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-04-26 20:54:38,451][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:54:38,453][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-04-26 20:54:38,453][gdb0][DEBUG ] create a keyring file
[2017-04-26 20:54:38,455][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-04-26 20:54:38,455][gdb0][DEBUG ] find the location of an executable
[2017-04-26 20:54:38,457][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:54:38,578][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:54:38,579][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:54:38,595][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:54:38,610][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:54:38,618][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:38,618][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:54:38,618][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:54:38,634][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:38,634][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:38,634][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:38,634][gdb0][WARNING] Traceback (most recent call last):
[2017-04-26 20:54:38,634][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-04-26 20:54:38,634][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-04-26 20:54:38,634][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-04-26 20:54:38,634][gdb0][WARNING]     main(sys.argv[1:])
[2017-04-26 20:54:38,635][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-04-26 20:54:38,635][gdb0][WARNING]     args.func(args)
[2017-04-26 20:54:38,635][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-04-26 20:54:38,635][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-04-26 20:54:38,635][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2018, in prepare
[2017-04-26 20:54:38,635][gdb0][WARNING]     self.prepare_locked()
[2017-04-26 20:54:38,635][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2049, in prepare_locked
[2017-04-26 20:54:38,635][gdb0][WARNING]     self.data.prepare(self.journal)
[2017-04-26 20:54:38,635][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2816, in prepare
[2017-04-26 20:54:38,635][gdb0][WARNING]     self.prepare_device(*to_prepare_list)
[2017-04-26 20:54:38,635][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2971, in prepare_device
[2017-04-26 20:54:38,635][gdb0][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2017-04-26 20:54:38,635][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2878, in prepare_device
[2017-04-26 20:54:38,635][gdb0][WARNING]     self.sanity_checks()
[2017-04-26 20:54:38,635][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2841, in sanity_checks
[2017-04-26 20:54:38,636][gdb0][WARNING]     check_partitions=not self.args.dmcrypt)
[2017-04-26 20:54:38,636][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-04-26 20:54:38,636][gdb0][WARNING]     raise Error('Device is mounted', partition)
[2017-04-26 20:54:38,636][gdb0][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-04-26 20:54:38,643][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-04-26 20:54:38,644][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:54:38,644][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-04-26 20:54:48,184][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 20:54:48,184][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-04-26 20:54:48,184][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 20:54:48,184][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 20:54:48,184][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-04-26 20:54:48,184][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdafa991908>
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fdafabe7aa0>
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 20:54:48,185][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-04-26 20:54:48,186][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-04-26 20:54:48,421][gdb0][DEBUG ] connection detected need for sudo
[2017-04-26 20:54:48,649][gdb0][DEBUG ] connected to host: gdb0 
[2017-04-26 20:54:48,649][gdb0][DEBUG ] detect platform information from remote host
[2017-04-26 20:54:48,665][gdb0][DEBUG ] detect machine type
[2017-04-26 20:54:48,669][gdb0][DEBUG ] find the location of an executable
[2017-04-26 20:54:48,670][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-04-26 20:54:48,671][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-04-26 20:54:48,671][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 20:54:48,673][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-04-26 20:54:48,673][gdb0][DEBUG ] find the location of an executable
[2017-04-26 20:54:48,675][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-04-26 20:54:48,796][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-04-26 20:54:48,799][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:54:48,815][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:54:48,831][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-04-26 20:54:48,838][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:48,839][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-04-26 20:54:48,839][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-04-26 20:54:48,855][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:48,855][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:48,855][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:48,855][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:54:48,856][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:54:48,856][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-04-26 20:54:48,863][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-04-26 20:54:48,871][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-04-26 20:54:48,878][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-04-26 20:54:48,886][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:48,886][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-04-26 20:54:48,886][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:48,886][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-04-26 20:54:48,918][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-04-26 20:54:48,918][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-04-26 20:54:48,918][gdb0][WARNING] 10+0 records in
[2017-04-26 20:54:48,918][gdb0][WARNING] 10+0 records out
[2017-04-26 20:54:48,918][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.0069562 s, 1.5 GB/s
[2017-04-26 20:54:48,919][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-04-26 20:54:49,083][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-04-26 20:54:49,083][gdb0][WARNING] 10+0 records in
[2017-04-26 20:54:49,083][gdb0][WARNING] 10+0 records out
[2017-04-26 20:54:49,083][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00546889 s, 1.9 GB/s
[2017-04-26 20:54:49,083][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-04-26 20:54:49,083][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-04-26 20:54:49,084][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-04-26 20:54:49,084][gdb0][WARNING] backup header from main header.
[2017-04-26 20:54:49,084][gdb0][WARNING] 
[2017-04-26 20:54:49,084][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-04-26 20:54:49,084][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-04-26 20:54:49,084][gdb0][WARNING] 
[2017-04-26 20:54:49,084][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-04-26 20:54:49,084][gdb0][WARNING] 
[2017-04-26 20:54:50,101][gdb0][DEBUG ] ****************************************************************************
[2017-04-26 20:54:50,101][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-04-26 20:54:50,101][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-04-26 20:54:50,101][gdb0][DEBUG ] ****************************************************************************
[2017-04-26 20:54:50,102][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-04-26 20:54:50,102][gdb0][DEBUG ] other utilities.
[2017-04-26 20:54:50,102][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-04-26 20:54:51,119][gdb0][DEBUG ] Creating new GPT entries.
[2017-04-26 20:54:51,119][gdb0][DEBUG ] The operation has completed successfully.
[2017-04-26 20:54:51,119][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-04-26 20:54:51,119][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:54:51,135][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:54:51,167][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:54:51,170][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:51,170][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:51,170][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-04-26 20:54:51,170][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:51,170][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-04-26 20:54:51,171][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:f77cddb5-572e-4ead-9315-fca53e3ae028 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-04-26 20:54:52,188][gdb0][DEBUG ] Setting name!
[2017-04-26 20:54:52,188][gdb0][DEBUG ] partNum is 1
[2017-04-26 20:54:52,188][gdb0][DEBUG ] REALLY setting name!
[2017-04-26 20:54:52,188][gdb0][DEBUG ] The operation has completed successfully.
[2017-04-26 20:54:52,188][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:54:52,188][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:54:52,403][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:54:52,517][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:54:52,549][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:52,549][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:52,549][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-04-26 20:54:52,549][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/f77cddb5-572e-4ead-9315-fca53e3ae028
[2017-04-26 20:54:52,549][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-04-26 20:54:53,616][gdb0][DEBUG ] The operation has completed successfully.
[2017-04-26 20:54:53,617][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:54:53,617][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:54:53,781][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:54:53,896][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:54:53,928][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/f77cddb5-572e-4ead-9315-fca53e3ae028
[2017-04-26 20:54:53,928][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:53,928][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-04-26 20:54:53,928][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:53,928][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-04-26 20:54:53,928][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:53,928][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-04-26 20:54:53,928][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:ea93afe8-a41b-49c8-a887-ae1c0365c01e --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-04-26 20:54:54,995][gdb0][DEBUG ] Setting name!
[2017-04-26 20:54:54,996][gdb0][DEBUG ] partNum is 0
[2017-04-26 20:54:54,996][gdb0][DEBUG ] REALLY setting name!
[2017-04-26 20:54:54,996][gdb0][DEBUG ] The operation has completed successfully.
[2017-04-26 20:54:54,996][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-04-26 20:54:54,996][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:54:55,160][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:54:55,375][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:54:55,407][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:55,407][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:55,407][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-04-26 20:54:55,407][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-04-26 20:54:55,407][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-04-26 20:54:56,124][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-04-26 20:54:56,124][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.CE9ZA6 with options noatime,inode64
[2017-04-26 20:54:56,124][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-04-26 20:54:56,124][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.CE9ZA6
[2017-04-26 20:54:56,124][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-04-26 20:54:56,124][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-04-26 20:54:56,124][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-04-26 20:54:56,124][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-04-26 20:54:56,125][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-04-26 20:54:56,125][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-04-26 20:54:56,125][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-04-26 20:54:56,125][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.CE9ZA6
[2017-04-26 20:54:56,125][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.CE9ZA6/ceph_fsid.14595.tmp
[2017-04-26 20:54:56,125][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.CE9ZA6/fsid.14595.tmp
[2017-04-26 20:54:56,125][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.CE9ZA6/magic.14595.tmp
[2017-04-26 20:54:56,128][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.CE9ZA6/journal_uuid.14595.tmp
[2017-04-26 20:54:56,129][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.CE9ZA6/journal -> /dev/disk/by-partuuid/f77cddb5-572e-4ead-9315-fca53e3ae028
[2017-04-26 20:54:56,129][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.CE9ZA6
[2017-04-26 20:54:56,132][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.CE9ZA6
[2017-04-26 20:54:56,132][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.CE9ZA6
[2017-04-26 20:54:56,164][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-04-26 20:54:56,164][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-04-26 20:54:57,181][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-04-26 20:54:57,181][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-04-26 20:54:57,181][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-04-26 20:54:57,182][gdb0][DEBUG ] The operation has completed successfully.
[2017-04-26 20:54:57,182][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-04-26 20:54:57,182][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:54:57,182][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-04-26 20:54:57,396][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-04-26 20:54:57,404][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-04-26 20:54:57,422][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-04-26 20:55:02,544][gdb0][INFO  ] checking OSD status...
[2017-04-26 20:55:02,544][gdb0][DEBUG ] find the location of an executable
[2017-04-26 20:55:02,546][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-04-26 20:55:02,712][gdb0][WARNING] there are 2 OSDs out
[2017-04-26 20:55:02,712][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-04-26 21:01:13,616][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 21:01:13,616][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-04-26 21:01:13,616][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 21:01:13,617][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 21:01:13,617][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 21:01:13,617][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-04-26 21:01:13,617][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 21:01:13,617][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fea88b62518>
[2017-04-26 21:01:13,617][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 21:01:13,617][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-04-26 21:01:13,617][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fea89479938>
[2017-04-26 21:01:13,617][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 21:01:13,617][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 21:01:13,617][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-04-26 21:01:13,858][gdb0][DEBUG ] connection detected need for sudo
[2017-04-26 21:01:14,089][gdb0][DEBUG ] connected to host: gdb0 
[2017-04-26 21:01:14,090][gdb0][DEBUG ] detect platform information from remote host
[2017-04-26 21:01:14,106][gdb0][DEBUG ] detect machine type
[2017-04-26 21:01:14,110][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 21:01:14,113][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-04-26 21:01:14,345][gdb1][DEBUG ] connection detected need for sudo
[2017-04-26 21:01:14,573][gdb1][DEBUG ] connected to host: gdb1 
[2017-04-26 21:01:14,573][gdb1][DEBUG ] detect platform information from remote host
[2017-04-26 21:01:14,589][gdb1][DEBUG ] detect machine type
[2017-04-26 21:01:14,593][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 21:01:14,595][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-04-26 21:01:14,610][gdb3][DEBUG ] connection detected need for sudo
[2017-04-26 21:01:14,625][gdb3][DEBUG ] connected to host: gdb3 
[2017-04-26 21:01:14,625][gdb3][DEBUG ] detect platform information from remote host
[2017-04-26 21:01:14,642][gdb3][DEBUG ] detect machine type
[2017-04-26 21:01:14,644][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 22:42:28,051][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1669340518>
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f1669c57938>
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-26 22:42:28,052][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-26 22:42:28,052][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-04-26 22:42:28,297][gdb0][DEBUG ] connection detected need for sudo
[2017-04-26 22:42:28,524][gdb0][DEBUG ] connected to host: gdb0 
[2017-04-26 22:42:28,525][gdb0][DEBUG ] detect platform information from remote host
[2017-04-26 22:42:28,540][gdb0][DEBUG ] detect machine type
[2017-04-26 22:42:28,544][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 22:42:28,547][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-04-26 22:42:28,785][gdb1][DEBUG ] connection detected need for sudo
[2017-04-26 22:42:29,016][gdb1][DEBUG ] connected to host: gdb1 
[2017-04-26 22:42:29,017][gdb1][DEBUG ] detect platform information from remote host
[2017-04-26 22:42:29,033][gdb1][DEBUG ] detect machine type
[2017-04-26 22:42:29,036][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-26 22:42:29,038][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-04-26 22:42:29,054][gdb3][DEBUG ] connection detected need for sudo
[2017-04-26 22:42:29,067][gdb3][DEBUG ] connected to host: gdb3 
[2017-04-26 22:42:29,068][gdb3][DEBUG ] detect platform information from remote host
[2017-04-26 22:42:29,084][gdb3][DEBUG ] detect machine type
[2017-04-26 22:42:29,086][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-29 18:54:20,707][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-29 18:54:20,707][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-04-29 18:54:20,707][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-29 18:54:20,707][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-29 18:54:20,707][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-29 18:54:20,707][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-04-29 18:54:20,707][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-29 18:54:20,707][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f769bbc4518>
[2017-04-29 18:54:20,707][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-29 18:54:20,707][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-04-29 18:54:20,708][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f769c4db938>
[2017-04-29 18:54:20,708][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-29 18:54:20,708][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-29 18:54:20,708][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-04-29 18:54:20,953][gdb0][DEBUG ] connection detected need for sudo
[2017-04-29 18:54:21,184][gdb0][DEBUG ] connected to host: gdb0 
[2017-04-29 18:54:21,185][gdb0][DEBUG ] detect platform information from remote host
[2017-04-29 18:54:21,201][gdb0][DEBUG ] detect machine type
[2017-04-29 18:54:21,205][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-29 18:54:21,207][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-04-29 18:54:21,207][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-04-29 18:54:21,463][gdb1][DEBUG ] connection detected need for sudo
[2017-04-29 18:54:21,698][gdb1][DEBUG ] connected to host: gdb1 
[2017-04-29 18:54:21,698][gdb1][DEBUG ] detect platform information from remote host
[2017-04-29 18:54:21,714][gdb1][DEBUG ] detect machine type
[2017-04-29 18:54:21,718][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-29 18:54:21,720][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-04-29 18:54:21,720][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-04-29 18:54:21,735][gdb3][DEBUG ] connection detected need for sudo
[2017-04-29 18:54:21,749][gdb3][DEBUG ] connected to host: gdb3 
[2017-04-29 18:54:21,749][gdb3][DEBUG ] detect platform information from remote host
[2017-04-29 18:54:21,766][gdb3][DEBUG ] detect machine type
[2017-04-29 18:54:21,768][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-29 18:54:21,770][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-04-29 18:54:21,770][ceph_deploy][ERROR ] GenericError: Failed to configure 3 admin hosts

[2017-04-29 18:54:30,695][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0 gdb1 gdb3
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ]  username                      : None
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fee26433518>
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fee26d4a938>
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-04-29 18:54:30,696][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-04-29 18:54:30,696][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-04-29 18:54:30,929][gdb0][DEBUG ] connection detected need for sudo
[2017-04-29 18:54:31,161][gdb0][DEBUG ] connected to host: gdb0 
[2017-04-29 18:54:31,161][gdb0][DEBUG ] detect platform information from remote host
[2017-04-29 18:54:31,177][gdb0][DEBUG ] detect machine type
[2017-04-29 18:54:31,181][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-29 18:54:31,184][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-04-29 18:54:31,432][gdb1][DEBUG ] connection detected need for sudo
[2017-04-29 18:54:31,665][gdb1][DEBUG ] connected to host: gdb1 
[2017-04-29 18:54:31,665][gdb1][DEBUG ] detect platform information from remote host
[2017-04-29 18:54:31,682][gdb1][DEBUG ] detect machine type
[2017-04-29 18:54:31,686][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-04-29 18:54:31,689][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-04-29 18:54:31,703][gdb3][DEBUG ] connection detected need for sudo
[2017-04-29 18:54:31,716][gdb3][DEBUG ] connected to host: gdb3 
[2017-04-29 18:54:31,717][gdb3][DEBUG ] detect platform information from remote host
[2017-04-29 18:54:31,732][gdb3][DEBUG ] detect machine type
[2017-04-29 18:54:31,734][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:20:05,071][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-01 00:20:05,071][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1 gdb0 gdb3
[2017-05-01 00:20:05,071][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-01 00:20:05,071][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-01 00:20:05,071][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-01 00:20:05,071][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-01 00:20:05,071][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-01 00:20:05,071][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9705336518>
[2017-05-01 00:20:05,072][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-01 00:20:05,072][ceph_deploy.cli][INFO  ]  client                        : ['gdb1', 'gdb0', 'gdb3']
[2017-05-01 00:20:05,072][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f9705c4d938>
[2017-05-01 00:20:05,072][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-01 00:20:05,072][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-01 00:20:05,072][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-01 00:20:05,317][gdb1][DEBUG ] connection detected need for sudo
[2017-05-01 00:20:05,553][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-01 00:20:05,553][gdb1][DEBUG ] detect platform information from remote host
[2017-05-01 00:20:05,569][gdb1][DEBUG ] detect machine type
[2017-05-01 00:20:05,573][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:20:05,575][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-01 00:20:05,575][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-01 00:20:05,814][gdb0][DEBUG ] connection detected need for sudo
[2017-05-01 00:20:06,041][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-01 00:20:06,041][gdb0][DEBUG ] detect platform information from remote host
[2017-05-01 00:20:06,057][gdb0][DEBUG ] detect machine type
[2017-05-01 00:20:06,061][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:20:06,064][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-01 00:20:06,064][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-01 00:20:06,079][gdb3][DEBUG ] connection detected need for sudo
[2017-05-01 00:20:06,092][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-01 00:20:06,093][gdb3][DEBUG ] detect platform information from remote host
[2017-05-01 00:20:06,108][gdb3][DEBUG ] detect machine type
[2017-05-01 00:20:06,110][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:20:06,112][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-01 00:20:06,112][ceph_deploy][ERROR ] GenericError: Failed to configure 3 admin hosts

[2017-05-01 00:20:25,644][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-01 00:20:25,644][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1 gdb0 gdb3
[2017-05-01 00:20:25,644][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-01 00:20:25,644][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-01 00:20:25,645][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-01 00:20:25,645][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-01 00:20:25,645][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-01 00:20:25,645][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5d32452518>
[2017-05-01 00:20:25,645][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-01 00:20:25,645][ceph_deploy.cli][INFO  ]  client                        : ['gdb1', 'gdb0', 'gdb3']
[2017-05-01 00:20:25,645][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f5d32d69938>
[2017-05-01 00:20:25,645][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-01 00:20:25,645][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-01 00:20:25,645][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-01 00:20:25,889][gdb1][DEBUG ] connection detected need for sudo
[2017-05-01 00:20:26,117][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-01 00:20:26,117][gdb1][DEBUG ] detect platform information from remote host
[2017-05-01 00:20:26,133][gdb1][DEBUG ] detect machine type
[2017-05-01 00:20:26,137][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:20:26,139][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-01 00:20:26,369][gdb0][DEBUG ] connection detected need for sudo
[2017-05-01 00:20:26,600][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-01 00:20:26,601][gdb0][DEBUG ] detect platform information from remote host
[2017-05-01 00:20:26,617][gdb0][DEBUG ] detect machine type
[2017-05-01 00:20:26,621][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:20:26,624][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-01 00:20:26,639][gdb3][DEBUG ] connection detected need for sudo
[2017-05-01 00:20:26,653][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-01 00:20:26,654][gdb3][DEBUG ] detect platform information from remote host
[2017-05-01 00:20:26,669][gdb3][DEBUG ] detect machine type
[2017-05-01 00:20:26,672][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:40:56,355][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-01 00:40:56,356][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1 gdb0 gdb3
[2017-05-01 00:40:56,356][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-01 00:40:56,356][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-01 00:40:56,356][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-01 00:40:56,356][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-01 00:40:56,356][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-01 00:40:56,356][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f916dd66518>
[2017-05-01 00:40:56,356][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-01 00:40:56,356][ceph_deploy.cli][INFO  ]  client                        : ['gdb1', 'gdb0', 'gdb3']
[2017-05-01 00:40:56,356][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f916e67d938>
[2017-05-01 00:40:56,356][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-01 00:40:56,357][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-01 00:40:56,357][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-01 00:40:56,594][gdb1][DEBUG ] connection detected need for sudo
[2017-05-01 00:40:56,816][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-01 00:40:56,817][gdb1][DEBUG ] detect platform information from remote host
[2017-05-01 00:40:56,833][gdb1][DEBUG ] detect machine type
[2017-05-01 00:40:56,837][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:40:56,839][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-01 00:40:56,839][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-01 00:40:57,070][gdb0][DEBUG ] connection detected need for sudo
[2017-05-01 00:40:57,301][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-01 00:40:57,302][gdb0][DEBUG ] detect platform information from remote host
[2017-05-01 00:40:57,317][gdb0][DEBUG ] detect machine type
[2017-05-01 00:40:57,321][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:40:57,323][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-01 00:40:57,323][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-01 00:40:57,338][gdb3][DEBUG ] connection detected need for sudo
[2017-05-01 00:40:57,352][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-01 00:40:57,352][gdb3][DEBUG ] detect platform information from remote host
[2017-05-01 00:40:57,369][gdb3][DEBUG ] detect machine type
[2017-05-01 00:40:57,371][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:40:57,372][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-01 00:40:57,373][ceph_deploy][ERROR ] GenericError: Failed to configure 3 admin hosts

[2017-05-01 00:41:04,145][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1 gdb0 gdb3
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5308954518>
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ]  client                        : ['gdb1', 'gdb0', 'gdb3']
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f530926b938>
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-01 00:41:04,146][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-01 00:41:04,146][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-01 00:41:04,390][gdb1][DEBUG ] connection detected need for sudo
[2017-05-01 00:41:04,617][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-01 00:41:04,617][gdb1][DEBUG ] detect platform information from remote host
[2017-05-01 00:41:04,633][gdb1][DEBUG ] detect machine type
[2017-05-01 00:41:04,637][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:41:04,639][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-01 00:41:04,869][gdb0][DEBUG ] connection detected need for sudo
[2017-05-01 00:41:05,101][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-01 00:41:05,102][gdb0][DEBUG ] detect platform information from remote host
[2017-05-01 00:41:05,117][gdb0][DEBUG ] detect machine type
[2017-05-01 00:41:05,121][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:41:05,124][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-01 00:41:05,139][gdb3][DEBUG ] connection detected need for sudo
[2017-05-01 00:41:05,153][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-01 00:41:05,154][gdb3][DEBUG ] detect platform information from remote host
[2017-05-01 00:41:05,169][gdb3][DEBUG ] detect machine type
[2017-05-01 00:41:05,171][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:42:48,058][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-01 00:42:48,058][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-01 00:42:48,058][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-01 00:42:48,058][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-01 00:42:48,058][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-01 00:42:48,058][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-01 00:42:48,058][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-01 00:42:48,058][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-01 00:42:48,058][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f23ef51be60>
[2017-05-01 00:42:48,058][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-01 00:42:48,058][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f23ef4f0b18>
[2017-05-01 00:42:48,058][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-01 00:42:48,059][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-01 00:42:48,059][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-01 00:42:48,059][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-01 00:42:48,059][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-01 00:42:48,085][gdb3][DEBUG ] connection detected need for sudo
[2017-05-01 00:42:48,099][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-01 00:42:48,100][gdb3][DEBUG ] detect platform information from remote host
[2017-05-01 00:42:48,116][gdb3][DEBUG ] detect machine type
[2017-05-01 00:42:48,119][gdb3][DEBUG ] find the location of an executable
[2017-05-01 00:42:48,119][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-01 00:42:48,119][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-01 00:42:48,119][gdb3][DEBUG ] get remote short hostname
[2017-05-01 00:42:48,119][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-01 00:42:48,120][gdb3][DEBUG ] get remote short hostname
[2017-05-01 00:42:48,120][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-01 00:42:48,120][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-01 00:42:48,122][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-01 00:42:48,122][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-01 00:42:48,122][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-01 00:42:48,123][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-01 00:42:48,124][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-01 00:42:48,195][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-01 00:42:48,262][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-01 00:42:50,276][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-01 00:42:50,341][gdb3][DEBUG ] ********************************************************************************
[2017-05-01 00:42:50,341][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-01 00:42:50,342][gdb3][DEBUG ] {
[2017-05-01 00:42:50,342][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-01 00:42:50,342][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-01 00:42:50,342][gdb3][DEBUG ]   "features": {
[2017-05-01 00:42:50,342][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-01 00:42:50,342][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-01 00:42:50,342][gdb3][DEBUG ]       "kraken", 
[2017-05-01 00:42:50,342][gdb3][DEBUG ]       "luminous"
[2017-05-01 00:42:50,342][gdb3][DEBUG ]     ], 
[2017-05-01 00:42:50,342][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-01 00:42:50,342][gdb3][DEBUG ]     "required_mon": [
[2017-05-01 00:42:50,342][gdb3][DEBUG ]       "kraken", 
[2017-05-01 00:42:50,342][gdb3][DEBUG ]       "luminous"
[2017-05-01 00:42:50,342][gdb3][DEBUG ]     ]
[2017-05-01 00:42:50,342][gdb3][DEBUG ]   }, 
[2017-05-01 00:42:50,343][gdb3][DEBUG ]   "monmap": {
[2017-05-01 00:42:50,343][gdb3][DEBUG ]     "created": "2017-04-26 20:27:26.031008", 
[2017-05-01 00:42:50,343][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-01 00:42:50,343][gdb3][DEBUG ]     "features": {
[2017-05-01 00:42:50,343][gdb3][DEBUG ]       "optional": [], 
[2017-05-01 00:42:50,343][gdb3][DEBUG ]       "persistent": [
[2017-05-01 00:42:50,343][gdb3][DEBUG ]         "kraken", 
[2017-05-01 00:42:50,343][gdb3][DEBUG ]         "luminous"
[2017-05-01 00:42:50,343][gdb3][DEBUG ]       ]
[2017-05-01 00:42:50,343][gdb3][DEBUG ]     }, 
[2017-05-01 00:42:50,343][gdb3][DEBUG ]     "fsid": "5f73f0a8-714e-4c1f-8410-722fc431c29c", 
[2017-05-01 00:42:50,343][gdb3][DEBUG ]     "modified": "2017-04-26 20:27:26.318310", 
[2017-05-01 00:42:50,343][gdb3][DEBUG ]     "mons": [
[2017-05-01 00:42:50,343][gdb3][DEBUG ]       {
[2017-05-01 00:42:50,343][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-01 00:42:50,343][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-01 00:42:50,343][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-01 00:42:50,344][gdb3][DEBUG ]         "rank": 0
[2017-05-01 00:42:50,344][gdb3][DEBUG ]       }
[2017-05-01 00:42:50,344][gdb3][DEBUG ]     ]
[2017-05-01 00:42:50,344][gdb3][DEBUG ]   }, 
[2017-05-01 00:42:50,344][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-01 00:42:50,344][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-01 00:42:50,344][gdb3][DEBUG ]   "quorum": [
[2017-05-01 00:42:50,344][gdb3][DEBUG ]     0
[2017-05-01 00:42:50,344][gdb3][DEBUG ]   ], 
[2017-05-01 00:42:50,344][gdb3][DEBUG ]   "rank": 0, 
[2017-05-01 00:42:50,344][gdb3][DEBUG ]   "state": "leader", 
[2017-05-01 00:42:50,344][gdb3][DEBUG ]   "sync_provider": []
[2017-05-01 00:42:50,344][gdb3][DEBUG ] }
[2017-05-01 00:42:50,344][gdb3][DEBUG ] ********************************************************************************
[2017-05-01 00:42:50,344][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-01 00:42:50,345][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-01 00:42:50,410][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-01 00:42:50,428][gdb3][DEBUG ] connection detected need for sudo
[2017-05-01 00:42:50,441][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-01 00:42:50,442][gdb3][DEBUG ] detect platform information from remote host
[2017-05-01 00:42:50,458][gdb3][DEBUG ] detect machine type
[2017-05-01 00:42:50,460][gdb3][DEBUG ] find the location of an executable
[2017-05-01 00:42:50,461][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-01 00:42:50,526][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-01 00:42:50,526][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-01 00:42:50,526][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-01 00:42:50,528][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpsNDAMt
[2017-05-01 00:42:50,542][gdb3][DEBUG ] connection detected need for sudo
[2017-05-01 00:42:50,555][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-01 00:42:50,556][gdb3][DEBUG ] detect platform information from remote host
[2017-05-01 00:42:50,571][gdb3][DEBUG ] detect machine type
[2017-05-01 00:42:50,574][gdb3][DEBUG ] get remote short hostname
[2017-05-01 00:42:50,574][gdb3][DEBUG ] fetch remote file
[2017-05-01 00:42:50,575][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-01 00:42:50,641][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-01 00:42:50,807][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-01 00:42:50,973][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-01 00:42:51,140][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-01 00:42:51,306][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-01 00:42:51,471][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-05-01 00:42:51,471][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-05-01 00:42:51,472][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-05-01 00:42:51,472][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-01 00:42:51,472][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-05-01 00:42:51,472][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-05-01 00:42:51,472][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpsNDAMt
[2017-05-05 21:50:15,675][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-05 21:50:15,676][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-05 21:50:15,676][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-05 21:50:15,676][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-05 21:50:15,677][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-05 21:50:15,677][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-05 21:50:15,677][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-05 21:50:15,677][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-05 21:50:15,677][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fae103d5e60>
[2017-05-05 21:50:15,677][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-05 21:50:15,677][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fae103aab18>
[2017-05-05 21:50:15,677][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-05 21:50:15,677][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-05 21:50:15,677][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-05 21:50:15,678][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-05 21:50:15,678][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-05 21:50:15,704][gdb3][DEBUG ] connection detected need for sudo
[2017-05-05 21:50:15,719][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-05 21:50:15,719][gdb3][DEBUG ] detect platform information from remote host
[2017-05-05 21:50:15,736][gdb3][DEBUG ] detect machine type
[2017-05-05 21:50:15,738][gdb3][DEBUG ] find the location of an executable
[2017-05-05 21:50:15,738][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-05 21:50:15,738][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-05 21:50:15,739][gdb3][DEBUG ] get remote short hostname
[2017-05-05 21:50:15,739][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-05 21:50:15,739][gdb3][DEBUG ] get remote short hostname
[2017-05-05 21:50:15,739][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-05 21:50:15,740][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-05 21:50:15,741][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-05 21:50:15,742][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-05 21:50:15,742][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-05 21:50:15,742][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-05 21:50:15,742][gdb3][DEBUG ] create the monitor keyring file
[2017-05-05 21:50:15,744][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-05 21:50:15,782][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-05 21:50:15,782][gdb3][DEBUG ] ceph-mon: set fsid to 5f73f0a8-714e-4c1f-8410-722fc431c29c
[2017-05-05 21:50:15,790][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-05 21:50:15,790][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-05 21:50:15,790][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-05 21:50:15,791][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-05 21:50:15,792][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-05 21:50:15,863][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-05 21:50:15,932][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-05 21:50:17,971][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-05 21:50:18,036][gdb3][DEBUG ] ********************************************************************************
[2017-05-05 21:50:18,036][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-05 21:50:18,036][gdb3][DEBUG ] {
[2017-05-05 21:50:18,036][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-05 21:50:18,036][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-05 21:50:18,036][gdb3][DEBUG ]   "features": {
[2017-05-05 21:50:18,036][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-05 21:50:18,037][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-05 21:50:18,037][gdb3][DEBUG ]       "kraken", 
[2017-05-05 21:50:18,037][gdb3][DEBUG ]       "luminous"
[2017-05-05 21:50:18,037][gdb3][DEBUG ]     ], 
[2017-05-05 21:50:18,037][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-05 21:50:18,037][gdb3][DEBUG ]     "required_mon": [
[2017-05-05 21:50:18,037][gdb3][DEBUG ]       "kraken", 
[2017-05-05 21:50:18,037][gdb3][DEBUG ]       "luminous"
[2017-05-05 21:50:18,037][gdb3][DEBUG ]     ]
[2017-05-05 21:50:18,037][gdb3][DEBUG ]   }, 
[2017-05-05 21:50:18,037][gdb3][DEBUG ]   "monmap": {
[2017-05-05 21:50:18,037][gdb3][DEBUG ]     "created": "2017-05-05 21:50:15.767309", 
[2017-05-05 21:50:18,037][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-05 21:50:18,037][gdb3][DEBUG ]     "features": {
[2017-05-05 21:50:18,037][gdb3][DEBUG ]       "optional": [], 
[2017-05-05 21:50:18,038][gdb3][DEBUG ]       "persistent": [
[2017-05-05 21:50:18,038][gdb3][DEBUG ]         "kraken", 
[2017-05-05 21:50:18,038][gdb3][DEBUG ]         "luminous"
[2017-05-05 21:50:18,038][gdb3][DEBUG ]       ]
[2017-05-05 21:50:18,038][gdb3][DEBUG ]     }, 
[2017-05-05 21:50:18,038][gdb3][DEBUG ]     "fsid": "5f73f0a8-714e-4c1f-8410-722fc431c29c", 
[2017-05-05 21:50:18,038][gdb3][DEBUG ]     "modified": "2017-05-05 21:50:16.010217", 
[2017-05-05 21:50:18,038][gdb3][DEBUG ]     "mons": [
[2017-05-05 21:50:18,038][gdb3][DEBUG ]       {
[2017-05-05 21:50:18,038][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-05 21:50:18,038][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-05 21:50:18,038][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-05 21:50:18,038][gdb3][DEBUG ]         "rank": 0
[2017-05-05 21:50:18,038][gdb3][DEBUG ]       }
[2017-05-05 21:50:18,038][gdb3][DEBUG ]     ]
[2017-05-05 21:50:18,038][gdb3][DEBUG ]   }, 
[2017-05-05 21:50:18,039][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-05 21:50:18,039][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-05 21:50:18,039][gdb3][DEBUG ]   "quorum": [
[2017-05-05 21:50:18,039][gdb3][DEBUG ]     0
[2017-05-05 21:50:18,039][gdb3][DEBUG ]   ], 
[2017-05-05 21:50:18,039][gdb3][DEBUG ]   "rank": 0, 
[2017-05-05 21:50:18,039][gdb3][DEBUG ]   "state": "leader", 
[2017-05-05 21:50:18,039][gdb3][DEBUG ]   "sync_provider": []
[2017-05-05 21:50:18,039][gdb3][DEBUG ] }
[2017-05-05 21:50:18,039][gdb3][DEBUG ] ********************************************************************************
[2017-05-05 21:50:18,039][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-05 21:50:18,040][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-05 21:50:18,105][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-05 21:50:18,124][gdb3][DEBUG ] connection detected need for sudo
[2017-05-05 21:50:18,138][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-05 21:50:18,139][gdb3][DEBUG ] detect platform information from remote host
[2017-05-05 21:50:18,156][gdb3][DEBUG ] detect machine type
[2017-05-05 21:50:18,158][gdb3][DEBUG ] find the location of an executable
[2017-05-05 21:50:18,159][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-05 21:50:18,224][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-05 21:50:18,225][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-05 21:50:18,225][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-05 21:50:18,227][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpemlGyc
[2017-05-05 21:50:18,242][gdb3][DEBUG ] connection detected need for sudo
[2017-05-05 21:50:18,257][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-05 21:50:18,257][gdb3][DEBUG ] detect platform information from remote host
[2017-05-05 21:50:18,274][gdb3][DEBUG ] detect machine type
[2017-05-05 21:50:18,277][gdb3][DEBUG ] get remote short hostname
[2017-05-05 21:50:18,277][gdb3][DEBUG ] fetch remote file
[2017-05-05 21:50:18,278][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-05 21:50:18,344][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-05 21:50:18,510][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-05 21:50:18,676][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-05 21:50:18,843][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-05 21:50:19,009][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-05 21:50:19,175][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-05 21:50:19,341][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-05 21:50:19,508][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-05 21:50:19,674][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-05 21:50:19,840][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-05 21:50:20,006][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.client.admin.keyring' and backing up old key as 'ceph.client.admin.keyring-20170505215020'
[2017-05-05 21:50:20,007][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mds.keyring' and backing up old key as 'ceph.bootstrap-mds.keyring-20170505215020'
[2017-05-05 21:50:20,007][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170505215020'
[2017-05-05 21:50:20,008][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-05 21:50:20,008][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-osd.keyring' and backing up old key as 'ceph.bootstrap-osd.keyring-20170505215020'
[2017-05-05 21:50:20,009][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-rgw.keyring' and backing up old key as 'ceph.bootstrap-rgw.keyring-20170505215020'
[2017-05-05 21:50:20,009][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpemlGyc
[2017-05-05 21:50:43,061][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-05 21:50:43,061][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-05 21:50:43,061][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-05 21:50:43,062][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-05 21:50:43,062][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-05 21:50:43,062][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-05 21:50:43,062][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-05 21:50:43,062][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-05 21:50:43,062][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff29c3e2e60>
[2017-05-05 21:50:43,062][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-05 21:50:43,062][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7ff29c3b7b18>
[2017-05-05 21:50:43,062][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-05 21:50:43,062][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-05 21:50:43,062][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-05 21:50:43,063][ceph_deploy.mon][WARNING] keyring (ceph.mon.keyring) not found, creating a new one
[2017-05-05 21:50:43,063][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-05 21:50:43,063][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-05 21:50:43,063][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-05 21:50:43,064][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-05 21:50:43,090][gdb3][DEBUG ] connection detected need for sudo
[2017-05-05 21:50:43,105][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-05 21:50:43,106][gdb3][DEBUG ] detect platform information from remote host
[2017-05-05 21:50:43,123][gdb3][DEBUG ] detect machine type
[2017-05-05 21:50:43,125][gdb3][DEBUG ] find the location of an executable
[2017-05-05 21:50:43,126][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-05 21:50:43,126][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-05 21:50:43,126][gdb3][DEBUG ] get remote short hostname
[2017-05-05 21:50:43,126][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-05 21:50:43,126][gdb3][DEBUG ] get remote short hostname
[2017-05-05 21:50:43,127][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-05 21:50:43,127][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-05 21:50:43,129][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-05 21:50:43,129][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-05 21:50:43,129][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-05 21:50:43,130][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-05 21:50:43,131][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-05 21:50:43,203][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-05 21:50:43,275][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-05 21:50:45,288][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-05 21:50:45,353][gdb3][DEBUG ] ********************************************************************************
[2017-05-05 21:50:45,354][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-05 21:50:45,354][gdb3][DEBUG ] {
[2017-05-05 21:50:45,354][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-05 21:50:45,354][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-05 21:50:45,354][gdb3][DEBUG ]   "features": {
[2017-05-05 21:50:45,354][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-05 21:50:45,354][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-05 21:50:45,354][gdb3][DEBUG ]       "kraken", 
[2017-05-05 21:50:45,354][gdb3][DEBUG ]       "luminous"
[2017-05-05 21:50:45,354][gdb3][DEBUG ]     ], 
[2017-05-05 21:50:45,355][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-05 21:50:45,355][gdb3][DEBUG ]     "required_mon": [
[2017-05-05 21:50:45,355][gdb3][DEBUG ]       "kraken", 
[2017-05-05 21:50:45,355][gdb3][DEBUG ]       "luminous"
[2017-05-05 21:50:45,355][gdb3][DEBUG ]     ]
[2017-05-05 21:50:45,355][gdb3][DEBUG ]   }, 
[2017-05-05 21:50:45,355][gdb3][DEBUG ]   "monmap": {
[2017-05-05 21:50:45,355][gdb3][DEBUG ]     "created": "2017-05-05 21:50:15.767309", 
[2017-05-05 21:50:45,355][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-05 21:50:45,355][gdb3][DEBUG ]     "features": {
[2017-05-05 21:50:45,355][gdb3][DEBUG ]       "optional": [], 
[2017-05-05 21:50:45,355][gdb3][DEBUG ]       "persistent": [
[2017-05-05 21:50:45,355][gdb3][DEBUG ]         "kraken", 
[2017-05-05 21:50:45,355][gdb3][DEBUG ]         "luminous"
[2017-05-05 21:50:45,355][gdb3][DEBUG ]       ]
[2017-05-05 21:50:45,355][gdb3][DEBUG ]     }, 
[2017-05-05 21:50:45,355][gdb3][DEBUG ]     "fsid": "5f73f0a8-714e-4c1f-8410-722fc431c29c", 
[2017-05-05 21:50:45,356][gdb3][DEBUG ]     "modified": "2017-05-05 21:50:16.010217", 
[2017-05-05 21:50:45,356][gdb3][DEBUG ]     "mons": [
[2017-05-05 21:50:45,356][gdb3][DEBUG ]       {
[2017-05-05 21:50:45,356][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-05 21:50:45,356][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-05 21:50:45,356][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-05 21:50:45,356][gdb3][DEBUG ]         "rank": 0
[2017-05-05 21:50:45,356][gdb3][DEBUG ]       }
[2017-05-05 21:50:45,356][gdb3][DEBUG ]     ]
[2017-05-05 21:50:45,356][gdb3][DEBUG ]   }, 
[2017-05-05 21:50:45,356][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-05 21:50:45,356][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-05 21:50:45,356][gdb3][DEBUG ]   "quorum": [
[2017-05-05 21:50:45,356][gdb3][DEBUG ]     0
[2017-05-05 21:50:45,356][gdb3][DEBUG ]   ], 
[2017-05-05 21:50:45,356][gdb3][DEBUG ]   "rank": 0, 
[2017-05-05 21:50:45,357][gdb3][DEBUG ]   "state": "leader", 
[2017-05-05 21:50:45,357][gdb3][DEBUG ]   "sync_provider": []
[2017-05-05 21:50:45,357][gdb3][DEBUG ] }
[2017-05-05 21:50:45,357][gdb3][DEBUG ] ********************************************************************************
[2017-05-05 21:50:45,357][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-05 21:50:45,358][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-05 21:50:45,423][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-05 21:50:45,438][gdb3][DEBUG ] connection detected need for sudo
[2017-05-05 21:50:45,452][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-05 21:50:45,453][gdb3][DEBUG ] detect platform information from remote host
[2017-05-05 21:50:45,469][gdb3][DEBUG ] detect machine type
[2017-05-05 21:50:45,472][gdb3][DEBUG ] find the location of an executable
[2017-05-05 21:50:45,473][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-05 21:50:45,538][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-05 21:50:45,538][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-05 21:50:45,538][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-05 21:50:45,540][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpXluFtR
[2017-05-05 21:50:45,555][gdb3][DEBUG ] connection detected need for sudo
[2017-05-05 21:50:45,569][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-05 21:50:45,570][gdb3][DEBUG ] detect platform information from remote host
[2017-05-05 21:50:45,586][gdb3][DEBUG ] detect machine type
[2017-05-05 21:50:45,589][gdb3][DEBUG ] get remote short hostname
[2017-05-05 21:50:45,589][gdb3][DEBUG ] fetch remote file
[2017-05-05 21:50:45,591][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-05 21:50:45,657][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-05 21:50:45,823][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-05 21:50:45,989][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-05 21:50:46,155][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-05 21:50:46,322][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-05 21:50:46,487][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-05 21:50:46,487][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-05 21:50:46,487][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring
[2017-05-05 21:50:46,488][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.mon.keyring' and backing up old key as 'ceph.mon.keyring-20170505215046'
[2017-05-05 21:50:46,488][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-05 21:50:46,488][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-05 21:50:46,488][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpXluFtR
[2017-05-05 21:51:34,572][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-05 21:51:34,572][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-05 21:51:34,572][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-05 21:51:34,572][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-05 21:51:34,572][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-05 21:51:34,572][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-05 21:51:34,573][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-05 21:51:34,573][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3fd6c84518>
[2017-05-05 21:51:34,573][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-05 21:51:34,573][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-05 21:51:34,573][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f3fd759b938>
[2017-05-05 21:51:34,573][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-05 21:51:34,573][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-05 21:51:34,573][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-05 21:51:34,599][gdb3][DEBUG ] connection detected need for sudo
[2017-05-05 21:51:34,613][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-05 21:51:34,614][gdb3][DEBUG ] detect platform information from remote host
[2017-05-05 21:51:34,630][gdb3][DEBUG ] detect machine type
[2017-05-05 21:51:34,633][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-05 21:52:41,945][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-05 21:52:41,945][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-05 21:52:41,945][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-05 21:52:41,945][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-05 21:52:41,945][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-05 21:52:41,945][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-05 21:52:41,945][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-05 21:52:41,945][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-05 21:52:41,945][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb5a30f5908>
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb5a334baa0>
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-05 21:52:41,946][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-05 21:52:41,947][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-05 21:52:42,186][gdb1][DEBUG ] connection detected need for sudo
[2017-05-05 21:52:42,417][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-05 21:52:42,417][gdb1][DEBUG ] detect platform information from remote host
[2017-05-05 21:52:42,434][gdb1][DEBUG ] detect machine type
[2017-05-05 21:52:42,438][gdb1][DEBUG ] find the location of an executable
[2017-05-05 21:52:42,439][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-05 21:52:42,439][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-05 21:52:42,439][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-05 21:52:42,442][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-05 21:52:42,442][gdb1][DEBUG ] create a keyring file
[2017-05-05 21:52:42,444][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-05 21:52:42,444][gdb1][DEBUG ] find the location of an executable
[2017-05-05 21:52:42,446][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-05 21:52:42,617][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-05 21:52:42,617][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-05 21:52:42,617][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-05 21:52:42,633][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-05 21:52:42,640][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:42,640][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-05 21:52:42,640][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-05 21:52:42,656][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:42,656][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:42,656][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:42,656][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-05 21:52:42,656][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-05 21:52:42,656][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-05 21:52:42,771][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-05 21:52:42,771][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-05 21:52:42,771][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-05 21:52:42,771][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:42,771][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-05 21:52:42,771][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:42,771][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-05 21:52:42,779][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-05 21:52:42,779][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-05 21:52:42,786][gdb1][WARNING] 10+0 records in
[2017-05-05 21:52:42,786][gdb1][WARNING] 10+0 records out
[2017-05-05 21:52:42,787][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00563795 s, 1.9 GB/s
[2017-05-05 21:52:42,787][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-05 21:52:42,851][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-05 21:52:43,015][gdb1][WARNING] 10+0 records in
[2017-05-05 21:52:43,015][gdb1][WARNING] 10+0 records out
[2017-05-05 21:52:43,015][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.11428 s, 91.8 MB/s
[2017-05-05 21:52:43,015][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-05 21:52:43,015][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-05 21:52:43,016][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-05 21:52:43,016][gdb1][WARNING] backup header from main header.
[2017-05-05 21:52:43,016][gdb1][WARNING] 
[2017-05-05 21:52:43,016][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-05 21:52:43,016][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-05 21:52:43,016][gdb1][WARNING] 
[2017-05-05 21:52:43,016][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-05 21:52:43,016][gdb1][WARNING] 
[2017-05-05 21:52:44,033][gdb1][DEBUG ] ****************************************************************************
[2017-05-05 21:52:44,033][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-05 21:52:44,033][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-05 21:52:44,034][gdb1][DEBUG ] ****************************************************************************
[2017-05-05 21:52:44,034][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-05 21:52:44,034][gdb1][DEBUG ] other utilities.
[2017-05-05 21:52:44,034][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-05 21:52:45,001][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-05 21:52:45,001][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-05 21:52:45,001][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-05 21:52:45,001][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:52:45,017][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:52:45,081][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:52:45,081][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:45,081][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:45,081][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-05-05 21:52:45,081][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:45,081][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-05 21:52:45,081][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:9a309cfa-e034-4db8-9850-dc89a7a89e6b --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-05 21:52:46,099][gdb1][DEBUG ] Setting name!
[2017-05-05 21:52:46,099][gdb1][DEBUG ] partNum is 1
[2017-05-05 21:52:46,099][gdb1][DEBUG ] REALLY setting name!
[2017-05-05 21:52:46,099][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-05 21:52:46,099][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-05 21:52:46,099][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:52:46,414][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:52:46,629][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:52:46,843][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:46,843][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:46,843][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-05 21:52:46,843][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/9a309cfa-e034-4db8-9850-dc89a7a89e6b
[2017-05-05 21:52:46,844][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-05 21:52:47,860][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-05 21:52:47,861][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-05 21:52:47,861][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:52:48,075][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:52:48,240][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:52:48,304][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/9a309cfa-e034-4db8-9850-dc89a7a89e6b
[2017-05-05 21:52:48,304][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:48,304][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-05 21:52:48,304][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:48,304][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-05 21:52:48,304][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:48,304][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-05 21:52:48,305][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:5ee9826f-64a0-4c8a-b241-dccfc25d18fa --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-05 21:52:49,322][gdb1][DEBUG ] Setting name!
[2017-05-05 21:52:49,322][gdb1][DEBUG ] partNum is 0
[2017-05-05 21:52:49,322][gdb1][DEBUG ] REALLY setting name!
[2017-05-05 21:52:49,322][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-05 21:52:49,322][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-05 21:52:49,322][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:52:49,537][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:52:49,751][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:52:49,966][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:49,966][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:49,966][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-05 21:52:49,966][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-05 21:52:49,966][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-05 21:52:50,682][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-05 21:52:50,682][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-05 21:52:50,682][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-05 21:52:50,683][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-05 21:52:50,683][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-05 21:52:50,683][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-05 21:52:50,683][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-05 21:52:50,683][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-05 21:52:50,683][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-05 21:52:50,683][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.fBMjue with options noatime,inode64
[2017-05-05 21:52:50,683][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.fBMjue
[2017-05-05 21:52:50,715][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.fBMjue
[2017-05-05 21:52:50,715][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.fBMjue/ceph_fsid.13181.tmp
[2017-05-05 21:52:50,715][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.fBMjue/fsid.13181.tmp
[2017-05-05 21:52:50,715][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.fBMjue/magic.13181.tmp
[2017-05-05 21:52:50,715][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.fBMjue/journal_uuid.13181.tmp
[2017-05-05 21:52:50,715][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.fBMjue/journal -> /dev/disk/by-partuuid/9a309cfa-e034-4db8-9850-dc89a7a89e6b
[2017-05-05 21:52:50,715][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.fBMjue
[2017-05-05 21:52:50,717][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.fBMjue
[2017-05-05 21:52:50,717][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.fBMjue
[2017-05-05 21:52:50,749][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:52:50,749][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-05 21:52:51,766][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-05 21:52:51,766][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-05 21:52:51,766][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-05 21:52:51,766][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-05 21:52:51,766][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-05 21:52:51,766][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:52:51,766][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:52:51,981][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:52:52,012][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-05 21:52:52,046][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-05 21:52:57,219][gdb1][INFO  ] checking OSD status...
[2017-05-05 21:52:57,219][gdb1][DEBUG ] find the location of an executable
[2017-05-05 21:52:57,222][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-05 21:52:57,337][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-05 21:53:41,040][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-05 21:53:41,041][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-05 21:53:41,042][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-05 21:53:41,042][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff554a6b908>
[2017-05-05 21:53:41,042][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-05 21:53:41,042][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-05 21:53:41,042][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff554cc1aa0>
[2017-05-05 21:53:41,042][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-05 21:53:41,042][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-05 21:53:41,042][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-05 21:53:41,042][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-05 21:53:41,284][gdb0][DEBUG ] connection detected need for sudo
[2017-05-05 21:53:41,512][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-05 21:53:41,513][gdb0][DEBUG ] detect platform information from remote host
[2017-05-05 21:53:41,530][gdb0][DEBUG ] detect machine type
[2017-05-05 21:53:41,534][gdb0][DEBUG ] find the location of an executable
[2017-05-05 21:53:41,535][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-05 21:53:41,535][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-05 21:53:41,535][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-05 21:53:41,538][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-05 21:53:41,538][gdb0][DEBUG ] find the location of an executable
[2017-05-05 21:53:41,540][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-05 21:53:41,660][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-05 21:53:41,676][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-05 21:53:41,692][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-05 21:53:41,707][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-05 21:53:41,715][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:41,715][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-05 21:53:41,715][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-05 21:53:41,731][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:41,731][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:41,731][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:41,731][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-05 21:53:41,731][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-05 21:53:41,731][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-05 21:53:41,735][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-05 21:53:41,750][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-05 21:53:41,752][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-05 21:53:41,767][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:41,767][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-05 21:53:41,767][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:41,767][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-05 21:53:41,783][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-05 21:53:41,783][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-05 21:53:41,815][gdb0][WARNING] 10+0 records in
[2017-05-05 21:53:41,815][gdb0][WARNING] 10+0 records out
[2017-05-05 21:53:41,815][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.030735 s, 341 MB/s
[2017-05-05 21:53:41,815][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-05 21:53:41,831][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-05 21:53:41,995][gdb0][WARNING] 10+0 records in
[2017-05-05 21:53:41,995][gdb0][WARNING] 10+0 records out
[2017-05-05 21:53:41,996][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.12989 s, 80.7 MB/s
[2017-05-05 21:53:41,996][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-05 21:53:41,996][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-05 21:53:41,996][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-05 21:53:41,996][gdb0][WARNING] backup header from main header.
[2017-05-05 21:53:41,996][gdb0][WARNING] 
[2017-05-05 21:53:41,996][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-05 21:53:41,996][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-05 21:53:41,996][gdb0][WARNING] 
[2017-05-05 21:53:41,996][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-05 21:53:41,996][gdb0][WARNING] 
[2017-05-05 21:53:43,013][gdb0][DEBUG ] ****************************************************************************
[2017-05-05 21:53:43,014][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-05 21:53:43,014][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-05 21:53:43,014][gdb0][DEBUG ] ****************************************************************************
[2017-05-05 21:53:43,014][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-05 21:53:43,014][gdb0][DEBUG ] other utilities.
[2017-05-05 21:53:43,014][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-05 21:53:44,031][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-05 21:53:44,031][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-05 21:53:44,031][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-05 21:53:44,031][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:53:44,032][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:53:44,063][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:53:44,071][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:44,071][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:44,071][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-05-05 21:53:44,071][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:44,071][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-05 21:53:44,071][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:2835a107-040c-4b23-9012-afa5bb12c0f6 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-05 21:53:45,088][gdb0][DEBUG ] Setting name!
[2017-05-05 21:53:45,088][gdb0][DEBUG ] partNum is 1
[2017-05-05 21:53:45,089][gdb0][DEBUG ] REALLY setting name!
[2017-05-05 21:53:45,089][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-05 21:53:45,089][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-05 21:53:45,089][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:53:45,303][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:53:45,468][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:53:45,683][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:45,683][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:45,683][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-05 21:53:45,683][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/2835a107-040c-4b23-9012-afa5bb12c0f6
[2017-05-05 21:53:45,683][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-05 21:53:46,700][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-05 21:53:46,700][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-05 21:53:46,700][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:53:46,915][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:53:47,029][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:53:47,244][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/2835a107-040c-4b23-9012-afa5bb12c0f6
[2017-05-05 21:53:47,244][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:47,244][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-05 21:53:47,244][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:47,244][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-05 21:53:47,244][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:47,244][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-05 21:53:47,244][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:c6e9ab6b-19e8-463d-a3e9-d008f6f4d345 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-05 21:53:48,262][gdb0][DEBUG ] Setting name!
[2017-05-05 21:53:48,262][gdb0][DEBUG ] partNum is 0
[2017-05-05 21:53:48,262][gdb0][DEBUG ] REALLY setting name!
[2017-05-05 21:53:48,262][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-05 21:53:48,262][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-05 21:53:48,262][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:53:48,477][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:53:48,691][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:53:48,755][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:48,755][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:48,755][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-05 21:53:48,755][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-05 21:53:48,756][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-05 21:53:49,472][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-05 21:53:49,472][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-05 21:53:49,472][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-05 21:53:49,472][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-05 21:53:49,472][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-05 21:53:49,472][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.YHLFBB with options noatime,inode64
[2017-05-05 21:53:49,472][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-05 21:53:49,472][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.YHLFBB
[2017-05-05 21:53:49,472][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-05 21:53:49,472][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-05 21:53:49,473][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-05 21:53:49,473][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.YHLFBB
[2017-05-05 21:53:49,473][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.YHLFBB/ceph_fsid.4985.tmp
[2017-05-05 21:53:49,473][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.YHLFBB/fsid.4985.tmp
[2017-05-05 21:53:49,473][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.YHLFBB/magic.4985.tmp
[2017-05-05 21:53:49,473][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.YHLFBB/journal_uuid.4985.tmp
[2017-05-05 21:53:49,473][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.YHLFBB/journal -> /dev/disk/by-partuuid/2835a107-040c-4b23-9012-afa5bb12c0f6
[2017-05-05 21:53:49,473][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.YHLFBB
[2017-05-05 21:53:49,473][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.YHLFBB
[2017-05-05 21:53:49,473][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.YHLFBB
[2017-05-05 21:53:49,505][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:53:49,505][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-05 21:53:50,572][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-05 21:53:50,573][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-05 21:53:50,573][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:53:50,737][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:53:51,052][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:53:51,266][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-05 21:53:51,269][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-05 21:53:56,391][gdb0][INFO  ] checking OSD status...
[2017-05-05 21:53:56,391][gdb0][DEBUG ] find the location of an executable
[2017-05-05 21:53:56,393][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-05 21:53:56,508][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-05 21:54:00,061][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-05 21:54:00,061][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-05-05 21:54:00,061][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-05 21:54:00,061][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-05 21:54:00,061][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6d8160a908>
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6d81860aa0>
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-05 21:54:00,062][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-05 21:54:00,063][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-05 21:54:00,301][gdb0][DEBUG ] connection detected need for sudo
[2017-05-05 21:54:00,532][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-05 21:54:00,533][gdb0][DEBUG ] detect platform information from remote host
[2017-05-05 21:54:00,548][gdb0][DEBUG ] detect machine type
[2017-05-05 21:54:00,552][gdb0][DEBUG ] find the location of an executable
[2017-05-05 21:54:00,553][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-05 21:54:00,553][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-05 21:54:00,553][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-05 21:54:00,556][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-05 21:54:00,556][gdb0][DEBUG ] create a keyring file
[2017-05-05 21:54:00,557][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-05 21:54:00,558][gdb0][DEBUG ] find the location of an executable
[2017-05-05 21:54:00,560][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-05 21:54:00,680][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-05 21:54:00,684][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-05 21:54:00,699][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-05 21:54:00,715][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-05 21:54:00,723][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:00,723][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-05 21:54:00,723][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-05 21:54:00,738][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:00,739][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:00,739][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:00,739][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-05 21:54:00,739][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-05 21:54:00,739][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-05 21:54:00,747][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-05 21:54:00,754][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-05 21:54:00,762][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-05 21:54:00,769][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:00,769][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-05 21:54:00,769][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:00,770][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-05 21:54:00,785][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-05 21:54:00,785][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-05 21:54:00,950][gdb0][WARNING] 10+0 records in
[2017-05-05 21:54:00,950][gdb0][WARNING] 10+0 records out
[2017-05-05 21:54:00,950][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.158665 s, 66.1 MB/s
[2017-05-05 21:54:00,950][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-05 21:54:00,966][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-05 21:54:00,966][gdb0][WARNING] 10+0 records in
[2017-05-05 21:54:00,966][gdb0][WARNING] 10+0 records out
[2017-05-05 21:54:00,966][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00511901 s, 2.0 GB/s
[2017-05-05 21:54:00,966][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-05 21:54:00,967][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-05 21:54:00,971][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-05 21:54:00,971][gdb0][WARNING] backup header from main header.
[2017-05-05 21:54:00,971][gdb0][WARNING] 
[2017-05-05 21:54:00,971][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-05 21:54:00,971][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-05 21:54:00,971][gdb0][WARNING] 
[2017-05-05 21:54:00,971][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-05 21:54:00,971][gdb0][WARNING] 
[2017-05-05 21:54:02,039][gdb0][DEBUG ] ****************************************************************************
[2017-05-05 21:54:02,039][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-05 21:54:02,039][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-05 21:54:02,039][gdb0][DEBUG ] ****************************************************************************
[2017-05-05 21:54:02,039][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-05 21:54:02,039][gdb0][DEBUG ] other utilities.
[2017-05-05 21:54:02,039][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-05 21:54:03,056][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-05 21:54:03,057][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-05 21:54:03,057][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-05 21:54:03,057][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:54:03,057][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:54:03,057][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:54:03,073][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:03,073][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:03,073][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-05-05 21:54:03,073][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:03,073][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-05 21:54:03,073][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:49e7ece5-0404-4f43-ad37-f45bc47af4be --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-05 21:54:04,140][gdb0][DEBUG ] Setting name!
[2017-05-05 21:54:04,140][gdb0][DEBUG ] partNum is 1
[2017-05-05 21:54:04,140][gdb0][DEBUG ] REALLY setting name!
[2017-05-05 21:54:04,141][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-05 21:54:04,141][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-05 21:54:04,141][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:54:04,305][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:54:04,469][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:54:04,501][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:04,501][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:04,501][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-05 21:54:04,501][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/49e7ece5-0404-4f43-ad37-f45bc47af4be
[2017-05-05 21:54:04,502][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-05 21:54:05,518][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-05 21:54:05,519][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-05 21:54:05,519][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:54:05,683][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:54:05,847][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:54:06,062][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/49e7ece5-0404-4f43-ad37-f45bc47af4be
[2017-05-05 21:54:06,062][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:06,062][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-05 21:54:06,062][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:06,062][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-05 21:54:06,062][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:06,063][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-05 21:54:06,063][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:b911aa1e-add6-4727-be4b-ed5c6466bc95 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-05 21:54:07,080][gdb0][DEBUG ] Setting name!
[2017-05-05 21:54:07,080][gdb0][DEBUG ] partNum is 0
[2017-05-05 21:54:07,080][gdb0][DEBUG ] REALLY setting name!
[2017-05-05 21:54:07,080][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-05 21:54:07,080][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-05 21:54:07,080][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:54:07,245][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:54:07,459][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:54:07,674][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:07,674][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:07,674][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-05 21:54:07,674][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-05 21:54:07,674][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-05 21:54:08,340][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-05 21:54:08,340][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-05 21:54:08,341][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-05 21:54:08,341][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-05 21:54:08,341][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-05 21:54:08,341][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-05 21:54:08,341][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-05 21:54:08,341][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-05 21:54:08,341][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-05 21:54:08,341][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.U0Rfjm with options noatime,inode64
[2017-05-05 21:54:08,341][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.U0Rfjm
[2017-05-05 21:54:08,345][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.U0Rfjm
[2017-05-05 21:54:08,346][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.U0Rfjm/ceph_fsid.6005.tmp
[2017-05-05 21:54:08,353][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.U0Rfjm/fsid.6005.tmp
[2017-05-05 21:54:08,355][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.U0Rfjm/magic.6005.tmp
[2017-05-05 21:54:08,358][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.U0Rfjm/journal_uuid.6005.tmp
[2017-05-05 21:54:08,359][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.U0Rfjm/journal -> /dev/disk/by-partuuid/49e7ece5-0404-4f43-ad37-f45bc47af4be
[2017-05-05 21:54:08,359][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.U0Rfjm
[2017-05-05 21:54:08,361][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.U0Rfjm
[2017-05-05 21:54:08,361][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.U0Rfjm
[2017-05-05 21:54:08,392][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-05 21:54:08,393][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-05 21:54:09,410][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-05 21:54:09,410][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-05 21:54:09,410][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-05 21:54:09,410][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-05 21:54:09,410][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-05 21:54:09,410][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:54:09,410][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-05 21:54:09,624][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-05 21:54:09,625][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-05 21:54:09,634][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-05 21:54:14,756][gdb0][INFO  ] checking OSD status...
[2017-05-05 21:54:14,757][gdb0][DEBUG ] find the location of an executable
[2017-05-05 21:54:14,759][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-05 21:54:14,874][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-05 21:55:58,784][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-05 21:55:58,784][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-05-05 21:55:58,784][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-05 21:55:58,784][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-05 21:55:58,784][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-05 21:55:58,784][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-05 21:55:58,784][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-05 21:55:58,785][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa796fb9518>
[2017-05-05 21:55:58,785][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-05 21:55:58,785][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-05-05 21:55:58,785][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fa7978d0938>
[2017-05-05 21:55:58,785][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-05 21:55:58,785][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-05 21:55:58,785][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-05 21:55:59,025][gdb0][DEBUG ] connection detected need for sudo
[2017-05-05 21:55:59,249][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-05 21:55:59,249][gdb0][DEBUG ] detect platform information from remote host
[2017-05-05 21:55:59,266][gdb0][DEBUG ] detect machine type
[2017-05-05 21:55:59,270][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-05 21:55:59,272][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-05 21:55:59,501][gdb1][DEBUG ] connection detected need for sudo
[2017-05-05 21:55:59,724][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-05 21:55:59,724][gdb1][DEBUG ] detect platform information from remote host
[2017-05-05 21:55:59,740][gdb1][DEBUG ] detect machine type
[2017-05-05 21:55:59,744][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-05 21:55:59,747][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-05 21:55:59,762][gdb3][DEBUG ] connection detected need for sudo
[2017-05-05 21:55:59,776][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-05 21:55:59,777][gdb3][DEBUG ] detect platform information from remote host
[2017-05-05 21:55:59,793][gdb3][DEBUG ] detect machine type
[2017-05-05 21:55:59,796][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-10 17:54:23,514][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-10 17:54:23,516][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-10 17:54:23,516][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-10 17:54:23,516][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-10 17:54:23,516][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-10 17:54:23,516][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-10 17:54:23,516][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-10 17:54:23,516][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-10 17:54:23,516][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fda9d509ea8>
[2017-05-10 17:54:23,516][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-10 17:54:23,517][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fda9d4ddb18>
[2017-05-10 17:54:23,517][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-10 17:54:23,517][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-10 17:54:23,517][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-10 17:54:23,518][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-10 17:54:23,519][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-10 17:54:23,556][gdb3][DEBUG ] connection detected need for sudo
[2017-05-10 17:54:23,570][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-10 17:54:23,571][gdb3][DEBUG ] detect platform information from remote host
[2017-05-10 17:54:23,587][gdb3][DEBUG ] detect machine type
[2017-05-10 17:54:23,590][gdb3][DEBUG ] find the location of an executable
[2017-05-10 17:54:23,590][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-10 17:54:23,590][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-10 17:54:23,590][gdb3][DEBUG ] get remote short hostname
[2017-05-10 17:54:23,591][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-10 17:54:23,591][gdb3][DEBUG ] get remote short hostname
[2017-05-10 17:54:23,591][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-10 17:54:23,592][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-10 17:54:23,593][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-10 17:54:23,594][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-10 17:54:23,595][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-10 17:54:23,595][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-10 17:54:23,596][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-10 17:54:23,670][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-10 17:54:23,743][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-10 17:54:25,814][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-10 17:54:25,879][gdb3][DEBUG ] ********************************************************************************
[2017-05-10 17:54:25,879][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-10 17:54:25,879][gdb3][DEBUG ] {
[2017-05-10 17:54:25,879][gdb3][DEBUG ]   "election_epoch": 5, 
[2017-05-10 17:54:25,879][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-10 17:54:25,879][gdb3][DEBUG ]   "features": {
[2017-05-10 17:54:25,879][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-10 17:54:25,879][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-10 17:54:25,879][gdb3][DEBUG ]       "kraken", 
[2017-05-10 17:54:25,879][gdb3][DEBUG ]       "luminous"
[2017-05-10 17:54:25,880][gdb3][DEBUG ]     ], 
[2017-05-10 17:54:25,880][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-10 17:54:25,880][gdb3][DEBUG ]     "required_mon": [
[2017-05-10 17:54:25,880][gdb3][DEBUG ]       "kraken", 
[2017-05-10 17:54:25,880][gdb3][DEBUG ]       "luminous"
[2017-05-10 17:54:25,880][gdb3][DEBUG ]     ]
[2017-05-10 17:54:25,880][gdb3][DEBUG ]   }, 
[2017-05-10 17:54:25,880][gdb3][DEBUG ]   "monmap": {
[2017-05-10 17:54:25,880][gdb3][DEBUG ]     "created": "2017-05-05 21:50:15.767309", 
[2017-05-10 17:54:25,880][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-10 17:54:25,880][gdb3][DEBUG ]     "features": {
[2017-05-10 17:54:25,880][gdb3][DEBUG ]       "optional": [], 
[2017-05-10 17:54:25,880][gdb3][DEBUG ]       "persistent": [
[2017-05-10 17:54:25,880][gdb3][DEBUG ]         "kraken", 
[2017-05-10 17:54:25,880][gdb3][DEBUG ]         "luminous"
[2017-05-10 17:54:25,880][gdb3][DEBUG ]       ]
[2017-05-10 17:54:25,880][gdb3][DEBUG ]     }, 
[2017-05-10 17:54:25,881][gdb3][DEBUG ]     "fsid": "5f73f0a8-714e-4c1f-8410-722fc431c29c", 
[2017-05-10 17:54:25,881][gdb3][DEBUG ]     "modified": "2017-05-05 21:50:16.010217", 
[2017-05-10 17:54:25,881][gdb3][DEBUG ]     "mons": [
[2017-05-10 17:54:25,881][gdb3][DEBUG ]       {
[2017-05-10 17:54:25,881][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-10 17:54:25,881][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-10 17:54:25,881][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-10 17:54:25,881][gdb3][DEBUG ]         "rank": 0
[2017-05-10 17:54:25,881][gdb3][DEBUG ]       }
[2017-05-10 17:54:25,881][gdb3][DEBUG ]     ]
[2017-05-10 17:54:25,881][gdb3][DEBUG ]   }, 
[2017-05-10 17:54:25,881][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-10 17:54:25,881][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-10 17:54:25,881][gdb3][DEBUG ]   "quorum": [
[2017-05-10 17:54:25,881][gdb3][DEBUG ]     0
[2017-05-10 17:54:25,881][gdb3][DEBUG ]   ], 
[2017-05-10 17:54:25,882][gdb3][DEBUG ]   "rank": 0, 
[2017-05-10 17:54:25,882][gdb3][DEBUG ]   "state": "leader", 
[2017-05-10 17:54:25,882][gdb3][DEBUG ]   "sync_provider": []
[2017-05-10 17:54:25,882][gdb3][DEBUG ] }
[2017-05-10 17:54:25,882][gdb3][DEBUG ] ********************************************************************************
[2017-05-10 17:54:25,882][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-10 17:54:25,883][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-10 17:54:25,948][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-10 17:54:25,963][gdb3][DEBUG ] connection detected need for sudo
[2017-05-10 17:54:25,976][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-10 17:54:25,977][gdb3][DEBUG ] detect platform information from remote host
[2017-05-10 17:54:25,993][gdb3][DEBUG ] detect machine type
[2017-05-10 17:54:25,996][gdb3][DEBUG ] find the location of an executable
[2017-05-10 17:54:25,997][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-10 17:54:26,062][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-10 17:54:26,062][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-10 17:54:26,062][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-10 17:54:26,064][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpW0hV5s
[2017-05-10 17:54:26,079][gdb3][DEBUG ] connection detected need for sudo
[2017-05-10 17:54:26,093][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-10 17:54:26,094][gdb3][DEBUG ] detect platform information from remote host
[2017-05-10 17:54:26,110][gdb3][DEBUG ] detect machine type
[2017-05-10 17:54:26,113][gdb3][DEBUG ] get remote short hostname
[2017-05-10 17:54:26,113][gdb3][DEBUG ] fetch remote file
[2017-05-10 17:54:26,114][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-10 17:54:26,180][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-10 17:54:26,346][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-10 17:54:26,512][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-10 17:54:26,679][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-10 17:54:26,845][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-10 17:54:27,011][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-05-10 17:54:27,012][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-05-10 17:54:27,013][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-05-10 17:54:27,013][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-10 17:54:27,013][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-05-10 17:54:27,014][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-05-10 17:54:27,014][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpW0hV5s
[2017-05-10 17:54:39,652][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-10 17:54:39,652][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-05-10 17:54:39,652][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-10 17:54:39,652][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-10 17:54:39,652][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-10 17:54:39,652][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-10 17:54:39,652][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-10 17:54:39,652][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0c11307560>
[2017-05-10 17:54:39,652][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-10 17:54:39,652][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-05-10 17:54:39,653][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f0c11c1e938>
[2017-05-10 17:54:39,653][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-10 17:54:39,653][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-10 17:54:39,653][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-10 17:54:39,909][gdb0][DEBUG ] connection detected need for sudo
[2017-05-10 17:54:40,159][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-10 17:54:40,159][gdb0][DEBUG ] detect platform information from remote host
[2017-05-10 17:54:40,179][gdb0][DEBUG ] detect machine type
[2017-05-10 17:54:40,182][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-10 17:54:40,185][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-10 17:54:40,425][gdb1][DEBUG ] connection detected need for sudo
[2017-05-10 17:54:40,672][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-10 17:54:40,672][gdb1][DEBUG ] detect platform information from remote host
[2017-05-10 17:54:40,690][gdb1][DEBUG ] detect machine type
[2017-05-10 17:54:40,694][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-10 17:54:40,696][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-10 17:54:40,696][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-10 17:54:40,711][gdb3][DEBUG ] connection detected need for sudo
[2017-05-10 17:54:40,725][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-10 17:54:40,726][gdb3][DEBUG ] detect platform information from remote host
[2017-05-10 17:54:40,742][gdb3][DEBUG ] detect machine type
[2017-05-10 17:54:40,744][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-10 17:54:40,746][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-10 17:55:21,515][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb3
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0e87150560>
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f0e87a67938>
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-10 17:55:21,516][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-10 17:55:21,516][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-10 17:55:21,543][gdb3][DEBUG ] connection detected need for sudo
[2017-05-10 17:55:21,557][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-10 17:55:21,557][gdb3][DEBUG ] detect platform information from remote host
[2017-05-10 17:55:21,574][gdb3][DEBUG ] detect machine type
[2017-05-10 17:55:21,576][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-12 16:06:09,839][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-12 16:06:09,840][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-12 16:06:09,840][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-12 16:06:09,841][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-12 16:06:09,841][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-12 16:06:09,841][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-12 16:06:09,841][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-12 16:06:09,841][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-12 16:06:09,841][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f88dcdc0e60>
[2017-05-12 16:06:09,841][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-12 16:06:09,841][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f88dcd95b18>
[2017-05-12 16:06:09,841][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-12 16:06:09,841][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-12 16:06:09,841][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-12 16:06:09,843][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-12 16:06:09,843][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-12 16:06:09,881][gdb3][DEBUG ] connection detected need for sudo
[2017-05-12 16:06:09,897][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-12 16:06:09,898][gdb3][DEBUG ] detect platform information from remote host
[2017-05-12 16:06:09,915][gdb3][DEBUG ] detect machine type
[2017-05-12 16:06:09,917][gdb3][DEBUG ] find the location of an executable
[2017-05-12 16:06:09,918][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-12 16:06:09,918][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-12 16:06:09,918][gdb3][DEBUG ] get remote short hostname
[2017-05-12 16:06:09,918][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-12 16:06:09,919][gdb3][DEBUG ] get remote short hostname
[2017-05-12 16:06:09,919][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-12 16:06:09,919][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-12 16:06:09,921][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-12 16:06:09,922][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-12 16:06:09,922][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-12 16:06:09,923][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-12 16:06:09,924][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-12 16:06:09,999][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-12 16:06:10,067][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-12 16:06:12,137][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-12 16:06:12,202][gdb3][DEBUG ] ********************************************************************************
[2017-05-12 16:06:12,202][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-12 16:06:12,203][gdb3][DEBUG ] {
[2017-05-12 16:06:12,203][gdb3][DEBUG ]   "election_epoch": 6, 
[2017-05-12 16:06:12,203][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-12 16:06:12,203][gdb3][DEBUG ]   "features": {
[2017-05-12 16:06:12,203][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-12 16:06:12,203][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-12 16:06:12,204][gdb3][DEBUG ]       "kraken", 
[2017-05-12 16:06:12,204][gdb3][DEBUG ]       "luminous"
[2017-05-12 16:06:12,204][gdb3][DEBUG ]     ], 
[2017-05-12 16:06:12,204][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-12 16:06:12,204][gdb3][DEBUG ]     "required_mon": [
[2017-05-12 16:06:12,204][gdb3][DEBUG ]       "kraken", 
[2017-05-12 16:06:12,204][gdb3][DEBUG ]       "luminous"
[2017-05-12 16:06:12,204][gdb3][DEBUG ]     ]
[2017-05-12 16:06:12,204][gdb3][DEBUG ]   }, 
[2017-05-12 16:06:12,204][gdb3][DEBUG ]   "monmap": {
[2017-05-12 16:06:12,205][gdb3][DEBUG ]     "created": "2017-05-05 21:50:15.767309", 
[2017-05-12 16:06:12,205][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-12 16:06:12,205][gdb3][DEBUG ]     "features": {
[2017-05-12 16:06:12,205][gdb3][DEBUG ]       "optional": [], 
[2017-05-12 16:06:12,205][gdb3][DEBUG ]       "persistent": [
[2017-05-12 16:06:12,205][gdb3][DEBUG ]         "kraken", 
[2017-05-12 16:06:12,205][gdb3][DEBUG ]         "luminous"
[2017-05-12 16:06:12,205][gdb3][DEBUG ]       ]
[2017-05-12 16:06:12,205][gdb3][DEBUG ]     }, 
[2017-05-12 16:06:12,205][gdb3][DEBUG ]     "fsid": "5f73f0a8-714e-4c1f-8410-722fc431c29c", 
[2017-05-12 16:06:12,206][gdb3][DEBUG ]     "modified": "2017-05-05 21:50:16.010217", 
[2017-05-12 16:06:12,206][gdb3][DEBUG ]     "mons": [
[2017-05-12 16:06:12,206][gdb3][DEBUG ]       {
[2017-05-12 16:06:12,206][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-12 16:06:12,206][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-12 16:06:12,206][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-12 16:06:12,206][gdb3][DEBUG ]         "rank": 0
[2017-05-12 16:06:12,206][gdb3][DEBUG ]       }
[2017-05-12 16:06:12,206][gdb3][DEBUG ]     ]
[2017-05-12 16:06:12,206][gdb3][DEBUG ]   }, 
[2017-05-12 16:06:12,207][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-12 16:06:12,207][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-12 16:06:12,207][gdb3][DEBUG ]   "quorum": [
[2017-05-12 16:06:12,207][gdb3][DEBUG ]     0
[2017-05-12 16:06:12,207][gdb3][DEBUG ]   ], 
[2017-05-12 16:06:12,207][gdb3][DEBUG ]   "rank": 0, 
[2017-05-12 16:06:12,207][gdb3][DEBUG ]   "state": "leader", 
[2017-05-12 16:06:12,207][gdb3][DEBUG ]   "sync_provider": []
[2017-05-12 16:06:12,207][gdb3][DEBUG ] }
[2017-05-12 16:06:12,207][gdb3][DEBUG ] ********************************************************************************
[2017-05-12 16:06:12,208][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-12 16:06:12,209][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-12 16:06:12,274][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-12 16:06:12,296][gdb3][DEBUG ] connection detected need for sudo
[2017-05-12 16:06:12,312][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-12 16:06:12,313][gdb3][DEBUG ] detect platform information from remote host
[2017-05-12 16:06:12,330][gdb3][DEBUG ] detect machine type
[2017-05-12 16:06:12,332][gdb3][DEBUG ] find the location of an executable
[2017-05-12 16:06:12,334][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-12 16:06:12,399][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-12 16:06:12,399][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-12 16:06:12,400][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-12 16:06:12,401][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmp64XZj3
[2017-05-12 16:06:12,423][gdb3][DEBUG ] connection detected need for sudo
[2017-05-12 16:06:12,440][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-12 16:06:12,440][gdb3][DEBUG ] detect platform information from remote host
[2017-05-12 16:06:12,458][gdb3][DEBUG ] detect machine type
[2017-05-12 16:06:12,460][gdb3][DEBUG ] get remote short hostname
[2017-05-12 16:06:12,461][gdb3][DEBUG ] fetch remote file
[2017-05-12 16:06:12,462][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-12 16:06:12,528][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-12 16:06:12,695][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-12 16:06:12,863][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-12 16:06:13,030][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-12 16:06:13,197][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-12 16:06:13,363][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-05-12 16:06:13,364][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-05-12 16:06:13,365][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-05-12 16:06:13,365][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-12 16:06:13,366][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-05-12 16:06:13,367][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-05-12 16:06:13,367][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp64XZj3
[2017-05-13 18:51:36,345][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-13 18:51:36,346][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-13 18:51:36,346][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-13 18:51:36,347][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-13 18:51:36,347][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-13 18:51:36,347][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-13 18:51:36,347][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-13 18:51:36,347][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-13 18:51:36,347][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f849445ce60>
[2017-05-13 18:51:36,347][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-13 18:51:36,347][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f8494431b18>
[2017-05-13 18:51:36,347][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-13 18:51:36,347][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-13 18:51:36,347][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-13 18:51:36,348][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-13 18:51:36,348][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-13 18:51:36,386][gdb3][DEBUG ] connection detected need for sudo
[2017-05-13 18:51:36,400][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-13 18:51:36,401][gdb3][DEBUG ] detect platform information from remote host
[2017-05-13 18:51:36,417][gdb3][DEBUG ] detect machine type
[2017-05-13 18:51:36,419][gdb3][DEBUG ] find the location of an executable
[2017-05-13 18:51:36,420][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-13 18:51:36,420][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-13 18:51:36,420][gdb3][DEBUG ] get remote short hostname
[2017-05-13 18:51:36,420][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-13 18:51:36,420][gdb3][DEBUG ] get remote short hostname
[2017-05-13 18:51:36,421][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-13 18:51:36,421][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-13 18:51:36,423][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-13 18:51:36,423][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-13 18:51:36,424][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-13 18:51:36,424][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-13 18:51:36,425][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-13 18:51:36,498][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-13 18:51:36,567][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-13 18:51:38,605][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-13 18:51:38,670][gdb3][DEBUG ] ********************************************************************************
[2017-05-13 18:51:38,670][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-13 18:51:38,670][gdb3][DEBUG ] {
[2017-05-13 18:51:38,670][gdb3][DEBUG ]   "election_epoch": 7, 
[2017-05-13 18:51:38,671][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-13 18:51:38,671][gdb3][DEBUG ]   "features": {
[2017-05-13 18:51:38,671][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-13 18:51:38,671][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-13 18:51:38,671][gdb3][DEBUG ]       "kraken", 
[2017-05-13 18:51:38,671][gdb3][DEBUG ]       "luminous"
[2017-05-13 18:51:38,671][gdb3][DEBUG ]     ], 
[2017-05-13 18:51:38,671][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-13 18:51:38,671][gdb3][DEBUG ]     "required_mon": [
[2017-05-13 18:51:38,671][gdb3][DEBUG ]       "kraken", 
[2017-05-13 18:51:38,671][gdb3][DEBUG ]       "luminous"
[2017-05-13 18:51:38,671][gdb3][DEBUG ]     ]
[2017-05-13 18:51:38,671][gdb3][DEBUG ]   }, 
[2017-05-13 18:51:38,671][gdb3][DEBUG ]   "monmap": {
[2017-05-13 18:51:38,671][gdb3][DEBUG ]     "created": "2017-05-05 21:50:15.767309", 
[2017-05-13 18:51:38,671][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-13 18:51:38,671][gdb3][DEBUG ]     "features": {
[2017-05-13 18:51:38,672][gdb3][DEBUG ]       "optional": [], 
[2017-05-13 18:51:38,672][gdb3][DEBUG ]       "persistent": [
[2017-05-13 18:51:38,672][gdb3][DEBUG ]         "kraken", 
[2017-05-13 18:51:38,672][gdb3][DEBUG ]         "luminous"
[2017-05-13 18:51:38,672][gdb3][DEBUG ]       ]
[2017-05-13 18:51:38,672][gdb3][DEBUG ]     }, 
[2017-05-13 18:51:38,672][gdb3][DEBUG ]     "fsid": "5f73f0a8-714e-4c1f-8410-722fc431c29c", 
[2017-05-13 18:51:38,672][gdb3][DEBUG ]     "modified": "2017-05-05 21:50:16.010217", 
[2017-05-13 18:51:38,672][gdb3][DEBUG ]     "mons": [
[2017-05-13 18:51:38,672][gdb3][DEBUG ]       {
[2017-05-13 18:51:38,672][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-13 18:51:38,672][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-13 18:51:38,672][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-13 18:51:38,672][gdb3][DEBUG ]         "rank": 0
[2017-05-13 18:51:38,672][gdb3][DEBUG ]       }
[2017-05-13 18:51:38,672][gdb3][DEBUG ]     ]
[2017-05-13 18:51:38,672][gdb3][DEBUG ]   }, 
[2017-05-13 18:51:38,673][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-13 18:51:38,673][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-13 18:51:38,673][gdb3][DEBUG ]   "quorum": [
[2017-05-13 18:51:38,673][gdb3][DEBUG ]     0
[2017-05-13 18:51:38,673][gdb3][DEBUG ]   ], 
[2017-05-13 18:51:38,673][gdb3][DEBUG ]   "rank": 0, 
[2017-05-13 18:51:38,673][gdb3][DEBUG ]   "state": "leader", 
[2017-05-13 18:51:38,673][gdb3][DEBUG ]   "sync_provider": []
[2017-05-13 18:51:38,673][gdb3][DEBUG ] }
[2017-05-13 18:51:38,673][gdb3][DEBUG ] ********************************************************************************
[2017-05-13 18:51:38,673][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-13 18:51:38,674][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-13 18:51:38,739][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-13 18:51:38,754][gdb3][DEBUG ] connection detected need for sudo
[2017-05-13 18:51:38,768][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-13 18:51:38,768][gdb3][DEBUG ] detect platform information from remote host
[2017-05-13 18:51:38,785][gdb3][DEBUG ] detect machine type
[2017-05-13 18:51:38,787][gdb3][DEBUG ] find the location of an executable
[2017-05-13 18:51:38,789][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-13 18:51:38,854][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-13 18:51:38,854][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-13 18:51:38,854][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-13 18:51:38,856][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpNibfYr
[2017-05-13 18:51:38,871][gdb3][DEBUG ] connection detected need for sudo
[2017-05-13 18:51:38,885][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-13 18:51:38,885][gdb3][DEBUG ] detect platform information from remote host
[2017-05-13 18:51:38,901][gdb3][DEBUG ] detect machine type
[2017-05-13 18:51:38,904][gdb3][DEBUG ] get remote short hostname
[2017-05-13 18:51:38,904][gdb3][DEBUG ] fetch remote file
[2017-05-13 18:51:38,905][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-13 18:51:38,972][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-13 18:51:39,138][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-13 18:51:39,305][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-13 18:51:39,471][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-13 18:51:39,637][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-13 18:51:39,803][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-05-13 18:51:39,804][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-05-13 18:51:39,805][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-05-13 18:51:39,805][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-13 18:51:39,806][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-05-13 18:51:39,806][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-05-13 18:51:39,807][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpNibfYr
[2017-05-14 02:18:56,599][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9a4b27efc8>
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f9a4bb911b8>
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:18:56,600][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:18:56,601][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-14 02:18:56,601][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-14 02:18:56,601][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-14 02:18:56,601][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-14 02:18:56,626][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:18:56,640][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:18:56,641][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:18:56,657][gdb3][DEBUG ] detect machine type
[2017-05-14 02:18:56,660][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 02:18:56,660][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-14 02:18:56,661][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-14 02:18:56,699][gdb3][DEBUG ] Reading package lists...
[2017-05-14 02:18:56,863][gdb3][DEBUG ] Building dependency tree...
[2017-05-14 02:18:56,863][gdb3][DEBUG ] Reading state information...
[2017-05-14 02:18:56,927][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-14 02:18:56,928][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-14 02:18:56,928][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-14 02:18:56,928][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-14 02:18:56,928][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-14 02:18:56,928][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 ntp python-blinker python-cephfs
[2017-05-14 02:18:56,928][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-05-14 02:18:56,928][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-05-14 02:18:56,928][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-05-14 02:18:56,928][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-05-14 02:18:56,928][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-05-14 02:18:56,928][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-14 02:18:56,944][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-14 02:18:56,944][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-14 02:18:57,058][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 26 not upgraded.
[2017-05-14 02:18:57,058][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-14 02:18:57,122][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 79236 files and directories currently installed.)
[2017-05-14 02:18:57,122][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-14 02:18:57,287][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-14 02:18:57,351][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-14 02:18:57,383][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-14 02:18:57,601][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-14 02:18:57,716][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-14 02:18:57,881][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-14 02:18:57,913][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-14 02:18:57,945][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-14 02:18:58,109][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-14 02:18:58,173][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-14 02:18:58,174][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-14 02:18:58,342][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-14 02:18:58,373][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-14 02:18:58,538][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-14 02:18:58,570][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-14 02:18:58,577][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-14 02:18:58,692][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-14 02:18:59,458][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-14 02:18:59,621][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:18:59,621][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-14 02:18:59,621][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:18:59,621][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:18:59,621][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:18:59,621][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 02:18:59,621][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:18:59,621][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb1f5669710>
[2017-05-14 02:18:59,622][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:18:59,622][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-14 02:18:59,622][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fb1f5f76230>
[2017-05-14 02:18:59,622][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:18:59,622][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:18:59,622][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-14 02:18:59,647][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:18:59,661][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:18:59,662][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:18:59,678][gdb3][DEBUG ] detect machine type
[2017-05-14 02:18:59,680][gdb3][DEBUG ] find the location of an executable
[2017-05-14 02:18:59,695][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:18:59,709][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:18:59,709][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:18:59,726][gdb3][DEBUG ] detect machine type
[2017-05-14 02:18:59,728][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 02:18:59,728][gdb3][INFO  ] purging data on gdb3
[2017-05-14 02:18:59,729][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-14 02:18:59,742][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-14 02:18:59,908][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:18:59,909][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-14 02:18:59,909][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:18:59,909][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:18:59,909][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:18:59,909][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 02:18:59,909][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:18:59,909][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f91564419e0>
[2017-05-14 02:18:59,909][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:18:59,909][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f9156d04848>
[2017-05-14 02:18:59,909][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:18:59,909][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:19:28,315][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:19:28,315][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-14 02:19:28,315][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:19:28,315][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:19:28,315][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:19:28,315][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 02:19:28,315][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:19:28,316][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f06d5cc6560>
[2017-05-14 02:19:28,316][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:19:28,316][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-14 02:19:28,316][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-14 02:19:28,316][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f06d634a758>
[2017-05-14 02:19:28,316][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-14 02:19:28,316][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:19:28,316][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-14 02:19:28,316][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:19:28,316][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-14 02:19:28,316][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-14 02:19:28,316][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-14 02:19:28,342][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:19:28,355][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:19:28,356][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:19:28,372][gdb3][DEBUG ] detect machine type
[2017-05-14 02:19:28,374][gdb3][DEBUG ] find the location of an executable
[2017-05-14 02:19:28,375][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-14 02:19:28,386][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-14 02:19:28,393][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-14 02:19:28,393][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-14 02:19:28,393][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-14 02:19:28,393][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-14 02:19:28,393][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-14 02:19:28,393][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-14 02:19:28,393][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-14 02:19:28,393][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-14 02:19:28,554][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:19:28,554][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-14 02:19:28,554][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:19:28,555][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:19:28,555][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:19:28,555][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 02:19:28,555][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-14 02:19:28,555][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:19:28,555][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feccb99ee60>
[2017-05-14 02:19:28,555][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:19:28,555][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7feccb973b18>
[2017-05-14 02:19:28,555][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:19:28,555][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-14 02:19:28,555][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:19:28,556][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-14 02:19:28,556][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-14 02:19:28,581][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:19:28,595][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:19:28,596][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:19:28,611][gdb3][DEBUG ] detect machine type
[2017-05-14 02:19:28,614][gdb3][DEBUG ] find the location of an executable
[2017-05-14 02:19:28,614][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-14 02:19:28,614][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-14 02:19:28,614][gdb3][DEBUG ] get remote short hostname
[2017-05-14 02:19:28,614][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-14 02:19:28,615][gdb3][DEBUG ] get remote short hostname
[2017-05-14 02:19:28,615][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-14 02:19:28,616][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 02:19:28,617][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-14 02:19:28,617][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-14 02:19:28,617][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-14 02:19:28,618][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-14 02:19:28,618][gdb3][DEBUG ] create the monitor keyring file
[2017-05-14 02:19:28,619][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-14 02:19:28,657][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-14 02:19:28,657][gdb3][DEBUG ] ceph-mon: set fsid to fa24701f-4a7e-4b49-8d84-75f8be59d10e
[2017-05-14 02:19:28,664][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-14 02:19:28,665][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-14 02:19:28,665][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-14 02:19:28,665][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-14 02:19:28,666][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-14 02:19:28,742][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-14 02:19:28,810][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-14 02:19:30,848][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 02:19:30,913][gdb3][DEBUG ] ********************************************************************************
[2017-05-14 02:19:30,913][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-14 02:19:30,914][gdb3][DEBUG ] {
[2017-05-14 02:19:30,914][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-14 02:19:30,914][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-14 02:19:30,914][gdb3][DEBUG ]   "features": {
[2017-05-14 02:19:30,914][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-14 02:19:30,914][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-14 02:19:30,914][gdb3][DEBUG ]       "kraken", 
[2017-05-14 02:19:30,914][gdb3][DEBUG ]       "luminous"
[2017-05-14 02:19:30,914][gdb3][DEBUG ]     ], 
[2017-05-14 02:19:30,914][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-14 02:19:30,914][gdb3][DEBUG ]     "required_mon": [
[2017-05-14 02:19:30,914][gdb3][DEBUG ]       "kraken", 
[2017-05-14 02:19:30,914][gdb3][DEBUG ]       "luminous"
[2017-05-14 02:19:30,914][gdb3][DEBUG ]     ]
[2017-05-14 02:19:30,915][gdb3][DEBUG ]   }, 
[2017-05-14 02:19:30,915][gdb3][DEBUG ]   "monmap": {
[2017-05-14 02:19:30,915][gdb3][DEBUG ]     "created": "2017-05-14 02:19:28.643028", 
[2017-05-14 02:19:30,915][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-14 02:19:30,915][gdb3][DEBUG ]     "features": {
[2017-05-14 02:19:30,915][gdb3][DEBUG ]       "optional": [], 
[2017-05-14 02:19:30,915][gdb3][DEBUG ]       "persistent": [
[2017-05-14 02:19:30,915][gdb3][DEBUG ]         "kraken", 
[2017-05-14 02:19:30,915][gdb3][DEBUG ]         "luminous"
[2017-05-14 02:19:30,915][gdb3][DEBUG ]       ]
[2017-05-14 02:19:30,915][gdb3][DEBUG ]     }, 
[2017-05-14 02:19:30,915][gdb3][DEBUG ]     "fsid": "fa24701f-4a7e-4b49-8d84-75f8be59d10e", 
[2017-05-14 02:19:30,915][gdb3][DEBUG ]     "modified": "2017-05-14 02:19:28.894093", 
[2017-05-14 02:19:30,915][gdb3][DEBUG ]     "mons": [
[2017-05-14 02:19:30,915][gdb3][DEBUG ]       {
[2017-05-14 02:19:30,915][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-14 02:19:30,916][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-14 02:19:30,916][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-14 02:19:30,916][gdb3][DEBUG ]         "rank": 0
[2017-05-14 02:19:30,916][gdb3][DEBUG ]       }
[2017-05-14 02:19:30,916][gdb3][DEBUG ]     ]
[2017-05-14 02:19:30,916][gdb3][DEBUG ]   }, 
[2017-05-14 02:19:30,916][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-14 02:19:30,916][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-14 02:19:30,916][gdb3][DEBUG ]   "quorum": [
[2017-05-14 02:19:30,916][gdb3][DEBUG ]     0
[2017-05-14 02:19:30,916][gdb3][DEBUG ]   ], 
[2017-05-14 02:19:30,916][gdb3][DEBUG ]   "rank": 0, 
[2017-05-14 02:19:30,916][gdb3][DEBUG ]   "state": "leader", 
[2017-05-14 02:19:30,916][gdb3][DEBUG ]   "sync_provider": []
[2017-05-14 02:19:30,916][gdb3][DEBUG ] }
[2017-05-14 02:19:30,916][gdb3][DEBUG ] ********************************************************************************
[2017-05-14 02:19:30,916][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-14 02:19:30,917][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 02:19:30,982][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-14 02:19:30,998][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:19:31,011][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:19:31,012][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:19:31,028][gdb3][DEBUG ] detect machine type
[2017-05-14 02:19:31,030][gdb3][DEBUG ] find the location of an executable
[2017-05-14 02:19:31,031][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 02:19:31,096][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-14 02:19:31,096][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-14 02:19:31,096][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-14 02:19:31,098][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpYs4Mvc
[2017-05-14 02:19:31,113][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:19:31,127][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:19:31,128][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:19:31,144][gdb3][DEBUG ] detect machine type
[2017-05-14 02:19:31,147][gdb3][DEBUG ] get remote short hostname
[2017-05-14 02:19:31,147][gdb3][DEBUG ] fetch remote file
[2017-05-14 02:19:31,148][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 02:19:31,214][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-14 02:19:31,380][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-14 02:19:31,546][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-14 02:19:31,713][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-14 02:19:31,879][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-14 02:19:32,045][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-14 02:19:32,211][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-14 02:19:32,378][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-14 02:19:32,544][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-14 02:19:32,710][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-14 02:19:32,875][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-14 02:19:32,876][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-14 02:19:32,876][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170514021932'
[2017-05-14 02:19:32,876][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-14 02:19:32,876][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-14 02:19:32,876][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-14 02:19:32,876][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpYs4Mvc
[2017-05-14 02:19:58,516][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:19:58,517][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-14 02:19:58,517][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:19:58,517][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:19:58,517][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-14 02:19:58,517][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-14 02:19:58,517][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-14 02:19:58,517][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:19:58,517][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-14 02:19:58,517][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-14 02:19:58,517][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-14 02:19:58,517][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-14 02:19:58,518][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-14 02:19:58,518][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:19:58,518][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa6d2d5e908>
[2017-05-14 02:19:58,518][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:19:58,518][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-14 02:19:58,518][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa6d2fb4aa0>
[2017-05-14 02:19:58,518][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:19:58,518][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:19:58,518][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-14 02:19:58,518][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-14 02:19:58,762][gdb1][DEBUG ] connection detected need for sudo
[2017-05-14 02:19:58,993][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-14 02:19:58,993][gdb1][DEBUG ] detect platform information from remote host
[2017-05-14 02:19:59,010][gdb1][DEBUG ] detect machine type
[2017-05-14 02:19:59,014][gdb1][DEBUG ] find the location of an executable
[2017-05-14 02:19:59,015][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 02:19:59,016][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-14 02:19:59,016][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 02:19:59,018][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-14 02:19:59,019][gdb1][DEBUG ] find the location of an executable
[2017-05-14 02:19:59,021][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-14 02:19:59,141][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-14 02:19:59,157][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:19:59,165][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:19:59,180][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:19:59,188][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-14 02:19:59,204][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:19:59,204][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:19:59,204][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:19:59,204][gdb1][WARNING] Traceback (most recent call last):
[2017-05-14 02:19:59,204][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-14 02:19:59,204][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-14 02:19:59,204][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-14 02:19:59,204][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-14 02:19:59,204][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2029, in main
[2017-05-14 02:19:59,204][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2018, in prepare
[2017-05-14 02:19:59,205][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2049, in prepare_locked
[2017-05-14 02:19:59,205][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2816, in prepare
[2017-05-14 02:19:59,205][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2971, in prepare_device
[2017-05-14 02:19:59,205][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2878, in prepare_device
[2017-05-14 02:19:59,205][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2841, in sanity_checks
[2017-05-14 02:19:59,205][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-05-14 02:19:59,205][gdb1][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-05-14 02:19:59,207][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-14 02:19:59,207][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-14 02:19:59,207][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-14 02:21:05,303][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:21:05,303][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-14 02:21:05,303][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:21:05,303][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:21:05,303][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-14 02:21:05,303][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-14 02:21:05,303][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-14 02:21:05,303][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f73f1797908>
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f73f19edaa0>
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:21:05,304][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-14 02:21:05,305][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-14 02:21:05,542][gdb1][DEBUG ] connection detected need for sudo
[2017-05-14 02:21:05,777][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-14 02:21:05,778][gdb1][DEBUG ] detect platform information from remote host
[2017-05-14 02:21:05,794][gdb1][DEBUG ] detect machine type
[2017-05-14 02:21:05,798][gdb1][DEBUG ] find the location of an executable
[2017-05-14 02:21:05,799][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 02:21:05,799][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-14 02:21:05,799][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 02:21:05,802][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-14 02:21:05,802][gdb1][DEBUG ] find the location of an executable
[2017-05-14 02:21:05,804][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-14 02:21:05,925][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-14 02:21:05,940][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:21:05,948][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:21:05,964][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:21:05,971][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:05,971][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-14 02:21:05,971][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-14 02:21:05,987][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:05,987][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:05,987][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:05,987][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-14 02:21:05,995][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-14 02:21:06,002][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-14 02:21:06,018][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-14 02:21:06,021][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:06,021][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-14 02:21:06,021][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:06,022][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-14 02:21:06,025][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-14 02:21:06,028][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-14 02:21:06,028][gdb1][WARNING] backup header from main header.
[2017-05-14 02:21:06,029][gdb1][WARNING] 
[2017-05-14 02:21:07,046][gdb1][DEBUG ] ****************************************************************************
[2017-05-14 02:21:07,046][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-14 02:21:07,046][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-14 02:21:07,046][gdb1][DEBUG ] ****************************************************************************
[2017-05-14 02:21:07,046][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-14 02:21:07,047][gdb1][DEBUG ] other utilities.
[2017-05-14 02:21:07,047][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-14 02:21:08,064][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-14 02:21:08,064][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:21:08,064][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-14 02:21:08,064][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:08,080][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:21:08,111][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:08,119][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:08,119][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:08,119][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-05-14 02:21:08,119][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:08,119][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-14 02:21:08,120][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:7606d85d-f6ce-48d5-a990-a2fa8dbcf8b3 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-14 02:21:09,136][gdb1][DEBUG ] Setting name!
[2017-05-14 02:21:09,136][gdb1][DEBUG ] partNum is 1
[2017-05-14 02:21:09,137][gdb1][DEBUG ] REALLY setting name!
[2017-05-14 02:21:09,137][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:21:09,137][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 02:21:09,137][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:09,351][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:21:09,515][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:09,730][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:09,730][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:09,730][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-14 02:21:09,730][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/7606d85d-f6ce-48d5-a990-a2fa8dbcf8b3
[2017-05-14 02:21:09,730][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-14 02:21:10,747][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:21:10,748][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 02:21:10,748][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:10,912][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:21:11,076][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:11,092][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/7606d85d-f6ce-48d5-a990-a2fa8dbcf8b3
[2017-05-14 02:21:11,092][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:11,092][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-14 02:21:11,092][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:11,092][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-14 02:21:11,092][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:11,092][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-14 02:21:11,093][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f36e9204-6c0c-4c30-8421-7332c87b0b7f --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-14 02:21:12,160][gdb1][DEBUG ] Setting name!
[2017-05-14 02:21:12,160][gdb1][DEBUG ] partNum is 0
[2017-05-14 02:21:12,160][gdb1][DEBUG ] REALLY setting name!
[2017-05-14 02:21:12,160][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:21:12,160][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 02:21:12,160][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:12,324][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:21:12,489][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:12,703][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:12,704][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:12,704][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-14 02:21:12,704][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-14 02:21:12,704][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-14 02:21:13,470][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-14 02:21:13,470][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-14 02:21:13,470][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-14 02:21:13,471][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-14 02:21:13,471][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-14 02:21:13,471][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-14 02:21:13,471][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-14 02:21:13,471][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-14 02:21:13,471][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-14 02:21:13,471][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.UfxMUp with options noatime,inode64
[2017-05-14 02:21:13,471][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.UfxMUp
[2017-05-14 02:21:13,471][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.UfxMUp
[2017-05-14 02:21:13,471][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.UfxMUp/ceph_fsid.14335.tmp
[2017-05-14 02:21:13,471][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.UfxMUp/fsid.14335.tmp
[2017-05-14 02:21:13,471][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.UfxMUp/magic.14335.tmp
[2017-05-14 02:21:13,471][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.UfxMUp/journal_uuid.14335.tmp
[2017-05-14 02:21:13,472][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.UfxMUp/journal -> /dev/disk/by-partuuid/7606d85d-f6ce-48d5-a990-a2fa8dbcf8b3
[2017-05-14 02:21:13,472][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.UfxMUp
[2017-05-14 02:21:13,472][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.UfxMUp
[2017-05-14 02:21:13,472][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.UfxMUp
[2017-05-14 02:21:13,536][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:13,536][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-14 02:21:14,553][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:21:14,553][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 02:21:14,553][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:14,768][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:21:14,932][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:14,964][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-14 02:21:14,998][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-14 02:21:20,121][gdb1][INFO  ] checking OSD status...
[2017-05-14 02:21:20,121][gdb1][DEBUG ] find the location of an executable
[2017-05-14 02:21:20,124][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-14 02:21:20,239][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-14 02:21:53,899][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:21:53,899][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-14 02:21:53,899][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fac7c023908>
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-14 02:21:53,900][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fac7c279aa0>
[2017-05-14 02:21:53,901][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:21:53,901][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:21:53,901][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-14 02:21:53,901][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-14 02:21:54,142][gdb1][DEBUG ] connection detected need for sudo
[2017-05-14 02:21:54,369][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-14 02:21:54,370][gdb1][DEBUG ] detect platform information from remote host
[2017-05-14 02:21:54,386][gdb1][DEBUG ] detect machine type
[2017-05-14 02:21:54,390][gdb1][DEBUG ] find the location of an executable
[2017-05-14 02:21:54,391][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 02:21:54,391][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-14 02:21:54,391][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 02:21:54,394][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-14 02:21:54,394][gdb1][DEBUG ] create a keyring file
[2017-05-14 02:21:54,396][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-14 02:21:54,396][gdb1][DEBUG ] find the location of an executable
[2017-05-14 02:21:54,398][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-14 02:21:54,519][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-14 02:21:54,535][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:21:54,542][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:21:54,558][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:21:54,566][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:54,566][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-14 02:21:54,566][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-14 02:21:54,582][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:54,582][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:54,582][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:54,582][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-14 02:21:54,582][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-14 02:21:54,582][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-14 02:21:54,590][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-14 02:21:54,598][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-14 02:21:54,605][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-14 02:21:54,621][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:54,621][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-14 02:21:54,621][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:54,621][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-14 02:21:54,653][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-14 02:21:54,653][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-14 02:21:54,818][gdb1][WARNING] 10+0 records in
[2017-05-14 02:21:54,818][gdb1][WARNING] 10+0 records out
[2017-05-14 02:21:54,818][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.130838 s, 80.1 MB/s
[2017-05-14 02:21:54,818][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-14 02:21:54,818][gdb1][DEBUG ] /dev/xvdb2: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-14 02:21:54,818][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-14 02:21:54,818][gdb1][WARNING] 10+0 records in
[2017-05-14 02:21:54,818][gdb1][WARNING] 10+0 records out
[2017-05-14 02:21:54,818][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00576455 s, 1.8 GB/s
[2017-05-14 02:21:54,818][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-14 02:21:54,819][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-14 02:21:54,819][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-14 02:21:54,819][gdb1][WARNING] backup header from main header.
[2017-05-14 02:21:54,819][gdb1][WARNING] 
[2017-05-14 02:21:54,819][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-14 02:21:54,819][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-14 02:21:54,819][gdb1][WARNING] 
[2017-05-14 02:21:54,819][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-14 02:21:54,819][gdb1][WARNING] 
[2017-05-14 02:21:55,886][gdb1][DEBUG ] ****************************************************************************
[2017-05-14 02:21:55,886][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-14 02:21:55,887][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-14 02:21:55,887][gdb1][DEBUG ] ****************************************************************************
[2017-05-14 02:21:55,887][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-14 02:21:55,887][gdb1][DEBUG ] other utilities.
[2017-05-14 02:21:55,887][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-14 02:21:56,853][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-14 02:21:56,854][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:21:56,854][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-14 02:21:56,854][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:56,886][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:21:56,901][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:56,933][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:56,934][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:56,934][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-05-14 02:21:56,934][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:56,934][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-14 02:21:56,934][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:9b223141-df8a-4fe2-9b68-b46599e3b20e --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-14 02:21:57,951][gdb1][DEBUG ] Setting name!
[2017-05-14 02:21:57,951][gdb1][DEBUG ] partNum is 1
[2017-05-14 02:21:57,951][gdb1][DEBUG ] REALLY setting name!
[2017-05-14 02:21:57,951][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:21:57,952][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 02:21:57,952][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:58,166][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:21:58,431][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:58,645][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:58,646][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:21:58,646][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-14 02:21:58,646][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/9b223141-df8a-4fe2-9b68-b46599e3b20e
[2017-05-14 02:21:58,646][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-14 02:21:59,713][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:21:59,713][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 02:21:59,713][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:21:59,877][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:22:00,092][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:00,108][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/9b223141-df8a-4fe2-9b68-b46599e3b20e
[2017-05-14 02:22:00,108][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:00,108][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-14 02:22:00,108][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:00,108][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-14 02:22:00,108][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:00,108][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-14 02:22:00,108][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:8fd1ba0f-5ab3-4dc5-a0ae-8fe39c77b724 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-14 02:22:01,175][gdb1][DEBUG ] Setting name!
[2017-05-14 02:22:01,175][gdb1][DEBUG ] partNum is 0
[2017-05-14 02:22:01,175][gdb1][DEBUG ] REALLY setting name!
[2017-05-14 02:22:01,175][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:22:01,176][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 02:22:01,176][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:01,390][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:22:01,605][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:01,669][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:01,669][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:01,669][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-14 02:22:01,669][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-14 02:22:01,669][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-14 02:22:02,385][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-14 02:22:02,385][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-14 02:22:02,386][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-14 02:22:02,386][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-14 02:22:02,386][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-14 02:22:02,386][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-14 02:22:02,386][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-14 02:22:02,386][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-14 02:22:02,386][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-14 02:22:02,386][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.s_fV7v with options noatime,inode64
[2017-05-14 02:22:02,386][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.s_fV7v
[2017-05-14 02:22:02,402][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.s_fV7v
[2017-05-14 02:22:02,418][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.s_fV7v/ceph_fsid.15239.tmp
[2017-05-14 02:22:02,433][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.s_fV7v/fsid.15239.tmp
[2017-05-14 02:22:02,433][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.s_fV7v/magic.15239.tmp
[2017-05-14 02:22:02,434][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.s_fV7v/journal_uuid.15239.tmp
[2017-05-14 02:22:02,434][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.s_fV7v/journal -> /dev/disk/by-partuuid/9b223141-df8a-4fe2-9b68-b46599e3b20e
[2017-05-14 02:22:02,434][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.s_fV7v
[2017-05-14 02:22:02,437][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.s_fV7v
[2017-05-14 02:22:02,437][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.s_fV7v
[2017-05-14 02:22:02,469][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:02,469][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-14 02:22:03,486][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-14 02:22:03,486][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-14 02:22:03,487][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-14 02:22:03,487][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-14 02:22:03,487][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 02:22:03,487][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:03,487][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:22:03,701][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:03,733][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-14 02:22:03,751][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-14 02:22:08,924][gdb1][INFO  ] checking OSD status...
[2017-05-14 02:22:08,924][gdb1][DEBUG ] find the location of an executable
[2017-05-14 02:22:08,927][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-14 02:22:09,042][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-14 02:22:13,182][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:22:13,182][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-05-14 02:22:13,182][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:22:13,182][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:22:13,182][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-14 02:22:13,182][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0399c42908>
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0399e98aa0>
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:22:13,183][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-14 02:22:13,184][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-14 02:22:13,430][gdb0][DEBUG ] connection detected need for sudo
[2017-05-14 02:22:13,652][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-14 02:22:13,653][gdb0][DEBUG ] detect platform information from remote host
[2017-05-14 02:22:13,669][gdb0][DEBUG ] detect machine type
[2017-05-14 02:22:13,673][gdb0][DEBUG ] find the location of an executable
[2017-05-14 02:22:13,674][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-14 02:22:13,674][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-14 02:22:13,674][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 02:22:13,677][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-14 02:22:13,677][gdb0][DEBUG ] create a keyring file
[2017-05-14 02:22:13,679][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-14 02:22:13,679][gdb0][DEBUG ] find the location of an executable
[2017-05-14 02:22:13,681][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-14 02:22:13,802][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-14 02:22:13,809][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:22:13,825][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:22:13,841][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-14 02:22:13,848][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:13,848][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-14 02:22:13,849][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-14 02:22:13,864][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:13,864][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:13,864][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:13,865][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-14 02:22:13,872][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-14 02:22:13,880][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-14 02:22:13,887][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-14 02:22:13,895][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:13,895][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-14 02:22:13,895][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:13,895][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-14 02:22:13,911][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-14 02:22:13,911][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-14 02:22:13,911][gdb0][WARNING] backup header from main header.
[2017-05-14 02:22:13,911][gdb0][WARNING] 
[2017-05-14 02:22:14,928][gdb0][DEBUG ] ****************************************************************************
[2017-05-14 02:22:14,928][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-14 02:22:14,928][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-14 02:22:14,928][gdb0][DEBUG ] ****************************************************************************
[2017-05-14 02:22:14,928][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-14 02:22:14,929][gdb0][DEBUG ] other utilities.
[2017-05-14 02:22:14,929][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-14 02:22:15,946][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-14 02:22:15,946][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-14 02:22:15,946][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-14 02:22:15,946][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:15,954][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:22:15,986][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:15,993][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:15,993][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:15,993][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-05-14 02:22:15,993][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:15,993][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-14 02:22:15,994][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:f522327c-3274-4ae9-8d24-c5c64947f62f --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-14 02:22:17,010][gdb0][DEBUG ] Setting name!
[2017-05-14 02:22:17,011][gdb0][DEBUG ] partNum is 1
[2017-05-14 02:22:17,011][gdb0][DEBUG ] REALLY setting name!
[2017-05-14 02:22:17,011][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-14 02:22:17,011][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 02:22:17,011][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:17,226][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:22:17,340][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:17,555][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:17,555][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:17,555][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-14 02:22:17,555][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/f522327c-3274-4ae9-8d24-c5c64947f62f
[2017-05-14 02:22:17,555][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-14 02:22:18,572][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-14 02:22:18,572][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 02:22:18,573][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:18,737][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:22:18,851][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:19,015][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/f522327c-3274-4ae9-8d24-c5c64947f62f
[2017-05-14 02:22:19,016][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:19,016][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-14 02:22:19,016][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:19,016][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-14 02:22:19,016][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:19,016][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-14 02:22:19,016][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:0577768c-beab-402a-b9ef-fee2c2566902 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-14 02:22:20,033][gdb0][DEBUG ] Setting name!
[2017-05-14 02:22:20,033][gdb0][DEBUG ] partNum is 0
[2017-05-14 02:22:20,033][gdb0][DEBUG ] REALLY setting name!
[2017-05-14 02:22:20,033][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-14 02:22:20,033][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-14 02:22:20,034][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:20,198][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:22:20,362][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:20,394][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:20,394][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:20,394][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-14 02:22:20,394][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-14 02:22:20,394][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-14 02:22:21,110][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-14 02:22:21,111][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-14 02:22:21,111][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-14 02:22:21,111][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-14 02:22:21,111][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-14 02:22:21,111][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-14 02:22:21,111][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-14 02:22:21,111][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-14 02:22:21,111][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-14 02:22:21,111][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.bAXHx3 with options noatime,inode64
[2017-05-14 02:22:21,111][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.bAXHx3
[2017-05-14 02:22:21,111][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.bAXHx3
[2017-05-14 02:22:21,111][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.bAXHx3/ceph_fsid.7066.tmp
[2017-05-14 02:22:21,111][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.bAXHx3/fsid.7066.tmp
[2017-05-14 02:22:21,112][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.bAXHx3/magic.7066.tmp
[2017-05-14 02:22:21,112][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.bAXHx3/journal_uuid.7066.tmp
[2017-05-14 02:22:21,112][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.bAXHx3/journal -> /dev/disk/by-partuuid/f522327c-3274-4ae9-8d24-c5c64947f62f
[2017-05-14 02:22:21,112][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.bAXHx3
[2017-05-14 02:22:21,113][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.bAXHx3
[2017-05-14 02:22:21,113][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.bAXHx3
[2017-05-14 02:22:21,145][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-14 02:22:21,145][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-14 02:22:22,212][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-14 02:22:22,212][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-14 02:22:22,212][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-14 02:22:22,212][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-14 02:22:22,213][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-14 02:22:22,213][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:22,213][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-14 02:22:22,327][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-14 02:22:22,391][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-14 02:22:22,394][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-14 02:22:27,516][gdb0][INFO  ] checking OSD status...
[2017-05-14 02:22:27,516][gdb0][DEBUG ] find the location of an executable
[2017-05-14 02:22:27,519][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-14 02:22:27,634][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-14 02:22:58,379][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f14578f5518>
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f145820c938>
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 02:22:58,380][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 02:22:58,381][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-14 02:22:58,622][gdb0][DEBUG ] connection detected need for sudo
[2017-05-14 02:22:58,853][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-14 02:22:58,853][gdb0][DEBUG ] detect platform information from remote host
[2017-05-14 02:22:58,870][gdb0][DEBUG ] detect machine type
[2017-05-14 02:22:58,874][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 02:22:58,876][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-14 02:22:59,110][gdb1][DEBUG ] connection detected need for sudo
[2017-05-14 02:22:59,341][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-14 02:22:59,341][gdb1][DEBUG ] detect platform information from remote host
[2017-05-14 02:22:59,357][gdb1][DEBUG ] detect machine type
[2017-05-14 02:22:59,361][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 02:22:59,364][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-14 02:22:59,379][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 02:22:59,393][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 02:22:59,394][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 02:22:59,410][gdb3][DEBUG ] detect machine type
[2017-05-14 02:22:59,413][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 19:29:10,070][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-14 19:29:10,073][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-14 19:29:10,073][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-14 19:29:10,073][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-14 19:29:10,074][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-14 19:29:10,074][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-14 19:29:10,074][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-14 19:29:10,074][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-14 19:29:10,074][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f763992fe60>
[2017-05-14 19:29:10,074][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-14 19:29:10,074][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f7639904b18>
[2017-05-14 19:29:10,074][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-14 19:29:10,074][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-14 19:29:10,074][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-14 19:29:10,076][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-14 19:29:10,076][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-14 19:29:10,115][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 19:29:10,129][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 19:29:10,130][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 19:29:10,146][gdb3][DEBUG ] detect machine type
[2017-05-14 19:29:10,148][gdb3][DEBUG ] find the location of an executable
[2017-05-14 19:29:10,148][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-14 19:29:10,148][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-14 19:29:10,149][gdb3][DEBUG ] get remote short hostname
[2017-05-14 19:29:10,149][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-14 19:29:10,149][gdb3][DEBUG ] get remote short hostname
[2017-05-14 19:29:10,149][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-14 19:29:10,150][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-14 19:29:10,151][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-14 19:29:10,152][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-14 19:29:10,153][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-14 19:29:10,153][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-14 19:29:10,154][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-14 19:29:10,231][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-14 19:29:10,301][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-14 19:29:12,371][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 19:29:12,436][gdb3][DEBUG ] ********************************************************************************
[2017-05-14 19:29:12,436][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-14 19:29:12,436][gdb3][DEBUG ] {
[2017-05-14 19:29:12,436][gdb3][DEBUG ]   "election_epoch": 5, 
[2017-05-14 19:29:12,436][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-14 19:29:12,436][gdb3][DEBUG ]   "features": {
[2017-05-14 19:29:12,437][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-14 19:29:12,437][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-14 19:29:12,437][gdb3][DEBUG ]       "kraken", 
[2017-05-14 19:29:12,437][gdb3][DEBUG ]       "luminous"
[2017-05-14 19:29:12,437][gdb3][DEBUG ]     ], 
[2017-05-14 19:29:12,437][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-14 19:29:12,437][gdb3][DEBUG ]     "required_mon": [
[2017-05-14 19:29:12,437][gdb3][DEBUG ]       "kraken", 
[2017-05-14 19:29:12,437][gdb3][DEBUG ]       "luminous"
[2017-05-14 19:29:12,437][gdb3][DEBUG ]     ]
[2017-05-14 19:29:12,437][gdb3][DEBUG ]   }, 
[2017-05-14 19:29:12,437][gdb3][DEBUG ]   "monmap": {
[2017-05-14 19:29:12,437][gdb3][DEBUG ]     "created": "2017-05-14 02:19:28.643028", 
[2017-05-14 19:29:12,437][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-14 19:29:12,437][gdb3][DEBUG ]     "features": {
[2017-05-14 19:29:12,437][gdb3][DEBUG ]       "optional": [], 
[2017-05-14 19:29:12,438][gdb3][DEBUG ]       "persistent": [
[2017-05-14 19:29:12,438][gdb3][DEBUG ]         "kraken", 
[2017-05-14 19:29:12,438][gdb3][DEBUG ]         "luminous"
[2017-05-14 19:29:12,438][gdb3][DEBUG ]       ]
[2017-05-14 19:29:12,438][gdb3][DEBUG ]     }, 
[2017-05-14 19:29:12,438][gdb3][DEBUG ]     "fsid": "fa24701f-4a7e-4b49-8d84-75f8be59d10e", 
[2017-05-14 19:29:12,438][gdb3][DEBUG ]     "modified": "2017-05-14 02:19:28.894093", 
[2017-05-14 19:29:12,438][gdb3][DEBUG ]     "mons": [
[2017-05-14 19:29:12,438][gdb3][DEBUG ]       {
[2017-05-14 19:29:12,438][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-14 19:29:12,438][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-14 19:29:12,438][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-14 19:29:12,438][gdb3][DEBUG ]         "rank": 0
[2017-05-14 19:29:12,438][gdb3][DEBUG ]       }
[2017-05-14 19:29:12,438][gdb3][DEBUG ]     ]
[2017-05-14 19:29:12,438][gdb3][DEBUG ]   }, 
[2017-05-14 19:29:12,439][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-14 19:29:12,439][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-14 19:29:12,439][gdb3][DEBUG ]   "quorum": [
[2017-05-14 19:29:12,439][gdb3][DEBUG ]     0
[2017-05-14 19:29:12,439][gdb3][DEBUG ]   ], 
[2017-05-14 19:29:12,439][gdb3][DEBUG ]   "rank": 0, 
[2017-05-14 19:29:12,439][gdb3][DEBUG ]   "state": "leader", 
[2017-05-14 19:29:12,439][gdb3][DEBUG ]   "sync_provider": []
[2017-05-14 19:29:12,439][gdb3][DEBUG ] }
[2017-05-14 19:29:12,439][gdb3][DEBUG ] ********************************************************************************
[2017-05-14 19:29:12,439][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-14 19:29:12,440][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 19:29:12,505][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-14 19:29:12,521][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 19:29:12,535][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 19:29:12,535][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 19:29:12,552][gdb3][DEBUG ] detect machine type
[2017-05-14 19:29:12,554][gdb3][DEBUG ] find the location of an executable
[2017-05-14 19:29:12,555][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 19:29:12,620][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-14 19:29:12,620][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-14 19:29:12,620][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-14 19:29:12,622][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpd1mMYj
[2017-05-14 19:29:12,637][gdb3][DEBUG ] connection detected need for sudo
[2017-05-14 19:29:12,650][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-14 19:29:12,651][gdb3][DEBUG ] detect platform information from remote host
[2017-05-14 19:29:12,667][gdb3][DEBUG ] detect machine type
[2017-05-14 19:29:12,669][gdb3][DEBUG ] get remote short hostname
[2017-05-14 19:29:12,670][gdb3][DEBUG ] fetch remote file
[2017-05-14 19:29:12,671][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-14 19:29:12,736][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-14 19:29:12,903][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-14 19:29:13,069][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-14 19:29:13,235][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-14 19:29:13,401][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-14 19:29:13,567][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-05-14 19:29:13,568][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-05-14 19:29:13,568][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-05-14 19:29:13,569][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-14 19:29:13,569][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-05-14 19:29:13,569][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-05-14 19:29:13,570][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpd1mMYj
[2017-05-15 05:22:11,765][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:22:11,765][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk 0:/dev/xvdb
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  disk                          : [('0', '/dev/xvdb', None)]
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f44d05a0908>
[2017-05-15 05:22:11,766][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:22:11,767][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:22:11,767][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f44d07f6aa0>
[2017-05-15 05:22:11,767][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:22:11,767][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:22:11,767][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:22:11,767][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks 0:/dev/xvdb:
[2017-05-15 05:22:16,026][ceph_deploy.osd][ERROR ] connecting to host: 0 resulted in errors: HostNotFound 0
[2017-05-15 05:22:16,027][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-15 05:22:16,191][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:22:16,192][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-05-15 05:22:16,192][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:22:16,192][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:22:16,192][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:22:16,192][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 05:22:16,192][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:22:16,192][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f72b880f518>
[2017-05-15 05:22:16,192][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:22:16,192][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-05-15 05:22:16,193][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f72b9126938>
[2017-05-15 05:22:16,193][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:22:16,193][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:22:16,193][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-15 05:22:16,467][gdb0][DEBUG ] connection detected need for sudo
[2017-05-15 05:22:16,686][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-15 05:22:16,686][gdb0][DEBUG ] detect platform information from remote host
[2017-05-15 05:22:16,705][gdb0][DEBUG ] detect machine type
[2017-05-15 05:22:16,709][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:22:16,711][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-15 05:22:16,950][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:22:17,175][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:22:17,175][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:22:17,192][gdb1][DEBUG ] detect machine type
[2017-05-15 05:22:17,196][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:22:17,198][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-15 05:22:17,198][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-15 05:22:17,213][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 05:22:17,228][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 05:22:17,228][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 05:22:17,245][gdb3][DEBUG ] detect machine type
[2017-05-15 05:22:17,247][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:22:17,249][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-15 05:22:27,700][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:22:27,700][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-15 05:22:27,700][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:22:27,700][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:22:27,700][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:22:27,700][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 05:22:27,700][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:22:27,700][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f914edb4fc8>
[2017-05-15 05:22:27,700][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:22:27,701][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-15 05:22:27,701][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f914f6c71b8>
[2017-05-15 05:22:27,701][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:22:27,701][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:22:27,701][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-15 05:22:27,701][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-15 05:22:27,701][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-15 05:22:27,701][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-15 05:22:27,727][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 05:22:27,741][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 05:22:27,741][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 05:22:27,758][gdb3][DEBUG ] detect machine type
[2017-05-15 05:22:27,760][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:22:27,760][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-15 05:22:27,761][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-15 05:22:27,799][gdb3][DEBUG ] Reading package lists...
[2017-05-15 05:22:27,963][gdb3][DEBUG ] Building dependency tree...
[2017-05-15 05:22:27,964][gdb3][DEBUG ] Reading state information...
[2017-05-15 05:22:28,028][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-15 05:22:28,028][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-15 05:22:28,028][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-15 05:22:28,028][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-15 05:22:28,028][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-15 05:22:28,029][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 ntp python-blinker python-cephfs
[2017-05-15 05:22:28,029][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-05-15 05:22:28,029][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-05-15 05:22:28,029][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-05-15 05:22:28,029][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-05-15 05:22:28,029][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-05-15 05:22:28,029][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-15 05:22:28,045][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-15 05:22:28,045][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-15 05:22:28,310][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 26 not upgraded.
[2017-05-15 05:22:28,310][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-15 05:22:28,675][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 79236 files and directories currently installed.)
[2017-05-15 05:22:28,675][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-15 05:22:28,840][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-15 05:22:28,954][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-15 05:22:28,956][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-15 05:22:29,171][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-15 05:22:29,285][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-15 05:22:29,451][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-15 05:22:29,483][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-15 05:22:29,515][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-15 05:22:29,686][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-15 05:22:29,750][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-15 05:22:29,766][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-15 05:22:29,930][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-15 05:22:29,930][gdb3][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-05-15 05:22:29,994][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-15 05:22:30,108][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-15 05:22:30,173][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-15 05:22:30,181][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-15 05:22:30,395][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-15 05:22:31,161][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-15 05:22:31,323][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6f9cda5710>
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f6f9d6b2230>
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:22:31,324][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:22:31,324][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-15 05:22:31,350][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 05:22:31,364][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 05:22:31,364][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 05:22:31,381][gdb3][DEBUG ] detect machine type
[2017-05-15 05:22:31,383][gdb3][DEBUG ] find the location of an executable
[2017-05-15 05:22:31,398][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 05:22:31,411][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 05:22:31,411][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 05:22:31,427][gdb3][DEBUG ] detect machine type
[2017-05-15 05:22:31,430][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:22:31,430][gdb3][INFO  ] purging data on gdb3
[2017-05-15 05:22:31,431][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-15 05:22:31,444][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-15 05:22:31,611][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:22:31,611][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-15 05:22:31,611][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:22:31,611][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:22:31,611][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:22:31,611][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 05:22:31,611][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:22:31,611][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faa14a979e0>
[2017-05-15 05:22:31,611][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:22:31,612][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7faa1535a848>
[2017-05-15 05:22:31,612][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:22:31,612][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:22:50,986][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:22:50,986][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-15 05:22:50,986][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:22:50,986][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:22:50,986][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:22:50,987][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 05:22:50,987][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:22:50,987][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9349d45560>
[2017-05-15 05:22:50,987][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:22:50,987][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-15 05:22:50,987][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-15 05:22:50,987][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f934a3c9758>
[2017-05-15 05:22:50,987][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-15 05:22:50,988][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:22:50,988][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-15 05:22:50,988][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:22:50,988][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-15 05:22:50,988][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-15 05:22:50,988][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-15 05:22:51,014][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 05:22:51,028][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 05:22:51,028][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 05:22:51,045][gdb3][DEBUG ] detect machine type
[2017-05-15 05:22:51,047][gdb3][DEBUG ] find the location of an executable
[2017-05-15 05:22:51,048][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-15 05:22:51,059][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-15 05:22:51,065][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-15 05:22:51,066][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-15 05:22:51,066][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-15 05:22:51,066][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-15 05:22:51,066][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-15 05:22:51,066][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-15 05:22:51,066][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-15 05:22:51,066][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-15 05:22:51,231][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:22:51,231][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-15 05:22:51,232][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:22:51,232][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:22:51,232][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:22:51,232][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 05:22:51,232][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-15 05:22:51,232][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:22:51,232][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f390a032e60>
[2017-05-15 05:22:51,232][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:22:51,233][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f390a007b18>
[2017-05-15 05:22:51,233][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:22:51,233][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-15 05:22:51,233][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:22:51,234][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-15 05:22:51,234][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-15 05:22:51,260][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 05:22:51,273][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 05:22:51,274][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 05:22:51,290][gdb3][DEBUG ] detect machine type
[2017-05-15 05:22:51,292][gdb3][DEBUG ] find the location of an executable
[2017-05-15 05:22:51,293][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-15 05:22:51,293][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-15 05:22:51,293][gdb3][DEBUG ] get remote short hostname
[2017-05-15 05:22:51,293][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-15 05:22:51,293][gdb3][DEBUG ] get remote short hostname
[2017-05-15 05:22:51,294][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-15 05:22:51,294][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:22:51,296][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-15 05:22:51,296][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-15 05:22:51,296][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-15 05:22:51,297][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-15 05:22:51,297][gdb3][DEBUG ] create the monitor keyring file
[2017-05-15 05:22:51,298][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-15 05:22:51,336][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-15 05:22:51,336][gdb3][DEBUG ] ceph-mon: set fsid to 2abd18d5-dd31-4b52-9bf1-36a5d85a68f9
[2017-05-15 05:22:51,339][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-15 05:22:51,343][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-15 05:22:51,343][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-15 05:22:51,344][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-15 05:22:51,345][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 05:22:51,414][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-15 05:22:51,481][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-15 05:22:53,551][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-15 05:22:53,616][gdb3][DEBUG ] ********************************************************************************
[2017-05-15 05:22:53,617][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-15 05:22:53,617][gdb3][DEBUG ] {
[2017-05-15 05:22:53,617][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-15 05:22:53,617][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-15 05:22:53,617][gdb3][DEBUG ]   "features": {
[2017-05-15 05:22:53,617][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-15 05:22:53,617][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-15 05:22:53,617][gdb3][DEBUG ]       "kraken", 
[2017-05-15 05:22:53,617][gdb3][DEBUG ]       "luminous"
[2017-05-15 05:22:53,618][gdb3][DEBUG ]     ], 
[2017-05-15 05:22:53,618][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-15 05:22:53,618][gdb3][DEBUG ]     "required_mon": [
[2017-05-15 05:22:53,618][gdb3][DEBUG ]       "kraken", 
[2017-05-15 05:22:53,618][gdb3][DEBUG ]       "luminous"
[2017-05-15 05:22:53,618][gdb3][DEBUG ]     ]
[2017-05-15 05:22:53,618][gdb3][DEBUG ]   }, 
[2017-05-15 05:22:53,618][gdb3][DEBUG ]   "monmap": {
[2017-05-15 05:22:53,618][gdb3][DEBUG ]     "created": "2017-05-15 05:22:51.321209", 
[2017-05-15 05:22:53,618][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-15 05:22:53,618][gdb3][DEBUG ]     "features": {
[2017-05-15 05:22:53,618][gdb3][DEBUG ]       "optional": [], 
[2017-05-15 05:22:53,618][gdb3][DEBUG ]       "persistent": [
[2017-05-15 05:22:53,618][gdb3][DEBUG ]         "kraken", 
[2017-05-15 05:22:53,618][gdb3][DEBUG ]         "luminous"
[2017-05-15 05:22:53,618][gdb3][DEBUG ]       ]
[2017-05-15 05:22:53,619][gdb3][DEBUG ]     }, 
[2017-05-15 05:22:53,619][gdb3][DEBUG ]     "fsid": "2abd18d5-dd31-4b52-9bf1-36a5d85a68f9", 
[2017-05-15 05:22:53,619][gdb3][DEBUG ]     "modified": "2017-05-15 05:22:51.564549", 
[2017-05-15 05:22:53,619][gdb3][DEBUG ]     "mons": [
[2017-05-15 05:22:53,619][gdb3][DEBUG ]       {
[2017-05-15 05:22:53,619][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-15 05:22:53,619][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-15 05:22:53,619][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-15 05:22:53,619][gdb3][DEBUG ]         "rank": 0
[2017-05-15 05:22:53,619][gdb3][DEBUG ]       }
[2017-05-15 05:22:53,619][gdb3][DEBUG ]     ]
[2017-05-15 05:22:53,619][gdb3][DEBUG ]   }, 
[2017-05-15 05:22:53,619][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-15 05:22:53,619][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-15 05:22:53,619][gdb3][DEBUG ]   "quorum": [
[2017-05-15 05:22:53,619][gdb3][DEBUG ]     0
[2017-05-15 05:22:53,620][gdb3][DEBUG ]   ], 
[2017-05-15 05:22:53,620][gdb3][DEBUG ]   "rank": 0, 
[2017-05-15 05:22:53,620][gdb3][DEBUG ]   "state": "leader", 
[2017-05-15 05:22:53,620][gdb3][DEBUG ]   "sync_provider": []
[2017-05-15 05:22:53,620][gdb3][DEBUG ] }
[2017-05-15 05:22:53,620][gdb3][DEBUG ] ********************************************************************************
[2017-05-15 05:22:53,620][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-15 05:22:53,621][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-15 05:22:53,686][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-15 05:22:53,701][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 05:22:53,715][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 05:22:53,715][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 05:22:53,731][gdb3][DEBUG ] detect machine type
[2017-05-15 05:22:53,734][gdb3][DEBUG ] find the location of an executable
[2017-05-15 05:22:53,735][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-15 05:22:53,800][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-15 05:22:53,800][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-15 05:22:53,800][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-15 05:22:53,802][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpHCMCCp
[2017-05-15 05:22:53,817][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 05:22:53,830][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 05:22:53,830][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 05:22:53,847][gdb3][DEBUG ] detect machine type
[2017-05-15 05:22:53,849][gdb3][DEBUG ] get remote short hostname
[2017-05-15 05:22:53,849][gdb3][DEBUG ] fetch remote file
[2017-05-15 05:22:53,851][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-15 05:22:53,916][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-15 05:22:54,083][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-15 05:22:54,249][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-15 05:22:54,415][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-15 05:22:54,581][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-15 05:22:54,747][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-15 05:22:54,913][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-15 05:22:55,079][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-15 05:22:55,246][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-15 05:22:55,412][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-15 05:22:55,577][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-15 05:22:55,577][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-15 05:22:55,577][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170515052255'
[2017-05-15 05:22:55,578][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-15 05:22:55,578][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-15 05:22:55,578][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-15 05:22:55,578][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpHCMCCp
[2017-05-15 05:23:53,667][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:23:53,667][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-15 05:23:53,668][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:23:53,668][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:23:53,668][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:23:53,668][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 05:23:53,668][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:23:53,668][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8049cf1518>
[2017-05-15 05:23:53,668][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:23:53,668][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-15 05:23:53,668][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f804a608938>
[2017-05-15 05:23:53,668][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:23:53,668][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:23:53,668][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-15 05:23:53,694][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 05:23:53,708][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 05:23:53,709][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 05:23:53,726][gdb3][DEBUG ] detect machine type
[2017-05-15 05:23:53,728][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:25:24,291][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:25:24,292][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-05-15 05:25:24,292][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:25:24,292][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:25:24,292][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:25:24,292][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-15 05:25:24,292][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:25:24,292][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:25:24,292][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:25:24,292][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:25:24,292][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:25:24,293][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:25:24,293][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:25:24,293][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:25:24,293][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f21ac83f908>
[2017-05-15 05:25:24,293][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:25:24,293][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:25:24,293][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f21aca95aa0>
[2017-05-15 05:25:24,293][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:25:24,293][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:25:24,293][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:25:24,293][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-15 05:25:24,536][gdb0][DEBUG ] connection detected need for sudo
[2017-05-15 05:25:24,767][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-15 05:25:24,768][gdb0][DEBUG ] detect platform information from remote host
[2017-05-15 05:25:24,785][gdb0][DEBUG ] detect machine type
[2017-05-15 05:25:24,789][gdb0][DEBUG ] find the location of an executable
[2017-05-15 05:25:24,790][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:25:24,790][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-15 05:25:24,790][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:25:24,793][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-15 05:25:24,793][gdb0][DEBUG ] find the location of an executable
[2017-05-15 05:25:24,795][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:25:24,915][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 05:25:24,931][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:25:24,947][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:25:24,963][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:25:24,978][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:24,978][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 05:25:24,978][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 05:25:24,994][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:24,994][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:24,994][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:24,994][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:25:24,994][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:25:24,995][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-15 05:25:25,002][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-15 05:25:25,018][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-15 05:25:25,021][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-15 05:25:25,037][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:25,037][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-15 05:25:25,037][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:25,037][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-15 05:25:25,053][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-15 05:25:25,053][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-15 05:25:25,069][gdb0][WARNING] 10+0 records in
[2017-05-15 05:25:25,069][gdb0][WARNING] 10+0 records out
[2017-05-15 05:25:25,069][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00792432 s, 1.3 GB/s
[2017-05-15 05:25:25,069][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-15 05:25:25,183][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-15 05:25:25,183][gdb0][WARNING] 10+0 records in
[2017-05-15 05:25:25,183][gdb0][WARNING] 10+0 records out
[2017-05-15 05:25:25,184][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00548934 s, 1.9 GB/s
[2017-05-15 05:25:25,184][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-15 05:25:25,184][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-15 05:25:25,184][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-15 05:25:25,184][gdb0][WARNING] backup header from main header.
[2017-05-15 05:25:25,184][gdb0][WARNING] 
[2017-05-15 05:25:25,184][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-15 05:25:25,184][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-15 05:25:25,184][gdb0][WARNING] 
[2017-05-15 05:25:25,184][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-15 05:25:25,184][gdb0][WARNING] 
[2017-05-15 05:25:26,251][gdb0][DEBUG ] ****************************************************************************
[2017-05-15 05:25:26,252][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-15 05:25:26,252][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-15 05:25:26,252][gdb0][DEBUG ] ****************************************************************************
[2017-05-15 05:25:26,252][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-15 05:25:26,252][gdb0][DEBUG ] other utilities.
[2017-05-15 05:25:26,252][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-15 05:25:27,269][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-15 05:25:27,269][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:25:27,269][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-15 05:25:27,270][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:25:27,270][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:25:27,271][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:25:27,287][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:27,287][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:27,287][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-05-15 05:25:27,287][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:27,287][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-15 05:25:27,287][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:782b4b90-90ad-4778-9b19-ad6324ecda23 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-15 05:25:28,304][gdb0][DEBUG ] Setting name!
[2017-05-15 05:25:28,304][gdb0][DEBUG ] partNum is 1
[2017-05-15 05:25:28,304][gdb0][DEBUG ] REALLY setting name!
[2017-05-15 05:25:28,304][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:25:28,304][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:25:28,304][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:25:28,519][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:25:28,733][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:25:28,749][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:28,749][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:28,749][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:25:28,750][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/782b4b90-90ad-4778-9b19-ad6324ecda23
[2017-05-15 05:25:28,750][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-15 05:25:29,766][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:25:29,767][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:25:29,767][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:25:29,981][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:25:30,196][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:25:30,212][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/782b4b90-90ad-4778-9b19-ad6324ecda23
[2017-05-15 05:25:30,212][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:30,212][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-15 05:25:30,212][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:30,212][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-15 05:25:30,212][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:30,213][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-15 05:25:30,213][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:d0a1eb28-2fdb-43df-af58-fdc3fde192d4 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-15 05:25:31,229][gdb0][DEBUG ] Setting name!
[2017-05-15 05:25:31,229][gdb0][DEBUG ] partNum is 0
[2017-05-15 05:25:31,230][gdb0][DEBUG ] REALLY setting name!
[2017-05-15 05:25:31,230][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:25:31,230][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:25:31,230][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:25:31,444][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:25:31,709][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:25:31,923][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:31,924][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:31,924][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:25:31,924][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-15 05:25:31,924][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-15 05:25:32,589][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-15 05:25:32,590][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-15 05:25:32,590][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-15 05:25:32,590][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-15 05:25:32,590][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-15 05:25:32,590][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-15 05:25:32,590][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-15 05:25:32,590][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-15 05:25:32,590][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-15 05:25:32,590][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.vkdCwR with options noatime,inode64
[2017-05-15 05:25:32,590][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.vkdCwR
[2017-05-15 05:25:32,590][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.vkdCwR
[2017-05-15 05:25:32,591][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.vkdCwR/ceph_fsid.4408.tmp
[2017-05-15 05:25:32,592][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.vkdCwR/fsid.4408.tmp
[2017-05-15 05:25:32,595][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.vkdCwR/magic.4408.tmp
[2017-05-15 05:25:32,596][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.vkdCwR/journal_uuid.4408.tmp
[2017-05-15 05:25:32,600][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.vkdCwR/journal -> /dev/disk/by-partuuid/782b4b90-90ad-4778-9b19-ad6324ecda23
[2017-05-15 05:25:32,600][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.vkdCwR
[2017-05-15 05:25:32,601][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.vkdCwR
[2017-05-15 05:25:32,601][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.vkdCwR
[2017-05-15 05:25:32,632][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:25:32,633][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-15 05:25:33,700][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:25:33,700][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:25:33,700][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:25:33,914][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:25:34,179][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:25:34,211][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-15 05:25:34,245][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 05:25:39,418][gdb0][INFO  ] checking OSD status...
[2017-05-15 05:25:39,418][gdb0][DEBUG ] find the location of an executable
[2017-05-15 05:25:39,421][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-15 05:25:39,536][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-15 05:25:39,700][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:25:39,700][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-05-15 05:25:39,700][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:25:39,701][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:25:39,701][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:25:39,701][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 05:25:39,701][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:25:39,701][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fedc5d42518>
[2017-05-15 05:25:39,701][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:25:39,701][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-05-15 05:25:39,701][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fedc6659938>
[2017-05-15 05:25:39,701][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:25:39,701][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:25:39,701][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-15 05:25:39,944][gdb0][DEBUG ] connection detected need for sudo
[2017-05-15 05:25:40,172][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-15 05:25:40,172][gdb0][DEBUG ] detect platform information from remote host
[2017-05-15 05:25:40,189][gdb0][DEBUG ] detect machine type
[2017-05-15 05:25:40,193][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:25:40,196][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-15 05:25:40,427][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:25:40,658][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:25:40,659][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:25:40,675][gdb1][DEBUG ] detect machine type
[2017-05-15 05:25:40,679][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:25:40,681][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-15 05:25:40,681][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-15 05:25:40,696][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 05:25:40,710][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 05:25:40,710][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 05:25:40,727][gdb3][DEBUG ] detect machine type
[2017-05-15 05:25:40,729][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:25:40,731][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-15 05:26:53,427][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:26:53,427][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-05-15 05:26:53,427][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:26:53,427][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:26:53,427][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:26:53,427][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-15 05:26:53,427][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:26:53,427][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7e31355908>
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7e315abaa0>
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:26:53,428][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:26:53,429][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-15 05:26:53,672][gdb0][DEBUG ] connection detected need for sudo
[2017-05-15 05:26:53,908][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-15 05:26:53,908][gdb0][DEBUG ] detect platform information from remote host
[2017-05-15 05:26:53,925][gdb0][DEBUG ] detect machine type
[2017-05-15 05:26:53,929][gdb0][DEBUG ] find the location of an executable
[2017-05-15 05:26:53,930][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:26:53,930][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-15 05:26:53,930][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:26:53,933][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-15 05:26:53,933][gdb0][DEBUG ] find the location of an executable
[2017-05-15 05:26:53,935][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:26:54,055][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 05:26:54,071][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:26:54,087][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:26:54,103][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:26:54,118][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:54,118][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 05:26:54,119][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 05:26:54,126][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:54,126][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:54,126][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:54,126][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:26:54,127][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:26:54,127][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-15 05:26:54,142][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-15 05:26:54,150][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-15 05:26:54,157][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-15 05:26:54,165][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:54,165][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-15 05:26:54,165][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:54,165][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-15 05:26:54,181][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-15 05:26:54,182][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-15 05:26:54,198][gdb0][WARNING] 10+0 records in
[2017-05-15 05:26:54,198][gdb0][WARNING] 10+0 records out
[2017-05-15 05:26:54,198][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00735456 s, 1.4 GB/s
[2017-05-15 05:26:54,198][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-15 05:26:54,362][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-15 05:26:54,362][gdb0][WARNING] 10+0 records in
[2017-05-15 05:26:54,363][gdb0][WARNING] 10+0 records out
[2017-05-15 05:26:54,363][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00501285 s, 2.1 GB/s
[2017-05-15 05:26:54,363][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-15 05:26:54,363][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-15 05:26:54,363][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-15 05:26:54,363][gdb0][WARNING] backup header from main header.
[2017-05-15 05:26:54,363][gdb0][WARNING] 
[2017-05-15 05:26:54,363][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-15 05:26:54,363][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-15 05:26:54,363][gdb0][WARNING] 
[2017-05-15 05:26:54,363][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-15 05:26:54,363][gdb0][WARNING] 
[2017-05-15 05:26:55,380][gdb0][DEBUG ] ****************************************************************************
[2017-05-15 05:26:55,380][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-15 05:26:55,381][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-15 05:26:55,381][gdb0][DEBUG ] ****************************************************************************
[2017-05-15 05:26:55,381][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-15 05:26:55,381][gdb0][DEBUG ] other utilities.
[2017-05-15 05:26:55,381][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-15 05:26:56,398][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-15 05:26:56,398][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:26:56,398][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-15 05:26:56,398][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:26:56,406][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:26:56,437][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:26:56,441][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:56,442][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:56,442][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-05-15 05:26:56,442][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:56,442][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-15 05:26:56,443][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:c8cbafdd-0f54-40fe-bbb7-2dddbe7e9e60 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-15 05:26:57,459][gdb0][DEBUG ] Setting name!
[2017-05-15 05:26:57,460][gdb0][DEBUG ] partNum is 1
[2017-05-15 05:26:57,460][gdb0][DEBUG ] REALLY setting name!
[2017-05-15 05:26:57,460][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:26:57,460][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:26:57,460][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:26:57,674][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:26:57,889][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:26:57,897][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:57,897][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:57,897][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:26:57,897][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/c8cbafdd-0f54-40fe-bbb7-2dddbe7e9e60
[2017-05-15 05:26:57,897][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-15 05:26:58,914][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:26:58,914][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:26:58,914][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:26:59,129][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:26:59,293][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:26:59,508][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/c8cbafdd-0f54-40fe-bbb7-2dddbe7e9e60
[2017-05-15 05:26:59,508][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:59,508][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-15 05:26:59,508][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:59,508][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-15 05:26:59,509][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:26:59,509][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-15 05:26:59,509][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:3c7f865b-729c-4a3e-b1c7-ac697b9e8e37 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-15 05:27:00,526][gdb0][DEBUG ] Setting name!
[2017-05-15 05:27:00,526][gdb0][DEBUG ] partNum is 0
[2017-05-15 05:27:00,526][gdb0][DEBUG ] REALLY setting name!
[2017-05-15 05:27:00,526][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:27:00,526][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:27:00,526][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:00,741][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:27:00,955][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:01,019][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:01,019][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:01,019][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:27:01,020][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-15 05:27:01,020][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-15 05:27:01,786][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-15 05:27:01,786][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-15 05:27:01,786][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-15 05:27:01,786][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-15 05:27:01,786][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-15 05:27:01,786][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-15 05:27:01,786][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-15 05:27:01,787][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-15 05:27:01,787][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-15 05:27:01,787][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.q52KSY with options noatime,inode64
[2017-05-15 05:27:01,787][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.q52KSY
[2017-05-15 05:27:01,802][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.q52KSY
[2017-05-15 05:27:01,806][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.q52KSY/ceph_fsid.5466.tmp
[2017-05-15 05:27:01,809][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.q52KSY/fsid.5466.tmp
[2017-05-15 05:27:01,810][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.q52KSY/magic.5466.tmp
[2017-05-15 05:27:01,814][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.q52KSY/journal_uuid.5466.tmp
[2017-05-15 05:27:01,815][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.q52KSY/journal -> /dev/disk/by-partuuid/c8cbafdd-0f54-40fe-bbb7-2dddbe7e9e60
[2017-05-15 05:27:01,815][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.q52KSY
[2017-05-15 05:27:01,816][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.q52KSY
[2017-05-15 05:27:01,817][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.q52KSY
[2017-05-15 05:27:01,848][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:01,848][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-15 05:27:02,865][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:27:02,866][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:27:02,866][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:03,080][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:27:03,295][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:03,560][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-15 05:27:03,562][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 05:27:08,684][gdb0][INFO  ] checking OSD status...
[2017-05-15 05:27:08,684][gdb0][DEBUG ] find the location of an executable
[2017-05-15 05:27:08,687][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-15 05:27:08,852][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-15 05:27:33,492][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:27:33,492][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-05-15 05:27:33,492][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:27:33,492][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:27:33,492][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:27:33,492][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-15 05:27:33,492][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb232099908>
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb2322efaa0>
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:27:33,493][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:27:33,494][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-15 05:27:33,740][gdb0][DEBUG ] connection detected need for sudo
[2017-05-15 05:27:33,968][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-15 05:27:33,968][gdb0][DEBUG ] detect platform information from remote host
[2017-05-15 05:27:33,985][gdb0][DEBUG ] detect machine type
[2017-05-15 05:27:33,989][gdb0][DEBUG ] find the location of an executable
[2017-05-15 05:27:33,990][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:27:33,990][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-15 05:27:33,990][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:27:33,992][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-15 05:27:33,993][gdb0][DEBUG ] create a keyring file
[2017-05-15 05:27:33,994][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-15 05:27:33,994][gdb0][DEBUG ] find the location of an executable
[2017-05-15 05:27:33,997][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:27:34,117][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 05:27:34,125][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:27:34,141][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:27:34,157][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:27:34,172][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:34,172][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 05:27:34,172][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 05:27:34,188][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:34,188][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:34,188][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:34,188][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:27:34,188][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:27:34,189][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-15 05:27:34,196][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-15 05:27:34,204][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-15 05:27:34,211][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-15 05:27:34,227][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:34,227][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-15 05:27:34,227][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:34,227][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-15 05:27:34,235][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-15 05:27:34,235][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-15 05:27:34,243][gdb0][WARNING] 10+0 records in
[2017-05-15 05:27:34,243][gdb0][WARNING] 10+0 records out
[2017-05-15 05:27:34,243][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00770432 s, 1.4 GB/s
[2017-05-15 05:27:34,243][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-15 05:27:34,357][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-15 05:27:34,357][gdb0][WARNING] 10+0 records in
[2017-05-15 05:27:34,357][gdb0][WARNING] 10+0 records out
[2017-05-15 05:27:34,357][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00553906 s, 1.9 GB/s
[2017-05-15 05:27:34,357][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-15 05:27:34,357][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-15 05:27:34,358][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-15 05:27:34,358][gdb0][WARNING] backup header from main header.
[2017-05-15 05:27:34,358][gdb0][WARNING] 
[2017-05-15 05:27:34,358][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-15 05:27:34,358][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-15 05:27:34,358][gdb0][WARNING] 
[2017-05-15 05:27:34,358][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-15 05:27:34,358][gdb0][WARNING] 
[2017-05-15 05:27:35,425][gdb0][DEBUG ] ****************************************************************************
[2017-05-15 05:27:35,425][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-15 05:27:35,425][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-15 05:27:35,426][gdb0][DEBUG ] ****************************************************************************
[2017-05-15 05:27:35,426][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-15 05:27:35,426][gdb0][DEBUG ] other utilities.
[2017-05-15 05:27:35,426][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-15 05:27:36,443][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-15 05:27:36,443][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:27:36,443][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-15 05:27:36,443][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:36,443][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:27:36,444][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:36,460][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:36,460][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:36,460][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-05-15 05:27:36,460][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:36,460][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-15 05:27:36,461][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:556aedcb-deb6-45f9-8b46-22499c86207f --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-15 05:27:37,477][gdb0][DEBUG ] Setting name!
[2017-05-15 05:27:37,478][gdb0][DEBUG ] partNum is 1
[2017-05-15 05:27:37,478][gdb0][DEBUG ] REALLY setting name!
[2017-05-15 05:27:37,478][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:27:37,478][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:27:37,478][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:37,692][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:27:37,907][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:38,122][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:38,122][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:38,122][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:27:38,122][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/556aedcb-deb6-45f9-8b46-22499c86207f
[2017-05-15 05:27:38,122][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-15 05:27:39,139][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:27:39,139][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:27:39,139][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:39,354][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:27:39,468][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:39,682][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/556aedcb-deb6-45f9-8b46-22499c86207f
[2017-05-15 05:27:39,683][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:39,683][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-15 05:27:39,683][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:39,683][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-15 05:27:39,683][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:39,683][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-15 05:27:39,683][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:d5556b0f-ce9c-4a9c-8c10-efefde471248 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-15 05:27:40,700][gdb0][DEBUG ] Setting name!
[2017-05-15 05:27:40,700][gdb0][DEBUG ] partNum is 0
[2017-05-15 05:27:40,700][gdb0][DEBUG ] REALLY setting name!
[2017-05-15 05:27:40,700][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:27:40,700][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:27:40,700][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:40,915][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:27:41,179][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:41,211][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:41,212][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:41,212][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:27:41,212][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-15 05:27:41,212][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-15 05:27:41,878][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-15 05:27:41,878][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-15 05:27:41,878][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-15 05:27:41,878][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-15 05:27:41,878][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-15 05:27:41,878][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-15 05:27:41,878][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-15 05:27:41,878][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-15 05:27:41,878][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-15 05:27:41,878][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.9qzdg7 with options noatime,inode64
[2017-05-15 05:27:41,879][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.9qzdg7
[2017-05-15 05:27:41,910][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.9qzdg7
[2017-05-15 05:27:41,910][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9qzdg7/ceph_fsid.6504.tmp
[2017-05-15 05:27:41,911][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9qzdg7/fsid.6504.tmp
[2017-05-15 05:27:41,911][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9qzdg7/magic.6504.tmp
[2017-05-15 05:27:41,911][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9qzdg7/journal_uuid.6504.tmp
[2017-05-15 05:27:41,914][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.9qzdg7/journal -> /dev/disk/by-partuuid/556aedcb-deb6-45f9-8b46-22499c86207f
[2017-05-15 05:27:41,914][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9qzdg7
[2017-05-15 05:27:41,914][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.9qzdg7
[2017-05-15 05:27:41,915][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.9qzdg7
[2017-05-15 05:27:41,946][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:41,947][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-15 05:27:42,963][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-15 05:27:42,963][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-15 05:27:42,964][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-15 05:27:42,964][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 05:27:42,964][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:27:42,964][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:42,964][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:27:43,128][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:27:43,160][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-15 05:27:43,178][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 05:27:48,300][gdb0][INFO  ] checking OSD status...
[2017-05-15 05:27:48,300][gdb0][DEBUG ] find the location of an executable
[2017-05-15 05:27:48,303][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-15 05:27:48,468][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-15 05:27:58,470][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:27:58,470][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 05:27:58,470][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:27:58,470][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:27:58,470][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:27:58,470][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 05:27:58,470][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:27:58,470][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:27:58,470][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:27:58,470][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:27:58,470][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:27:58,470][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:27:58,471][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:27:58,471][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:27:58,471][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcccda5d908>
[2017-05-15 05:27:58,471][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:27:58,471][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:27:58,471][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcccdcb3aa0>
[2017-05-15 05:27:58,471][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:27:58,471][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:27:58,471][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:27:58,471][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 05:27:58,708][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:27:58,899][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:27:58,900][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:27:58,916][gdb1][DEBUG ] detect machine type
[2017-05-15 05:27:58,920][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:27:58,920][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:27:58,920][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-15 05:27:58,921][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:27:58,923][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-15 05:27:58,923][gdb1][DEBUG ] create a keyring file
[2017-05-15 05:27:58,925][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-15 05:27:58,925][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:27:58,927][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:27:59,047][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 05:27:59,063][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:27:59,070][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:27:59,086][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:27:59,102][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:59,102][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 05:27:59,102][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 05:27:59,105][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:59,105][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:59,106][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:59,106][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:27:59,106][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:27:59,106][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-15 05:27:59,121][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-15 05:27:59,129][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-15 05:27:59,145][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-15 05:27:59,148][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:59,148][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-15 05:27:59,148][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:27:59,149][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-15 05:27:59,180][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-15 05:27:59,180][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-15 05:27:59,188][gdb1][WARNING] 10+0 records in
[2017-05-15 05:27:59,188][gdb1][WARNING] 10+0 records out
[2017-05-15 05:27:59,188][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00708027 s, 1.5 GB/s
[2017-05-15 05:27:59,188][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-15 05:27:59,353][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-15 05:27:59,353][gdb1][WARNING] 10+0 records in
[2017-05-15 05:27:59,353][gdb1][WARNING] 10+0 records out
[2017-05-15 05:27:59,353][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.0054319 s, 1.9 GB/s
[2017-05-15 05:27:59,353][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-15 05:27:59,353][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-15 05:27:59,353][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-15 05:27:59,353][gdb1][WARNING] backup header from main header.
[2017-05-15 05:27:59,353][gdb1][WARNING] 
[2017-05-15 05:27:59,353][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-15 05:27:59,353][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-15 05:27:59,354][gdb1][WARNING] 
[2017-05-15 05:27:59,354][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-15 05:27:59,354][gdb1][WARNING] 
[2017-05-15 05:28:00,371][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:28:00,371][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-15 05:28:00,371][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-15 05:28:00,371][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:28:00,371][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-15 05:28:00,371][gdb1][DEBUG ] other utilities.
[2017-05-15 05:28:00,371][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-15 05:28:01,388][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-15 05:28:01,388][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:28:01,389][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-15 05:28:01,389][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:28:01,389][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:28:01,421][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:28:01,452][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:28:01,453][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:28:01,453][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-05-15 05:28:01,453][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:28:01,453][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-15 05:28:01,453][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:12477e33-d11e-40a3-9228-1ad6ab83c5e2 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-15 05:28:02,470][gdb1][DEBUG ] Setting name!
[2017-05-15 05:28:02,470][gdb1][DEBUG ] partNum is 1
[2017-05-15 05:28:02,470][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:28:02,470][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:28:02,470][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:28:02,470][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:28:02,685][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:28:02,849][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:28:02,881][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:28:02,881][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:28:02,881][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:28:02,881][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/12477e33-d11e-40a3-9228-1ad6ab83c5e2
[2017-05-15 05:28:02,881][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-15 05:28:03,898][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:28:03,899][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:28:03,900][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:28:04,114][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:28:04,279][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:28:04,311][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/12477e33-d11e-40a3-9228-1ad6ab83c5e2
[2017-05-15 05:28:04,311][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:28:04,311][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-15 05:28:04,311][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:28:04,311][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-15 05:28:04,311][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:28:04,311][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-15 05:28:04,311][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:3daa8fd5-f1b1-4132-b64a-3d9aadb4f383 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-15 05:28:05,329][gdb1][DEBUG ] Setting name!
[2017-05-15 05:28:05,329][gdb1][DEBUG ] partNum is 0
[2017-05-15 05:28:05,329][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:28:05,329][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:28:05,329][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:28:05,329][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:28:05,544][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:28:05,758][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:28:05,973][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:28:05,973][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:28:05,973][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:28:05,973][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-15 05:28:05,973][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-15 05:28:06,639][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-15 05:28:06,639][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-15 05:28:06,639][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-15 05:28:06,639][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-15 05:28:06,639][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-15 05:28:06,640][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-15 05:28:06,640][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-15 05:28:06,640][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-15 05:28:06,640][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-15 05:28:06,640][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.mOSXSh with options noatime,inode64
[2017-05-15 05:28:06,640][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.mOSXSh
[2017-05-15 05:28:06,648][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.mOSXSh
[2017-05-15 05:28:06,648][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.mOSXSh/ceph_fsid.6189.tmp
[2017-05-15 05:28:06,664][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.mOSXSh/fsid.6189.tmp
[2017-05-15 05:28:06,664][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.mOSXSh/magic.6189.tmp
[2017-05-15 05:28:06,667][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.mOSXSh/journal_uuid.6189.tmp
[2017-05-15 05:28:06,667][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.mOSXSh/journal -> /dev/disk/by-partuuid/12477e33-d11e-40a3-9228-1ad6ab83c5e2
[2017-05-15 05:28:06,668][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.mOSXSh
[2017-05-15 05:28:06,670][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.mOSXSh
[2017-05-15 05:28:06,670][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.mOSXSh
[2017-05-15 05:28:06,734][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:28:06,734][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-15 05:28:07,751][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-15 05:28:07,751][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-15 05:28:07,751][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-15 05:28:07,751][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:28:07,751][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:28:07,751][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:28:07,751][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:28:07,966][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:28:07,966][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-15 05:28:07,984][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 05:28:13,106][gdb1][INFO  ] checking OSD status...
[2017-05-15 05:28:13,106][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:28:13,109][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-15 05:28:13,224][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-15 05:28:46,267][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:28:46,267][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb1
[2017-05-15 05:28:46,267][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:28:46,267][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:28:46,267][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:28:46,268][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 05:28:46,268][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:28:46,268][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fed9af5afc8>
[2017-05-15 05:28:46,268][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:28:46,268][ceph_deploy.cli][INFO  ]  host                          : ['gdb1']
[2017-05-15 05:28:46,268][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fed9b86d1b8>
[2017-05-15 05:28:46,268][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:28:46,268][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:28:46,268][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-15 05:28:46,268][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-15 05:28:46,268][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb1
[2017-05-15 05:28:46,268][ceph_deploy.install][DEBUG ] Detecting platform for host gdb1 ...
[2017-05-15 05:28:46,503][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:28:46,731][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:28:46,731][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:28:46,747][gdb1][DEBUG ] detect machine type
[2017-05-15 05:28:46,751][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:28:46,751][gdb1][INFO  ] Purging Ceph on gdb1
[2017-05-15 05:28:46,752][gdb1][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-15 05:28:46,791][gdb1][DEBUG ] Reading package lists...
[2017-05-15 05:28:46,955][gdb1][DEBUG ] Building dependency tree...
[2017-05-15 05:28:46,955][gdb1][DEBUG ] Reading state information...
[2017-05-15 05:28:47,019][gdb1][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-15 05:28:47,019][gdb1][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-15 05:28:47,019][gdb1][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-15 05:28:47,019][gdb1][DEBUG ]   bc ca-certificates-java ceph-fuse cmake cmake-data cython cython3
[2017-05-15 05:28:47,019][gdb1][DEBUG ]   default-jdk default-jdk-headless default-jre default-jre-headless dh-systemd
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   docutils-common fontconfig fonts-font-awesome fonts-lato icu-devtools
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   java-common javahelper javascript-common jq junit4 libaio-dev libaio1
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   libarchive13 libasound2 libasound2-data libasyncns0 libatk1.0-0
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   libatk1.0-data libatomic-ops-dev libavahi-client3 libavahi-common-data
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   libavahi-common3 libbabeltrace-ctf-dev libbabeltrace-dev libblkid-dev
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   libcairo2 libcephfs2 libcups2 libcurl3 libdatrie1 libdrm-amdgpu1
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   libdrm-intel1 libdrm-nouveau2 libdrm-radeon1 libexpat1-dev libfcgi-dev
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   libfcgi0ldbl libflac8 libfuse-dev libgdk-pixbuf2.0-0 libgdk-pixbuf2.0-common
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   libgif7 libgl1-mesa-dri libgl1-mesa-glx libglapi-mesa
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   libgoogle-perftools-dev libgoogle-perftools4 libgraphite2-3 libgtk2.0-0
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   libgtk2.0-common libhamcrest-java libharfbuzz0b libibverbs-dev libibverbs1
[2017-05-15 05:28:47,020][gdb1][DEBUG ]   libicu-dev libjs-jquery libjs-modernizr libjs-sphinxdoc libjs-underscore
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   libjsoncpp1 libkeyutils-dev liblcms2-2 libleveldb-dev libleveldb1v5
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   libllvm3.8 liblttng-ust-ctl2 liblttng-ust-dev liblttng-ust-python-agent0
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   liblttng-ust0 libnspr4 libnspr4-dev libnss3 libnss3-dev libnss3-nssdb
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   libogg0 libonig2 libpango-1.0-0 libpangocairo-1.0-0 libpangoft2-1.0-0
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   libpciaccess0 libpcre16-3 libpcre3-dev libpcre32-3 libpcrecpp0v5
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   libpixman-1-0 libpulse0 libpython-all-dev libpython-dev libpython2.7
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   libpython2.7-dev libpython3-all-dev libpython3-dev libpython3.5-dev
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   librados2 libradosstriper1 librbd1 librgw2 libselinux1-dev libsepol1-dev
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   libsnappy-dev libsnappy1v5 libsndfile1 libssl-dev libtcmalloc-minimal4
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   libthai-data libthai0 libudev-dev libunwind-dev libunwind8 libunwind8-dev
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   liburcu-dev liburcu4 libvorbis0a libvorbisenc2 libx11-xcb1 libxcb-dri2-0
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-render0 libxcb-shm0
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   libxcb-sync1 libxcomposite1 libxcursor1 libxdamage1 libxfixes3 libxi6
[2017-05-15 05:28:47,021][gdb1][DEBUG ]   libxinerama1 libxml2-dev libxrandr2 libxrender1 libxshmfence1 libxtst6
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   libxxf86vm1 openjdk-8-jdk openjdk-8-jdk-headless openjdk-8-jre
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   openjdk-8-jre-headless python-alabaster python-all python-all-dev
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   python-babel python-babel-localedata python-blinker python-cephfs
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-dev
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   python-docutils python-enum34 python-flask python-idna python-ipaddress
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   python-itsdangerous python-jinja2 python-markupsafe python-ndg-httpsclient
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   python-nose python-openssl python-pyasn1 python-pygments python-pyinotify
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   python-rados python-rbd python-requests python-rgw python-roman python-six
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   python-sphinx python-sphinx-rtd-theme python-tz python-urllib3
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   python-werkzeug python2.7-dev python3-all python3-all-dev python3-dev
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   python3.5-dev sphinx-common sphinx-rtd-theme-common uuid-dev valgrind
[2017-05-15 05:28:47,022][gdb1][DEBUG ]   x11-common xfslibs-dev xmlstarlet yasm
[2017-05-15 05:28:47,022][gdb1][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-15 05:28:47,086][gdb1][DEBUG ] The following packages will be REMOVED:
[2017-05-15 05:28:47,087][gdb1][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-15 05:28:47,351][gdb1][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 31 not upgraded.
[2017-05-15 05:28:47,351][gdb1][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-15 05:28:47,817][gdb1][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 96777 files and directories currently installed.)
[2017-05-15 05:28:47,817][gdb1][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-15 05:28:48,031][gdb1][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-15 05:28:48,095][gdb1][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-15 05:28:48,159][gdb1][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-15 05:28:48,374][gdb1][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-15 05:28:48,488][gdb1][DEBUG ] dpkg: warning: while removing ceph-osd, directory '/var/lib/ceph/osd' not empty so not removed
[2017-05-15 05:28:48,488][gdb1][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-15 05:28:48,653][gdb1][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-15 05:28:48,717][gdb1][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-15 05:28:48,732][gdb1][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-15 05:28:48,947][gdb1][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-15 05:28:49,011][gdb1][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-15 05:28:49,027][gdb1][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-15 05:28:49,191][gdb1][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-15 05:28:49,193][gdb1][DEBUG ] dpkg: warning: while removing ceph-base, directory '/usr/share/doc/ceph' not empty so not removed
[2017-05-15 05:28:49,193][gdb1][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[2017-05-15 05:28:49,193][gdb1][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-05-15 05:28:49,225][gdb1][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-15 05:28:49,389][gdb1][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-15 05:28:49,453][gdb1][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-15 05:28:49,469][gdb1][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-15 05:28:49,683][gdb1][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-15 05:28:50,750][gdb1][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-15 05:28:56,032][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:28:56,032][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 05:28:56,032][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:28:56,032][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:28:56,032][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcde5d8a908>
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcde5fe0aa0>
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:28:56,033][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:28:56,034][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:28:56,034][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 05:28:56,276][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:28:56,503][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:28:56,504][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:28:56,519][gdb1][DEBUG ] detect machine type
[2017-05-15 05:28:56,523][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:28:56,524][ceph_deploy.osd][ERROR ] ceph needs to be installed in remote host: gdb1
[2017-05-15 05:28:56,524][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-15 05:31:25,511][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:31:25,511][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 05:31:25,511][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:31:25,511][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:31:25,511][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:31:25,511][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 05:31:25,511][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9900068908>
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f99002beaa0>
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:31:25,512][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:31:25,513][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 05:31:25,751][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:31:25,979][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:31:25,979][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:31:25,995][gdb1][DEBUG ] detect machine type
[2017-05-15 05:31:25,999][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:31:26,000][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:31:26,000][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-15 05:31:26,000][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:31:26,004][ceph_deploy.osd][ERROR ] OSError: [Errno 2] No such file or directory: '/etc/ceph/tmpqNrM6j'
[2017-05-15 05:31:26,004][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-15 05:31:39,151][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:31:39,151][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb1
[2017-05-15 05:31:39,151][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:31:39,151][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:31:39,151][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:31:39,151][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 05:31:39,151][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:31:39,152][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff57daa0fc8>
[2017-05-15 05:31:39,152][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:31:39,152][ceph_deploy.cli][INFO  ]  host                          : ['gdb1']
[2017-05-15 05:31:39,152][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7ff57e3b31b8>
[2017-05-15 05:31:39,152][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:31:39,152][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:31:39,152][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-15 05:31:39,152][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-15 05:31:39,152][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb1
[2017-05-15 05:31:39,152][ceph_deploy.install][DEBUG ] Detecting platform for host gdb1 ...
[2017-05-15 05:31:39,387][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:31:39,611][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:31:39,611][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:31:39,627][gdb1][DEBUG ] detect machine type
[2017-05-15 05:31:39,631][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:31:39,631][gdb1][INFO  ] Purging Ceph on gdb1
[2017-05-15 05:31:39,632][gdb1][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-15 05:31:39,670][gdb1][DEBUG ] Reading package lists...
[2017-05-15 05:31:39,835][gdb1][DEBUG ] Building dependency tree...
[2017-05-15 05:31:39,835][gdb1][DEBUG ] Reading state information...
[2017-05-15 05:31:39,899][gdb1][DEBUG ] Package 'ceph' is not installed, so not removed
[2017-05-15 05:31:39,899][gdb1][DEBUG ] Package 'ceph-common' is not installed, so not removed
[2017-05-15 05:31:39,899][gdb1][DEBUG ] Package 'ceph-mds' is not installed, so not removed
[2017-05-15 05:31:39,899][gdb1][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-15 05:31:39,899][gdb1][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-15 05:31:39,899][gdb1][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-15 05:31:39,899][gdb1][DEBUG ]   bc ca-certificates-java ceph-fuse cmake cmake-data cython cython3
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   default-jdk default-jdk-headless default-jre default-jre-headless dh-systemd
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   docutils-common fontconfig fonts-font-awesome fonts-lato icu-devtools
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   java-common javahelper javascript-common jq junit4 libaio-dev libaio1
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   libarchive13 libasound2 libasound2-data libasyncns0 libatk1.0-0
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   libatk1.0-data libatomic-ops-dev libavahi-client3 libavahi-common-data
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   libavahi-common3 libbabeltrace-ctf-dev libbabeltrace-dev libblkid-dev
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   libcairo2 libcephfs2 libcups2 libcurl3 libdatrie1 libdrm-amdgpu1
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   libdrm-intel1 libdrm-nouveau2 libdrm-radeon1 libexpat1-dev libfcgi-dev
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   libfcgi0ldbl libflac8 libfuse-dev libgdk-pixbuf2.0-0 libgdk-pixbuf2.0-common
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   libgif7 libgl1-mesa-dri libgl1-mesa-glx libglapi-mesa
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   libgoogle-perftools-dev libgoogle-perftools4 libgraphite2-3 libgtk2.0-0
[2017-05-15 05:31:39,900][gdb1][DEBUG ]   libgtk2.0-common libhamcrest-java libharfbuzz0b libibverbs-dev libibverbs1
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   libicu-dev libjs-jquery libjs-modernizr libjs-sphinxdoc libjs-underscore
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   libjsoncpp1 libkeyutils-dev liblcms2-2 libleveldb-dev libleveldb1v5
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   libllvm3.8 liblttng-ust-ctl2 liblttng-ust-dev liblttng-ust-python-agent0
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   liblttng-ust0 libnspr4 libnspr4-dev libnss3 libnss3-dev libnss3-nssdb
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   libogg0 libonig2 libpango-1.0-0 libpangocairo-1.0-0 libpangoft2-1.0-0
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   libpciaccess0 libpcre16-3 libpcre3-dev libpcre32-3 libpcrecpp0v5
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   libpixman-1-0 libpulse0 libpython-all-dev libpython-dev libpython2.7
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   libpython2.7-dev libpython3-all-dev libpython3-dev libpython3.5-dev
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   librados2 libradosstriper1 librbd1 librgw2 libselinux1-dev libsepol1-dev
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   libsnappy-dev libsnappy1v5 libsndfile1 libssl-dev libtcmalloc-minimal4
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   libthai-data libthai0 libudev-dev libunwind-dev libunwind8 libunwind8-dev
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   liburcu-dev liburcu4 libvorbis0a libvorbisenc2 libx11-xcb1 libxcb-dri2-0
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-render0 libxcb-shm0
[2017-05-15 05:31:39,901][gdb1][DEBUG ]   libxcb-sync1 libxcomposite1 libxcursor1 libxdamage1 libxfixes3 libxi6
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   libxinerama1 libxml2-dev libxrandr2 libxrender1 libxshmfence1 libxtst6
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   libxxf86vm1 openjdk-8-jdk openjdk-8-jdk-headless openjdk-8-jre
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   openjdk-8-jre-headless python-alabaster python-all python-all-dev
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   python-babel python-babel-localedata python-blinker python-cephfs
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-dev
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   python-docutils python-enum34 python-flask python-idna python-ipaddress
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   python-itsdangerous python-jinja2 python-markupsafe python-ndg-httpsclient
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   python-nose python-openssl python-pyasn1 python-pygments python-pyinotify
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   python-rados python-rbd python-requests python-rgw python-roman python-six
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   python-sphinx python-sphinx-rtd-theme python-tz python-urllib3
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   python-werkzeug python2.7-dev python3-all python3-all-dev python3-dev
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   python3.5-dev sphinx-common sphinx-rtd-theme-common uuid-dev valgrind
[2017-05-15 05:31:39,902][gdb1][DEBUG ]   x11-common xfslibs-dev xmlstarlet yasm
[2017-05-15 05:31:39,902][gdb1][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-15 05:31:39,903][gdb1][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.
[2017-05-15 05:31:52,142][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:31:52,143][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:31:52,144][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:31:52,144][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f55ddfe5908>
[2017-05-15 05:31:52,144][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:31:52,144][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:31:52,144][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f55de23baa0>
[2017-05-15 05:31:52,144][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:31:52,144][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:31:52,144][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:31:52,144][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 05:31:52,379][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:31:52,606][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:31:52,607][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:31:52,623][gdb1][DEBUG ] detect machine type
[2017-05-15 05:31:52,626][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:31:52,627][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:31:52,627][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-15 05:31:52,627][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:31:52,629][ceph_deploy.osd][ERROR ] OSError: [Errno 2] No such file or directory: '/etc/ceph/tmpghTVqu'
[2017-05-15 05:31:52,629][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-15 05:32:10,455][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:32:10,455][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb1
[2017-05-15 05:32:10,456][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:32:10,456][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:32:10,456][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:32:10,456][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 05:32:10,456][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:32:10,456][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5a3ffe1710>
[2017-05-15 05:32:10,456][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:32:10,456][ceph_deploy.cli][INFO  ]  host                          : ['gdb1']
[2017-05-15 05:32:10,456][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f5a408ee230>
[2017-05-15 05:32:10,456][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:32:10,456][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:32:10,456][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb1
[2017-05-15 05:32:10,695][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:32:10,918][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:32:10,919][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:32:10,935][gdb1][DEBUG ] detect machine type
[2017-05-15 05:32:10,938][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:32:10,939][ceph_deploy.install][ERROR ] Ceph is still installed on: ['gdb1']
[2017-05-15 05:32:10,939][ceph_deploy][ERROR ] RuntimeError: refusing to purge data while Ceph is still installed

[2017-05-15 05:32:51,517][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:32:51,517][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 05:32:51,517][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:32:51,517][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:32:51,517][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f387a101908>
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f387a357aa0>
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:32:51,518][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:32:51,519][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:32:51,519][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 05:32:51,760][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:32:51,991][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:32:51,991][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:32:52,008][gdb1][DEBUG ] detect machine type
[2017-05-15 05:32:52,011][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:32:52,012][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:32:52,012][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-15 05:32:52,012][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:32:52,015][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-15 05:32:52,015][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:32:52,016][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:32:52,137][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 05:32:52,153][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:32:52,168][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:32:52,184][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:32:52,191][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:52,192][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 05:32:52,192][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 05:32:52,207][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:52,207][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:52,207][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:52,208][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:32:52,208][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:32:52,208][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-15 05:32:52,322][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-15 05:32:52,322][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-15 05:32:52,322][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-15 05:32:52,322][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:52,322][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-15 05:32:52,322][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:52,322][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-15 05:32:52,338][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-15 05:32:52,338][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-15 05:32:52,503][gdb1][WARNING] 10+0 records in
[2017-05-15 05:32:52,503][gdb1][WARNING] 10+0 records out
[2017-05-15 05:32:52,503][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.125353 s, 83.6 MB/s
[2017-05-15 05:32:52,503][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-15 05:32:52,503][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-15 05:32:52,503][gdb1][WARNING] 10+0 records in
[2017-05-15 05:32:52,503][gdb1][WARNING] 10+0 records out
[2017-05-15 05:32:52,503][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00503594 s, 2.1 GB/s
[2017-05-15 05:32:52,503][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-15 05:32:52,503][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-15 05:32:52,504][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-15 05:32:52,504][gdb1][WARNING] backup header from main header.
[2017-05-15 05:32:52,504][gdb1][WARNING] 
[2017-05-15 05:32:52,504][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-15 05:32:52,504][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-15 05:32:52,504][gdb1][WARNING] 
[2017-05-15 05:32:52,504][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-15 05:32:52,504][gdb1][WARNING] 
[2017-05-15 05:32:53,621][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:32:53,622][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-15 05:32:53,622][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-15 05:32:53,622][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:32:53,622][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-15 05:32:53,622][gdb1][DEBUG ] other utilities.
[2017-05-15 05:32:53,622][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-15 05:32:54,639][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-15 05:32:54,640][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:32:54,640][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-15 05:32:54,640][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:32:54,655][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:32:54,687][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:32:54,703][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:54,703][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:54,703][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-05-15 05:32:54,703][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:54,703][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-15 05:32:54,703][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:6066903b-da35-4cca-912e-21bcf43d2e47 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-15 05:32:55,771][gdb1][DEBUG ] Setting name!
[2017-05-15 05:32:55,771][gdb1][DEBUG ] partNum is 1
[2017-05-15 05:32:55,771][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:32:55,771][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:32:55,771][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:32:55,772][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:32:55,986][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:32:56,150][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:32:56,365][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:56,365][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:56,365][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:32:56,366][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/6066903b-da35-4cca-912e-21bcf43d2e47
[2017-05-15 05:32:56,366][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-15 05:32:57,383][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:32:57,383][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:32:57,383][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:32:57,547][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:32:57,812][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:32:58,027][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/6066903b-da35-4cca-912e-21bcf43d2e47
[2017-05-15 05:32:58,027][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:58,027][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-15 05:32:58,027][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:58,027][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-15 05:32:58,027][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:58,027][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-15 05:32:58,028][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f6e8dea6-9511-4b90-8377-9a816de6f905 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-15 05:32:59,045][gdb1][DEBUG ] Setting name!
[2017-05-15 05:32:59,045][gdb1][DEBUG ] partNum is 0
[2017-05-15 05:32:59,045][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:32:59,045][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:32:59,045][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:32:59,045][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:32:59,260][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:32:59,525][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:32:59,557][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:59,557][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:32:59,557][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:32:59,557][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-15 05:32:59,557][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-15 05:33:00,323][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-15 05:33:00,324][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-15 05:33:00,324][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-15 05:33:00,324][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-15 05:33:00,324][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-15 05:33:00,324][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-15 05:33:00,324][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-15 05:33:00,324][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-15 05:33:00,324][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-15 05:33:00,324][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.O4z34v with options noatime,inode64
[2017-05-15 05:33:00,324][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.O4z34v
[2017-05-15 05:33:00,325][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.O4z34v
[2017-05-15 05:33:00,325][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.O4z34v/ceph_fsid.12144.tmp
[2017-05-15 05:33:00,325][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.O4z34v/fsid.12144.tmp
[2017-05-15 05:33:00,328][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.O4z34v/magic.12144.tmp
[2017-05-15 05:33:00,329][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.O4z34v/journal_uuid.12144.tmp
[2017-05-15 05:33:00,333][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.O4z34v/journal -> /dev/disk/by-partuuid/6066903b-da35-4cca-912e-21bcf43d2e47
[2017-05-15 05:33:00,333][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.O4z34v
[2017-05-15 05:33:00,334][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.O4z34v
[2017-05-15 05:33:00,334][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.O4z34v
[2017-05-15 05:33:00,366][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:33:00,366][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-15 05:33:01,383][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-15 05:33:01,383][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-15 05:33:01,383][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-15 05:33:01,384][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:33:01,384][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:33:01,384][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:33:01,385][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:33:01,650][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:33:01,653][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-15 05:33:01,687][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 05:33:06,859][gdb1][INFO  ] checking OSD status...
[2017-05-15 05:33:06,860][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:33:06,862][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-15 05:33:06,977][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-15 05:34:23,472][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:34:23,472][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 05:34:23,472][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:34:23,472][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:34:23,472][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:34:23,472][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 05:34:23,472][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f583e125908>
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f583e37baa0>
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:34:23,473][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:34:23,474][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 05:34:23,711][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:34:23,943][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:34:23,944][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:34:23,960][gdb1][DEBUG ] detect machine type
[2017-05-15 05:34:23,963][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:34:23,964][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:34:23,964][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-15 05:34:23,965][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:34:23,967][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-15 05:34:23,967][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:34:23,969][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:34:24,140][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 05:34:24,140][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:34:24,140][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:34:24,140][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:34:24,141][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:24,141][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 05:34:24,141][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 05:34:24,156][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:24,156][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:24,156][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:24,157][gdb1][WARNING] Traceback (most recent call last):
[2017-05-15 05:34:24,157][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-15 05:34:24,157][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-15 05:34:24,157][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-15 05:34:24,157][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-15 05:34:24,157][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2029, in main
[2017-05-15 05:34:24,157][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2018, in prepare
[2017-05-15 05:34:24,157][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2049, in prepare_locked
[2017-05-15 05:34:24,157][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2816, in prepare
[2017-05-15 05:34:24,157][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2971, in prepare_device
[2017-05-15 05:34:24,157][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2878, in prepare_device
[2017-05-15 05:34:24,157][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2841, in sanity_checks
[2017-05-15 05:34:24,158][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-05-15 05:34:24,158][gdb1][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-05-15 05:34:24,162][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-15 05:34:24,162][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:34:24,162][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-15 05:34:56,532][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:34:56,533][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 05:34:56,533][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:34:56,533][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:34:56,533][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:34:56,533][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 05:34:56,533][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:34:56,533][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:34:56,533][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:34:56,533][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:34:56,533][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:34:56,533][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:34:56,534][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:34:56,534][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:34:56,534][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f684ba64908>
[2017-05-15 05:34:56,534][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:34:56,534][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:34:56,534][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f684bcbaaa0>
[2017-05-15 05:34:56,534][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:34:56,534][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:34:56,534][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:34:56,534][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 05:34:56,776][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:34:57,007][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:34:57,007][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:34:57,024][gdb1][DEBUG ] detect machine type
[2017-05-15 05:34:57,027][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:34:57,028][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:34:57,028][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-15 05:34:57,028][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:34:57,031][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-15 05:34:57,031][gdb1][DEBUG ] create a keyring file
[2017-05-15 05:34:57,032][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-15 05:34:57,032][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:34:57,034][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:34:57,154][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 05:34:57,170][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:34:57,186][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:34:57,194][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:34:57,209][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:57,209][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 05:34:57,209][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 05:34:57,217][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:57,217][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:57,217][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:57,218][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:34:57,218][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:34:57,218][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-15 05:34:57,233][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-15 05:34:57,241][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-15 05:34:57,248][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-15 05:34:57,264][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:57,264][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-15 05:34:57,264][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:57,264][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-15 05:34:57,296][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-15 05:34:57,296][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-15 05:34:57,304][gdb1][WARNING] 10+0 records in
[2017-05-15 05:34:57,304][gdb1][WARNING] 10+0 records out
[2017-05-15 05:34:57,304][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00694986 s, 1.5 GB/s
[2017-05-15 05:34:57,304][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-15 05:34:57,469][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-15 05:34:57,532][gdb1][WARNING] 10+0 records in
[2017-05-15 05:34:57,533][gdb1][WARNING] 10+0 records out
[2017-05-15 05:34:57,533][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.0601053 s, 174 MB/s
[2017-05-15 05:34:57,533][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-15 05:34:57,533][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-15 05:34:57,533][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-15 05:34:57,533][gdb1][WARNING] backup header from main header.
[2017-05-15 05:34:57,533][gdb1][WARNING] 
[2017-05-15 05:34:57,533][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-15 05:34:57,533][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-15 05:34:57,533][gdb1][WARNING] 
[2017-05-15 05:34:57,533][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-15 05:34:57,533][gdb1][WARNING] 
[2017-05-15 05:34:58,550][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:34:58,550][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-15 05:34:58,551][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-15 05:34:58,551][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:34:58,551][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-15 05:34:58,551][gdb1][DEBUG ] other utilities.
[2017-05-15 05:34:58,551][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-15 05:34:59,568][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-15 05:34:59,568][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:34:59,568][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-15 05:34:59,568][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:34:59,568][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:34:59,584][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:34:59,600][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:59,600][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:59,600][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-05-15 05:34:59,600][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:34:59,600][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-15 05:34:59,600][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:9eb0ad61-4c75-487d-b610-2bc00ec0b818 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-15 05:35:00,667][gdb1][DEBUG ] Setting name!
[2017-05-15 05:35:00,667][gdb1][DEBUG ] partNum is 1
[2017-05-15 05:35:00,667][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:35:00,667][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:35:00,668][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:35:00,668][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:35:00,832][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:35:01,097][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:35:01,311][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:35:01,311][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:35:01,311][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:35:01,311][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/9eb0ad61-4c75-487d-b610-2bc00ec0b818
[2017-05-15 05:35:01,312][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-15 05:35:02,329][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:35:02,329][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:35:02,329][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:35:02,544][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:35:02,708][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:35:02,772][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/9eb0ad61-4c75-487d-b610-2bc00ec0b818
[2017-05-15 05:35:02,772][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:35:02,772][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-15 05:35:02,772][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:35:02,773][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-15 05:35:02,773][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:35:02,773][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-15 05:35:02,773][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:de8caa4f-b593-409c-bcee-ce0804588604 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-15 05:35:03,790][gdb1][DEBUG ] Setting name!
[2017-05-15 05:35:03,790][gdb1][DEBUG ] partNum is 0
[2017-05-15 05:35:03,790][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:35:03,790][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:35:03,790][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:35:03,790][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:35:04,005][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:35:04,219][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:35:04,251][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:35:04,251][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:35:04,251][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:35:04,251][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-15 05:35:04,252][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-15 05:35:05,018][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-15 05:35:05,018][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-15 05:35:05,018][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-15 05:35:05,018][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-15 05:35:05,018][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-15 05:35:05,018][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-15 05:35:05,018][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-15 05:35:05,018][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-15 05:35:05,018][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-15 05:35:05,018][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.zQoUb1 with options noatime,inode64
[2017-05-15 05:35:05,019][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.zQoUb1
[2017-05-15 05:35:05,034][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.zQoUb1
[2017-05-15 05:35:05,034][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zQoUb1/ceph_fsid.16667.tmp
[2017-05-15 05:35:05,036][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zQoUb1/fsid.16667.tmp
[2017-05-15 05:35:05,039][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zQoUb1/magic.16667.tmp
[2017-05-15 05:35:05,042][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zQoUb1/journal_uuid.16667.tmp
[2017-05-15 05:35:05,042][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.zQoUb1/journal -> /dev/disk/by-partuuid/9eb0ad61-4c75-487d-b610-2bc00ec0b818
[2017-05-15 05:35:05,043][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.zQoUb1
[2017-05-15 05:35:05,046][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.zQoUb1
[2017-05-15 05:35:05,046][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.zQoUb1
[2017-05-15 05:35:05,078][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:35:05,078][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-15 05:35:06,095][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-15 05:35:06,095][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-15 05:35:06,095][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-15 05:35:06,095][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:35:06,095][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:35:06,095][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:35:06,095][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:35:06,210][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:35:06,210][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-15 05:35:06,212][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 05:35:11,334][gdb1][INFO  ] checking OSD status...
[2017-05-15 05:35:11,334][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:35:11,337][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-15 05:35:11,452][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-15 05:41:31,362][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:41:31,362][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 05:41:31,362][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:41:31,362][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f79ddc91908>
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f79ddee7aa0>
[2017-05-15 05:41:31,363][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:41:31,364][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:41:31,364][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:41:31,364][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 05:41:31,611][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:41:31,811][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:41:31,811][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:41:31,827][gdb1][DEBUG ] detect machine type
[2017-05-15 05:41:31,831][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:41:31,832][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:41:31,832][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-15 05:41:31,832][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:41:31,835][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-15 05:41:31,835][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:41:31,837][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:41:31,957][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 05:41:31,973][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:41:31,981][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:41:31,996][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:41:32,012][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:32,012][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 05:41:32,012][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 05:41:32,016][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:32,016][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:32,016][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:32,016][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-15 05:41:32,032][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-15 05:41:32,039][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-15 05:41:32,047][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-15 05:41:32,054][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:32,055][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-15 05:41:32,055][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:32,055][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-15 05:41:32,058][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-15 05:41:32,062][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-15 05:41:32,062][gdb1][WARNING] backup header from main header.
[2017-05-15 05:41:32,062][gdb1][WARNING] 
[2017-05-15 05:41:33,129][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:41:33,129][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-15 05:41:33,129][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-15 05:41:33,129][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:41:33,129][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-15 05:41:33,129][gdb1][DEBUG ] other utilities.
[2017-05-15 05:41:33,129][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-15 05:41:34,146][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-15 05:41:34,146][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:41:34,147][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-15 05:41:34,147][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:41:34,147][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:41:34,147][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:41:34,162][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:34,163][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:34,163][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-05-15 05:41:34,163][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:34,163][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-15 05:41:34,163][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:73017062-bb84-4ea5-a69c-c01474adbff2 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-15 05:41:35,180][gdb1][DEBUG ] Setting name!
[2017-05-15 05:41:35,180][gdb1][DEBUG ] partNum is 1
[2017-05-15 05:41:35,180][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:41:35,180][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:41:35,180][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:41:35,180][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:41:35,395][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:41:35,559][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:41:35,591][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:35,591][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:35,591][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:41:35,592][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/73017062-bb84-4ea5-a69c-c01474adbff2
[2017-05-15 05:41:35,592][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-15 05:41:36,608][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:41:36,609][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:41:36,609][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:41:36,823][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:41:36,988][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:41:37,019][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/73017062-bb84-4ea5-a69c-c01474adbff2
[2017-05-15 05:41:37,020][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:37,020][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-15 05:41:37,020][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:37,020][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-15 05:41:37,020][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:37,020][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-15 05:41:37,020][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:fc3b33ef-afd2-45c5-9981-2a7594f19bd6 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-15 05:41:38,037][gdb1][DEBUG ] Setting name!
[2017-05-15 05:41:38,037][gdb1][DEBUG ] partNum is 0
[2017-05-15 05:41:38,037][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:41:38,037][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:41:38,037][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:41:38,038][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:41:38,252][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:41:38,466][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:41:38,681][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:38,681][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:38,681][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:41:38,681][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-15 05:41:38,681][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-15 05:41:39,347][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-15 05:41:39,347][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-15 05:41:39,348][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-15 05:41:39,348][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-15 05:41:39,348][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-15 05:41:39,348][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-15 05:41:39,348][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-15 05:41:39,348][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-15 05:41:39,348][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-15 05:41:39,348][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.8osI83 with options noatime,inode64
[2017-05-15 05:41:39,348][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.8osI83
[2017-05-15 05:41:39,348][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.8osI83
[2017-05-15 05:41:39,348][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.8osI83/ceph_fsid.19349.tmp
[2017-05-15 05:41:39,356][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.8osI83/fsid.19349.tmp
[2017-05-15 05:41:39,359][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.8osI83/magic.19349.tmp
[2017-05-15 05:41:39,361][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.8osI83/journal_uuid.19349.tmp
[2017-05-15 05:41:39,362][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.8osI83/journal -> /dev/disk/by-partuuid/73017062-bb84-4ea5-a69c-c01474adbff2
[2017-05-15 05:41:39,362][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.8osI83
[2017-05-15 05:41:39,365][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.8osI83
[2017-05-15 05:41:39,365][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.8osI83
[2017-05-15 05:41:39,397][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:41:39,397][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-15 05:41:40,414][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-15 05:41:40,414][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-15 05:41:40,415][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-15 05:41:40,415][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:41:40,415][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:41:40,415][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:41:40,415][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:41:40,629][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:41:40,693][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-15 05:41:40,696][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 05:41:45,818][gdb1][INFO  ] checking OSD status...
[2017-05-15 05:41:45,818][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:41:45,821][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-15 05:41:45,936][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-15 05:42:23,664][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:42:23,664][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 05:42:23,664][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:42:23,664][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:42:23,664][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:42:23,664][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3b420fa908>
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3b42350aa0>
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:42:23,665][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:42:23,666][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 05:42:23,912][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:42:24,143][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:42:24,143][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:42:24,160][gdb1][DEBUG ] detect machine type
[2017-05-15 05:42:24,164][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:42:24,165][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:42:24,165][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-15 05:42:24,165][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:42:24,168][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-15 05:42:24,168][gdb1][DEBUG ] create a keyring file
[2017-05-15 05:42:24,169][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-15 05:42:24,169][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:42:24,171][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:42:24,292][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 05:42:24,308][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:42:24,315][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:42:24,331][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:42:24,347][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:24,347][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 05:42:24,347][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 05:42:24,354][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:24,355][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:24,355][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:24,355][gdb1][WARNING] Traceback (most recent call last):
[2017-05-15 05:42:24,355][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-15 05:42:24,355][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-15 05:42:24,355][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-15 05:42:24,355][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-15 05:42:24,355][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2029, in main
[2017-05-15 05:42:24,355][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2018, in prepare
[2017-05-15 05:42:24,355][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2049, in prepare_locked
[2017-05-15 05:42:24,355][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2816, in prepare
[2017-05-15 05:42:24,355][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2971, in prepare_device
[2017-05-15 05:42:24,355][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2878, in prepare_device
[2017-05-15 05:42:24,356][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2841, in sanity_checks
[2017-05-15 05:42:24,356][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-05-15 05:42:24,356][gdb1][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-05-15 05:42:24,363][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-15 05:42:24,363][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:42:24,364][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-15 05:42:44,225][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:42:44,226][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 05:42:44,226][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:42:44,226][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:42:44,226][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:42:44,226][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 05:42:44,226][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:42:44,226][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:42:44,226][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:42:44,226][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:42:44,226][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:42:44,226][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:42:44,227][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:42:44,227][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:42:44,227][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7eff9d32f908>
[2017-05-15 05:42:44,227][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:42:44,227][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:42:44,227][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7eff9d585aa0>
[2017-05-15 05:42:44,227][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:42:44,227][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:42:44,227][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:42:44,227][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 05:42:44,472][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:42:44,703][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:42:44,703][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:42:44,719][gdb1][DEBUG ] detect machine type
[2017-05-15 05:42:44,723][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:42:44,724][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:42:44,724][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-15 05:42:44,724][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:42:44,726][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-15 05:42:44,726][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:42:44,728][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:42:44,849][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 05:42:44,864][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:42:44,872][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:42:44,888][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:42:44,903][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:44,904][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 05:42:44,904][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 05:42:44,911][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:44,911][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:44,911][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:44,911][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-15 05:42:44,919][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-15 05:42:44,935][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-15 05:42:44,938][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-15 05:42:44,954][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:44,954][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-15 05:42:44,954][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:44,954][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-15 05:42:44,954][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-15 05:42:44,958][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-15 05:42:44,958][gdb1][WARNING] backup header from main header.
[2017-05-15 05:42:44,958][gdb1][WARNING] 
[2017-05-15 05:42:46,025][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:42:46,025][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-15 05:42:46,025][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-15 05:42:46,025][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:42:46,025][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-15 05:42:46,025][gdb1][DEBUG ] other utilities.
[2017-05-15 05:42:46,026][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-15 05:42:47,043][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-15 05:42:47,043][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:42:47,043][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-15 05:42:47,043][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:42:47,043][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:42:47,059][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:42:47,074][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:47,074][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:47,074][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-05-15 05:42:47,075][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:47,075][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-15 05:42:47,075][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:2ffed38f-864e-4803-b0fc-30fc29450629 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-15 05:42:48,092][gdb1][DEBUG ] Setting name!
[2017-05-15 05:42:48,092][gdb1][DEBUG ] partNum is 1
[2017-05-15 05:42:48,092][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:42:48,092][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:42:48,092][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:42:48,092][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:42:48,307][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:42:48,521][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:42:48,736][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:48,736][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:48,736][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:42:48,736][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/2ffed38f-864e-4803-b0fc-30fc29450629
[2017-05-15 05:42:48,736][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-15 05:42:49,753][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:42:49,753][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:42:49,754][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:42:49,968][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:42:50,132][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:42:50,347][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/2ffed38f-864e-4803-b0fc-30fc29450629
[2017-05-15 05:42:50,347][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:50,347][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-15 05:42:50,347][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:50,347][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-15 05:42:50,347][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:50,348][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-15 05:42:50,348][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:7b66cb1e-516d-4c55-808b-7b4a17e9f680 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-15 05:42:51,364][gdb1][DEBUG ] Setting name!
[2017-05-15 05:42:51,365][gdb1][DEBUG ] partNum is 0
[2017-05-15 05:42:51,365][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:42:51,365][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:42:51,365][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:42:51,365][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:42:51,579][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:42:51,744][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:42:51,958][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:51,958][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:51,959][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:42:51,959][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-15 05:42:51,959][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-15 05:42:52,725][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-15 05:42:52,725][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-15 05:42:52,725][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-15 05:42:52,725][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-15 05:42:52,725][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-15 05:42:52,726][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-15 05:42:52,726][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-15 05:42:52,726][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-15 05:42:52,726][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-15 05:42:52,726][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.Tb2Zzg with options noatime,inode64
[2017-05-15 05:42:52,726][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.Tb2Zzg
[2017-05-15 05:42:52,726][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.Tb2Zzg
[2017-05-15 05:42:52,726][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Tb2Zzg/ceph_fsid.20899.tmp
[2017-05-15 05:42:52,726][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Tb2Zzg/fsid.20899.tmp
[2017-05-15 05:42:52,730][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Tb2Zzg/magic.20899.tmp
[2017-05-15 05:42:52,731][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Tb2Zzg/journal_uuid.20899.tmp
[2017-05-15 05:42:52,734][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.Tb2Zzg/journal -> /dev/disk/by-partuuid/2ffed38f-864e-4803-b0fc-30fc29450629
[2017-05-15 05:42:52,734][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Tb2Zzg
[2017-05-15 05:42:52,734][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.Tb2Zzg
[2017-05-15 05:42:52,734][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Tb2Zzg
[2017-05-15 05:42:52,766][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:42:52,766][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-15 05:42:53,783][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-15 05:42:53,783][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-15 05:42:53,783][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-15 05:42:53,783][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:42:53,784][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:42:53,784][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:42:53,784][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:42:53,998][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:42:54,002][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-15 05:42:54,036][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 05:42:59,158][gdb1][INFO  ] checking OSD status...
[2017-05-15 05:42:59,158][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:42:59,160][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-15 05:42:59,275][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-15 05:44:20,614][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 05:44:20,615][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 05:44:20,616][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc02b7c0908>
[2017-05-15 05:44:20,616][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 05:44:20,616][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 05:44:20,616][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc02ba16aa0>
[2017-05-15 05:44:20,616][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 05:44:20,616][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 05:44:20,616][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 05:44:20,616][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 05:44:20,856][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 05:44:21,083][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 05:44:21,083][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 05:44:21,100][gdb1][DEBUG ] detect machine type
[2017-05-15 05:44:21,103][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:44:21,104][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 05:44:21,104][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-15 05:44:21,104][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 05:44:21,107][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-15 05:44:21,107][gdb1][DEBUG ] create a keyring file
[2017-05-15 05:44:21,108][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-15 05:44:21,108][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:44:21,110][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 05:44:21,281][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 05:44:21,281][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:44:21,281][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:44:21,281][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 05:44:21,281][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:21,281][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 05:44:21,281][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 05:44:21,297][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:21,297][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:21,297][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:21,297][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:44:21,297][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:44:21,297][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-15 05:44:21,305][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-15 05:44:21,312][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-15 05:44:21,328][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-15 05:44:21,332][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:21,332][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-15 05:44:21,332][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:21,332][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-05-15 05:44:21,364][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-05-15 05:44:21,364][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-05-15 05:44:21,364][gdb1][WARNING] 10+0 records in
[2017-05-15 05:44:21,364][gdb1][WARNING] 10+0 records out
[2017-05-15 05:44:21,364][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00706958 s, 1.5 GB/s
[2017-05-15 05:44:21,364][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-05-15 05:44:21,478][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-05-15 05:44:21,479][gdb1][WARNING] 10+0 records in
[2017-05-15 05:44:21,479][gdb1][WARNING] 10+0 records out
[2017-05-15 05:44:21,479][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00492408 s, 2.1 GB/s
[2017-05-15 05:44:21,479][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-15 05:44:21,479][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-15 05:44:21,480][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-15 05:44:21,480][gdb1][WARNING] backup header from main header.
[2017-05-15 05:44:21,481][gdb1][WARNING] 
[2017-05-15 05:44:21,481][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-05-15 05:44:21,481][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-05-15 05:44:21,481][gdb1][WARNING] 
[2017-05-15 05:44:21,481][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-05-15 05:44:21,481][gdb1][WARNING] 
[2017-05-15 05:44:22,548][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:44:22,548][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-15 05:44:22,548][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-15 05:44:22,548][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 05:44:22,548][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-15 05:44:22,549][gdb1][DEBUG ] other utilities.
[2017-05-15 05:44:22,549][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-15 05:44:23,566][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-15 05:44:23,566][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:44:23,566][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-15 05:44:23,566][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:44:23,598][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:44:23,614][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:44:23,629][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:23,630][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:23,630][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-05-15 05:44:23,630][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:23,630][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-15 05:44:23,630][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:391281ee-2f13-45c3-adc0-3fdf82bdf8b0 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-15 05:44:24,647][gdb1][DEBUG ] Setting name!
[2017-05-15 05:44:24,647][gdb1][DEBUG ] partNum is 1
[2017-05-15 05:44:24,647][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:44:24,647][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:44:24,647][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:44:24,647][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:44:24,862][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:44:25,077][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:44:25,291][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:25,291][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:25,291][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 05:44:25,291][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/391281ee-2f13-45c3-adc0-3fdf82bdf8b0
[2017-05-15 05:44:25,291][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-15 05:44:26,308][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:44:26,309][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:44:26,309][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:44:26,523][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:44:26,687][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:44:26,902][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/391281ee-2f13-45c3-adc0-3fdf82bdf8b0
[2017-05-15 05:44:26,902][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:26,902][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-15 05:44:26,902][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:26,902][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-15 05:44:26,902][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:26,903][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-15 05:44:26,903][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:b0b48c68-fa8f-4e4b-9351-1b81c3b052e9 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-15 05:44:27,920][gdb1][DEBUG ] Setting name!
[2017-05-15 05:44:27,920][gdb1][DEBUG ] partNum is 0
[2017-05-15 05:44:27,920][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 05:44:27,920][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:44:27,920][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 05:44:27,920][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:44:28,135][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:44:28,349][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:44:28,413][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:28,413][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:28,414][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 05:44:28,414][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-15 05:44:28,414][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-15 05:44:29,130][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-15 05:44:29,130][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-15 05:44:29,130][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-15 05:44:29,130][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-15 05:44:29,130][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-15 05:44:29,130][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-15 05:44:29,130][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-15 05:44:29,130][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-15 05:44:29,131][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-15 05:44:29,131][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.auOnv6 with options noatime,inode64
[2017-05-15 05:44:29,131][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.auOnv6
[2017-05-15 05:44:29,163][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.auOnv6
[2017-05-15 05:44:29,163][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.auOnv6/ceph_fsid.22424.tmp
[2017-05-15 05:44:29,163][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.auOnv6/fsid.22424.tmp
[2017-05-15 05:44:29,166][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.auOnv6/magic.22424.tmp
[2017-05-15 05:44:29,167][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.auOnv6/journal_uuid.22424.tmp
[2017-05-15 05:44:29,171][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.auOnv6/journal -> /dev/disk/by-partuuid/391281ee-2f13-45c3-adc0-3fdf82bdf8b0
[2017-05-15 05:44:29,171][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.auOnv6
[2017-05-15 05:44:29,171][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.auOnv6
[2017-05-15 05:44:29,171][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.auOnv6
[2017-05-15 05:44:29,235][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 05:44:29,235][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-15 05:44:30,252][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-15 05:44:30,252][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-15 05:44:30,252][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-15 05:44:30,252][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 05:44:30,253][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 05:44:30,253][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:44:30,253][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 05:44:30,517][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 05:44:30,533][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-15 05:44:30,551][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 05:44:35,673][gdb1][INFO  ] checking OSD status...
[2017-05-15 05:44:35,673][gdb1][DEBUG ] find the location of an executable
[2017-05-15 05:44:35,676][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-15 05:44:35,791][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-15 15:51:00,378][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa9305b2fc8>
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fa930ec51b8>
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 15:51:00,379][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 15:51:00,380][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-15 15:51:00,380][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-15 15:51:00,380][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-15 15:51:00,380][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-15 15:51:00,406][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 15:51:00,419][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 15:51:00,420][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 15:51:00,436][gdb3][DEBUG ] detect machine type
[2017-05-15 15:51:00,439][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 15:51:00,439][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-15 15:51:00,440][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-15 15:51:00,477][gdb3][DEBUG ] Reading package lists...
[2017-05-15 15:51:00,642][gdb3][DEBUG ] Building dependency tree...
[2017-05-15 15:51:00,642][gdb3][DEBUG ] Reading state information...
[2017-05-15 15:51:00,706][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-15 15:51:00,706][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-15 15:51:00,707][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-15 15:51:00,707][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-15 15:51:00,707][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-15 15:51:00,707][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 ntp python-blinker python-cephfs
[2017-05-15 15:51:00,707][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-05-15 15:51:00,707][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-05-15 15:51:00,707][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-05-15 15:51:00,708][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-05-15 15:51:00,708][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-05-15 15:51:00,708][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-15 15:51:00,740][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-15 15:51:00,740][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-15 15:51:00,854][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 26 not upgraded.
[2017-05-15 15:51:00,854][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-15 15:51:00,886][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 79236 files and directories currently installed.)
[2017-05-15 15:51:00,886][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-15 15:51:01,050][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-15 15:51:01,165][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-15 15:51:01,180][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-15 15:51:01,395][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-15 15:51:01,510][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-15 15:51:01,675][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-15 15:51:01,707][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-15 15:51:01,739][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-15 15:51:01,906][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-15 15:51:01,970][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-15 15:51:01,970][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-15 15:51:02,135][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-15 15:51:02,166][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-15 15:51:02,332][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-15 15:51:02,364][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-15 15:51:02,380][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-15 15:51:02,494][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-15 15:51:03,260][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-15 15:51:03,426][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 15:51:03,426][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-15 15:51:03,426][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 15:51:03,427][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 15:51:03,427][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 15:51:03,427][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 15:51:03,427][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 15:51:03,427][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fefa9aa1710>
[2017-05-15 15:51:03,427][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 15:51:03,427][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-15 15:51:03,427][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fefaa3ae230>
[2017-05-15 15:51:03,427][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 15:51:03,427][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 15:51:03,427][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-15 15:51:03,454][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 15:51:03,467][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 15:51:03,468][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 15:51:03,484][gdb3][DEBUG ] detect machine type
[2017-05-15 15:51:03,486][gdb3][DEBUG ] find the location of an executable
[2017-05-15 15:51:03,502][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 15:51:03,515][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 15:51:03,515][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 15:51:03,531][gdb3][DEBUG ] detect machine type
[2017-05-15 15:51:03,534][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 15:51:03,534][gdb3][INFO  ] purging data on gdb3
[2017-05-15 15:51:03,535][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-15 15:51:03,547][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-15 15:51:03,715][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 15:51:03,716][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-15 15:51:03,716][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 15:51:03,716][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 15:51:03,716][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 15:51:03,716][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 15:51:03,716][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 15:51:03,716][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4977ed39e0>
[2017-05-15 15:51:03,716][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 15:51:03,716][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f4978796848>
[2017-05-15 15:51:03,716][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 15:51:03,716][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 15:51:26,858][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 15:51:26,858][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-15 15:51:26,858][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 15:51:26,858][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 15:51:26,858][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f037c28a560>
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f037c90e758>
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 15:51:26,859][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-15 15:51:26,859][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-15 15:51:26,860][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-15 15:51:26,885][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 15:51:26,900][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 15:51:26,900][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 15:51:26,917][gdb3][DEBUG ] detect machine type
[2017-05-15 15:51:26,919][gdb3][DEBUG ] find the location of an executable
[2017-05-15 15:51:26,920][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-15 15:51:26,932][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-15 15:51:26,938][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-15 15:51:26,938][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-15 15:51:26,938][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-15 15:51:26,938][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-15 15:51:26,939][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-15 15:51:26,939][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-15 15:51:26,939][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-15 15:51:26,939][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-15 15:51:27,106][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 15:51:27,106][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-15 15:51:27,106][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 15:51:27,106][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 15:51:27,106][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 15:51:27,107][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 15:51:27,107][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-15 15:51:27,107][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 15:51:27,107][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5bb5a9ce60>
[2017-05-15 15:51:27,107][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 15:51:27,107][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f5bb5a71b18>
[2017-05-15 15:51:27,107][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 15:51:27,107][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-15 15:51:27,107][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 15:51:27,108][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-15 15:51:27,108][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-15 15:51:27,134][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 15:51:27,149][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 15:51:27,149][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 15:51:27,166][gdb3][DEBUG ] detect machine type
[2017-05-15 15:51:27,168][gdb3][DEBUG ] find the location of an executable
[2017-05-15 15:51:27,169][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-15 15:51:27,169][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-15 15:51:27,169][gdb3][DEBUG ] get remote short hostname
[2017-05-15 15:51:27,169][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-15 15:51:27,169][gdb3][DEBUG ] get remote short hostname
[2017-05-15 15:51:27,170][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-15 15:51:27,170][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 15:51:27,171][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-15 15:51:27,172][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-15 15:51:27,172][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-15 15:51:27,172][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-15 15:51:27,173][gdb3][DEBUG ] create the monitor keyring file
[2017-05-15 15:51:27,174][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-15 15:51:27,212][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-15 15:51:27,212][gdb3][DEBUG ] ceph-mon: set fsid to 46a856ae-ecba-4352-8fe5-621ec7158a1f
[2017-05-15 15:51:27,220][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-15 15:51:27,220][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-15 15:51:27,220][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-15 15:51:27,221][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-15 15:51:27,222][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 15:51:27,291][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-15 15:51:27,358][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-15 15:51:29,397][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-15 15:51:29,462][gdb3][DEBUG ] ********************************************************************************
[2017-05-15 15:51:29,462][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-15 15:51:29,463][gdb3][DEBUG ] {
[2017-05-15 15:51:29,463][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-15 15:51:29,463][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-15 15:51:29,463][gdb3][DEBUG ]   "features": {
[2017-05-15 15:51:29,463][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-15 15:51:29,463][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-15 15:51:29,463][gdb3][DEBUG ]       "kraken", 
[2017-05-15 15:51:29,463][gdb3][DEBUG ]       "luminous"
[2017-05-15 15:51:29,463][gdb3][DEBUG ]     ], 
[2017-05-15 15:51:29,463][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-15 15:51:29,463][gdb3][DEBUG ]     "required_mon": [
[2017-05-15 15:51:29,463][gdb3][DEBUG ]       "kraken", 
[2017-05-15 15:51:29,463][gdb3][DEBUG ]       "luminous"
[2017-05-15 15:51:29,464][gdb3][DEBUG ]     ]
[2017-05-15 15:51:29,464][gdb3][DEBUG ]   }, 
[2017-05-15 15:51:29,464][gdb3][DEBUG ]   "monmap": {
[2017-05-15 15:51:29,464][gdb3][DEBUG ]     "created": "2017-05-15 15:51:27.197421", 
[2017-05-15 15:51:29,464][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-15 15:51:29,464][gdb3][DEBUG ]     "features": {
[2017-05-15 15:51:29,464][gdb3][DEBUG ]       "optional": [], 
[2017-05-15 15:51:29,464][gdb3][DEBUG ]       "persistent": [
[2017-05-15 15:51:29,464][gdb3][DEBUG ]         "kraken", 
[2017-05-15 15:51:29,464][gdb3][DEBUG ]         "luminous"
[2017-05-15 15:51:29,464][gdb3][DEBUG ]       ]
[2017-05-15 15:51:29,464][gdb3][DEBUG ]     }, 
[2017-05-15 15:51:29,464][gdb3][DEBUG ]     "fsid": "46a856ae-ecba-4352-8fe5-621ec7158a1f", 
[2017-05-15 15:51:29,464][gdb3][DEBUG ]     "modified": "2017-05-15 15:51:27.435930", 
[2017-05-15 15:51:29,464][gdb3][DEBUG ]     "mons": [
[2017-05-15 15:51:29,464][gdb3][DEBUG ]       {
[2017-05-15 15:51:29,464][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-15 15:51:29,465][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-15 15:51:29,465][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-15 15:51:29,465][gdb3][DEBUG ]         "rank": 0
[2017-05-15 15:51:29,465][gdb3][DEBUG ]       }
[2017-05-15 15:51:29,465][gdb3][DEBUG ]     ]
[2017-05-15 15:51:29,465][gdb3][DEBUG ]   }, 
[2017-05-15 15:51:29,465][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-15 15:51:29,465][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-15 15:51:29,465][gdb3][DEBUG ]   "quorum": [
[2017-05-15 15:51:29,465][gdb3][DEBUG ]     0
[2017-05-15 15:51:29,465][gdb3][DEBUG ]   ], 
[2017-05-15 15:51:29,465][gdb3][DEBUG ]   "rank": 0, 
[2017-05-15 15:51:29,465][gdb3][DEBUG ]   "state": "leader", 
[2017-05-15 15:51:29,465][gdb3][DEBUG ]   "sync_provider": []
[2017-05-15 15:51:29,465][gdb3][DEBUG ] }
[2017-05-15 15:51:29,465][gdb3][DEBUG ] ********************************************************************************
[2017-05-15 15:51:29,466][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-15 15:51:29,466][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-15 15:51:29,531][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-15 15:51:29,547][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 15:51:29,560][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 15:51:29,561][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 15:51:29,577][gdb3][DEBUG ] detect machine type
[2017-05-15 15:51:29,580][gdb3][DEBUG ] find the location of an executable
[2017-05-15 15:51:29,581][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-15 15:51:29,646][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-15 15:51:29,646][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-15 15:51:29,646][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-15 15:51:29,648][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpIVfn67
[2017-05-15 15:51:29,663][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 15:51:29,676][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 15:51:29,677][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 15:51:29,693][gdb3][DEBUG ] detect machine type
[2017-05-15 15:51:29,695][gdb3][DEBUG ] get remote short hostname
[2017-05-15 15:51:29,696][gdb3][DEBUG ] fetch remote file
[2017-05-15 15:51:29,697][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-15 15:51:29,763][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-15 15:51:29,929][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-15 15:51:30,095][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-15 15:51:30,261][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-15 15:51:30,427][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-15 15:51:30,593][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-15 15:51:30,759][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-15 15:51:30,925][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-15 15:51:31,092][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-15 15:51:31,258][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-15 15:51:31,423][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-15 15:51:31,423][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-15 15:51:31,423][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170515155131'
[2017-05-15 15:51:31,424][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-15 15:51:31,424][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-15 15:51:31,424][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-15 15:51:31,424][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpIVfn67
[2017-05-15 15:53:14,902][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 15:53:14,902][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd79881b908>
[2017-05-15 15:53:14,903][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 15:53:14,904][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 15:53:14,904][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd798a71aa0>
[2017-05-15 15:53:14,904][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 15:53:14,904][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 15:53:14,904][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 15:53:14,904][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-15 15:53:15,283][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 15:53:15,475][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 15:53:15,476][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 15:53:15,494][gdb1][DEBUG ] detect machine type
[2017-05-15 15:53:15,498][gdb1][DEBUG ] find the location of an executable
[2017-05-15 15:53:15,499][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 15:53:15,499][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-15 15:53:15,500][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 15:53:15,503][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-15 15:53:15,503][gdb1][DEBUG ] create a keyring file
[2017-05-15 15:53:15,505][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-15 15:53:15,505][gdb1][DEBUG ] find the location of an executable
[2017-05-15 15:53:15,508][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 15:53:15,678][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 15:53:15,679][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 15:53:15,695][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 15:53:15,711][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 15:53:15,727][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:15,727][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 15:53:15,727][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 15:53:15,730][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:15,730][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:15,730][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:15,731][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-15 15:53:15,845][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-15 15:53:15,845][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-15 15:53:15,845][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-15 15:53:15,848][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:15,848][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-15 15:53:15,848][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:15,849][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-15 15:53:15,864][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-15 15:53:15,872][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-15 15:53:15,872][gdb1][WARNING] backup header from main header.
[2017-05-15 15:53:15,872][gdb1][WARNING] 
[2017-05-15 15:53:16,939][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 15:53:16,939][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-15 15:53:16,939][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-15 15:53:16,939][gdb1][DEBUG ] ****************************************************************************
[2017-05-15 15:53:16,939][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-15 15:53:16,940][gdb1][DEBUG ] other utilities.
[2017-05-15 15:53:16,940][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-15 15:53:17,957][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-15 15:53:17,957][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 15:53:17,957][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-15 15:53:17,957][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 15:53:17,957][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 15:53:17,973][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 15:53:17,976][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:17,976][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:17,976][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-05-15 15:53:17,976][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:17,976][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-15 15:53:17,977][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:246a091f-e396-4d1a-b6b3-b7d0a3cc2976 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-15 15:53:19,044][gdb1][DEBUG ] Setting name!
[2017-05-15 15:53:19,044][gdb1][DEBUG ] partNum is 1
[2017-05-15 15:53:19,044][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 15:53:19,044][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 15:53:19,044][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 15:53:19,044][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 15:53:19,359][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 15:53:19,523][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 15:53:19,587][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:19,588][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:19,588][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 15:53:19,588][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/246a091f-e396-4d1a-b6b3-b7d0a3cc2976
[2017-05-15 15:53:19,588][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-15 15:53:20,605][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 15:53:20,605][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 15:53:20,605][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 15:53:20,819][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 15:53:20,984][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 15:53:21,000][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/246a091f-e396-4d1a-b6b3-b7d0a3cc2976
[2017-05-15 15:53:21,000][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:21,000][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-15 15:53:21,000][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:21,000][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-15 15:53:21,000][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:21,000][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-15 15:53:21,000][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:a5b2637a-f2ef-4c40-93fa-f0215fb217cc --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-15 15:53:22,067][gdb1][DEBUG ] Setting name!
[2017-05-15 15:53:22,068][gdb1][DEBUG ] partNum is 0
[2017-05-15 15:53:22,068][gdb1][DEBUG ] REALLY setting name!
[2017-05-15 15:53:22,068][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 15:53:22,068][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 15:53:22,068][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 15:53:22,232][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 15:53:22,497][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 15:53:22,529][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:22,529][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:22,529][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 15:53:22,529][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-15 15:53:22,529][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-15 15:53:23,345][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-15 15:53:23,346][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-15 15:53:23,346][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-15 15:53:23,346][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-15 15:53:23,346][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-15 15:53:23,346][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-15 15:53:23,346][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-15 15:53:23,346][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-15 15:53:23,346][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-15 15:53:23,346][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.Dlkb8r with options noatime,inode64
[2017-05-15 15:53:23,346][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.Dlkb8r
[2017-05-15 15:53:23,346][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.Dlkb8r
[2017-05-15 15:53:23,362][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Dlkb8r/ceph_fsid.23586.tmp
[2017-05-15 15:53:23,363][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Dlkb8r/fsid.23586.tmp
[2017-05-15 15:53:23,367][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Dlkb8r/magic.23586.tmp
[2017-05-15 15:53:23,370][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Dlkb8r/journal_uuid.23586.tmp
[2017-05-15 15:53:23,373][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.Dlkb8r/journal -> /dev/disk/by-partuuid/246a091f-e396-4d1a-b6b3-b7d0a3cc2976
[2017-05-15 15:53:23,374][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.Dlkb8r
[2017-05-15 15:53:23,374][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.Dlkb8r
[2017-05-15 15:53:23,374][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.Dlkb8r
[2017-05-15 15:53:23,406][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 15:53:23,406][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-15 15:53:24,423][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-15 15:53:24,423][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-15 15:53:24,423][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-15 15:53:24,423][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-15 15:53:24,423][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 15:53:24,423][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 15:53:24,424][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 15:53:24,639][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 15:53:24,703][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-15 15:53:24,705][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 15:53:29,928][gdb1][INFO  ] checking OSD status...
[2017-05-15 15:53:29,928][gdb1][DEBUG ] find the location of an executable
[2017-05-15 15:53:29,930][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-15 15:53:30,045][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-15 15:53:30,210][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 15:53:30,210][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-05-15 15:53:30,210][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 15:53:30,210][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 15:53:30,210][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 15:53:30,210][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 15:53:30,210][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 15:53:30,211][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6900b5d518>
[2017-05-15 15:53:30,211][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 15:53:30,211][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-05-15 15:53:30,211][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f6901474938>
[2017-05-15 15:53:30,211][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 15:53:30,211][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 15:53:30,211][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-15 15:53:30,479][gdb0][DEBUG ] connection detected need for sudo
[2017-05-15 15:53:30,735][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-15 15:53:30,735][gdb0][DEBUG ] detect platform information from remote host
[2017-05-15 15:53:30,751][gdb0][DEBUG ] detect machine type
[2017-05-15 15:53:30,756][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 15:53:30,757][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-15 15:53:30,758][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-15 15:53:30,988][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 15:53:31,215][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 15:53:31,215][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 15:53:31,232][gdb1][DEBUG ] detect machine type
[2017-05-15 15:53:31,236][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 15:53:31,239][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-15 15:53:31,254][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 15:53:31,268][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 15:53:31,268][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 15:53:31,285][gdb3][DEBUG ] detect machine type
[2017-05-15 15:53:31,287][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 15:53:31,289][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-15 17:01:12,956][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 17:01:12,956][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-05-15 17:01:12,956][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 17:01:12,956][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 17:01:12,956][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-15 17:01:12,956][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdc252fd908>
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fdc25553aa0>
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 17:01:12,957][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-15 17:01:12,958][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-15 17:01:13,196][gdb0][DEBUG ] connection detected need for sudo
[2017-05-15 17:01:13,431][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-15 17:01:13,432][gdb0][DEBUG ] detect platform information from remote host
[2017-05-15 17:01:13,448][gdb0][DEBUG ] detect machine type
[2017-05-15 17:01:13,452][gdb0][DEBUG ] find the location of an executable
[2017-05-15 17:01:13,453][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-15 17:01:13,453][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-15 17:01:13,453][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 17:01:13,455][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-15 17:01:13,455][gdb0][DEBUG ] create a keyring file
[2017-05-15 17:01:13,457][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-15 17:01:13,457][gdb0][DEBUG ] find the location of an executable
[2017-05-15 17:01:13,459][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-15 17:01:13,580][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-15 17:01:13,587][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 17:01:13,603][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 17:01:13,619][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-15 17:01:13,634][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:13,635][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-05-15 17:01:13,635][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-15 17:01:13,650][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:13,650][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:13,650][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:13,651][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-15 17:01:13,654][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-15 17:01:13,670][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-15 17:01:13,673][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-15 17:01:13,689][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:13,689][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-15 17:01:13,689][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:13,689][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-15 17:01:13,689][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-15 17:01:13,689][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-15 17:01:13,689][gdb0][WARNING] backup header from main header.
[2017-05-15 17:01:13,689][gdb0][WARNING] 
[2017-05-15 17:01:14,756][gdb0][DEBUG ] ****************************************************************************
[2017-05-15 17:01:14,757][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-15 17:01:14,757][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-15 17:01:14,757][gdb0][DEBUG ] ****************************************************************************
[2017-05-15 17:01:14,757][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-15 17:01:14,757][gdb0][DEBUG ] other utilities.
[2017-05-15 17:01:14,757][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-15 17:01:15,724][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-15 17:01:15,724][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 17:01:15,724][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-15 17:01:15,724][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 17:01:15,740][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 17:01:15,772][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 17:01:15,772][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:15,772][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:15,773][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-05-15 17:01:15,773][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:15,773][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-05-15 17:01:15,773][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:3c506fef-441e-4876-84c6-21046e29987d --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-15 17:01:16,790][gdb0][DEBUG ] Setting name!
[2017-05-15 17:01:16,790][gdb0][DEBUG ] partNum is 1
[2017-05-15 17:01:16,791][gdb0][DEBUG ] REALLY setting name!
[2017-05-15 17:01:16,791][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 17:01:16,791][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 17:01:16,791][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 17:01:17,005][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 17:01:17,169][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 17:01:17,201][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:17,201][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:17,202][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-15 17:01:17,202][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/3c506fef-441e-4876-84c6-21046e29987d
[2017-05-15 17:01:17,202][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-15 17:01:18,269][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 17:01:18,269][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 17:01:18,269][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 17:01:18,434][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 17:01:18,598][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 17:01:18,813][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/3c506fef-441e-4876-84c6-21046e29987d
[2017-05-15 17:01:18,813][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:18,813][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-15 17:01:18,813][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:18,813][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-15 17:01:18,813][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:18,813][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-05-15 17:01:18,813][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:83485fb8-9d8c-45f7-a730-99b8be22a1b9 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-15 17:01:19,880][gdb0][DEBUG ] Setting name!
[2017-05-15 17:01:19,880][gdb0][DEBUG ] partNum is 0
[2017-05-15 17:01:19,880][gdb0][DEBUG ] REALLY setting name!
[2017-05-15 17:01:19,880][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 17:01:19,880][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-15 17:01:19,881][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 17:01:20,045][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 17:01:20,259][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 17:01:20,423][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:20,424][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:20,424][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-15 17:01:20,424][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-15 17:01:20,424][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-15 17:01:21,140][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-05-15 17:01:21,140][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-15 17:01:21,140][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-15 17:01:21,140][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-05-15 17:01:21,140][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-15 17:01:21,141][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-15 17:01:21,141][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-05-15 17:01:21,141][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-15 17:01:21,141][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-15 17:01:21,141][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.X5NR6f with options noatime,inode64
[2017-05-15 17:01:21,141][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.X5NR6f
[2017-05-15 17:01:21,141][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.X5NR6f
[2017-05-15 17:01:21,141][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.X5NR6f/ceph_fsid.10421.tmp
[2017-05-15 17:01:21,141][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.X5NR6f/fsid.10421.tmp
[2017-05-15 17:01:21,141][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.X5NR6f/magic.10421.tmp
[2017-05-15 17:01:21,142][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.X5NR6f/journal_uuid.10421.tmp
[2017-05-15 17:01:21,144][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.X5NR6f/journal -> /dev/disk/by-partuuid/3c506fef-441e-4876-84c6-21046e29987d
[2017-05-15 17:01:21,144][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.X5NR6f
[2017-05-15 17:01:21,147][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.X5NR6f
[2017-05-15 17:01:21,147][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.X5NR6f
[2017-05-15 17:01:21,179][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-15 17:01:21,179][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-15 17:01:22,196][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-15 17:01:22,196][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-15 17:01:22,196][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-15 17:01:22,197][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-15 17:01:22,197][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-15 17:01:22,197][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 17:01:22,197][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-15 17:01:22,361][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-15 17:01:22,425][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-15 17:01:22,428][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-15 17:01:27,551][gdb0][INFO  ] checking OSD status...
[2017-05-15 17:01:27,551][gdb0][DEBUG ] find the location of an executable
[2017-05-15 17:01:27,553][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-15 17:01:27,669][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-15 17:01:27,832][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-15 17:01:27,832][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-05-15 17:01:27,832][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-15 17:01:27,832][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-15 17:01:27,832][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-15 17:01:27,832][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-15 17:01:27,832][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-15 17:01:27,832][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f754a9fa518>
[2017-05-15 17:01:27,832][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-15 17:01:27,832][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-05-15 17:01:27,832][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f754b311938>
[2017-05-15 17:01:27,833][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-15 17:01:27,833][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-15 17:01:27,833][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-15 17:01:28,076][gdb0][DEBUG ] connection detected need for sudo
[2017-05-15 17:01:28,303][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-15 17:01:28,304][gdb0][DEBUG ] detect platform information from remote host
[2017-05-15 17:01:28,320][gdb0][DEBUG ] detect machine type
[2017-05-15 17:01:28,324][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 17:01:28,327][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-15 17:01:28,702][gdb1][DEBUG ] connection detected need for sudo
[2017-05-15 17:01:28,947][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-15 17:01:28,948][gdb1][DEBUG ] detect platform information from remote host
[2017-05-15 17:01:28,965][gdb1][DEBUG ] detect machine type
[2017-05-15 17:01:28,969][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-15 17:01:28,972][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-15 17:01:28,987][gdb3][DEBUG ] connection detected need for sudo
[2017-05-15 17:01:29,001][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-15 17:01:29,002][gdb3][DEBUG ] detect platform information from remote host
[2017-05-15 17:01:29,018][gdb3][DEBUG ] detect machine type
[2017-05-15 17:01:29,020][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-16 00:08:13,531][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-16 00:08:13,532][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-16 00:08:13,532][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-16 00:08:13,532][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-16 00:08:13,533][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-16 00:08:13,533][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-16 00:08:13,533][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-16 00:08:13,533][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-16 00:08:13,533][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5c8ba42e60>
[2017-05-16 00:08:13,533][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-16 00:08:13,533][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f5c8ba17b18>
[2017-05-16 00:08:13,533][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-16 00:08:13,533][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-16 00:08:13,533][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-16 00:08:13,535][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-16 00:08:13,535][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-16 00:08:13,572][gdb3][DEBUG ] connection detected need for sudo
[2017-05-16 00:08:13,586][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-16 00:08:13,587][gdb3][DEBUG ] detect platform information from remote host
[2017-05-16 00:08:13,602][gdb3][DEBUG ] detect machine type
[2017-05-16 00:08:13,605][gdb3][DEBUG ] find the location of an executable
[2017-05-16 00:08:13,605][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-16 00:08:13,605][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-16 00:08:13,605][gdb3][DEBUG ] get remote short hostname
[2017-05-16 00:08:13,606][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-16 00:08:13,606][gdb3][DEBUG ] get remote short hostname
[2017-05-16 00:08:13,606][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-16 00:08:13,607][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-16 00:08:13,608][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-16 00:08:13,609][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-16 00:08:13,610][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-16 00:08:13,610][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-16 00:08:13,611][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-16 00:08:13,683][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-16 00:08:13,757][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-16 00:08:15,827][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-16 00:08:15,892][gdb3][DEBUG ] ********************************************************************************
[2017-05-16 00:08:15,893][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-16 00:08:15,893][gdb3][DEBUG ] {
[2017-05-16 00:08:15,893][gdb3][DEBUG ]   "election_epoch": 5, 
[2017-05-16 00:08:15,893][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-16 00:08:15,893][gdb3][DEBUG ]   "features": {
[2017-05-16 00:08:15,893][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-16 00:08:15,893][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-16 00:08:15,893][gdb3][DEBUG ]       "kraken", 
[2017-05-16 00:08:15,893][gdb3][DEBUG ]       "luminous"
[2017-05-16 00:08:15,894][gdb3][DEBUG ]     ], 
[2017-05-16 00:08:15,894][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-16 00:08:15,894][gdb3][DEBUG ]     "required_mon": [
[2017-05-16 00:08:15,894][gdb3][DEBUG ]       "kraken", 
[2017-05-16 00:08:15,894][gdb3][DEBUG ]       "luminous"
[2017-05-16 00:08:15,894][gdb3][DEBUG ]     ]
[2017-05-16 00:08:15,894][gdb3][DEBUG ]   }, 
[2017-05-16 00:08:15,894][gdb3][DEBUG ]   "monmap": {
[2017-05-16 00:08:15,894][gdb3][DEBUG ]     "created": "2017-05-15 15:51:27.197421", 
[2017-05-16 00:08:15,894][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-16 00:08:15,894][gdb3][DEBUG ]     "features": {
[2017-05-16 00:08:15,894][gdb3][DEBUG ]       "optional": [], 
[2017-05-16 00:08:15,894][gdb3][DEBUG ]       "persistent": [
[2017-05-16 00:08:15,894][gdb3][DEBUG ]         "kraken", 
[2017-05-16 00:08:15,894][gdb3][DEBUG ]         "luminous"
[2017-05-16 00:08:15,894][gdb3][DEBUG ]       ]
[2017-05-16 00:08:15,894][gdb3][DEBUG ]     }, 
[2017-05-16 00:08:15,895][gdb3][DEBUG ]     "fsid": "46a856ae-ecba-4352-8fe5-621ec7158a1f", 
[2017-05-16 00:08:15,895][gdb3][DEBUG ]     "modified": "2017-05-15 15:51:27.435930", 
[2017-05-16 00:08:15,895][gdb3][DEBUG ]     "mons": [
[2017-05-16 00:08:15,895][gdb3][DEBUG ]       {
[2017-05-16 00:08:15,895][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-16 00:08:15,895][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-16 00:08:15,895][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-16 00:08:15,895][gdb3][DEBUG ]         "rank": 0
[2017-05-16 00:08:15,895][gdb3][DEBUG ]       }
[2017-05-16 00:08:15,895][gdb3][DEBUG ]     ]
[2017-05-16 00:08:15,895][gdb3][DEBUG ]   }, 
[2017-05-16 00:08:15,895][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-16 00:08:15,895][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-16 00:08:15,895][gdb3][DEBUG ]   "quorum": [
[2017-05-16 00:08:15,895][gdb3][DEBUG ]     0
[2017-05-16 00:08:15,895][gdb3][DEBUG ]   ], 
[2017-05-16 00:08:15,896][gdb3][DEBUG ]   "rank": 0, 
[2017-05-16 00:08:15,896][gdb3][DEBUG ]   "state": "leader", 
[2017-05-16 00:08:15,896][gdb3][DEBUG ]   "sync_provider": []
[2017-05-16 00:08:15,896][gdb3][DEBUG ] }
[2017-05-16 00:08:15,896][gdb3][DEBUG ] ********************************************************************************
[2017-05-16 00:08:15,896][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-16 00:08:15,897][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-16 00:08:15,962][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-16 00:08:15,979][gdb3][DEBUG ] connection detected need for sudo
[2017-05-16 00:08:15,992][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-16 00:08:15,993][gdb3][DEBUG ] detect platform information from remote host
[2017-05-16 00:08:16,009][gdb3][DEBUG ] detect machine type
[2017-05-16 00:08:16,011][gdb3][DEBUG ] find the location of an executable
[2017-05-16 00:08:16,012][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-16 00:08:16,077][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-16 00:08:16,078][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-16 00:08:16,078][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-16 00:08:16,080][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpJhhi2u
[2017-05-16 00:08:16,095][gdb3][DEBUG ] connection detected need for sudo
[2017-05-16 00:08:16,108][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-16 00:08:16,109][gdb3][DEBUG ] detect platform information from remote host
[2017-05-16 00:08:16,125][gdb3][DEBUG ] detect machine type
[2017-05-16 00:08:16,127][gdb3][DEBUG ] get remote short hostname
[2017-05-16 00:08:16,128][gdb3][DEBUG ] fetch remote file
[2017-05-16 00:08:16,129][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-16 00:08:16,195][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-16 00:08:16,361][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-16 00:08:16,527][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-16 00:08:16,693][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-16 00:08:16,859][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-16 00:08:17,025][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-05-16 00:08:17,026][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-05-16 00:08:17,027][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-05-16 00:08:17,027][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-16 00:08:17,027][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-05-16 00:08:17,028][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-05-16 00:08:17,028][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpJhhi2u
[2017-05-17 20:50:20,319][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-17 20:50:20,320][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-17 20:50:20,320][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-17 20:50:20,320][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-17 20:50:20,321][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-17 20:50:20,321][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-17 20:50:20,321][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-17 20:50:20,321][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-17 20:50:20,321][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f44b2d27e60>
[2017-05-17 20:50:20,321][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-17 20:50:20,321][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f44b2cfcb18>
[2017-05-17 20:50:20,321][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-17 20:50:20,321][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-17 20:50:20,321][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-17 20:50:20,323][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-17 20:50:20,323][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-17 20:50:20,361][gdb3][DEBUG ] connection detected need for sudo
[2017-05-17 20:50:20,392][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-17 20:50:20,393][gdb3][DEBUG ] detect platform information from remote host
[2017-05-17 20:50:20,409][gdb3][DEBUG ] detect machine type
[2017-05-17 20:50:20,412][gdb3][DEBUG ] find the location of an executable
[2017-05-17 20:50:20,412][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-17 20:50:20,412][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-17 20:50:20,412][gdb3][DEBUG ] get remote short hostname
[2017-05-17 20:50:20,413][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-17 20:50:20,413][gdb3][DEBUG ] get remote short hostname
[2017-05-17 20:50:20,413][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-17 20:50:20,414][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-17 20:50:20,420][ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-17 20:50:20,420][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2017-05-17 20:50:30,551][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf mon create-initial
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3d0c05ce60>
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f3d0c031b18>
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-17 20:50:30,552][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-17 20:50:30,553][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-17 20:50:30,553][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-17 20:50:30,553][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-17 20:50:30,579][gdb3][DEBUG ] connection detected need for sudo
[2017-05-17 20:50:30,593][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-17 20:50:30,593][gdb3][DEBUG ] detect platform information from remote host
[2017-05-17 20:50:30,610][gdb3][DEBUG ] detect machine type
[2017-05-17 20:50:30,612][gdb3][DEBUG ] find the location of an executable
[2017-05-17 20:50:30,612][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-17 20:50:30,612][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-17 20:50:30,613][gdb3][DEBUG ] get remote short hostname
[2017-05-17 20:50:30,613][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-17 20:50:30,613][gdb3][DEBUG ] get remote short hostname
[2017-05-17 20:50:30,613][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-17 20:50:30,614][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-17 20:50:30,615][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-17 20:50:30,616][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-17 20:50:30,617][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-17 20:50:30,618][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-17 20:50:30,619][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-17 20:50:30,693][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-17 20:50:30,760][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-17 20:50:32,831][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-17 20:50:32,896][gdb3][DEBUG ] ********************************************************************************
[2017-05-17 20:50:32,896][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-17 20:50:32,896][gdb3][DEBUG ] {
[2017-05-17 20:50:32,896][gdb3][DEBUG ]   "election_epoch": 6, 
[2017-05-17 20:50:32,896][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-17 20:50:32,897][gdb3][DEBUG ]   "features": {
[2017-05-17 20:50:32,897][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-17 20:50:32,897][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-17 20:50:32,897][gdb3][DEBUG ]       "kraken", 
[2017-05-17 20:50:32,897][gdb3][DEBUG ]       "luminous"
[2017-05-17 20:50:32,897][gdb3][DEBUG ]     ], 
[2017-05-17 20:50:32,897][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-17 20:50:32,897][gdb3][DEBUG ]     "required_mon": [
[2017-05-17 20:50:32,897][gdb3][DEBUG ]       "kraken", 
[2017-05-17 20:50:32,897][gdb3][DEBUG ]       "luminous"
[2017-05-17 20:50:32,897][gdb3][DEBUG ]     ]
[2017-05-17 20:50:32,897][gdb3][DEBUG ]   }, 
[2017-05-17 20:50:32,897][gdb3][DEBUG ]   "monmap": {
[2017-05-17 20:50:32,897][gdb3][DEBUG ]     "created": "2017-05-15 15:51:27.197421", 
[2017-05-17 20:50:32,897][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-17 20:50:32,898][gdb3][DEBUG ]     "features": {
[2017-05-17 20:50:32,898][gdb3][DEBUG ]       "optional": [], 
[2017-05-17 20:50:32,898][gdb3][DEBUG ]       "persistent": [
[2017-05-17 20:50:32,898][gdb3][DEBUG ]         "kraken", 
[2017-05-17 20:50:32,898][gdb3][DEBUG ]         "luminous"
[2017-05-17 20:50:32,898][gdb3][DEBUG ]       ]
[2017-05-17 20:50:32,898][gdb3][DEBUG ]     }, 
[2017-05-17 20:50:32,898][gdb3][DEBUG ]     "fsid": "46a856ae-ecba-4352-8fe5-621ec7158a1f", 
[2017-05-17 20:50:32,898][gdb3][DEBUG ]     "modified": "2017-05-15 15:51:27.435930", 
[2017-05-17 20:50:32,898][gdb3][DEBUG ]     "mons": [
[2017-05-17 20:50:32,898][gdb3][DEBUG ]       {
[2017-05-17 20:50:32,898][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-17 20:50:32,898][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-17 20:50:32,898][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-17 20:50:32,898][gdb3][DEBUG ]         "rank": 0
[2017-05-17 20:50:32,898][gdb3][DEBUG ]       }
[2017-05-17 20:50:32,898][gdb3][DEBUG ]     ]
[2017-05-17 20:50:32,899][gdb3][DEBUG ]   }, 
[2017-05-17 20:50:32,899][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-17 20:50:32,899][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-17 20:50:32,899][gdb3][DEBUG ]   "quorum": [
[2017-05-17 20:50:32,899][gdb3][DEBUG ]     0
[2017-05-17 20:50:32,899][gdb3][DEBUG ]   ], 
[2017-05-17 20:50:32,899][gdb3][DEBUG ]   "rank": 0, 
[2017-05-17 20:50:32,899][gdb3][DEBUG ]   "state": "leader", 
[2017-05-17 20:50:32,899][gdb3][DEBUG ]   "sync_provider": []
[2017-05-17 20:50:32,899][gdb3][DEBUG ] }
[2017-05-17 20:50:32,899][gdb3][DEBUG ] ********************************************************************************
[2017-05-17 20:50:32,899][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-17 20:50:32,900][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-17 20:50:32,965][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-17 20:50:32,981][gdb3][DEBUG ] connection detected need for sudo
[2017-05-17 20:50:32,995][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-17 20:50:32,996][gdb3][DEBUG ] detect platform information from remote host
[2017-05-17 20:50:33,012][gdb3][DEBUG ] detect machine type
[2017-05-17 20:50:33,015][gdb3][DEBUG ] find the location of an executable
[2017-05-17 20:50:33,016][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-17 20:50:33,081][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-17 20:50:33,081][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-17 20:50:33,081][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-17 20:50:33,083][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpNCDzJg
[2017-05-17 20:50:33,099][gdb3][DEBUG ] connection detected need for sudo
[2017-05-17 20:50:33,112][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-17 20:50:33,113][gdb3][DEBUG ] detect platform information from remote host
[2017-05-17 20:50:33,129][gdb3][DEBUG ] detect machine type
[2017-05-17 20:50:33,132][gdb3][DEBUG ] get remote short hostname
[2017-05-17 20:50:33,132][gdb3][DEBUG ] fetch remote file
[2017-05-17 20:50:33,133][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-17 20:50:33,199][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-17 20:50:33,366][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-17 20:50:33,532][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-17 20:50:33,698][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-17 20:50:33,864][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-17 20:50:34,030][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-05-17 20:50:34,031][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-05-17 20:50:34,032][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-05-17 20:50:34,032][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-17 20:50:34,032][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-05-17 20:50:34,033][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-05-17 20:50:34,033][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpNCDzJg
[2017-06-06 23:17:47,455][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:17:47,456][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-06 23:17:47,456][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:17:47,456][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:17:47,456][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:17:47,456][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:17:47,456][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:17:47,456][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2f97a4b0e0>
[2017-06-06 23:17:47,457][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:17:47,457][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-06 23:17:47,457][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f2f983581b8>
[2017-06-06 23:17:47,457][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:17:47,457][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:17:47,457][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-06 23:17:47,457][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-06 23:17:47,457][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-06 23:17:47,457][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-06 23:17:47,494][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:17:47,508][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:17:47,509][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:17:47,526][gdb3][DEBUG ] detect machine type
[2017-06-06 23:17:47,528][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-06 23:17:47,528][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-06 23:17:47,529][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-06 23:17:47,569][gdb3][DEBUG ] Reading package lists...
[2017-06-06 23:17:47,733][gdb3][DEBUG ] Building dependency tree...
[2017-06-06 23:17:47,733][gdb3][DEBUG ] Reading state information...
[2017-06-06 23:17:47,797][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-06 23:17:47,797][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-06 23:17:47,798][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-06 23:17:47,798][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-06 23:17:47,798][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-06 23:17:47,798][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-06 23:17:47,798][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
[2017-06-06 23:17:47,798][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws
[2017-06-06 23:17:47,798][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws ntp python-blinker python-cephfs
[2017-06-06 23:17:47,798][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-06-06 23:17:47,798][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-06-06 23:17:47,798][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-06-06 23:17:47,798][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-06-06 23:17:47,798][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-06-06 23:17:47,799][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-06 23:17:47,814][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-06 23:17:47,814][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-06 23:17:47,979][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 37 not upgraded.
[2017-06-06 23:17:47,979][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-06 23:17:47,983][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 129562 files and directories currently installed.)
[2017-06-06 23:17:47,986][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-06 23:17:48,200][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-06 23:17:48,315][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-06 23:17:48,315][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-06 23:17:48,529][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-06 23:17:48,643][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-06 23:17:48,811][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-06 23:17:48,842][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-06 23:17:48,874][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-06 23:17:49,089][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-06 23:17:49,121][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-06 23:17:49,136][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-06 23:17:49,301][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-06 23:17:49,304][gdb3][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-06-06 23:17:49,336][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-06 23:17:49,501][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-06 23:17:49,533][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-06 23:17:49,540][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-06 23:17:49,754][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-06-06 23:17:50,521][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-06 23:17:50,687][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:17:50,687][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-06 23:17:50,687][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:17:50,688][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:17:50,688][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:17:50,688][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:17:50,688][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:17:50,688][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f034fd0c758>
[2017-06-06 23:17:50,688][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:17:50,688][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-06 23:17:50,688][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f0350619230>
[2017-06-06 23:17:50,688][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:17:50,688][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:17:50,688][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-06 23:17:50,714][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:17:50,728][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:17:50,728][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:17:50,744][gdb3][DEBUG ] detect machine type
[2017-06-06 23:17:50,747][gdb3][DEBUG ] find the location of an executable
[2017-06-06 23:17:50,762][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:17:50,777][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:17:50,778][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:17:50,795][gdb3][DEBUG ] detect machine type
[2017-06-06 23:17:50,797][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-06 23:17:50,797][gdb3][INFO  ] purging data on gdb3
[2017-06-06 23:17:50,798][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-06 23:17:50,811][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-06 23:17:50,984][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:17:50,985][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-06 23:17:50,985][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:17:50,985][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:17:50,985][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:17:50,985][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:17:50,985][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:17:50,985][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc373019a28>
[2017-06-06 23:17:50,985][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:17:50,985][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fc3738dc848>
[2017-06-06 23:17:50,985][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:17:50,985][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:18:56,226][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:18:56,226][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-06 23:18:56,226][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:18:56,226][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc1336115a8>
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fc133c95758>
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:18:56,227][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-06 23:18:56,227][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-06 23:18:56,228][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-06 23:18:56,254][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:18:56,268][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:18:56,268][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:18:56,285][gdb3][DEBUG ] detect machine type
[2017-06-06 23:18:56,287][gdb3][DEBUG ] find the location of an executable
[2017-06-06 23:18:56,288][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-06 23:18:56,300][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-06 23:18:56,306][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-06 23:18:56,306][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-06 23:18:56,306][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-06 23:18:56,307][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-06 23:18:56,307][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-06 23:18:56,307][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-06 23:18:56,307][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-06 23:18:56,307][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-06 23:18:56,476][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:18:56,476][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-06 23:18:56,476][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:18:56,477][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:18:56,477][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:18:56,477][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:18:56,477][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-06 23:18:56,477][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:18:56,477][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe92225eea8>
[2017-06-06 23:18:56,477][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:18:56,477][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fe922232b18>
[2017-06-06 23:18:56,477][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:18:56,478][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-06 23:18:56,478][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:18:56,478][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-06 23:18:56,479][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-06 23:18:56,505][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:18:56,520][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:18:56,520][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:18:56,537][gdb3][DEBUG ] detect machine type
[2017-06-06 23:18:56,539][gdb3][DEBUG ] find the location of an executable
[2017-06-06 23:18:56,539][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-06 23:18:56,540][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-06 23:18:56,540][gdb3][DEBUG ] get remote short hostname
[2017-06-06 23:18:56,540][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-06 23:18:56,540][gdb3][DEBUG ] get remote short hostname
[2017-06-06 23:18:56,540][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-06 23:18:56,541][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:18:56,542][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-06 23:18:56,543][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-06 23:18:56,543][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-06 23:18:56,543][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-06 23:18:56,543][gdb3][DEBUG ] create the monitor keyring file
[2017-06-06 23:18:56,544][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-06 23:18:56,582][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-06 23:18:56,583][gdb3][DEBUG ] ceph-mon: set fsid to c5ebcdfa-bacf-4217-a9c1-bf47efc2961f
[2017-06-06 23:18:56,584][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-06 23:18:56,587][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-06 23:18:56,588][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-06 23:18:56,588][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-06 23:18:56,589][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-06 23:18:56,657][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-06 23:18:56,725][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-06 23:18:58,764][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-06 23:18:58,829][gdb3][DEBUG ] ********************************************************************************
[2017-06-06 23:18:58,829][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-06 23:18:58,829][gdb3][DEBUG ] {
[2017-06-06 23:18:58,830][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-06 23:18:58,830][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-06 23:18:58,830][gdb3][DEBUG ]   "features": {
[2017-06-06 23:18:58,830][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-06 23:18:58,830][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-06 23:18:58,830][gdb3][DEBUG ]       "kraken", 
[2017-06-06 23:18:58,830][gdb3][DEBUG ]       "luminous"
[2017-06-06 23:18:58,830][gdb3][DEBUG ]     ], 
[2017-06-06 23:18:58,830][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-06 23:18:58,830][gdb3][DEBUG ]     "required_mon": [
[2017-06-06 23:18:58,830][gdb3][DEBUG ]       "kraken", 
[2017-06-06 23:18:58,830][gdb3][DEBUG ]       "luminous"
[2017-06-06 23:18:58,830][gdb3][DEBUG ]     ]
[2017-06-06 23:18:58,830][gdb3][DEBUG ]   }, 
[2017-06-06 23:18:58,830][gdb3][DEBUG ]   "monmap": {
[2017-06-06 23:18:58,830][gdb3][DEBUG ]     "created": "2017-06-06 23:18:56.567881", 
[2017-06-06 23:18:58,831][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-06 23:18:58,831][gdb3][DEBUG ]     "features": {
[2017-06-06 23:18:58,831][gdb3][DEBUG ]       "optional": [], 
[2017-06-06 23:18:58,831][gdb3][DEBUG ]       "persistent": [
[2017-06-06 23:18:58,831][gdb3][DEBUG ]         "kraken", 
[2017-06-06 23:18:58,831][gdb3][DEBUG ]         "luminous"
[2017-06-06 23:18:58,831][gdb3][DEBUG ]       ]
[2017-06-06 23:18:58,831][gdb3][DEBUG ]     }, 
[2017-06-06 23:18:58,831][gdb3][DEBUG ]     "fsid": "c5ebcdfa-bacf-4217-a9c1-bf47efc2961f", 
[2017-06-06 23:18:58,831][gdb3][DEBUG ]     "modified": "2017-06-06 23:18:56.800139", 
[2017-06-06 23:18:58,831][gdb3][DEBUG ]     "mons": [
[2017-06-06 23:18:58,831][gdb3][DEBUG ]       {
[2017-06-06 23:18:58,831][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-06 23:18:58,831][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-06 23:18:58,831][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-06 23:18:58,831][gdb3][DEBUG ]         "rank": 0
[2017-06-06 23:18:58,832][gdb3][DEBUG ]       }
[2017-06-06 23:18:58,832][gdb3][DEBUG ]     ]
[2017-06-06 23:18:58,832][gdb3][DEBUG ]   }, 
[2017-06-06 23:18:58,832][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-06 23:18:58,832][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-06 23:18:58,832][gdb3][DEBUG ]   "quorum": [
[2017-06-06 23:18:58,832][gdb3][DEBUG ]     0
[2017-06-06 23:18:58,832][gdb3][DEBUG ]   ], 
[2017-06-06 23:18:58,832][gdb3][DEBUG ]   "rank": 0, 
[2017-06-06 23:18:58,832][gdb3][DEBUG ]   "state": "leader", 
[2017-06-06 23:18:58,832][gdb3][DEBUG ]   "sync_provider": []
[2017-06-06 23:18:58,832][gdb3][DEBUG ] }
[2017-06-06 23:18:58,832][gdb3][DEBUG ] ********************************************************************************
[2017-06-06 23:18:58,832][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-06 23:18:58,833][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-06 23:18:58,898][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-06 23:18:58,917][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:18:58,930][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:18:58,931][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:18:58,948][gdb3][DEBUG ] detect machine type
[2017-06-06 23:18:58,950][gdb3][DEBUG ] find the location of an executable
[2017-06-06 23:18:58,951][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-06 23:18:59,016][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-06 23:18:59,017][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-06 23:18:59,017][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-06 23:18:59,019][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpYLtIH0
[2017-06-06 23:18:59,034][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:18:59,048][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:18:59,048][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:18:59,065][gdb3][DEBUG ] detect machine type
[2017-06-06 23:18:59,067][gdb3][DEBUG ] get remote short hostname
[2017-06-06 23:18:59,068][gdb3][DEBUG ] fetch remote file
[2017-06-06 23:18:59,069][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-06 23:18:59,135][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-06 23:18:59,301][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-06 23:18:59,517][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-06 23:18:59,683][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-06 23:18:59,849][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-06 23:19:00,016][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-06 23:19:00,182][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-06 23:19:00,348][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-06 23:19:00,514][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-06 23:19:00,680][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-06 23:19:00,846][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-06 23:19:00,846][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-06 23:19:00,848][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170606231900'
[2017-06-06 23:19:00,848][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-06 23:19:00,848][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-06 23:19:00,849][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-06 23:19:00,850][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpYLtIH0
[2017-06-06 23:20:13,411][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:20:13,411][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-06-06 23:20:13,411][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:20:13,411][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:20:13,411][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-06 23:20:13,411][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-06-06 23:20:13,411][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f734814e950>
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f73483a4aa0>
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:20:13,412][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-06-06 23:20:13,413][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-06-06 23:20:13,413][ceph_deploy][ERROR ] RuntimeError: bootstrap-osd keyring not found; run 'gatherkeys'

[2017-06-06 23:20:13,577][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:20:13,577][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-06-06 23:20:13,577][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:20:13,577][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:20:13,577][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:20:13,577][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:20:13,577][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:20:13,577][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1343243560>
[2017-06-06 23:20:13,578][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:20:13,578][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-06-06 23:20:13,578][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f1343b5a938>
[2017-06-06 23:20:13,578][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:20:13,578][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:20:13,578][ceph_deploy][ERROR ] RuntimeError: ceph.client.admin.keyring not found

[2017-06-06 23:20:23,067][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:20:23,068][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-06 23:20:23,068][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:20:23,068][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:20:23,068][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:20:23,068][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:20:23,068][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:20:23,068][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5c819a05a8>
[2017-06-06 23:20:23,068][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:20:23,068][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-06 23:20:23,068][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-06 23:20:23,069][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f5c82024758>
[2017-06-06 23:20:23,069][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-06 23:20:23,069][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:20:23,069][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-06 23:20:23,069][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:20:23,069][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-06 23:20:23,069][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-06 23:20:23,069][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-06 23:20:23,096][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:20:23,110][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:20:23,110][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:20:23,127][gdb3][DEBUG ] detect machine type
[2017-06-06 23:20:23,129][gdb3][DEBUG ] find the location of an executable
[2017-06-06 23:20:23,130][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-06 23:20:23,141][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-06 23:20:23,148][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-06 23:20:23,148][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-06 23:20:23,148][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-06 23:20:23,148][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-06 23:20:23,148][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-06 23:20:23,149][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-06 23:20:23,149][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-06 23:20:23,149][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-06 23:20:23,314][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:20:23,315][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-06 23:20:23,315][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:20:23,315][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:20:23,315][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:20:23,315][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:20:23,315][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-06 23:20:23,315][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:20:23,315][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f46f85f8ea8>
[2017-06-06 23:20:23,315][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:20:23,315][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f46f85ccb18>
[2017-06-06 23:20:23,315][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:20:23,316][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-06 23:20:23,316][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:20:23,316][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-06 23:20:23,316][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-06 23:20:23,342][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:20:23,356][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:20:23,357][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:20:23,373][gdb3][DEBUG ] detect machine type
[2017-06-06 23:20:23,376][gdb3][DEBUG ] find the location of an executable
[2017-06-06 23:20:23,376][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-06 23:20:23,376][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-06 23:20:23,376][gdb3][DEBUG ] get remote short hostname
[2017-06-06 23:20:23,376][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-06 23:20:23,377][gdb3][DEBUG ] get remote short hostname
[2017-06-06 23:20:23,377][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-06 23:20:23,377][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:20:23,381][ceph_deploy.mon][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-06-06 23:20:23,381][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2017-06-06 23:20:29,142][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:20:29,142][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-06 23:20:29,142][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:20:29,142][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:20:29,143][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:20:29,143][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:20:29,143][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:20:29,143][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fba4fe730e0>
[2017-06-06 23:20:29,143][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:20:29,143][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-06 23:20:29,143][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fba507801b8>
[2017-06-06 23:20:29,143][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:20:29,143][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:20:29,143][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-06 23:20:29,143][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-06 23:20:29,143][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-06 23:20:29,143][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-06 23:20:29,170][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:20:29,185][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:20:29,186][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:20:29,202][gdb3][DEBUG ] detect machine type
[2017-06-06 23:20:29,205][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-06 23:20:29,205][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-06 23:20:29,206][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-06 23:20:29,244][gdb3][DEBUG ] Reading package lists...
[2017-06-06 23:20:29,409][gdb3][DEBUG ] Building dependency tree...
[2017-06-06 23:20:29,409][gdb3][DEBUG ] Reading state information...
[2017-06-06 23:20:29,473][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-06 23:20:29,473][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-06 23:20:29,474][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-06 23:20:29,474][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-06 23:20:29,474][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-06 23:20:29,474][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-06 23:20:29,474][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
[2017-06-06 23:20:29,474][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws
[2017-06-06 23:20:29,474][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws ntp python-blinker python-cephfs
[2017-06-06 23:20:29,474][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-06-06 23:20:29,475][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-06-06 23:20:29,475][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-06-06 23:20:29,475][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-06-06 23:20:29,475][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-06-06 23:20:29,475][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-06 23:20:29,507][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-06 23:20:29,507][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-06 23:20:29,621][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 37 not upgraded.
[2017-06-06 23:20:29,622][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-06 23:20:29,691][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 129562 files and directories currently installed.)
[2017-06-06 23:20:29,691][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-06 23:20:29,859][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-06 23:20:29,923][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-06 23:20:29,988][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-06 23:20:30,202][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-06 23:20:30,316][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-06 23:20:30,431][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-06 23:20:30,545][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-06 23:20:30,545][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-06 23:20:30,710][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-06 23:20:30,774][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-06 23:20:30,806][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-06 23:20:30,972][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-06 23:20:31,004][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-06 23:20:31,169][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-06 23:20:31,233][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-06 23:20:31,233][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-06 23:20:31,347][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-06-06 23:20:32,163][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-06 23:20:32,328][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4fbaba5758>
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f4fbb4b2230>
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:20:32,329][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:20:32,329][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-06 23:20:32,356][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:20:32,370][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:20:32,370][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:20:32,387][gdb3][DEBUG ] detect machine type
[2017-06-06 23:20:32,389][gdb3][DEBUG ] find the location of an executable
[2017-06-06 23:20:32,405][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:20:32,419][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:20:32,419][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:20:32,436][gdb3][DEBUG ] detect machine type
[2017-06-06 23:20:32,439][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-06 23:20:32,439][gdb3][INFO  ] purging data on gdb3
[2017-06-06 23:20:32,440][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-06 23:20:32,453][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-06 23:20:32,626][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:20:32,626][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-06 23:20:32,626][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:20:32,626][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:20:32,626][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:20:32,626][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:20:32,626][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:20:32,626][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8f47bbda28>
[2017-06-06 23:20:32,626][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:20:32,627][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f8f48480848>
[2017-06-06 23:20:32,627][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:20:32,627][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:20:47,577][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:20:47,577][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-06 23:20:47,577][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7a854a35a8>
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f7a85b27758>
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:20:47,578][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-06 23:20:47,578][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-06 23:20:47,579][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-06 23:20:47,605][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:20:47,618][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:20:47,619][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:20:47,636][gdb3][DEBUG ] detect machine type
[2017-06-06 23:20:47,638][gdb3][DEBUG ] find the location of an executable
[2017-06-06 23:20:47,639][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-06 23:20:47,650][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-06 23:20:47,657][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-06 23:20:47,657][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-06 23:20:47,657][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-06 23:20:47,657][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-06 23:20:47,657][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-06 23:20:47,657][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-06 23:20:47,657][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-06 23:20:47,658][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-06 23:20:47,823][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:20:47,824][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-06 23:20:47,824][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:20:47,824][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:20:47,824][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:20:47,824][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:20:47,824][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-06 23:20:47,824][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:20:47,824][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f53184c5ea8>
[2017-06-06 23:20:47,824][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:20:47,824][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f5318499b18>
[2017-06-06 23:20:47,824][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:20:47,825][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-06 23:20:47,825][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:20:47,825][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-06 23:20:47,825][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-06 23:20:47,851][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:20:47,865][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:20:47,866][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:20:47,882][gdb3][DEBUG ] detect machine type
[2017-06-06 23:20:47,884][gdb3][DEBUG ] find the location of an executable
[2017-06-06 23:20:47,885][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-06 23:20:47,885][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-06 23:20:47,885][gdb3][DEBUG ] get remote short hostname
[2017-06-06 23:20:47,885][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-06 23:20:47,885][gdb3][DEBUG ] get remote short hostname
[2017-06-06 23:20:47,886][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-06 23:20:47,886][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:20:47,888][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-06 23:20:47,888][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-06 23:20:47,888][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-06 23:20:47,889][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-06 23:20:47,889][gdb3][DEBUG ] create the monitor keyring file
[2017-06-06 23:20:47,890][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-06 23:20:47,928][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-06 23:20:47,928][gdb3][DEBUG ] ceph-mon: set fsid to d6478420-ca9b-4fd4-8ad7-195def2f5287
[2017-06-06 23:20:47,929][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-06 23:20:47,932][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-06 23:20:47,933][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-06 23:20:47,933][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-06 23:20:47,934][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-06 23:20:48,005][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-06 23:20:48,072][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-06 23:20:50,113][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-06 23:20:50,178][gdb3][DEBUG ] ********************************************************************************
[2017-06-06 23:20:50,178][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-06 23:20:50,178][gdb3][DEBUG ] {
[2017-06-06 23:20:50,178][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-06 23:20:50,179][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-06 23:20:50,179][gdb3][DEBUG ]   "features": {
[2017-06-06 23:20:50,179][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-06 23:20:50,179][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-06 23:20:50,179][gdb3][DEBUG ]       "kraken", 
[2017-06-06 23:20:50,179][gdb3][DEBUG ]       "luminous"
[2017-06-06 23:20:50,179][gdb3][DEBUG ]     ], 
[2017-06-06 23:20:50,179][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-06 23:20:50,179][gdb3][DEBUG ]     "required_mon": [
[2017-06-06 23:20:50,179][gdb3][DEBUG ]       "kraken", 
[2017-06-06 23:20:50,179][gdb3][DEBUG ]       "luminous"
[2017-06-06 23:20:50,179][gdb3][DEBUG ]     ]
[2017-06-06 23:20:50,179][gdb3][DEBUG ]   }, 
[2017-06-06 23:20:50,179][gdb3][DEBUG ]   "monmap": {
[2017-06-06 23:20:50,179][gdb3][DEBUG ]     "created": "2017-06-06 23:20:47.912808", 
[2017-06-06 23:20:50,179][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-06 23:20:50,180][gdb3][DEBUG ]     "features": {
[2017-06-06 23:20:50,180][gdb3][DEBUG ]       "optional": [], 
[2017-06-06 23:20:50,180][gdb3][DEBUG ]       "persistent": [
[2017-06-06 23:20:50,180][gdb3][DEBUG ]         "kraken", 
[2017-06-06 23:20:50,180][gdb3][DEBUG ]         "luminous"
[2017-06-06 23:20:50,180][gdb3][DEBUG ]       ]
[2017-06-06 23:20:50,180][gdb3][DEBUG ]     }, 
[2017-06-06 23:20:50,180][gdb3][DEBUG ]     "fsid": "d6478420-ca9b-4fd4-8ad7-195def2f5287", 
[2017-06-06 23:20:50,180][gdb3][DEBUG ]     "modified": "2017-06-06 23:20:48.149549", 
[2017-06-06 23:20:50,180][gdb3][DEBUG ]     "mons": [
[2017-06-06 23:20:50,180][gdb3][DEBUG ]       {
[2017-06-06 23:20:50,180][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-06 23:20:50,180][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-06 23:20:50,180][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-06 23:20:50,180][gdb3][DEBUG ]         "rank": 0
[2017-06-06 23:20:50,180][gdb3][DEBUG ]       }
[2017-06-06 23:20:50,180][gdb3][DEBUG ]     ]
[2017-06-06 23:20:50,181][gdb3][DEBUG ]   }, 
[2017-06-06 23:20:50,181][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-06 23:20:50,181][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-06 23:20:50,181][gdb3][DEBUG ]   "quorum": [
[2017-06-06 23:20:50,181][gdb3][DEBUG ]     0
[2017-06-06 23:20:50,181][gdb3][DEBUG ]   ], 
[2017-06-06 23:20:50,181][gdb3][DEBUG ]   "rank": 0, 
[2017-06-06 23:20:50,181][gdb3][DEBUG ]   "state": "leader", 
[2017-06-06 23:20:50,181][gdb3][DEBUG ]   "sync_provider": []
[2017-06-06 23:20:50,181][gdb3][DEBUG ] }
[2017-06-06 23:20:50,181][gdb3][DEBUG ] ********************************************************************************
[2017-06-06 23:20:50,181][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-06 23:20:50,182][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-06 23:20:50,247][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-06 23:20:50,262][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:20:50,277][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:20:50,278][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:20:50,295][gdb3][DEBUG ] detect machine type
[2017-06-06 23:20:50,297][gdb3][DEBUG ] find the location of an executable
[2017-06-06 23:20:50,299][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-06 23:20:50,364][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-06 23:20:50,364][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-06 23:20:50,364][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-06 23:20:50,366][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpKK4H45
[2017-06-06 23:20:50,382][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:20:50,396][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:20:50,396][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:20:50,413][gdb3][DEBUG ] detect machine type
[2017-06-06 23:20:50,415][gdb3][DEBUG ] get remote short hostname
[2017-06-06 23:20:50,416][gdb3][DEBUG ] fetch remote file
[2017-06-06 23:20:50,417][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-06 23:20:50,483][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-06 23:20:50,649][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-06 23:20:50,816][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-06 23:20:50,982][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-06 23:20:51,198][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-06 23:20:51,364][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-06 23:20:51,530][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-06 23:20:51,697][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-06 23:20:51,863][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-06 23:20:52,029][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-06 23:20:52,194][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-06 23:20:52,194][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-06 23:20:52,195][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring
[2017-06-06 23:20:52,195][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-06 23:20:52,195][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-06 23:20:52,195][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-06 23:20:52,195][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpKK4H45
[2017-06-06 23:20:58,937][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:20:58,937][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-06-06 23:20:58,937][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:20:58,937][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:20:58,937][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-06 23:20:58,937][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-06-06 23:20:58,937][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-06 23:20:58,937][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:20:58,937][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff1002f5950>
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff10054baa0>
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:20:58,938][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-06-06 23:20:58,938][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-06-06 23:20:59,193][gdb0][DEBUG ] connection detected need for sudo
[2017-06-06 23:20:59,425][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-06 23:20:59,426][gdb0][DEBUG ] detect platform information from remote host
[2017-06-06 23:20:59,444][gdb0][DEBUG ] detect machine type
[2017-06-06 23:20:59,448][gdb0][DEBUG ] find the location of an executable
[2017-06-06 23:20:59,449][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-06 23:20:59,449][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-06 23:20:59,449][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:20:59,452][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-06-06 23:20:59,452][gdb0][DEBUG ] find the location of an executable
[2017-06-06 23:20:59,454][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-06 23:20:59,675][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-06 23:20:59,675][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:20:59,675][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:20:59,683][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:20:59,698][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:20:59,699][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-06-06 23:20:59,699][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-06 23:20:59,714][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:20:59,715][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:20:59,715][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:20:59,715][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-06-06 23:20:59,722][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-06-06 23:20:59,730][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-06-06 23:20:59,745][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-06-06 23:20:59,749][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:20:59,749][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-06-06 23:20:59,749][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:20:59,749][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-06-06 23:20:59,750][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-06-06 23:20:59,758][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-06-06 23:20:59,758][gdb0][WARNING] backup header from main header.
[2017-06-06 23:20:59,758][gdb0][WARNING] 
[2017-06-06 23:21:00,825][gdb0][DEBUG ] ****************************************************************************
[2017-06-06 23:21:00,825][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-06-06 23:21:00,825][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-06-06 23:21:00,826][gdb0][DEBUG ] ****************************************************************************
[2017-06-06 23:21:00,826][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-06-06 23:21:00,826][gdb0][DEBUG ] other utilities.
[2017-06-06 23:21:00,826][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-06-06 23:21:01,843][gdb0][DEBUG ] Creating new GPT entries.
[2017-06-06 23:21:01,843][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-06 23:21:01,843][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-06-06 23:21:01,843][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:21:01,843][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:21:01,859][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:21:01,875][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:01,875][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:01,875][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-06-06 23:21:01,875][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:01,875][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-06-06 23:21:01,875][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:8f30a735-b9e1-44a5-ae56-d242b1a8d467 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-06-06 23:21:02,892][gdb0][DEBUG ] Setting name!
[2017-06-06 23:21:02,892][gdb0][DEBUG ] partNum is 1
[2017-06-06 23:21:02,892][gdb0][DEBUG ] REALLY setting name!
[2017-06-06 23:21:02,893][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-06 23:21:02,893][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-06 23:21:02,893][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:21:03,107][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:21:03,272][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:21:03,272][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:03,272][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:03,272][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-06-06 23:21:03,272][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/8f30a735-b9e1-44a5-ae56-d242b1a8d467
[2017-06-06 23:21:03,272][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-06-06 23:21:04,289][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-06 23:21:04,291][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-06 23:21:04,291][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:21:04,505][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:21:04,619][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:21:04,620][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/8f30a735-b9e1-44a5-ae56-d242b1a8d467
[2017-06-06 23:21:04,620][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:04,620][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-06-06 23:21:04,620][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:04,620][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-06-06 23:21:04,620][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:04,620][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-06-06 23:21:04,620][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:975673d7-d62f-4d22-a5b0-3dadbd2a938a --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-06-06 23:21:05,638][gdb0][DEBUG ] Setting name!
[2017-06-06 23:21:05,638][gdb0][DEBUG ] partNum is 0
[2017-06-06 23:21:05,638][gdb0][DEBUG ] REALLY setting name!
[2017-06-06 23:21:05,638][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-06 23:21:05,638][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-06 23:21:05,638][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:21:05,853][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:21:06,067][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:21:06,282][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:06,282][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:06,282][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-06 23:21:06,282][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-06-06 23:21:06,282][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-06-06 23:21:06,999][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-06-06 23:21:06,999][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-06-06 23:21:06,999][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-06-06 23:21:06,999][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-06-06 23:21:06,999][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-06-06 23:21:06,999][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-06-06 23:21:06,999][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-06-06 23:21:06,999][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-06-06 23:21:07,000][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-06-06 23:21:07,000][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.mrI7Pf with options noatime,inode64
[2017-06-06 23:21:07,000][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.mrI7Pf
[2017-06-06 23:21:07,000][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.mrI7Pf
[2017-06-06 23:21:07,000][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.mrI7Pf/ceph_fsid.7560.tmp
[2017-06-06 23:21:07,001][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.mrI7Pf/fsid.7560.tmp
[2017-06-06 23:21:07,004][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.mrI7Pf/magic.7560.tmp
[2017-06-06 23:21:07,012][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.mrI7Pf/journal_uuid.7560.tmp
[2017-06-06 23:21:07,012][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.mrI7Pf/journal -> /dev/disk/by-partuuid/8f30a735-b9e1-44a5-ae56-d242b1a8d467
[2017-06-06 23:21:07,012][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.mrI7Pf
[2017-06-06 23:21:07,012][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.mrI7Pf
[2017-06-06 23:21:07,012][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.mrI7Pf
[2017-06-06 23:21:07,044][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:07,044][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-06-06 23:21:08,112][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-06 23:21:08,112][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-06 23:21:08,112][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:21:08,276][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:21:08,491][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:21:08,756][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-06-06 23:21:08,758][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-06 23:21:13,880][gdb0][INFO  ] checking OSD status...
[2017-06-06 23:21:13,881][gdb0][DEBUG ] find the location of an executable
[2017-06-06 23:21:13,883][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-06 23:21:13,998][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-06 23:21:14,166][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:21:14,166][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-06-06 23:21:14,166][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:21:14,166][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:21:14,166][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:21:14,166][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:21:14,166][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:21:14,166][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f499e9b6560>
[2017-06-06 23:21:14,166][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:21:14,166][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-06-06 23:21:14,167][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f499f2cd938>
[2017-06-06 23:21:14,167][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:21:14,167][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:21:14,167][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-06 23:21:14,405][gdb0][DEBUG ] connection detected need for sudo
[2017-06-06 23:21:14,637][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-06 23:21:14,638][gdb0][DEBUG ] detect platform information from remote host
[2017-06-06 23:21:14,654][gdb0][DEBUG ] detect machine type
[2017-06-06 23:21:14,658][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:21:14,660][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-06 23:21:14,891][gdb1][DEBUG ] connection detected need for sudo
[2017-06-06 23:21:15,122][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-06 23:21:15,122][gdb1][DEBUG ] detect platform information from remote host
[2017-06-06 23:21:15,141][gdb1][DEBUG ] detect machine type
[2017-06-06 23:21:15,145][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:21:15,147][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-06-06 23:21:15,147][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-06 23:21:15,162][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:21:15,177][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:21:15,177][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:21:15,194][gdb3][DEBUG ] detect machine type
[2017-06-06 23:21:15,196][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:21:15,198][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-06-06 23:21:57,280][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:21:57,280][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-06-06 23:21:57,280][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:21:57,280][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:21:57,280][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-06 23:21:57,280][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-06-06 23:21:57,280][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-06 23:21:57,280][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:21:57,280][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0328b42950>
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0328d98aa0>
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:21:57,281][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-06-06 23:21:57,282][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-06-06 23:21:57,527][gdb1][DEBUG ] connection detected need for sudo
[2017-06-06 23:21:57,753][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-06 23:21:57,754][gdb1][DEBUG ] detect platform information from remote host
[2017-06-06 23:21:57,770][gdb1][DEBUG ] detect machine type
[2017-06-06 23:21:57,774][gdb1][DEBUG ] find the location of an executable
[2017-06-06 23:21:57,774][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-06 23:21:57,774][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-06 23:21:57,775][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:21:57,777][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-06-06 23:21:57,777][gdb1][DEBUG ] find the location of an executable
[2017-06-06 23:21:57,779][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-06 23:21:58,000][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-06 23:21:58,000][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:21:58,016][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:21:58,032][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:21:58,039][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-06 23:21:58,055][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:58,055][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:58,056][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:58,056][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-06 23:21:58,056][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-06-06 23:21:58,088][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-06-06 23:21:58,088][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-06-06 23:21:58,103][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-06-06 23:21:58,107][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:58,107][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-06-06 23:21:58,107][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:21:58,107][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-06-06 23:21:58,139][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-06-06 23:21:58,139][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-06-06 23:21:58,142][gdb1][WARNING] 10+0 records in
[2017-06-06 23:21:58,142][gdb1][WARNING] 10+0 records out
[2017-06-06 23:21:58,143][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00817436 s, 1.3 GB/s
[2017-06-06 23:21:58,143][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-06-06 23:21:58,143][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-06-06 23:21:58,146][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-06-06 23:21:58,146][gdb1][WARNING] backup header from main header.
[2017-06-06 23:21:58,147][gdb1][WARNING] 
[2017-06-06 23:21:58,147][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-06-06 23:21:58,147][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-06-06 23:21:58,147][gdb1][WARNING] 
[2017-06-06 23:21:58,147][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-06-06 23:21:58,147][gdb1][WARNING] 
[2017-06-06 23:21:59,315][gdb1][DEBUG ] ****************************************************************************
[2017-06-06 23:21:59,315][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-06-06 23:21:59,315][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-06-06 23:21:59,315][gdb1][DEBUG ] ****************************************************************************
[2017-06-06 23:21:59,315][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-06-06 23:21:59,315][gdb1][DEBUG ] other utilities.
[2017-06-06 23:21:59,315][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-06-06 23:22:00,332][gdb1][DEBUG ] Creating new GPT entries.
[2017-06-06 23:22:00,333][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-06 23:22:00,333][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-06-06 23:22:00,333][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:22:00,333][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:22:00,348][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:22:00,380][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:22:00,380][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-06-06 23:22:00,381][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:22:00,381][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-06-06 23:22:00,381][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:22:00,381][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-06-06 23:22:00,381][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:4c973395-9765-4e89-ba7e-fb8afee4d776 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-06-06 23:22:01,398][gdb1][DEBUG ] Setting name!
[2017-06-06 23:22:01,398][gdb1][DEBUG ] partNum is 0
[2017-06-06 23:22:01,398][gdb1][DEBUG ] REALLY setting name!
[2017-06-06 23:22:01,398][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-06 23:22:01,398][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-06 23:22:01,398][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:22:01,512][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:22:01,727][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:22:01,743][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:22:01,743][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:22:01,743][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-06 23:22:01,743][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-06-06 23:22:01,743][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-06-06 23:22:02,459][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8388543 blks
[2017-06-06 23:22:02,459][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-06-06 23:22:02,459][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-06-06 23:22:02,459][gdb1][DEBUG ] data     =                       bsize=4096   blocks=33554171, imaxpct=25
[2017-06-06 23:22:02,460][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-06-06 23:22:02,460][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-06-06 23:22:02,460][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=16383, version=2
[2017-06-06 23:22:02,460][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-06-06 23:22:02,460][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-06-06 23:22:02,460][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.LMqTVw with options noatime,inode64
[2017-06-06 23:22:02,460][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.LMqTVw
[2017-06-06 23:22:02,492][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.LMqTVw
[2017-06-06 23:22:02,492][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.LMqTVw/ceph_fsid.22719.tmp
[2017-06-06 23:22:02,492][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.LMqTVw/fsid.22719.tmp
[2017-06-06 23:22:02,495][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.LMqTVw/magic.22719.tmp
[2017-06-06 23:22:02,496][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.LMqTVw
[2017-06-06 23:22:02,499][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.LMqTVw
[2017-06-06 23:22:02,499][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.LMqTVw
[2017-06-06 23:22:02,531][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:22:02,531][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-06-06 23:22:03,548][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-06 23:22:03,548][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-06 23:22:03,548][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:22:03,763][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:22:03,877][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:22:03,941][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-06-06 23:22:03,944][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-06 23:22:09,116][gdb1][INFO  ] checking OSD status...
[2017-06-06 23:22:09,116][gdb1][DEBUG ] find the location of an executable
[2017-06-06 23:22:09,119][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-06 23:22:09,234][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-06 23:22:09,400][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:22:09,400][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-06-06 23:22:09,400][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:22:09,401][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:22:09,401][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:22:09,401][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-06 23:22:09,401][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:22:09,401][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc7317bd560>
[2017-06-06 23:22:09,401][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:22:09,401][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-06-06 23:22:09,401][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fc7320d4938>
[2017-06-06 23:22:09,401][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:22:09,401][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:22:09,401][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-06 23:22:09,642][gdb0][DEBUG ] connection detected need for sudo
[2017-06-06 23:22:09,874][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-06 23:22:09,874][gdb0][DEBUG ] detect platform information from remote host
[2017-06-06 23:22:09,891][gdb0][DEBUG ] detect machine type
[2017-06-06 23:22:09,895][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:22:09,897][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-06 23:22:10,126][gdb1][DEBUG ] connection detected need for sudo
[2017-06-06 23:22:10,317][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-06 23:22:10,318][gdb1][DEBUG ] detect platform information from remote host
[2017-06-06 23:22:10,334][gdb1][DEBUG ] detect machine type
[2017-06-06 23:22:10,338][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:22:10,341][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-06 23:22:10,356][gdb3][DEBUG ] connection detected need for sudo
[2017-06-06 23:22:10,370][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-06 23:22:10,371][gdb3][DEBUG ] detect platform information from remote host
[2017-06-06 23:22:10,387][gdb3][DEBUG ] detect machine type
[2017-06-06 23:22:10,390][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:23:23,762][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:23:23,762][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-06-06 23:23:23,762][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:23:23,762][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:23:23,762][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-06 23:23:23,762][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-06-06 23:23:23,762][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-06 23:23:23,762][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:23:23,762][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3596321950>
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3596577aa0>
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:23:23,763][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-06-06 23:23:23,764][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-06-06 23:23:24,002][gdb0][DEBUG ] connection detected need for sudo
[2017-06-06 23:23:24,229][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-06 23:23:24,230][gdb0][DEBUG ] detect platform information from remote host
[2017-06-06 23:23:24,246][gdb0][DEBUG ] detect machine type
[2017-06-06 23:23:24,249][gdb0][DEBUG ] find the location of an executable
[2017-06-06 23:23:24,250][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-06 23:23:24,250][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-06 23:23:24,251][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:23:24,253][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-06-06 23:23:24,253][gdb0][DEBUG ] create a keyring file
[2017-06-06 23:23:24,255][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-06-06 23:23:24,255][gdb0][DEBUG ] find the location of an executable
[2017-06-06 23:23:24,257][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-06 23:23:24,378][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-06 23:23:24,381][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:23:24,397][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:23:24,413][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:23:24,420][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:24,421][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-06-06 23:23:24,421][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-06 23:23:24,436][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:24,436][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:24,436][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:24,437][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-06 23:23:24,437][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-06-06 23:23:24,437][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-06-06 23:23:24,444][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-06-06 23:23:24,452][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-06-06 23:23:24,459][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-06-06 23:23:24,475][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:24,475][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-06-06 23:23:24,475][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:24,475][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-06-06 23:23:24,483][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-06-06 23:23:24,483][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-06-06 23:23:24,647][gdb0][WARNING] 10+0 records in
[2017-06-06 23:23:24,648][gdb0][WARNING] 10+0 records out
[2017-06-06 23:23:24,648][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.137301 s, 76.4 MB/s
[2017-06-06 23:23:24,648][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-06-06 23:23:24,648][gdb0][DEBUG ] /dev/xvdb2: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-06-06 23:23:24,648][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-06-06 23:23:24,648][gdb0][WARNING] 10+0 records in
[2017-06-06 23:23:24,648][gdb0][WARNING] 10+0 records out
[2017-06-06 23:23:24,648][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00481285 s, 2.2 GB/s
[2017-06-06 23:23:24,648][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-06-06 23:23:24,648][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-06-06 23:23:24,648][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-06-06 23:23:24,648][gdb0][WARNING] backup header from main header.
[2017-06-06 23:23:24,649][gdb0][WARNING] 
[2017-06-06 23:23:24,649][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-06-06 23:23:24,649][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-06-06 23:23:24,649][gdb0][WARNING] 
[2017-06-06 23:23:24,649][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-06-06 23:23:24,649][gdb0][WARNING] 
[2017-06-06 23:23:25,716][gdb0][DEBUG ] ****************************************************************************
[2017-06-06 23:23:25,716][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-06-06 23:23:25,717][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-06-06 23:23:25,717][gdb0][DEBUG ] ****************************************************************************
[2017-06-06 23:23:25,717][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-06-06 23:23:25,717][gdb0][DEBUG ] other utilities.
[2017-06-06 23:23:25,717][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-06-06 23:23:26,734][gdb0][DEBUG ] Creating new GPT entries.
[2017-06-06 23:23:26,734][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-06 23:23:26,734][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-06-06 23:23:26,734][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:23:26,734][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:23:26,766][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:23:26,768][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:26,768][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:26,768][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-06-06 23:23:26,768][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:26,768][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-06-06 23:23:26,768][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:5afb804e-9b26-4aa0-8983-f400b57a790f --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-06-06 23:23:27,835][gdb0][DEBUG ] Setting name!
[2017-06-06 23:23:27,835][gdb0][DEBUG ] partNum is 1
[2017-06-06 23:23:27,836][gdb0][DEBUG ] REALLY setting name!
[2017-06-06 23:23:27,836][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-06 23:23:27,836][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-06 23:23:27,836][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:23:28,000][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:23:28,164][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:23:28,379][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:28,379][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:28,379][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-06-06 23:23:28,379][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/5afb804e-9b26-4aa0-8983-f400b57a790f
[2017-06-06 23:23:28,379][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-06-06 23:23:29,397][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-06 23:23:29,397][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-06 23:23:29,397][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:23:29,561][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:23:29,726][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:23:29,940][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/5afb804e-9b26-4aa0-8983-f400b57a790f
[2017-06-06 23:23:29,940][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:29,940][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-06-06 23:23:29,941][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:29,941][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-06-06 23:23:29,941][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:29,941][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-06-06 23:23:29,941][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:aef4f57e-292a-43ef-af55-e3fe44654eb1 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-06-06 23:23:30,958][gdb0][DEBUG ] Setting name!
[2017-06-06 23:23:30,958][gdb0][DEBUG ] partNum is 0
[2017-06-06 23:23:30,958][gdb0][DEBUG ] REALLY setting name!
[2017-06-06 23:23:30,958][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-06 23:23:30,958][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-06 23:23:30,958][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:23:31,123][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:23:31,337][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:23:31,552][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:31,552][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:31,552][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-06 23:23:31,552][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-06-06 23:23:31,552][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-06-06 23:23:32,268][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-06-06 23:23:32,269][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-06-06 23:23:32,269][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-06-06 23:23:32,269][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-06-06 23:23:32,269][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-06-06 23:23:32,269][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-06-06 23:23:32,269][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-06-06 23:23:32,269][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-06-06 23:23:32,269][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-06-06 23:23:32,269][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.hRv7aG with options noatime,inode64
[2017-06-06 23:23:32,269][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.hRv7aG
[2017-06-06 23:23:32,269][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.hRv7aG
[2017-06-06 23:23:32,270][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.hRv7aG/ceph_fsid.8848.tmp
[2017-06-06 23:23:32,270][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.hRv7aG/fsid.8848.tmp
[2017-06-06 23:23:32,277][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.hRv7aG/magic.8848.tmp
[2017-06-06 23:23:32,277][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.hRv7aG/journal_uuid.8848.tmp
[2017-06-06 23:23:32,278][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.hRv7aG/journal -> /dev/disk/by-partuuid/5afb804e-9b26-4aa0-8983-f400b57a790f
[2017-06-06 23:23:32,279][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.hRv7aG
[2017-06-06 23:23:32,282][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.hRv7aG
[2017-06-06 23:23:32,282][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.hRv7aG
[2017-06-06 23:23:32,314][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:23:32,314][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-06-06 23:23:33,331][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-06-06 23:23:33,331][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-06-06 23:23:33,331][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-06-06 23:23:33,331][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-06 23:23:33,331][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-06 23:23:33,332][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:23:33,332][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:23:33,546][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:23:33,578][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-06-06 23:23:33,613][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-06 23:23:38,786][gdb0][INFO  ] checking OSD status...
[2017-06-06 23:23:38,786][gdb0][DEBUG ] find the location of an executable
[2017-06-06 23:23:38,789][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-06 23:23:38,954][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-06 23:24:49,997][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-06 23:24:49,998][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9407ba1950>
[2017-06-06 23:24:49,999][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-06 23:24:49,999][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-06 23:24:49,999][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9407df7aa0>
[2017-06-06 23:24:49,999][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-06 23:24:49,999][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-06 23:24:49,999][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-06-06 23:24:49,999][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-06-06 23:24:50,238][gdb1][DEBUG ] connection detected need for sudo
[2017-06-06 23:24:50,434][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-06 23:24:50,434][gdb1][DEBUG ] detect platform information from remote host
[2017-06-06 23:24:50,451][gdb1][DEBUG ] detect machine type
[2017-06-06 23:24:50,455][gdb1][DEBUG ] find the location of an executable
[2017-06-06 23:24:50,456][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-06 23:24:50,456][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-06 23:24:50,456][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-06 23:24:50,458][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-06 23:24:50,458][gdb1][DEBUG ] create a keyring file
[2017-06-06 23:24:50,460][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-06-06 23:24:50,460][gdb1][DEBUG ] find the location of an executable
[2017-06-06 23:24:50,462][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-06 23:24:50,633][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-06 23:24:50,633][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:24:50,633][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:24:50,633][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-06 23:24:50,633][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-06 23:24:50,649][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:24:50,649][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:24:50,649][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:24:50,649][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-06 23:24:50,649][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-06-06 23:24:50,657][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-06-06 23:24:50,664][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-06-06 23:24:50,680][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-06-06 23:24:50,683][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:24:50,683][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-06-06 23:24:50,683][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:24:50,683][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-06-06 23:24:50,715][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-06-06 23:24:50,715][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-06-06 23:24:50,715][gdb1][WARNING] 10+0 records in
[2017-06-06 23:24:50,715][gdb1][WARNING] 10+0 records out
[2017-06-06 23:24:50,715][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00705506 s, 1.5 GB/s
[2017-06-06 23:24:50,716][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-06-06 23:24:50,716][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-06-06 23:24:50,717][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-06-06 23:24:50,717][gdb1][WARNING] backup header from main header.
[2017-06-06 23:24:50,717][gdb1][WARNING] 
[2017-06-06 23:24:50,717][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-06-06 23:24:50,717][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-06-06 23:24:50,717][gdb1][WARNING] 
[2017-06-06 23:24:50,718][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-06-06 23:24:50,718][gdb1][WARNING] 
[2017-06-06 23:24:51,835][gdb1][DEBUG ] ****************************************************************************
[2017-06-06 23:24:51,835][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-06-06 23:24:51,835][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-06-06 23:24:51,835][gdb1][DEBUG ] ****************************************************************************
[2017-06-06 23:24:51,835][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-06-06 23:24:51,835][gdb1][DEBUG ] other utilities.
[2017-06-06 23:24:51,835][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-06-06 23:24:52,853][gdb1][DEBUG ] Creating new GPT entries.
[2017-06-06 23:24:52,853][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-06 23:24:52,853][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-06-06 23:24:52,853][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:24:52,869][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:24:52,900][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:24:52,901][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:24:52,901][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-06-06 23:24:52,901][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:24:52,901][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-06-06 23:24:52,901][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:24:52,901][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-06-06 23:24:52,901][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:7d793d80-9904-438c-8dbc-bf44e75c1339 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-06-06 23:24:53,968][gdb1][DEBUG ] Setting name!
[2017-06-06 23:24:53,969][gdb1][DEBUG ] partNum is 0
[2017-06-06 23:24:53,969][gdb1][DEBUG ] REALLY setting name!
[2017-06-06 23:24:53,969][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-06 23:24:53,969][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-06 23:24:53,969][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:24:54,033][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:24:54,198][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:24:54,230][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:24:54,230][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:24:54,230][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-06 23:24:54,230][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-06-06 23:24:54,230][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-06-06 23:24:54,946][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8388543 blks
[2017-06-06 23:24:54,946][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-06-06 23:24:54,947][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-06-06 23:24:54,947][gdb1][DEBUG ] data     =                       bsize=4096   blocks=33554171, imaxpct=25
[2017-06-06 23:24:54,947][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-06-06 23:24:54,947][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-06-06 23:24:54,947][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=16383, version=2
[2017-06-06 23:24:54,947][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-06-06 23:24:54,947][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-06-06 23:24:54,947][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.SXXnXX with options noatime,inode64
[2017-06-06 23:24:54,947][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.SXXnXX
[2017-06-06 23:24:54,979][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.SXXnXX
[2017-06-06 23:24:54,979][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.SXXnXX/ceph_fsid.23600.tmp
[2017-06-06 23:24:54,979][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.SXXnXX/fsid.23600.tmp
[2017-06-06 23:24:54,979][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.SXXnXX/magic.23600.tmp
[2017-06-06 23:24:54,979][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.SXXnXX
[2017-06-06 23:24:54,981][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.SXXnXX
[2017-06-06 23:24:54,981][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.SXXnXX
[2017-06-06 23:24:55,013][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-06 23:24:55,013][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-06-06 23:24:56,029][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-06-06 23:24:56,030][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-06-06 23:24:56,030][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-06-06 23:24:56,030][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-06 23:24:56,030][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-06 23:24:56,030][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:24:56,030][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-06 23:24:56,062][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-06 23:24:56,078][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-06-06 23:24:56,090][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-06 23:25:01,212][gdb1][INFO  ] checking OSD status...
[2017-06-06 23:25:01,213][gdb1][DEBUG ] find the location of an executable
[2017-06-06 23:25:01,215][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-06 23:25:01,381][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-09 16:10:15,586][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4390f880e0>
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f43918951b8>
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:10:15,588][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:10:15,588][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-09 16:10:15,588][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-09 16:10:15,589][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-09 16:10:15,589][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-09 16:10:15,628][gdb3][DEBUG ] connection detected need for sudo
[2017-06-09 16:10:15,642][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-09 16:10:15,643][gdb3][DEBUG ] detect platform information from remote host
[2017-06-09 16:10:15,659][gdb3][DEBUG ] detect machine type
[2017-06-09 16:10:15,661][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-09 16:10:15,662][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-09 16:10:15,662][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-09 16:10:15,702][gdb3][DEBUG ] Reading package lists...
[2017-06-09 16:10:15,867][gdb3][DEBUG ] Building dependency tree...
[2017-06-09 16:10:15,867][gdb3][DEBUG ] Reading state information...
[2017-06-09 16:10:15,931][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-09 16:10:15,931][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-09 16:10:15,931][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-09 16:10:15,931][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-09 16:10:15,932][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-09 16:10:15,932][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-09 16:10:15,932][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
[2017-06-09 16:10:15,932][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws
[2017-06-09 16:10:15,932][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws ntp python-blinker python-cephfs
[2017-06-09 16:10:15,932][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-06-09 16:10:15,932][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-06-09 16:10:15,932][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-06-09 16:10:15,933][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-06-09 16:10:15,933][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-06-09 16:10:15,933][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-09 16:10:15,997][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-09 16:10:15,997][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-09 16:10:16,262][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 40 not upgraded.
[2017-06-09 16:10:16,262][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-09 16:10:16,626][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 129562 files and directories currently installed.)
[2017-06-09 16:10:16,630][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-09 16:10:16,849][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-09 16:10:16,913][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-09 16:10:16,977][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-09 16:10:17,192][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-09 16:10:17,306][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-09 16:10:17,474][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-09 16:10:17,506][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-09 16:10:17,570][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-09 16:10:17,735][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-09 16:10:17,801][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-09 16:10:17,809][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-09 16:10:17,976][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-09 16:10:17,992][gdb3][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-06-09 16:10:18,024][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-09 16:10:18,138][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-09 16:10:18,253][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-09 16:10:18,253][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-09 16:10:18,417][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-06-09 16:10:19,284][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-09 16:10:19,447][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f57d28b3758>
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f57d31c0230>
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:10:19,448][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:10:19,449][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-09 16:10:19,474][gdb3][DEBUG ] connection detected need for sudo
[2017-06-09 16:10:19,488][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-09 16:10:19,488][gdb3][DEBUG ] detect platform information from remote host
[2017-06-09 16:10:19,505][gdb3][DEBUG ] detect machine type
[2017-06-09 16:10:19,507][gdb3][DEBUG ] find the location of an executable
[2017-06-09 16:10:19,523][gdb3][DEBUG ] connection detected need for sudo
[2017-06-09 16:10:19,536][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-09 16:10:19,537][gdb3][DEBUG ] detect platform information from remote host
[2017-06-09 16:10:19,553][gdb3][DEBUG ] detect machine type
[2017-06-09 16:10:19,556][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-09 16:10:19,556][gdb3][INFO  ] purging data on gdb3
[2017-06-09 16:10:19,557][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-09 16:10:19,570][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-09 16:10:19,741][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:10:19,742][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-09 16:10:19,742][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:10:19,742][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:10:19,742][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:10:19,742][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-09 16:10:19,742][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:10:19,742][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f16b1857a28>
[2017-06-09 16:10:19,742][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:10:19,742][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f16b211a848>
[2017-06-09 16:10:19,742][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:10:19,742][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:12:15,160][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:12:15,160][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-09 16:12:15,160][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:12:15,160][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd4d34cc5a8>
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fd4d3b50758>
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:12:15,161][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-09 16:12:15,161][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-09 16:12:15,162][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-09 16:12:15,188][gdb3][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:15,202][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-09 16:12:15,202][gdb3][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:15,218][gdb3][DEBUG ] detect machine type
[2017-06-09 16:12:15,221][gdb3][DEBUG ] find the location of an executable
[2017-06-09 16:12:15,222][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-09 16:12:15,233][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-09 16:12:15,239][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-09 16:12:15,239][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-09 16:12:15,240][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-09 16:12:15,240][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-09 16:12:15,240][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-09 16:12:15,240][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-09 16:12:15,240][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-09 16:12:15,240][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-09 16:12:15,406][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:12:15,406][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-09 16:12:15,406][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:12:15,407][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:12:15,407][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:12:15,407][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-09 16:12:15,407][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-09 16:12:15,407][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:12:15,407][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f941c9c5ea8>
[2017-06-09 16:12:15,407][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:12:15,407][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f941c999b18>
[2017-06-09 16:12:15,407][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:12:15,407][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-09 16:12:15,407][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:12:15,408][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-09 16:12:15,408][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-09 16:12:15,434][gdb3][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:15,448][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-09 16:12:15,449][gdb3][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:15,465][gdb3][DEBUG ] detect machine type
[2017-06-09 16:12:15,468][gdb3][DEBUG ] find the location of an executable
[2017-06-09 16:12:15,468][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-09 16:12:15,468][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-09 16:12:15,468][gdb3][DEBUG ] get remote short hostname
[2017-06-09 16:12:15,468][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-09 16:12:15,469][gdb3][DEBUG ] get remote short hostname
[2017-06-09 16:12:15,469][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-09 16:12:15,469][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:12:15,471][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-09 16:12:15,471][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-09 16:12:15,471][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-09 16:12:15,472][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-09 16:12:15,472][gdb3][DEBUG ] create the monitor keyring file
[2017-06-09 16:12:15,473][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-09 16:12:15,511][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-09 16:12:15,511][gdb3][DEBUG ] ceph-mon: set fsid to 045a9557-cdbd-4e61-a08b-89339e9bba2a
[2017-06-09 16:12:15,515][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-09 16:12:15,516][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-09 16:12:15,516][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-09 16:12:15,517][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-09 16:12:15,518][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-09 16:12:15,585][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-09 16:12:15,655][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-09 16:12:17,725][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-09 16:12:17,790][gdb3][DEBUG ] ********************************************************************************
[2017-06-09 16:12:17,790][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-09 16:12:17,791][gdb3][DEBUG ] {
[2017-06-09 16:12:17,791][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-09 16:12:17,791][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-09 16:12:17,791][gdb3][DEBUG ]   "features": {
[2017-06-09 16:12:17,791][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-09 16:12:17,791][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-09 16:12:17,791][gdb3][DEBUG ]       "kraken", 
[2017-06-09 16:12:17,791][gdb3][DEBUG ]       "luminous"
[2017-06-09 16:12:17,791][gdb3][DEBUG ]     ], 
[2017-06-09 16:12:17,791][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-09 16:12:17,791][gdb3][DEBUG ]     "required_mon": [
[2017-06-09 16:12:17,792][gdb3][DEBUG ]       "kraken", 
[2017-06-09 16:12:17,792][gdb3][DEBUG ]       "luminous"
[2017-06-09 16:12:17,792][gdb3][DEBUG ]     ]
[2017-06-09 16:12:17,792][gdb3][DEBUG ]   }, 
[2017-06-09 16:12:17,792][gdb3][DEBUG ]   "monmap": {
[2017-06-09 16:12:17,792][gdb3][DEBUG ]     "created": "2017-06-09 16:12:15.496124", 
[2017-06-09 16:12:17,792][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-09 16:12:17,792][gdb3][DEBUG ]     "features": {
[2017-06-09 16:12:17,792][gdb3][DEBUG ]       "optional": [], 
[2017-06-09 16:12:17,792][gdb3][DEBUG ]       "persistent": [
[2017-06-09 16:12:17,792][gdb3][DEBUG ]         "kraken", 
[2017-06-09 16:12:17,792][gdb3][DEBUG ]         "luminous"
[2017-06-09 16:12:17,792][gdb3][DEBUG ]       ]
[2017-06-09 16:12:17,792][gdb3][DEBUG ]     }, 
[2017-06-09 16:12:17,792][gdb3][DEBUG ]     "fsid": "045a9557-cdbd-4e61-a08b-89339e9bba2a", 
[2017-06-09 16:12:17,792][gdb3][DEBUG ]     "modified": "2017-06-09 16:12:15.732934", 
[2017-06-09 16:12:17,792][gdb3][DEBUG ]     "mons": [
[2017-06-09 16:12:17,792][gdb3][DEBUG ]       {
[2017-06-09 16:12:17,793][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-09 16:12:17,793][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-09 16:12:17,793][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-09 16:12:17,793][gdb3][DEBUG ]         "rank": 0
[2017-06-09 16:12:17,793][gdb3][DEBUG ]       }
[2017-06-09 16:12:17,793][gdb3][DEBUG ]     ]
[2017-06-09 16:12:17,793][gdb3][DEBUG ]   }, 
[2017-06-09 16:12:17,793][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-09 16:12:17,793][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-09 16:12:17,793][gdb3][DEBUG ]   "quorum": [
[2017-06-09 16:12:17,793][gdb3][DEBUG ]     0
[2017-06-09 16:12:17,793][gdb3][DEBUG ]   ], 
[2017-06-09 16:12:17,793][gdb3][DEBUG ]   "rank": 0, 
[2017-06-09 16:12:17,793][gdb3][DEBUG ]   "state": "leader", 
[2017-06-09 16:12:17,793][gdb3][DEBUG ]   "sync_provider": []
[2017-06-09 16:12:17,793][gdb3][DEBUG ] }
[2017-06-09 16:12:17,793][gdb3][DEBUG ] ********************************************************************************
[2017-06-09 16:12:17,794][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-09 16:12:17,794][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-09 16:12:17,860][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-09 16:12:17,876][gdb3][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:17,890][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-09 16:12:17,890][gdb3][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:17,906][gdb3][DEBUG ] detect machine type
[2017-06-09 16:12:17,909][gdb3][DEBUG ] find the location of an executable
[2017-06-09 16:12:17,910][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-09 16:12:17,975][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-09 16:12:17,975][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-09 16:12:17,975][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-09 16:12:17,977][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmplmpVVq
[2017-06-09 16:12:17,992][gdb3][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:18,005][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-09 16:12:18,006][gdb3][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:18,022][gdb3][DEBUG ] detect machine type
[2017-06-09 16:12:18,025][gdb3][DEBUG ] get remote short hostname
[2017-06-09 16:12:18,025][gdb3][DEBUG ] fetch remote file
[2017-06-09 16:12:18,026][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-09 16:12:18,092][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-09 16:12:18,258][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-09 16:12:18,424][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-09 16:12:18,590][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-09 16:12:18,757][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-09 16:12:18,923][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-09 16:12:19,089][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-09 16:12:19,255][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-09 16:12:19,421][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-09 16:12:19,587][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-09 16:12:19,753][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-09 16:12:19,753][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-09 16:12:19,754][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170609161219'
[2017-06-09 16:12:19,754][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-09 16:12:19,754][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-09 16:12:19,754][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-09 16:12:19,754][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmplmpVVq
[2017-06-09 16:12:28,030][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:12:28,030][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-06-09 16:12:28,030][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:12:28,030][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:12:28,030][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-09 16:12:28,030][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-06-09 16:12:28,030][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1dde638950>
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1dde88eaa0>
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:12:28,031][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-06-09 16:12:28,032][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-06-09 16:12:28,280][gdb0][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:28,508][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-09 16:12:28,509][gdb0][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:28,526][gdb0][DEBUG ] detect machine type
[2017-06-09 16:12:28,530][gdb0][DEBUG ] find the location of an executable
[2017-06-09 16:12:28,531][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-09 16:12:28,531][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-09 16:12:28,531][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:12:28,534][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-06-09 16:12:28,534][gdb0][DEBUG ] find the location of an executable
[2017-06-09 16:12:28,536][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-09 16:12:28,656][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-09 16:12:28,664][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:12:28,680][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:12:28,695][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:12:28,703][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:12:28,703][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-06-09 16:12:28,703][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-09 16:12:28,719][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:12:28,719][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:12:28,719][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:12:28,720][gdb0][WARNING] Traceback (most recent call last):
[2017-06-09 16:12:28,720][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-09 16:12:28,720][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-09 16:12:28,720][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-09 16:12:28,720][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-09 16:12:28,721][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-09 16:12:28,721][gdb0][WARNING]     args.func(args)
[2017-06-09 16:12:28,721][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-06-09 16:12:28,721][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-06-09 16:12:28,721][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2018, in prepare
[2017-06-09 16:12:28,721][gdb0][WARNING]     self.prepare_locked()
[2017-06-09 16:12:28,722][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2049, in prepare_locked
[2017-06-09 16:12:28,722][gdb0][WARNING]     self.data.prepare(self.journal)
[2017-06-09 16:12:28,722][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2816, in prepare
[2017-06-09 16:12:28,722][gdb0][WARNING]     self.prepare_device(*to_prepare_list)
[2017-06-09 16:12:28,722][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2971, in prepare_device
[2017-06-09 16:12:28,722][gdb0][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2017-06-09 16:12:28,722][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2878, in prepare_device
[2017-06-09 16:12:28,722][gdb0][WARNING]     self.sanity_checks()
[2017-06-09 16:12:28,722][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2841, in sanity_checks
[2017-06-09 16:12:28,722][gdb0][WARNING]     check_partitions=not self.args.dmcrypt)
[2017-06-09 16:12:28,722][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-06-09 16:12:28,722][gdb0][WARNING]     raise Error('Device is mounted', partition)
[2017-06-09 16:12:28,722][gdb0][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-06-09 16:12:28,731][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-09 16:12:28,731][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-09 16:12:28,731][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-06-09 16:12:28,904][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:12:28,904][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-06-09 16:12:28,904][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:12:28,904][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:12:28,904][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:12:28,904][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-09 16:12:28,904][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:12:28,904][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f34ef6f9560>
[2017-06-09 16:12:28,904][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:12:28,905][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-06-09 16:12:28,905][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f34f0010938>
[2017-06-09 16:12:28,905][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:12:28,905][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:12:28,905][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-09 16:12:29,144][gdb0][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:29,372][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-09 16:12:29,373][gdb0][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:29,388][gdb0][DEBUG ] detect machine type
[2017-06-09 16:12:29,392][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:12:29,394][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-09 16:12:29,629][gdb1][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:29,870][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-09 16:12:29,871][gdb1][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:29,889][gdb1][DEBUG ] detect machine type
[2017-06-09 16:12:29,893][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:12:29,895][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-06-09 16:12:29,895][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-09 16:12:29,910][gdb3][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:29,925][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-09 16:12:29,925][gdb3][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:29,942][gdb3][DEBUG ] detect machine type
[2017-06-09 16:12:29,944][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:12:29,946][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-06-09 16:12:43,283][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-09 16:12:43,284][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:12:43,285][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4aae00e950>
[2017-06-09 16:12:43,285][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:12:43,285][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-09 16:12:43,285][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f4aae264aa0>
[2017-06-09 16:12:43,285][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:12:43,285][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:12:43,285][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-06-09 16:12:43,285][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-06-09 16:12:43,529][gdb0][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:43,761][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-09 16:12:43,761][gdb0][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:43,778][gdb0][DEBUG ] detect machine type
[2017-06-09 16:12:43,782][gdb0][DEBUG ] find the location of an executable
[2017-06-09 16:12:43,782][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-09 16:12:43,783][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-09 16:12:43,783][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:12:43,785][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-06-09 16:12:43,785][gdb0][DEBUG ] find the location of an executable
[2017-06-09 16:12:43,787][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-09 16:12:43,908][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-09 16:12:43,915][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:12:43,931][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:12:43,947][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:12:43,962][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:12:43,962][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-06-09 16:12:43,963][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-09 16:12:43,970][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:12:43,970][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:12:43,971][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:12:43,971][gdb0][WARNING] Traceback (most recent call last):
[2017-06-09 16:12:43,971][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-09 16:12:43,971][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-09 16:12:43,971][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-09 16:12:43,971][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-09 16:12:43,971][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-09 16:12:43,971][gdb0][WARNING]     args.func(args)
[2017-06-09 16:12:43,972][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-06-09 16:12:43,972][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-06-09 16:12:43,972][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2018, in prepare
[2017-06-09 16:12:43,972][gdb0][WARNING]     self.prepare_locked()
[2017-06-09 16:12:43,972][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2049, in prepare_locked
[2017-06-09 16:12:43,972][gdb0][WARNING]     self.data.prepare(self.journal)
[2017-06-09 16:12:43,972][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2816, in prepare
[2017-06-09 16:12:43,972][gdb0][WARNING]     self.prepare_device(*to_prepare_list)
[2017-06-09 16:12:43,972][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2971, in prepare_device
[2017-06-09 16:12:43,972][gdb0][WARNING]     super(PrepareFilestoreData, self).prepare_device(*to_prepare_list)
[2017-06-09 16:12:43,972][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2878, in prepare_device
[2017-06-09 16:12:43,972][gdb0][WARNING]     self.sanity_checks()
[2017-06-09 16:12:43,972][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2841, in sanity_checks
[2017-06-09 16:12:43,973][gdb0][WARNING]     check_partitions=not self.args.dmcrypt)
[2017-06-09 16:12:43,973][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-06-09 16:12:43,973][gdb0][WARNING]     raise Error('Device is mounted', partition)
[2017-06-09 16:12:43,973][gdb0][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-06-09 16:12:43,980][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-09 16:12:43,980][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-09 16:12:43,981][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-06-09 16:12:44,153][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:12:44,153][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-06-09 16:12:44,153][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:12:44,153][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:12:44,153][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:12:44,153][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-09 16:12:44,153][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:12:44,153][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7093084560>
[2017-06-09 16:12:44,153][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:12:44,153][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-06-09 16:12:44,153][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f709399b938>
[2017-06-09 16:12:44,154][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:12:44,154][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:12:44,154][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-09 16:12:44,393][gdb0][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:44,621][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-09 16:12:44,622][gdb0][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:44,637][gdb0][DEBUG ] detect machine type
[2017-06-09 16:12:44,641][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:12:44,644][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-09 16:12:44,883][gdb1][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:45,118][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-09 16:12:45,119][gdb1][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:45,136][gdb1][DEBUG ] detect machine type
[2017-06-09 16:12:45,140][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:12:45,142][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-06-09 16:12:45,142][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-09 16:12:45,157][gdb3][DEBUG ] connection detected need for sudo
[2017-06-09 16:12:45,171][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-09 16:12:45,172][gdb3][DEBUG ] detect platform information from remote host
[2017-06-09 16:12:45,188][gdb3][DEBUG ] detect machine type
[2017-06-09 16:12:45,191][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:12:45,193][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-06-09 16:14:15,736][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:14:15,736][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-06-09 16:14:15,736][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:14:15,736][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:14:15,736][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-09 16:14:15,736][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-06-09 16:14:15,736][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-09 16:14:15,736][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:14:15,736][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffb12de6950>
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ffb1303caa0>
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:14:15,737][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-06-09 16:14:15,738][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-06-09 16:14:15,974][gdb0][DEBUG ] connection detected need for sudo
[2017-06-09 16:14:16,205][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-09 16:14:16,206][gdb0][DEBUG ] detect platform information from remote host
[2017-06-09 16:14:16,222][gdb0][DEBUG ] detect machine type
[2017-06-09 16:14:16,225][gdb0][DEBUG ] find the location of an executable
[2017-06-09 16:14:16,226][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-09 16:14:16,226][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-09 16:14:16,227][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:14:16,229][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-06-09 16:14:16,229][gdb0][DEBUG ] create a keyring file
[2017-06-09 16:14:16,231][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-06-09 16:14:16,231][gdb0][DEBUG ] find the location of an executable
[2017-06-09 16:14:16,233][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-09 16:14:16,353][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-09 16:14:16,361][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:14:16,377][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:14:16,392][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:14:16,400][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:16,400][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-06-09 16:14:16,400][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-09 16:14:16,416][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:16,416][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:16,416][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:16,416][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-09 16:14:16,416][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-06-09 16:14:16,416][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-06-09 16:14:16,424][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-06-09 16:14:16,431][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-06-09 16:14:16,447][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-06-09 16:14:16,448][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:16,448][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-06-09 16:14:16,448][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:16,449][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-06-09 16:14:16,480][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-06-09 16:14:16,480][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-06-09 16:14:16,480][gdb0][WARNING] 10+0 records in
[2017-06-09 16:14:16,481][gdb0][WARNING] 10+0 records out
[2017-06-09 16:14:16,481][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00667955 s, 1.6 GB/s
[2017-06-09 16:14:16,481][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-06-09 16:14:16,595][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-06-09 16:14:16,595][gdb0][WARNING] 10+0 records in
[2017-06-09 16:14:16,595][gdb0][WARNING] 10+0 records out
[2017-06-09 16:14:16,595][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00476738 s, 2.2 GB/s
[2017-06-09 16:14:16,595][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-06-09 16:14:16,595][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-06-09 16:14:16,595][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-06-09 16:14:16,595][gdb0][WARNING] backup header from main header.
[2017-06-09 16:14:16,595][gdb0][WARNING] 
[2017-06-09 16:14:16,596][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-06-09 16:14:16,596][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-06-09 16:14:16,596][gdb0][WARNING] 
[2017-06-09 16:14:16,596][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-06-09 16:14:16,596][gdb0][WARNING] 
[2017-06-09 16:14:17,663][gdb0][DEBUG ] ****************************************************************************
[2017-06-09 16:14:17,663][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-06-09 16:14:17,663][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-06-09 16:14:17,663][gdb0][DEBUG ] ****************************************************************************
[2017-06-09 16:14:17,663][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-06-09 16:14:17,663][gdb0][DEBUG ] other utilities.
[2017-06-09 16:14:17,664][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-06-09 16:14:18,680][gdb0][DEBUG ] Creating new GPT entries.
[2017-06-09 16:14:18,680][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-09 16:14:18,681][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-06-09 16:14:18,681][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:14:18,681][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-09 16:14:18,696][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:14:18,712][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:18,712][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:18,712][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-06-09 16:14:18,712][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:18,712][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-06-09 16:14:18,713][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:85a65f59-f2f2-4089-a8ef-cb848214b128 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-06-09 16:14:19,729][gdb0][DEBUG ] Setting name!
[2017-06-09 16:14:19,730][gdb0][DEBUG ] partNum is 1
[2017-06-09 16:14:19,730][gdb0][DEBUG ] REALLY setting name!
[2017-06-09 16:14:19,730][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-09 16:14:19,730][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-09 16:14:19,730][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:14:19,944][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-09 16:14:20,109][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:14:20,112][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:20,112][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:20,113][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-06-09 16:14:20,113][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/85a65f59-f2f2-4089-a8ef-cb848214b128
[2017-06-09 16:14:20,113][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-06-09 16:14:21,130][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-09 16:14:21,130][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-09 16:14:21,130][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:14:21,344][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-09 16:14:21,509][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:14:21,541][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/85a65f59-f2f2-4089-a8ef-cb848214b128
[2017-06-09 16:14:21,541][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:21,541][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-06-09 16:14:21,541][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:21,541][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-06-09 16:14:21,541][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:21,541][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-06-09 16:14:21,541][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:758c3b67-2ac8-421a-a7e8-d3fa1ebc8f00 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-06-09 16:14:22,558][gdb0][DEBUG ] Setting name!
[2017-06-09 16:14:22,558][gdb0][DEBUG ] partNum is 0
[2017-06-09 16:14:22,558][gdb0][DEBUG ] REALLY setting name!
[2017-06-09 16:14:22,558][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-09 16:14:22,558][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-09 16:14:22,559][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:14:22,773][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-09 16:14:23,038][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:14:23,252][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:23,253][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:23,253][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-09 16:14:23,253][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-06-09 16:14:23,253][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-06-09 16:14:23,969][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-06-09 16:14:23,969][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-06-09 16:14:23,969][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-06-09 16:14:23,969][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-06-09 16:14:23,969][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-06-09 16:14:23,969][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-06-09 16:14:23,969][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-06-09 16:14:23,970][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-06-09 16:14:23,970][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-06-09 16:14:23,970][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.GAbGx0 with options noatime,inode64
[2017-06-09 16:14:23,970][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.GAbGx0
[2017-06-09 16:14:23,977][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.GAbGx0
[2017-06-09 16:14:23,977][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.GAbGx0/ceph_fsid.10578.tmp
[2017-06-09 16:14:23,981][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.GAbGx0/fsid.10578.tmp
[2017-06-09 16:14:23,982][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.GAbGx0/magic.10578.tmp
[2017-06-09 16:14:23,985][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.GAbGx0/journal_uuid.10578.tmp
[2017-06-09 16:14:23,987][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.GAbGx0/journal -> /dev/disk/by-partuuid/85a65f59-f2f2-4089-a8ef-cb848214b128
[2017-06-09 16:14:23,987][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.GAbGx0
[2017-06-09 16:14:23,988][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.GAbGx0
[2017-06-09 16:14:23,988][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.GAbGx0
[2017-06-09 16:14:24,020][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:14:24,020][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-06-09 16:14:25,037][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-06-09 16:14:25,037][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-06-09 16:14:25,037][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-06-09 16:14:25,038][gdb0][DEBUG ] The operation has completed successfully.
[2017-06-09 16:14:25,038][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-09 16:14:25,038][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:14:25,038][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-09 16:14:25,302][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:14:25,318][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-06-09 16:14:25,336][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-09 16:14:30,458][gdb0][INFO  ] checking OSD status...
[2017-06-09 16:14:30,458][gdb0][DEBUG ] find the location of an executable
[2017-06-09 16:14:30,461][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-09 16:14:30,626][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-09 16:14:30,790][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:14:30,790][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-06-09 16:14:30,790][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:14:30,790][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:14:30,790][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:14:30,790][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-09 16:14:30,790][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:14:30,791][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdce295e560>
[2017-06-09 16:14:30,791][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:14:30,791][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-06-09 16:14:30,791][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fdce3275938>
[2017-06-09 16:14:30,791][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:14:30,791][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:14:30,791][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-09 16:14:31,029][gdb0][DEBUG ] connection detected need for sudo
[2017-06-09 16:14:31,261][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-09 16:14:31,261][gdb0][DEBUG ] detect platform information from remote host
[2017-06-09 16:14:31,277][gdb0][DEBUG ] detect machine type
[2017-06-09 16:14:31,281][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:14:31,283][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-09 16:14:31,522][gdb1][DEBUG ] connection detected need for sudo
[2017-06-09 16:14:31,753][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-09 16:14:31,754][gdb1][DEBUG ] detect platform information from remote host
[2017-06-09 16:14:31,770][gdb1][DEBUG ] detect machine type
[2017-06-09 16:14:31,774][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:14:31,776][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-06-09 16:14:31,776][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-09 16:14:31,791][gdb3][DEBUG ] connection detected need for sudo
[2017-06-09 16:14:31,805][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-09 16:14:31,806][gdb3][DEBUG ] detect platform information from remote host
[2017-06-09 16:14:31,823][gdb3][DEBUG ] detect machine type
[2017-06-09 16:14:31,825][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:14:31,827][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-06-09 16:15:02,310][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:15:02,311][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6db6c95950>
[2017-06-09 16:15:02,312][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:15:02,312][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-09 16:15:02,312][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6db6eebaa0>
[2017-06-09 16:15:02,312][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:15:02,312][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:15:02,312][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-06-09 16:15:02,312][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-06-09 16:15:02,559][gdb1][DEBUG ] connection detected need for sudo
[2017-06-09 16:15:02,786][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-09 16:15:02,786][gdb1][DEBUG ] detect platform information from remote host
[2017-06-09 16:15:02,803][gdb1][DEBUG ] detect machine type
[2017-06-09 16:15:02,807][gdb1][DEBUG ] find the location of an executable
[2017-06-09 16:15:02,808][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-09 16:15:02,808][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-09 16:15:02,808][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:15:02,811][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-09 16:15:02,811][gdb1][DEBUG ] create a keyring file
[2017-06-09 16:15:02,813][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-06-09 16:15:02,813][gdb1][DEBUG ] find the location of an executable
[2017-06-09 16:15:02,815][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-09 16:15:02,985][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-09 16:15:02,985][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:15:02,985][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:15:02,985][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-09 16:15:03,001][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-09 16:15:03,009][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:15:03,009][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:15:03,009][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:15:03,009][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-09 16:15:03,009][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-06-09 16:15:03,017][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-06-09 16:15:03,032][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-06-09 16:15:03,040][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-06-09 16:15:03,055][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:15:03,055][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-06-09 16:15:03,056][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:15:03,056][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-06-09 16:15:03,071][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-06-09 16:15:03,071][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-06-09 16:15:03,186][gdb1][WARNING] 10+0 records in
[2017-06-09 16:15:03,186][gdb1][WARNING] 10+0 records out
[2017-06-09 16:15:03,186][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.103705 s, 101 MB/s
[2017-06-09 16:15:03,186][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-06-09 16:15:03,186][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-06-09 16:15:03,186][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-06-09 16:15:03,186][gdb1][WARNING] backup header from main header.
[2017-06-09 16:15:03,186][gdb1][WARNING] 
[2017-06-09 16:15:03,186][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-06-09 16:15:03,186][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-06-09 16:15:03,186][gdb1][WARNING] 
[2017-06-09 16:15:03,187][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-06-09 16:15:03,187][gdb1][WARNING] 
[2017-06-09 16:15:04,204][gdb1][DEBUG ] ****************************************************************************
[2017-06-09 16:15:04,204][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-06-09 16:15:04,204][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-06-09 16:15:04,204][gdb1][DEBUG ] ****************************************************************************
[2017-06-09 16:15:04,204][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-06-09 16:15:04,204][gdb1][DEBUG ] other utilities.
[2017-06-09 16:15:04,204][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-06-09 16:15:05,221][gdb1][DEBUG ] Creating new GPT entries.
[2017-06-09 16:15:05,221][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-09 16:15:05,222][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-06-09 16:15:05,222][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:15:05,229][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-09 16:15:05,261][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:15:05,277][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:15:05,277][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-06-09 16:15:05,277][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:15:05,277][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-06-09 16:15:05,277][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:15:05,277][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-06-09 16:15:05,277][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:73b7e788-0e6b-4f48-bca6-12bfc787e008 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-06-09 16:15:06,294][gdb1][DEBUG ] Setting name!
[2017-06-09 16:15:06,294][gdb1][DEBUG ] partNum is 0
[2017-06-09 16:15:06,294][gdb1][DEBUG ] REALLY setting name!
[2017-06-09 16:15:06,295][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-09 16:15:06,296][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-09 16:15:06,296][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:15:06,410][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-09 16:15:06,574][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:15:06,578][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:15:06,578][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:15:06,578][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-09 16:15:06,578][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-06-09 16:15:06,578][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-06-09 16:15:07,295][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8388543 blks
[2017-06-09 16:15:07,295][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-06-09 16:15:07,295][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-06-09 16:15:07,295][gdb1][DEBUG ] data     =                       bsize=4096   blocks=33554171, imaxpct=25
[2017-06-09 16:15:07,295][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-06-09 16:15:07,295][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-06-09 16:15:07,295][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=16383, version=2
[2017-06-09 16:15:07,295][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-06-09 16:15:07,295][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-06-09 16:15:07,295][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.RCPbou with options noatime,inode64
[2017-06-09 16:15:07,296][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.RCPbou
[2017-06-09 16:15:07,296][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.RCPbou
[2017-06-09 16:15:07,311][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.RCPbou/ceph_fsid.8793.tmp
[2017-06-09 16:15:07,311][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.RCPbou/fsid.8793.tmp
[2017-06-09 16:15:07,312][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.RCPbou/magic.8793.tmp
[2017-06-09 16:15:07,315][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.RCPbou
[2017-06-09 16:15:07,316][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.RCPbou
[2017-06-09 16:15:07,316][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.RCPbou
[2017-06-09 16:15:07,348][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-09 16:15:07,348][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-06-09 16:15:08,415][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-06-09 16:15:08,415][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-06-09 16:15:08,416][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-06-09 16:15:08,416][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-09 16:15:08,416][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-09 16:15:08,416][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:15:08,416][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-09 16:15:08,416][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-09 16:15:08,419][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-06-09 16:15:08,453][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-09 16:15:13,575][gdb1][INFO  ] checking OSD status...
[2017-06-09 16:15:13,576][gdb1][DEBUG ] find the location of an executable
[2017-06-09 16:15:13,578][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-09 16:15:13,693][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-09 16:15:13,857][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-09 16:15:13,857][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-06-09 16:15:13,857][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-09 16:15:13,858][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-09 16:15:13,858][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-09 16:15:13,858][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-09 16:15:13,858][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-09 16:15:13,858][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f37ef970560>
[2017-06-09 16:15:13,858][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-09 16:15:13,858][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-06-09 16:15:13,858][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f37f0287938>
[2017-06-09 16:15:13,858][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-09 16:15:13,858][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-09 16:15:13,858][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-09 16:15:14,101][gdb0][DEBUG ] connection detected need for sudo
[2017-06-09 16:15:14,329][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-09 16:15:14,330][gdb0][DEBUG ] detect platform information from remote host
[2017-06-09 16:15:14,346][gdb0][DEBUG ] detect machine type
[2017-06-09 16:15:14,350][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:15:14,352][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-09 16:15:14,591][gdb1][DEBUG ] connection detected need for sudo
[2017-06-09 16:15:14,827][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-09 16:15:14,827][gdb1][DEBUG ] detect platform information from remote host
[2017-06-09 16:15:14,844][gdb1][DEBUG ] detect machine type
[2017-06-09 16:15:14,848][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-09 16:15:14,851][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-09 16:15:14,866][gdb3][DEBUG ] connection detected need for sudo
[2017-06-09 16:15:14,881][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-09 16:15:14,881][gdb3][DEBUG ] detect platform information from remote host
[2017-06-09 16:15:14,897][gdb3][DEBUG ] detect machine type
[2017-06-09 16:15:14,900][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-13 17:36:11,408][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-13 17:36:11,409][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-13 17:36:11,409][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-13 17:36:11,409][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-13 17:36:11,410][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-13 17:36:11,410][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-13 17:36:11,410][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-13 17:36:11,410][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f99efd100e0>
[2017-06-13 17:36:11,410][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-13 17:36:11,410][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-13 17:36:11,410][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f99f061d1b8>
[2017-06-13 17:36:11,410][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-13 17:36:11,410][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-13 17:36:11,410][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-13 17:36:11,410][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-13 17:36:11,410][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-13 17:36:11,410][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-13 17:36:11,447][gdb3][DEBUG ] connection detected need for sudo
[2017-06-13 17:36:11,461][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-13 17:36:11,462][gdb3][DEBUG ] detect platform information from remote host
[2017-06-13 17:36:11,478][gdb3][DEBUG ] detect machine type
[2017-06-13 17:36:11,481][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-13 17:36:11,481][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-13 17:36:11,482][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-13 17:36:11,521][gdb3][DEBUG ] Reading package lists...
[2017-06-13 17:36:11,685][gdb3][DEBUG ] Building dependency tree...
[2017-06-13 17:36:11,686][gdb3][DEBUG ] Reading state information...
[2017-06-13 17:36:11,750][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-13 17:36:11,750][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-13 17:36:11,750][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-13 17:36:11,750][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-13 17:36:11,751][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-13 17:36:11,751][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-13 17:36:11,751][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
[2017-06-13 17:36:11,751][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws
[2017-06-13 17:36:11,751][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws ntp python-blinker python-cephfs
[2017-06-13 17:36:11,751][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-06-13 17:36:11,751][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-06-13 17:36:11,752][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-06-13 17:36:11,752][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-06-13 17:36:11,752][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-06-13 17:36:11,752][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-13 17:36:11,816][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-13 17:36:11,816][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-13 17:36:11,930][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 41 not upgraded.
[2017-06-13 17:36:11,931][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-13 17:36:11,962][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 129562 files and directories currently installed.)
[2017-06-13 17:36:11,970][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-13 17:36:12,185][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-13 17:36:12,249][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-13 17:36:12,281][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-13 17:36:12,545][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-13 17:36:12,609][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-13 17:36:12,774][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-13 17:36:12,837][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-13 17:36:12,869][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-13 17:36:13,036][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-13 17:36:13,150][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-13 17:36:13,152][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-13 17:36:13,316][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-13 17:36:13,316][gdb3][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-06-13 17:36:13,332][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-13 17:36:13,496][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-13 17:36:13,560][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-13 17:36:13,560][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-13 17:36:13,725][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-06-13 17:36:14,541][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-13 17:36:14,706][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-13 17:36:14,706][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-13 17:36:14,706][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-13 17:36:14,706][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-13 17:36:14,706][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-13 17:36:14,706][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-13 17:36:14,706][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-13 17:36:14,707][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f774b3e2758>
[2017-06-13 17:36:14,707][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-13 17:36:14,707][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-13 17:36:14,707][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f774bcef230>
[2017-06-13 17:36:14,707][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-13 17:36:14,707][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-13 17:36:14,707][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-13 17:36:14,733][gdb3][DEBUG ] connection detected need for sudo
[2017-06-13 17:36:14,747][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-13 17:36:14,747][gdb3][DEBUG ] detect platform information from remote host
[2017-06-13 17:36:14,763][gdb3][DEBUG ] detect machine type
[2017-06-13 17:36:14,766][gdb3][DEBUG ] find the location of an executable
[2017-06-13 17:36:14,782][gdb3][DEBUG ] connection detected need for sudo
[2017-06-13 17:36:14,796][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-13 17:36:14,796][gdb3][DEBUG ] detect platform information from remote host
[2017-06-13 17:36:14,813][gdb3][DEBUG ] detect machine type
[2017-06-13 17:36:14,815][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-13 17:36:14,815][gdb3][INFO  ] purging data on gdb3
[2017-06-13 17:36:14,816][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-13 17:36:14,829][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-13 17:36:15,001][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-13 17:36:15,002][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-13 17:36:15,002][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-13 17:36:15,002][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-13 17:36:15,002][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-13 17:36:15,002][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-13 17:36:15,002][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-13 17:36:15,002][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f43a99dfa28>
[2017-06-13 17:36:15,002][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-13 17:36:15,002][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f43aa2a2848>
[2017-06-13 17:36:15,002][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-13 17:36:15,002][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-13 17:36:36,709][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-13 17:36:36,710][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-13 17:36:36,710][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-13 17:36:36,710][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-13 17:36:36,710][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-13 17:36:36,710][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-13 17:36:36,710][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-13 17:36:36,710][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f01716125a8>
[2017-06-13 17:36:36,711][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-13 17:36:36,711][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-13 17:36:36,711][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-13 17:36:36,711][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f0171c96758>
[2017-06-13 17:36:36,711][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-13 17:36:36,711][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-13 17:36:36,711][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-13 17:36:36,711][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-13 17:36:36,711][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-13 17:36:36,711][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-13 17:36:36,712][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-13 17:36:36,738][gdb3][DEBUG ] connection detected need for sudo
[2017-06-13 17:36:36,752][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-13 17:36:36,752][gdb3][DEBUG ] detect platform information from remote host
[2017-06-13 17:36:36,769][gdb3][DEBUG ] detect machine type
[2017-06-13 17:36:36,771][gdb3][DEBUG ] find the location of an executable
[2017-06-13 17:36:36,772][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-13 17:36:36,783][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-13 17:36:36,789][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-13 17:36:36,790][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-13 17:36:36,790][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-13 17:36:36,790][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-13 17:36:36,790][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-13 17:36:36,790][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-13 17:36:36,790][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-13 17:36:36,790][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-13 17:36:36,955][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-13 17:36:36,955][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-13 17:36:36,955][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-13 17:36:36,955][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-13 17:36:36,955][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-13 17:36:36,955][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-13 17:36:36,956][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-13 17:36:36,956][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-13 17:36:36,956][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fae02e88ea8>
[2017-06-13 17:36:36,956][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-13 17:36:36,956][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fae02e5cb18>
[2017-06-13 17:36:36,956][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-13 17:36:36,956][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-13 17:36:36,956][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-13 17:36:36,957][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-13 17:36:36,957][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-13 17:36:36,983][gdb3][DEBUG ] connection detected need for sudo
[2017-06-13 17:36:36,996][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-13 17:36:36,997][gdb3][DEBUG ] detect platform information from remote host
[2017-06-13 17:36:37,013][gdb3][DEBUG ] detect machine type
[2017-06-13 17:36:37,016][gdb3][DEBUG ] find the location of an executable
[2017-06-13 17:36:37,016][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-13 17:36:37,016][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-13 17:36:37,016][gdb3][DEBUG ] get remote short hostname
[2017-06-13 17:36:37,017][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-13 17:36:37,017][gdb3][DEBUG ] get remote short hostname
[2017-06-13 17:36:37,017][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-13 17:36:37,018][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-13 17:36:37,019][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-13 17:36:37,019][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-13 17:36:37,020][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-13 17:36:37,020][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-13 17:36:37,020][gdb3][DEBUG ] create the monitor keyring file
[2017-06-13 17:36:37,021][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-13 17:36:37,060][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-13 17:36:37,060][gdb3][DEBUG ] ceph-mon: set fsid to d0ad43f5-ae26-41da-91c2-326d28bdb2ba
[2017-06-13 17:36:37,063][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-13 17:36:37,064][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-13 17:36:37,065][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-13 17:36:37,065][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-13 17:36:37,067][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-13 17:36:37,134][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-13 17:36:37,207][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-13 17:36:39,245][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-13 17:36:39,310][gdb3][DEBUG ] ********************************************************************************
[2017-06-13 17:36:39,310][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-13 17:36:39,311][gdb3][DEBUG ] {
[2017-06-13 17:36:39,311][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-13 17:36:39,311][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-13 17:36:39,311][gdb3][DEBUG ]   "features": {
[2017-06-13 17:36:39,311][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-13 17:36:39,311][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-13 17:36:39,311][gdb3][DEBUG ]       "kraken", 
[2017-06-13 17:36:39,311][gdb3][DEBUG ]       "luminous"
[2017-06-13 17:36:39,311][gdb3][DEBUG ]     ], 
[2017-06-13 17:36:39,311][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-13 17:36:39,311][gdb3][DEBUG ]     "required_mon": [
[2017-06-13 17:36:39,311][gdb3][DEBUG ]       "kraken", 
[2017-06-13 17:36:39,311][gdb3][DEBUG ]       "luminous"
[2017-06-13 17:36:39,311][gdb3][DEBUG ]     ]
[2017-06-13 17:36:39,311][gdb3][DEBUG ]   }, 
[2017-06-13 17:36:39,312][gdb3][DEBUG ]   "monmap": {
[2017-06-13 17:36:39,312][gdb3][DEBUG ]     "created": "2017-06-13 17:36:37.045976", 
[2017-06-13 17:36:39,312][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-13 17:36:39,312][gdb3][DEBUG ]     "features": {
[2017-06-13 17:36:39,312][gdb3][DEBUG ]       "optional": [], 
[2017-06-13 17:36:39,312][gdb3][DEBUG ]       "persistent": [
[2017-06-13 17:36:39,312][gdb3][DEBUG ]         "kraken", 
[2017-06-13 17:36:39,312][gdb3][DEBUG ]         "luminous"
[2017-06-13 17:36:39,312][gdb3][DEBUG ]       ]
[2017-06-13 17:36:39,312][gdb3][DEBUG ]     }, 
[2017-06-13 17:36:39,312][gdb3][DEBUG ]     "fsid": "d0ad43f5-ae26-41da-91c2-326d28bdb2ba", 
[2017-06-13 17:36:39,312][gdb3][DEBUG ]     "modified": "2017-06-13 17:36:37.285676", 
[2017-06-13 17:36:39,312][gdb3][DEBUG ]     "mons": [
[2017-06-13 17:36:39,312][gdb3][DEBUG ]       {
[2017-06-13 17:36:39,312][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-13 17:36:39,312][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-13 17:36:39,312][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-13 17:36:39,313][gdb3][DEBUG ]         "rank": 0
[2017-06-13 17:36:39,313][gdb3][DEBUG ]       }
[2017-06-13 17:36:39,313][gdb3][DEBUG ]     ]
[2017-06-13 17:36:39,313][gdb3][DEBUG ]   }, 
[2017-06-13 17:36:39,313][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-13 17:36:39,313][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-13 17:36:39,313][gdb3][DEBUG ]   "quorum": [
[2017-06-13 17:36:39,313][gdb3][DEBUG ]     0
[2017-06-13 17:36:39,313][gdb3][DEBUG ]   ], 
[2017-06-13 17:36:39,313][gdb3][DEBUG ]   "rank": 0, 
[2017-06-13 17:36:39,313][gdb3][DEBUG ]   "state": "leader", 
[2017-06-13 17:36:39,313][gdb3][DEBUG ]   "sync_provider": []
[2017-06-13 17:36:39,313][gdb3][DEBUG ] }
[2017-06-13 17:36:39,313][gdb3][DEBUG ] ********************************************************************************
[2017-06-13 17:36:39,313][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-13 17:36:39,314][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-13 17:36:39,379][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-13 17:36:39,395][gdb3][DEBUG ] connection detected need for sudo
[2017-06-13 17:36:39,409][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-13 17:36:39,410][gdb3][DEBUG ] detect platform information from remote host
[2017-06-13 17:36:39,427][gdb3][DEBUG ] detect machine type
[2017-06-13 17:36:39,429][gdb3][DEBUG ] find the location of an executable
[2017-06-13 17:36:39,431][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-13 17:36:39,496][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-13 17:36:39,496][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-13 17:36:39,496][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-13 17:36:39,498][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpl29n6w
[2017-06-13 17:36:39,513][gdb3][DEBUG ] connection detected need for sudo
[2017-06-13 17:36:39,527][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-13 17:36:39,527][gdb3][DEBUG ] detect platform information from remote host
[2017-06-13 17:36:39,544][gdb3][DEBUG ] detect machine type
[2017-06-13 17:36:39,546][gdb3][DEBUG ] get remote short hostname
[2017-06-13 17:36:39,547][gdb3][DEBUG ] fetch remote file
[2017-06-13 17:36:39,548][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-13 17:36:39,614][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-13 17:36:39,780][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-13 17:36:39,947][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-13 17:36:40,113][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-13 17:36:40,279][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-13 17:36:40,445][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-13 17:36:40,612][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-13 17:36:40,778][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-13 17:36:40,944][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-13 17:36:41,110][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-13 17:36:41,275][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-13 17:36:41,276][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-13 17:36:41,276][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170613173641'
[2017-06-13 17:36:41,277][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-13 17:36:41,277][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-13 17:36:41,277][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-13 17:36:41,277][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpl29n6w
[2017-06-13 17:37:01,876][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-13 17:37:01,876][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-06-13 17:37:01,876][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-13 17:37:01,876][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-13 17:37:01,876][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-13 17:37:01,876][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-06-13 17:37:01,876][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe75397f950>
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe753bd5aa0>
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-13 17:37:01,877][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-06-13 17:37:01,878][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-06-13 17:37:02,147][gdb1][DEBUG ] connection detected need for sudo
[2017-06-13 17:37:02,390][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-13 17:37:02,390][gdb1][DEBUG ] detect platform information from remote host
[2017-06-13 17:37:02,409][gdb1][DEBUG ] detect machine type
[2017-06-13 17:37:02,413][gdb1][DEBUG ] find the location of an executable
[2017-06-13 17:37:02,413][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-13 17:37:02,414][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-13 17:37:02,414][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-13 17:37:02,417][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-06-13 17:37:02,417][gdb1][DEBUG ] find the location of an executable
[2017-06-13 17:37:02,419][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-13 17:37:02,589][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-13 17:37:02,590][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-13 17:37:02,590][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-13 17:37:02,605][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-13 17:37:02,621][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-13 17:37:02,629][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:02,629][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:02,629][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:02,629][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-13 17:37:02,629][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-06-13 17:37:02,693][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-06-13 17:37:02,693][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-06-13 17:37:02,709][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-06-13 17:37:02,717][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:02,717][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-06-13 17:37:02,717][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:02,717][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-06-13 17:37:02,749][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-06-13 17:37:02,749][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-06-13 17:37:02,749][gdb1][WARNING] 10+0 records in
[2017-06-13 17:37:02,749][gdb1][WARNING] 10+0 records out
[2017-06-13 17:37:02,749][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00691647 s, 1.5 GB/s
[2017-06-13 17:37:02,749][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-06-13 17:37:02,749][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-06-13 17:37:02,753][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-06-13 17:37:02,753][gdb1][WARNING] backup header from main header.
[2017-06-13 17:37:02,753][gdb1][WARNING] 
[2017-06-13 17:37:02,753][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-06-13 17:37:02,753][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-06-13 17:37:02,753][gdb1][WARNING] 
[2017-06-13 17:37:02,753][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-06-13 17:37:02,753][gdb1][WARNING] 
[2017-06-13 17:37:03,871][gdb1][DEBUG ] ****************************************************************************
[2017-06-13 17:37:03,871][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-06-13 17:37:03,871][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-06-13 17:37:03,871][gdb1][DEBUG ] ****************************************************************************
[2017-06-13 17:37:03,871][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-06-13 17:37:03,871][gdb1][DEBUG ] other utilities.
[2017-06-13 17:37:03,871][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-06-13 17:37:04,888][gdb1][DEBUG ] Creating new GPT entries.
[2017-06-13 17:37:04,889][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-13 17:37:04,889][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-06-13 17:37:04,889][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:04,889][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-13 17:37:04,953][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:04,953][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:04,953][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-06-13 17:37:04,953][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:04,954][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-06-13 17:37:04,954][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:04,954][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-06-13 17:37:04,954][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:e3d2da37-061e-441d-85f8-13276f5b5abf --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-06-13 17:37:05,971][gdb1][DEBUG ] Setting name!
[2017-06-13 17:37:05,971][gdb1][DEBUG ] partNum is 0
[2017-06-13 17:37:05,971][gdb1][DEBUG ] REALLY setting name!
[2017-06-13 17:37:05,971][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-13 17:37:05,972][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-13 17:37:05,972][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:06,136][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-13 17:37:06,250][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:06,314][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:06,315][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:06,315][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-13 17:37:06,315][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-06-13 17:37:06,315][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-06-13 17:37:06,981][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8388543 blks
[2017-06-13 17:37:06,981][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-06-13 17:37:06,981][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-06-13 17:37:06,981][gdb1][DEBUG ] data     =                       bsize=4096   blocks=33554171, imaxpct=25
[2017-06-13 17:37:06,981][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-06-13 17:37:06,981][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-06-13 17:37:06,982][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=16383, version=2
[2017-06-13 17:37:06,982][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-06-13 17:37:06,982][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-06-13 17:37:06,982][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.kXl5AW with options noatime,inode64
[2017-06-13 17:37:06,982][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.kXl5AW
[2017-06-13 17:37:06,982][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.kXl5AW
[2017-06-13 17:37:06,982][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.kXl5AW/ceph_fsid.15635.tmp
[2017-06-13 17:37:06,990][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.kXl5AW/fsid.15635.tmp
[2017-06-13 17:37:06,991][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.kXl5AW/magic.15635.tmp
[2017-06-13 17:37:06,992][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.kXl5AW
[2017-06-13 17:37:06,993][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.kXl5AW
[2017-06-13 17:37:06,993][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.kXl5AW
[2017-06-13 17:37:07,025][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:07,025][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-06-13 17:37:08,043][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-13 17:37:08,043][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-13 17:37:08,043][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:08,258][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-13 17:37:08,372][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:08,586][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-06-13 17:37:08,589][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-13 17:37:13,811][gdb1][INFO  ] checking OSD status...
[2017-06-13 17:37:13,812][gdb1][DEBUG ] find the location of an executable
[2017-06-13 17:37:13,814][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-13 17:37:13,929][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-13 17:37:14,099][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-13 17:37:14,099][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-06-13 17:37:14,099][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-13 17:37:14,099][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-13 17:37:14,099][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-13 17:37:14,099][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-13 17:37:14,099][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-13 17:37:14,099][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcceff10560>
[2017-06-13 17:37:14,099][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-13 17:37:14,099][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-06-13 17:37:14,100][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fccf0827938>
[2017-06-13 17:37:14,100][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-13 17:37:14,100][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-13 17:37:14,100][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-13 17:37:17,114][ceph_deploy.admin][ERROR ] connecting to host: gdb0 resulted in errors: HostNotFound gdb0
[2017-06-13 17:37:17,115][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-13 17:37:17,346][gdb1][DEBUG ] connection detected need for sudo
[2017-06-13 17:37:17,574][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-13 17:37:17,574][gdb1][DEBUG ] detect platform information from remote host
[2017-06-13 17:37:17,591][gdb1][DEBUG ] detect machine type
[2017-06-13 17:37:17,595][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-13 17:37:17,598][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-13 17:37:17,613][gdb3][DEBUG ] connection detected need for sudo
[2017-06-13 17:37:17,627][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-13 17:37:17,628][gdb3][DEBUG ] detect platform information from remote host
[2017-06-13 17:37:17,644][gdb3][DEBUG ] detect machine type
[2017-06-13 17:37:17,647][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-13 17:37:17,648][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-06-13 17:37:42,056][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-13 17:37:42,056][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-06-13 17:37:42,056][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-13 17:37:42,056][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-13 17:37:42,056][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-13 17:37:42,056][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-06-13 17:37:42,056][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-13 17:37:42,056][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-13 17:37:42,056][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2a0dba1950>
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2a0ddf7aa0>
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-13 17:37:42,057][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-06-13 17:37:42,058][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-06-13 17:37:42,306][gdb1][DEBUG ] connection detected need for sudo
[2017-06-13 17:37:42,541][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-13 17:37:42,542][gdb1][DEBUG ] detect platform information from remote host
[2017-06-13 17:37:42,559][gdb1][DEBUG ] detect machine type
[2017-06-13 17:37:42,563][gdb1][DEBUG ] find the location of an executable
[2017-06-13 17:37:42,564][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-13 17:37:42,564][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-13 17:37:42,564][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-13 17:37:42,566][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-13 17:37:42,567][gdb1][DEBUG ] create a keyring file
[2017-06-13 17:37:42,568][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-06-13 17:37:42,568][gdb1][DEBUG ] find the location of an executable
[2017-06-13 17:37:42,571][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-06-13 17:37:42,741][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-13 17:37:42,741][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-13 17:37:42,741][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-13 17:37:42,742][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-13 17:37:42,742][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:42,742][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-06-13 17:37:42,742][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-13 17:37:42,758][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:42,758][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:42,758][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:42,758][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-13 17:37:42,758][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-06-13 17:37:42,766][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-06-13 17:37:42,773][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-06-13 17:37:42,789][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-06-13 17:37:42,792][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:42,793][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-06-13 17:37:42,793][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:42,793][gdb1][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-06-13 17:37:42,808][gdb1][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-06-13 17:37:42,809][gdb1][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-06-13 17:37:42,816][gdb1][WARNING] 10+0 records in
[2017-06-13 17:37:42,816][gdb1][WARNING] 10+0 records out
[2017-06-13 17:37:42,816][gdb1][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00713132 s, 1.5 GB/s
[2017-06-13 17:37:42,816][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-06-13 17:37:42,816][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-06-13 17:37:42,817][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-06-13 17:37:42,817][gdb1][WARNING] backup header from main header.
[2017-06-13 17:37:42,817][gdb1][WARNING] 
[2017-06-13 17:37:42,817][gdb1][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-06-13 17:37:42,817][gdb1][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-06-13 17:37:42,817][gdb1][WARNING] 
[2017-06-13 17:37:42,817][gdb1][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-06-13 17:37:42,817][gdb1][WARNING] 
[2017-06-13 17:37:43,985][gdb1][DEBUG ] ****************************************************************************
[2017-06-13 17:37:43,985][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-06-13 17:37:43,985][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-06-13 17:37:43,985][gdb1][DEBUG ] ****************************************************************************
[2017-06-13 17:37:43,985][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-06-13 17:37:43,985][gdb1][DEBUG ] other utilities.
[2017-06-13 17:37:43,986][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-06-13 17:37:45,003][gdb1][DEBUG ] Creating new GPT entries.
[2017-06-13 17:37:45,003][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-13 17:37:45,003][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-06-13 17:37:45,003][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:45,003][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-13 17:37:45,004][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:45,011][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:45,011][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:45,012][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-06-13 17:37:45,012][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:45,012][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-06-13 17:37:45,012][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:63e36464-d09d-4078-afa8-665f4de5c541 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-06-13 17:37:46,029][gdb1][DEBUG ] Setting name!
[2017-06-13 17:37:46,029][gdb1][DEBUG ] partNum is 1
[2017-06-13 17:37:46,030][gdb1][DEBUG ] REALLY setting name!
[2017-06-13 17:37:46,030][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-13 17:37:46,030][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-13 17:37:46,030][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:46,244][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-13 17:37:46,409][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:46,623][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:46,624][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:46,624][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-06-13 17:37:46,624][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/63e36464-d09d-4078-afa8-665f4de5c541
[2017-06-13 17:37:46,624][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-06-13 17:37:47,641][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-13 17:37:47,642][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-13 17:37:47,642][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:47,806][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-13 17:37:48,020][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:48,235][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/63e36464-d09d-4078-afa8-665f4de5c541
[2017-06-13 17:37:48,235][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:48,235][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-06-13 17:37:48,235][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:48,235][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-06-13 17:37:48,236][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:48,236][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-06-13 17:37:48,236][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:f67d7d67-9fb9-44cc-85a0-3cf677854165 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-06-13 17:37:49,253][gdb1][DEBUG ] Setting name!
[2017-06-13 17:37:49,253][gdb1][DEBUG ] partNum is 0
[2017-06-13 17:37:49,253][gdb1][DEBUG ] REALLY setting name!
[2017-06-13 17:37:49,253][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-13 17:37:49,253][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-06-13 17:37:49,254][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:49,468][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-13 17:37:49,683][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:49,897][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:49,898][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:49,898][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-06-13 17:37:49,898][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-06-13 17:37:49,898][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-06-13 17:37:50,514][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-06-13 17:37:50,514][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-06-13 17:37:50,514][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-06-13 17:37:50,514][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-06-13 17:37:50,514][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-06-13 17:37:50,514][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-06-13 17:37:50,514][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-06-13 17:37:50,515][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-06-13 17:37:50,515][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-06-13 17:37:50,515][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.9fN1Tt with options noatime,inode64
[2017-06-13 17:37:50,515][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.9fN1Tt
[2017-06-13 17:37:50,515][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.9fN1Tt
[2017-06-13 17:37:50,531][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9fN1Tt/ceph_fsid.16275.tmp
[2017-06-13 17:37:50,531][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9fN1Tt/fsid.16275.tmp
[2017-06-13 17:37:50,546][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9fN1Tt/magic.16275.tmp
[2017-06-13 17:37:50,546][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9fN1Tt/journal_uuid.16275.tmp
[2017-06-13 17:37:50,548][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.9fN1Tt/journal -> /dev/disk/by-partuuid/63e36464-d09d-4078-afa8-665f4de5c541
[2017-06-13 17:37:50,548][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.9fN1Tt
[2017-06-13 17:37:50,551][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.9fN1Tt
[2017-06-13 17:37:50,551][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.9fN1Tt
[2017-06-13 17:37:50,567][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-06-13 17:37:50,567][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-06-13 17:37:51,584][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-06-13 17:37:51,584][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-06-13 17:37:51,584][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-06-13 17:37:51,584][gdb1][DEBUG ] The operation has completed successfully.
[2017-06-13 17:37:51,585][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-06-13 17:37:51,585][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:51,585][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-06-13 17:37:51,749][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-06-13 17:37:51,813][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-06-13 17:37:51,816][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-13 17:37:56,938][gdb1][INFO  ] checking OSD status...
[2017-06-13 17:37:56,938][gdb1][DEBUG ] find the location of an executable
[2017-06-13 17:37:56,941][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-13 17:37:57,106][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-13 17:37:57,270][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-13 17:37:57,270][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-06-13 17:37:57,270][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-13 17:37:57,270][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-13 17:37:57,270][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-13 17:37:57,270][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-13 17:37:57,271][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-13 17:37:57,271][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7aa1a1f560>
[2017-06-13 17:37:57,271][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-13 17:37:57,271][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-06-13 17:37:57,271][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f7aa2336938>
[2017-06-13 17:37:57,271][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-13 17:37:57,271][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-13 17:37:57,271][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-13 17:38:00,286][ceph_deploy.admin][ERROR ] connecting to host: gdb0 resulted in errors: HostNotFound gdb0
[2017-06-13 17:38:00,287][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-13 17:38:00,518][gdb1][DEBUG ] connection detected need for sudo
[2017-06-13 17:38:00,753][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-13 17:38:00,754][gdb1][DEBUG ] detect platform information from remote host
[2017-06-13 17:38:00,770][gdb1][DEBUG ] detect machine type
[2017-06-13 17:38:00,774][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-13 17:38:00,777][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-13 17:38:00,792][gdb3][DEBUG ] connection detected need for sudo
[2017-06-13 17:38:00,806][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-13 17:38:00,807][gdb3][DEBUG ] detect platform information from remote host
[2017-06-13 17:38:00,823][gdb3][DEBUG ] detect machine type
[2017-06-13 17:38:00,825][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-13 17:38:00,827][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-06-13 17:39:01,455][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0e0ab39560>
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f0e0b450938>
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-13 17:39:01,456][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-13 17:39:01,457][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-13 17:39:01,702][gdb1][DEBUG ] connection detected need for sudo
[2017-06-13 17:39:01,937][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-13 17:39:01,938][gdb1][DEBUG ] detect platform information from remote host
[2017-06-13 17:39:01,955][gdb1][DEBUG ] detect machine type
[2017-06-13 17:39:01,959][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:46:44,778][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:46:44,778][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-07-06 03:46:44,778][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:46:44,778][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:46:44,778][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:46:44,778][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:46:44,778][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:46:44,778][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd7429df128>
[2017-07-06 03:46:44,778][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:46:44,779][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 03:46:44,779][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fd7432ec1b8>
[2017-07-06 03:46:44,779][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:46:44,779][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:46:44,779][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-07-06 03:46:44,779][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-07-06 03:46:44,779][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-07-06 03:46:44,779][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-07-06 03:46:44,805][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:46:44,819][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:46:44,820][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:46:44,836][gdb3][DEBUG ] detect machine type
[2017-07-06 03:46:44,839][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:46:44,839][gdb3][INFO  ] Purging Ceph on gdb3
[2017-07-06 03:46:44,840][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-07-06 03:46:44,877][gdb3][DEBUG ] Reading package lists...
[2017-07-06 03:46:45,042][gdb3][DEBUG ] Building dependency tree...
[2017-07-06 03:46:45,042][gdb3][DEBUG ] Reading state information...
[2017-07-06 03:46:45,106][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-07-06 03:46:45,106][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-07-06 03:46:45,107][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-07-06 03:46:45,107][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-07-06 03:46:45,107][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-07-06 03:46:45,107][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-07-06 03:46:45,107][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-07-06 03:46:45,107][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1018 linux-headers-4.4.0-1013-aws
[2017-07-06 03:46:45,107][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-headers-4.4.0-1017-aws
[2017-07-06 03:46:45,108][gdb3][DEBUG ]   linux-headers-4.4.0-1018-aws linux-image-4.4.0-1013-aws
[2017-07-06 03:46:45,108][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws
[2017-07-06 03:46:45,108][gdb3][DEBUG ]   linux-image-4.4.0-1018-aws ntp python-blinker python-cephfs
[2017-07-06 03:46:45,108][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-07-06 03:46:45,108][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-07-06 03:46:45,108][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-07-06 03:46:45,108][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-07-06 03:46:45,108][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-07-06 03:46:45,108][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-07-06 03:46:45,140][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-07-06 03:46:45,140][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-07-06 03:46:45,254][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 60 not upgraded.
[2017-07-06 03:46:45,255][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-07-06 03:46:45,319][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 179644 files and directories currently installed.)
[2017-07-06 03:46:45,320][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-07-06 03:46:45,489][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-07-06 03:46:45,603][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-07-06 03:46:45,635][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-07-06 03:46:45,852][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-07-06 03:46:45,966][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-07-06 03:46:46,139][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-07-06 03:46:46,203][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-07-06 03:46:46,235][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-07-06 03:46:46,402][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-07-06 03:46:46,466][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-07-06 03:46:46,481][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-07-06 03:46:46,646][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-07-06 03:46:46,710][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-07-06 03:46:46,826][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-07-06 03:46:46,940][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-07-06 03:46:46,941][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-07-06 03:46:47,005][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-07-06 03:46:47,821][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-07-06 03:46:47,983][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:46:47,983][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-07-06 03:46:47,983][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:46:47,983][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:46:47,983][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:46:47,983][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:46:47,983][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:46:47,984][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5fb53f97a0>
[2017-07-06 03:46:47,984][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:46:47,984][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 03:46:47,984][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f5fb5d06230>
[2017-07-06 03:46:47,984][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:46:47,984][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:46:47,984][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-07-06 03:46:48,009][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:46:48,023][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:46:48,024][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:46:48,041][gdb3][DEBUG ] detect machine type
[2017-07-06 03:46:48,043][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:46:48,059][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:46:48,073][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:46:48,073][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:46:48,090][gdb3][DEBUG ] detect machine type
[2017-07-06 03:46:48,092][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:46:48,092][gdb3][INFO  ] purging data on gdb3
[2017-07-06 03:46:48,093][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-07-06 03:46:48,106][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-07-06 03:46:48,275][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:46:48,275][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-07-06 03:46:48,275][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:46:48,275][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:46:48,276][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:46:48,276][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:46:48,276][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:46:48,276][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7ea7631a70>
[2017-07-06 03:46:48,276][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:46:48,276][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f7ea7ef4848>
[2017-07-06 03:46:48,276][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:46:48,276][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:47:05,110][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb8250815f0>
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fb825705758>
[2017-07-06 03:47:05,111][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-07-06 03:47:05,112][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:47:05,112][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-07-06 03:47:05,112][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:47:05,112][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-07-06 03:47:05,112][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-07-06 03:47:05,112][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-07-06 03:47:05,138][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:47:05,152][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:47:05,152][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:47:05,169][gdb3][DEBUG ] detect machine type
[2017-07-06 03:47:05,171][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:47:05,172][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-07-06 03:47:05,183][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-07-06 03:47:05,189][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-07-06 03:47:05,190][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-07-06 03:47:05,190][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-07-06 03:47:05,190][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-07-06 03:47:05,190][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-07-06 03:47:05,190][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-07-06 03:47:05,190][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-07-06 03:47:05,190][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-07-06 03:47:05,356][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:47:05,356][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-07-06 03:47:05,356][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:47:05,356][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:47:05,356][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:47:05,356][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:47:05,356][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-07-06 03:47:05,356][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:47:05,356][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd072697ef0>
[2017-07-06 03:47:05,356][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:47:05,357][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fd07266bb18>
[2017-07-06 03:47:05,357][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:47:05,357][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-07-06 03:47:05,357][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:47:05,357][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-07-06 03:47:05,358][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-07-06 03:47:05,383][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:47:05,397][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:47:05,398][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:47:05,414][gdb3][DEBUG ] detect machine type
[2017-07-06 03:47:05,416][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:47:05,417][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-07-06 03:47:05,417][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-07-06 03:47:05,417][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:47:05,417][gdb3][DEBUG ] deploying mon to gdb3
[2017-07-06 03:47:05,417][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:47:05,418][gdb3][DEBUG ] remote hostname: gdb3
[2017-07-06 03:47:05,418][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:47:05,419][gdb3][DEBUG ] create the mon path if it does not exist
[2017-07-06 03:47:05,420][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 03:47:05,420][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 03:47:05,420][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 03:47:05,420][gdb3][DEBUG ] create the monitor keyring file
[2017-07-06 03:47:05,422][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-07-06 03:47:05,460][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-07-06 03:47:05,460][gdb3][DEBUG ] ceph-mon: set fsid to e090f789-86b8-4d35-8613-d3578aad8823
[2017-07-06 03:47:05,463][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-07-06 03:47:05,464][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 03:47:05,465][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-07-06 03:47:05,465][gdb3][DEBUG ] create the init path if it does not exist
[2017-07-06 03:47:05,467][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 03:47:05,537][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-07-06 03:47:05,605][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-07-06 03:47:07,643][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:47:07,708][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 03:47:07,708][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-07-06 03:47:07,708][gdb3][DEBUG ] {
[2017-07-06 03:47:07,708][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-07-06 03:47:07,708][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-07-06 03:47:07,708][gdb3][DEBUG ]   "features": {
[2017-07-06 03:47:07,709][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-07-06 03:47:07,709][gdb3][DEBUG ]     "quorum_mon": [
[2017-07-06 03:47:07,709][gdb3][DEBUG ]       "kraken", 
[2017-07-06 03:47:07,709][gdb3][DEBUG ]       "luminous"
[2017-07-06 03:47:07,709][gdb3][DEBUG ]     ], 
[2017-07-06 03:47:07,709][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-07-06 03:47:07,709][gdb3][DEBUG ]     "required_mon": [
[2017-07-06 03:47:07,709][gdb3][DEBUG ]       "kraken", 
[2017-07-06 03:47:07,709][gdb3][DEBUG ]       "luminous"
[2017-07-06 03:47:07,709][gdb3][DEBUG ]     ]
[2017-07-06 03:47:07,709][gdb3][DEBUG ]   }, 
[2017-07-06 03:47:07,709][gdb3][DEBUG ]   "monmap": {
[2017-07-06 03:47:07,709][gdb3][DEBUG ]     "created": "2017-07-06 03:47:05.444975", 
[2017-07-06 03:47:07,709][gdb3][DEBUG ]     "epoch": 2, 
[2017-07-06 03:47:07,709][gdb3][DEBUG ]     "features": {
[2017-07-06 03:47:07,709][gdb3][DEBUG ]       "optional": [], 
[2017-07-06 03:47:07,710][gdb3][DEBUG ]       "persistent": [
[2017-07-06 03:47:07,710][gdb3][DEBUG ]         "kraken", 
[2017-07-06 03:47:07,710][gdb3][DEBUG ]         "luminous"
[2017-07-06 03:47:07,710][gdb3][DEBUG ]       ]
[2017-07-06 03:47:07,710][gdb3][DEBUG ]     }, 
[2017-07-06 03:47:07,710][gdb3][DEBUG ]     "fsid": "e090f789-86b8-4d35-8613-d3578aad8823", 
[2017-07-06 03:47:07,710][gdb3][DEBUG ]     "modified": "2017-07-06 03:47:05.690101", 
[2017-07-06 03:47:07,710][gdb3][DEBUG ]     "mons": [
[2017-07-06 03:47:07,710][gdb3][DEBUG ]       {
[2017-07-06 03:47:07,710][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-07-06 03:47:07,710][gdb3][DEBUG ]         "name": "gdb3", 
[2017-07-06 03:47:07,710][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-07-06 03:47:07,710][gdb3][DEBUG ]         "rank": 0
[2017-07-06 03:47:07,710][gdb3][DEBUG ]       }
[2017-07-06 03:47:07,710][gdb3][DEBUG ]     ]
[2017-07-06 03:47:07,710][gdb3][DEBUG ]   }, 
[2017-07-06 03:47:07,710][gdb3][DEBUG ]   "name": "gdb3", 
[2017-07-06 03:47:07,710][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-07-06 03:47:07,711][gdb3][DEBUG ]   "quorum": [
[2017-07-06 03:47:07,711][gdb3][DEBUG ]     0
[2017-07-06 03:47:07,711][gdb3][DEBUG ]   ], 
[2017-07-06 03:47:07,711][gdb3][DEBUG ]   "rank": 0, 
[2017-07-06 03:47:07,711][gdb3][DEBUG ]   "state": "leader", 
[2017-07-06 03:47:07,711][gdb3][DEBUG ]   "sync_provider": []
[2017-07-06 03:47:07,711][gdb3][DEBUG ] }
[2017-07-06 03:47:07,711][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 03:47:07,711][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-07-06 03:47:07,712][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:47:07,777][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-07-06 03:47:07,793][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:47:07,807][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:47:07,808][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:47:07,824][gdb3][DEBUG ] detect machine type
[2017-07-06 03:47:07,827][gdb3][DEBUG ] find the location of an executable
[2017-07-06 03:47:07,828][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:47:07,893][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-07-06 03:47:07,893][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-07-06 03:47:07,893][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-07-06 03:47:07,895][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpT9E1o4
[2017-07-06 03:47:07,911][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:47:07,926][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:47:07,926][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:47:07,943][gdb3][DEBUG ] detect machine type
[2017-07-06 03:47:07,946][gdb3][DEBUG ] get remote short hostname
[2017-07-06 03:47:07,946][gdb3][DEBUG ] fetch remote file
[2017-07-06 03:47:07,948][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 03:47:08,013][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-07-06 03:47:08,179][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-07-06 03:47:08,346][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-07-06 03:47:08,512][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-07-06 03:47:08,678][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-07-06 03:47:08,844][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-07-06 03:47:09,010][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-07-06 03:47:09,176][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-07-06 03:47:09,342][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-07-06 03:47:09,509][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-07-06 03:47:09,674][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-07-06 03:47:09,674][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-07-06 03:47:09,674][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170706034709'
[2017-07-06 03:47:09,675][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-07-06 03:47:09,675][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-07-06 03:47:09,675][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-07-06 03:47:09,675][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpT9E1o4
[2017-07-06 03:47:13,994][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:47:13,994][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-07-06 03:47:13,995][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:47:13,995][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:47:13,995][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 03:47:13,995][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-07-06 03:47:13,995][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 03:47:13,995][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:47:13,995][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 03:47:13,995][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 03:47:13,996][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 03:47:13,996][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-07-06 03:47:13,996][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 03:47:13,996][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:47:13,996][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9162abb998>
[2017-07-06 03:47:13,996][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:47:13,996][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 03:47:13,996][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9162d11aa0>
[2017-07-06 03:47:13,996][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:47:13,997][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:47:13,997][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-07-06 03:47:13,997][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-07-06 03:47:14,238][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:47:14,464][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:47:14,465][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:47:14,481][gdb1][DEBUG ] detect machine type
[2017-07-06 03:47:14,485][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:47:14,486][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:47:14,486][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-07-06 03:47:14,487][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:47:14,489][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-07-06 03:47:14,490][gdb1][DEBUG ] create a keyring file
[2017-07-06 03:47:14,491][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-07-06 03:47:14,492][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:47:14,494][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-07-06 03:47:14,664][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:47:14,664][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:47:14,664][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:47:14,664][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:47:14,668][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:14,668][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-07-06 03:47:14,668][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 03:47:14,684][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:14,684][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:14,684][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:14,684][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-07-06 03:47:14,700][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-07-06 03:47:14,703][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-07-06 03:47:14,719][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-07-06 03:47:14,722][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:14,722][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-07-06 03:47:14,722][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:14,722][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-07-06 03:47:14,726][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-07-06 03:47:14,733][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-07-06 03:47:14,733][gdb1][WARNING] backup header from main header.
[2017-07-06 03:47:14,733][gdb1][WARNING] 
[2017-07-06 03:47:15,750][gdb1][DEBUG ] ****************************************************************************
[2017-07-06 03:47:15,750][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-07-06 03:47:15,750][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-07-06 03:47:15,751][gdb1][DEBUG ] ****************************************************************************
[2017-07-06 03:47:15,751][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-07-06 03:47:15,751][gdb1][DEBUG ] other utilities.
[2017-07-06 03:47:15,751][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-07-06 03:47:16,768][gdb1][DEBUG ] Creating new GPT entries.
[2017-07-06 03:47:16,768][gdb1][DEBUG ] The operation has completed successfully.
[2017-07-06 03:47:16,768][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-07-06 03:47:16,768][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 03:47:16,784][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 03:47:16,815][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 03:47:16,831][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:16,831][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:16,831][gdb1][WARNING] ptype_tobe_for_name: name = journal
[2017-07-06 03:47:16,831][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:16,831][gdb1][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-07-06 03:47:16,832][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:41465080-5ab3-429f-b535-c81a1b9a2da4 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-07-06 03:47:17,899][gdb1][DEBUG ] Setting name!
[2017-07-06 03:47:17,899][gdb1][DEBUG ] partNum is 1
[2017-07-06 03:47:17,899][gdb1][DEBUG ] REALLY setting name!
[2017-07-06 03:47:17,899][gdb1][DEBUG ] The operation has completed successfully.
[2017-07-06 03:47:17,899][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-07-06 03:47:17,899][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 03:47:18,114][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 03:47:18,278][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 03:47:18,342][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:18,342][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:18,342][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-07-06 03:47:18,342][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/41465080-5ab3-429f-b535-c81a1b9a2da4
[2017-07-06 03:47:18,342][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-07-06 03:47:19,359][gdb1][DEBUG ] The operation has completed successfully.
[2017-07-06 03:47:19,359][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-07-06 03:47:19,360][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 03:47:19,574][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 03:47:19,839][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 03:47:20,053][gdb1][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/41465080-5ab3-429f-b535-c81a1b9a2da4
[2017-07-06 03:47:20,053][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:20,053][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-07-06 03:47:20,053][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:20,054][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-07-06 03:47:20,054][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:20,054][gdb1][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-07-06 03:47:20,054][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:162b6ba4-36cc-4b6b-a575-6ca82786e8f3 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-07-06 03:47:21,070][gdb1][DEBUG ] Setting name!
[2017-07-06 03:47:21,071][gdb1][DEBUG ] partNum is 0
[2017-07-06 03:47:21,071][gdb1][DEBUG ] REALLY setting name!
[2017-07-06 03:47:21,071][gdb1][DEBUG ] The operation has completed successfully.
[2017-07-06 03:47:21,071][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-07-06 03:47:21,071][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 03:47:21,285][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 03:47:21,500][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 03:47:21,714][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:21,715][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:21,715][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-07-06 03:47:21,715][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-07-06 03:47:21,715][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-07-06 03:47:22,330][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-07-06 03:47:22,330][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-07-06 03:47:22,331][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-07-06 03:47:22,331][gdb1][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-07-06 03:47:22,331][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-07-06 03:47:22,331][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-07-06 03:47:22,331][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-07-06 03:47:22,331][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-07-06 03:47:22,331][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-07-06 03:47:22,331][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.voRpk0 with options noatime,inode64
[2017-07-06 03:47:22,331][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.voRpk0
[2017-07-06 03:47:22,395][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.voRpk0
[2017-07-06 03:47:22,395][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.voRpk0/ceph_fsid.9973.tmp
[2017-07-06 03:47:22,395][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.voRpk0/fsid.9973.tmp
[2017-07-06 03:47:22,395][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.voRpk0/magic.9973.tmp
[2017-07-06 03:47:22,395][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.voRpk0/journal_uuid.9973.tmp
[2017-07-06 03:47:22,396][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.voRpk0/journal -> /dev/disk/by-partuuid/41465080-5ab3-429f-b535-c81a1b9a2da4
[2017-07-06 03:47:22,396][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.voRpk0
[2017-07-06 03:47:22,396][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.voRpk0
[2017-07-06 03:47:22,396][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.voRpk0
[2017-07-06 03:47:22,397][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:47:22,397][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-07-06 03:47:23,414][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-07-06 03:47:23,414][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-07-06 03:47:23,414][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-07-06 03:47:23,414][gdb1][DEBUG ] The operation has completed successfully.
[2017-07-06 03:47:23,414][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-07-06 03:47:23,414][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 03:47:23,414][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 03:47:23,629][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 03:47:23,661][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-07-06 03:47:23,679][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 03:47:28,801][gdb1][INFO  ] checking OSD status...
[2017-07-06 03:47:28,801][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:47:28,803][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 03:47:28,918][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-07-06 03:47:29,082][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:47:29,082][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1 gdb3
[2017-07-06 03:47:29,082][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:47:29,082][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:47:29,082][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:47:29,082][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:47:29,082][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:47:29,082][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc4dea305a8>
[2017-07-06 03:47:29,082][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:47:29,082][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1', 'gdb3']
[2017-07-06 03:47:29,082][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fc4df347938>
[2017-07-06 03:47:29,083][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:47:29,083][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:47:29,083][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-07-06 03:47:32,098][ceph_deploy.admin][ERROR ] connecting to host: gdb0 resulted in errors: HostNotFound gdb0
[2017-07-06 03:47:32,098][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-07-06 03:47:32,333][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:47:32,561][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:47:32,562][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:47:32,578][gdb1][DEBUG ] detect machine type
[2017-07-06 03:47:32,582][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:47:32,584][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 03:47:32,599][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:47:32,613][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:47:32,614][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:47:32,631][gdb3][DEBUG ] detect machine type
[2017-07-06 03:47:32,633][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:47:32,635][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-07-06 03:50:13,328][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:50:13,329][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1 gdb3
[2017-07-06 03:50:13,329][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:50:13,329][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:50:13,329][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:50:13,329][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:50:13,329][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:50:13,329][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f88be8715a8>
[2017-07-06 03:50:13,329][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:50:13,329][ceph_deploy.cli][INFO  ]  client                        : ['gdb1', 'gdb3']
[2017-07-06 03:50:13,329][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f88bf188938>
[2017-07-06 03:50:13,330][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:50:13,330][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:50:13,330][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-07-06 03:50:13,573][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:50:13,804][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:50:13,805][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:50:13,821][gdb1][DEBUG ] detect machine type
[2017-07-06 03:50:13,825][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:50:13,827][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 03:50:13,842][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:50:13,856][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:50:13,857][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:50:13,873][gdb3][DEBUG ] detect machine type
[2017-07-06 03:50:13,875][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:51:44,547][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:51:44,547][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb1:/dev/xvdb
[2017-07-06 03:51:44,547][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:51:44,547][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:51:44,547][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 03:51:44,547][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-07-06 03:51:44,547][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcae8a10998>
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcae8c66aa0>
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:51:44,548][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-07-06 03:51:44,549][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-07-06 03:51:44,790][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:51:44,977][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:51:44,978][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:51:44,995][gdb1][DEBUG ] detect machine type
[2017-07-06 03:51:44,999][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:51:45,000][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 03:51:45,000][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-07-06 03:51:45,000][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:51:45,003][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-07-06 03:51:45,003][gdb1][DEBUG ] find the location of an executable
[2017-07-06 03:51:45,005][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-07-06 03:51:45,175][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 03:51:45,175][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:51:45,175][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:51:45,175][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 03:51:45,179][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:51:45,179][gdb1][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-07-06 03:51:45,179][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 03:51:45,195][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:51:45,195][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:51:45,195][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 03:51:45,195][gdb1][WARNING] Traceback (most recent call last):
[2017-07-06 03:51:45,195][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-07-06 03:51:45,195][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-07-06 03:51:45,195][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-07-06 03:51:45,195][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-07-06 03:51:45,195][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2029, in main
[2017-07-06 03:51:45,195][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2018, in prepare
[2017-07-06 03:51:45,195][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2049, in prepare_locked
[2017-07-06 03:51:45,196][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2816, in prepare
[2017-07-06 03:51:45,196][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2971, in prepare_device
[2017-07-06 03:51:45,196][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2878, in prepare_device
[2017-07-06 03:51:45,196][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2841, in sanity_checks
[2017-07-06 03:51:45,196][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-07-06 03:51:45,196][gdb1][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-07-06 03:51:45,199][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-07-06 03:51:45,199][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/local/bin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-07-06 03:51:45,200][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-07-06 03:51:45,371][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1 gdb3
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f33dd75f5a8>
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ]  client                        : ['gdb1', 'gdb3']
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f33de076938>
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 03:51:45,372][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 03:51:45,373][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-07-06 03:51:45,614][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 03:51:45,845][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 03:51:45,846][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 03:51:45,862][gdb1][DEBUG ] detect machine type
[2017-07-06 03:51:45,866][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 03:51:45,868][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 03:51:45,884][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 03:51:45,898][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 03:51:45,898][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 03:51:45,916][gdb3][DEBUG ] detect machine type
[2017-07-06 03:51:45,918][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:06:28,919][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff92c5eb128>
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7ff92cef81b8>
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:06:28,920][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:06:28,921][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-07-06 15:06:28,921][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-07-06 15:06:28,921][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-07-06 15:06:28,921][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-07-06 15:06:28,958][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 15:06:28,985][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 15:06:28,986][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 15:06:29,002][gdb3][DEBUG ] detect machine type
[2017-07-06 15:06:29,004][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 15:06:29,004][gdb3][INFO  ] Purging Ceph on gdb3
[2017-07-06 15:06:29,005][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-07-06 15:06:29,045][gdb3][DEBUG ] Reading package lists...
[2017-07-06 15:06:29,210][gdb3][DEBUG ] Building dependency tree...
[2017-07-06 15:06:29,210][gdb3][DEBUG ] Reading state information...
[2017-07-06 15:06:29,274][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-07-06 15:06:29,274][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-07-06 15:06:29,274][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-07-06 15:06:29,274][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-07-06 15:06:29,275][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-07-06 15:06:29,275][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-07-06 15:06:29,275][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-07-06 15:06:29,275][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1018 linux-headers-4.4.0-1013-aws
[2017-07-06 15:06:29,275][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-headers-4.4.0-1017-aws
[2017-07-06 15:06:29,275][gdb3][DEBUG ]   linux-headers-4.4.0-1018-aws linux-image-4.4.0-1013-aws
[2017-07-06 15:06:29,275][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws
[2017-07-06 15:06:29,276][gdb3][DEBUG ]   linux-image-4.4.0-1018-aws ntp python-blinker python-cephfs
[2017-07-06 15:06:29,276][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-07-06 15:06:29,276][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-07-06 15:06:29,276][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-07-06 15:06:29,276][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-07-06 15:06:29,276][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-07-06 15:06:29,276][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-07-06 15:06:29,292][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-07-06 15:06:29,292][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-07-06 15:06:29,557][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 60 not upgraded.
[2017-07-06 15:06:29,557][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-07-06 15:06:30,022][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 179644 files and directories currently installed.)
[2017-07-06 15:06:30,022][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-07-06 15:06:30,188][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-07-06 15:06:30,303][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-07-06 15:06:30,334][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-07-06 15:06:30,599][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-07-06 15:06:30,663][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-07-06 15:06:30,834][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-07-06 15:06:30,898][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-07-06 15:06:30,914][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-07-06 15:06:31,128][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-07-06 15:06:31,160][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-07-06 15:06:31,192][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-07-06 15:06:31,359][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-07-06 15:06:31,359][gdb3][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-07-06 15:06:31,375][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-07-06 15:06:31,539][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-07-06 15:06:31,571][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-07-06 15:06:31,571][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-07-06 15:06:31,786][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-07-06 15:06:32,602][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-07-06 15:06:32,767][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:06:32,767][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-07-06 15:06:32,768][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:06:32,768][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:06:32,768][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:06:32,768][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 15:06:32,768][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:06:32,768][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f68913c27a0>
[2017-07-06 15:06:32,768][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:06:32,768][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 15:06:32,768][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f6891ccf230>
[2017-07-06 15:06:32,768][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:06:32,768][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:06:32,768][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-07-06 15:06:32,795][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 15:06:32,809][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 15:06:32,809][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 15:06:32,826][gdb3][DEBUG ] detect machine type
[2017-07-06 15:06:32,828][gdb3][DEBUG ] find the location of an executable
[2017-07-06 15:06:32,844][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 15:06:32,858][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 15:06:32,858][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 15:06:32,875][gdb3][DEBUG ] detect machine type
[2017-07-06 15:06:32,877][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 15:06:32,877][gdb3][INFO  ] purging data on gdb3
[2017-07-06 15:06:32,878][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-07-06 15:06:32,891][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-07-06 15:06:33,061][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:06:33,061][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-07-06 15:06:33,061][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:06:33,061][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:06:33,062][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:06:33,062][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 15:06:33,062][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:06:33,062][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f33897d2a70>
[2017-07-06 15:06:33,062][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:06:33,062][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f338a095848>
[2017-07-06 15:06:33,062][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:06:33,062][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:06:46,679][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2315f815f0>
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f2316605758>
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:06:46,680][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-07-06 15:06:46,681][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:06:46,681][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-07-06 15:06:46,681][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-07-06 15:06:46,681][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-07-06 15:06:46,707][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 15:06:46,721][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 15:06:46,722][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 15:06:46,738][gdb3][DEBUG ] detect machine type
[2017-07-06 15:06:46,741][gdb3][DEBUG ] find the location of an executable
[2017-07-06 15:06:46,742][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-07-06 15:06:46,753][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-07-06 15:06:46,759][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-07-06 15:06:46,760][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-07-06 15:06:46,760][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-07-06 15:06:46,760][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-07-06 15:06:46,760][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-07-06 15:06:46,760][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-07-06 15:06:46,760][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-07-06 15:06:46,760][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-07-06 15:06:46,928][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:06:46,929][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-07-06 15:06:46,929][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:06:46,929][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:06:46,929][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:06:46,929][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 15:06:46,929][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-07-06 15:06:46,929][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:06:46,930][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc781e56ef0>
[2017-07-06 15:06:46,930][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:06:46,930][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fc781e2ab18>
[2017-07-06 15:06:46,930][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:06:46,930][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-07-06 15:06:46,930][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:06:46,931][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-07-06 15:06:46,931][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-07-06 15:06:46,958][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 15:06:46,972][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 15:06:46,972][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 15:06:46,989][gdb3][DEBUG ] detect machine type
[2017-07-06 15:06:46,992][gdb3][DEBUG ] find the location of an executable
[2017-07-06 15:06:46,992][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-07-06 15:06:46,992][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-07-06 15:06:46,992][gdb3][DEBUG ] get remote short hostname
[2017-07-06 15:06:46,993][gdb3][DEBUG ] deploying mon to gdb3
[2017-07-06 15:06:46,993][gdb3][DEBUG ] get remote short hostname
[2017-07-06 15:06:46,993][gdb3][DEBUG ] remote hostname: gdb3
[2017-07-06 15:06:46,994][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:06:46,995][gdb3][DEBUG ] create the mon path if it does not exist
[2017-07-06 15:06:46,995][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 15:06:46,996][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 15:06:46,996][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 15:06:46,996][gdb3][DEBUG ] create the monitor keyring file
[2017-07-06 15:06:46,997][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-07-06 15:06:47,067][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-07-06 15:06:47,068][gdb3][DEBUG ] ceph-mon: set fsid to 3e21a99d-3c53-44a3-804f-d73b265efe56
[2017-07-06 15:06:47,068][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-07-06 15:06:47,075][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 15:06:47,076][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-07-06 15:06:47,076][gdb3][DEBUG ] create the init path if it does not exist
[2017-07-06 15:06:47,077][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 15:06:47,145][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-07-06 15:06:47,213][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-07-06 15:06:49,282][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 15:06:49,398][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 15:06:49,398][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-07-06 15:06:49,398][gdb3][DEBUG ] {
[2017-07-06 15:06:49,398][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-07-06 15:06:49,398][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-07-06 15:06:49,398][gdb3][DEBUG ]   "features": {
[2017-07-06 15:06:49,398][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-07-06 15:06:49,399][gdb3][DEBUG ]     "quorum_mon": [
[2017-07-06 15:06:49,399][gdb3][DEBUG ]       "kraken", 
[2017-07-06 15:06:49,399][gdb3][DEBUG ]       "luminous"
[2017-07-06 15:06:49,399][gdb3][DEBUG ]     ], 
[2017-07-06 15:06:49,399][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-07-06 15:06:49,399][gdb3][DEBUG ]     "required_mon": [
[2017-07-06 15:06:49,399][gdb3][DEBUG ]       "kraken", 
[2017-07-06 15:06:49,399][gdb3][DEBUG ]       "luminous"
[2017-07-06 15:06:49,399][gdb3][DEBUG ]     ]
[2017-07-06 15:06:49,399][gdb3][DEBUG ]   }, 
[2017-07-06 15:06:49,399][gdb3][DEBUG ]   "monmap": {
[2017-07-06 15:06:49,399][gdb3][DEBUG ]     "created": "2017-07-06 15:06:47.044556", 
[2017-07-06 15:06:49,399][gdb3][DEBUG ]     "epoch": 2, 
[2017-07-06 15:06:49,399][gdb3][DEBUG ]     "features": {
[2017-07-06 15:06:49,399][gdb3][DEBUG ]       "optional": [], 
[2017-07-06 15:06:49,399][gdb3][DEBUG ]       "persistent": [
[2017-07-06 15:06:49,400][gdb3][DEBUG ]         "kraken", 
[2017-07-06 15:06:49,400][gdb3][DEBUG ]         "luminous"
[2017-07-06 15:06:49,400][gdb3][DEBUG ]       ]
[2017-07-06 15:06:49,400][gdb3][DEBUG ]     }, 
[2017-07-06 15:06:49,400][gdb3][DEBUG ]     "fsid": "3e21a99d-3c53-44a3-804f-d73b265efe56", 
[2017-07-06 15:06:49,400][gdb3][DEBUG ]     "modified": "2017-07-06 15:06:47.293420", 
[2017-07-06 15:06:49,400][gdb3][DEBUG ]     "mons": [
[2017-07-06 15:06:49,400][gdb3][DEBUG ]       {
[2017-07-06 15:06:49,400][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-07-06 15:06:49,400][gdb3][DEBUG ]         "name": "gdb3", 
[2017-07-06 15:06:49,400][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-07-06 15:06:49,400][gdb3][DEBUG ]         "rank": 0
[2017-07-06 15:06:49,400][gdb3][DEBUG ]       }
[2017-07-06 15:06:49,400][gdb3][DEBUG ]     ]
[2017-07-06 15:06:49,400][gdb3][DEBUG ]   }, 
[2017-07-06 15:06:49,400][gdb3][DEBUG ]   "name": "gdb3", 
[2017-07-06 15:06:49,400][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-07-06 15:06:49,401][gdb3][DEBUG ]   "quorum": [
[2017-07-06 15:06:49,401][gdb3][DEBUG ]     0
[2017-07-06 15:06:49,401][gdb3][DEBUG ]   ], 
[2017-07-06 15:06:49,401][gdb3][DEBUG ]   "rank": 0, 
[2017-07-06 15:06:49,401][gdb3][DEBUG ]   "state": "leader", 
[2017-07-06 15:06:49,401][gdb3][DEBUG ]   "sync_provider": []
[2017-07-06 15:06:49,401][gdb3][DEBUG ] }
[2017-07-06 15:06:49,401][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 15:06:49,401][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-07-06 15:06:49,402][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 15:06:49,467][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-07-06 15:06:49,485][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 15:06:49,499][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 15:06:49,499][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 15:06:49,516][gdb3][DEBUG ] detect machine type
[2017-07-06 15:06:49,518][gdb3][DEBUG ] find the location of an executable
[2017-07-06 15:06:49,519][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 15:06:49,584][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-07-06 15:06:49,584][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-07-06 15:06:49,585][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-07-06 15:06:49,586][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpfuL0Ao
[2017-07-06 15:06:49,602][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 15:06:49,616][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 15:06:49,616][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 15:06:49,633][gdb3][DEBUG ] detect machine type
[2017-07-06 15:06:49,635][gdb3][DEBUG ] get remote short hostname
[2017-07-06 15:06:49,636][gdb3][DEBUG ] fetch remote file
[2017-07-06 15:06:49,637][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 15:06:49,703][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-07-06 15:06:49,919][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-07-06 15:06:50,085][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-07-06 15:06:50,251][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-07-06 15:06:50,418][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-07-06 15:06:50,584][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-07-06 15:06:50,750][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-07-06 15:06:50,916][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-07-06 15:06:51,082][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-07-06 15:06:51,248][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-07-06 15:06:51,414][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-07-06 15:06:51,414][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-07-06 15:06:51,415][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170706150651'
[2017-07-06 15:06:51,416][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-07-06 15:06:51,416][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-07-06 15:06:51,416][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-07-06 15:06:51,416][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpfuL0Ao
[2017-07-06 15:07:08,054][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-07-06 15:07:08,055][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 15:07:08,056][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:07:08,056][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffa2ac80998>
[2017-07-06 15:07:08,056][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:07:08,056][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 15:07:08,056][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ffa2aed6aa0>
[2017-07-06 15:07:08,056][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:07:08,056][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:07:08,056][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-07-06 15:07:08,056][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-07-06 15:07:08,313][gdb0][DEBUG ] connection detected need for sudo
[2017-07-06 15:07:08,516][gdb0][DEBUG ] connected to host: gdb0 
[2017-07-06 15:07:08,517][gdb0][DEBUG ] detect platform information from remote host
[2017-07-06 15:07:08,534][gdb0][DEBUG ] detect machine type
[2017-07-06 15:07:08,538][gdb0][DEBUG ] find the location of an executable
[2017-07-06 15:07:08,539][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 15:07:08,539][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-07-06 15:07:08,539][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:07:08,542][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-07-06 15:07:08,543][gdb0][DEBUG ] find the location of an executable
[2017-07-06 15:07:08,544][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-07-06 15:07:08,665][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 15:07:08,672][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 15:07:08,688][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 15:07:08,704][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 15:07:08,720][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 15:07:08,727][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:07:08,727][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:07:08,728][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:07:08,728][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-07-06 15:07:08,728][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-07-06 15:07:08,728][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-07-06 15:07:08,743][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-07-06 15:07:08,745][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-07-06 15:07:08,760][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-07-06 15:07:08,764][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:07:08,764][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-07-06 15:07:08,764][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:07:08,764][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb1
[2017-07-06 15:07:08,796][gdb0][DEBUG ] /dev/xvdb1: 4 bytes were erased at offset 0x00000000 (xfs): 58 46 53 42
[2017-07-06 15:07:08,796][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb1 bs=1M count=10
[2017-07-06 15:07:08,910][gdb0][WARNING] 10+0 records in
[2017-07-06 15:07:08,910][gdb0][WARNING] 10+0 records out
[2017-07-06 15:07:08,910][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.122993 s, 85.3 MB/s
[2017-07-06 15:07:08,910][gdb0][WARNING] command_check_call: Running command: /sbin/wipefs --all /dev/xvdb2
[2017-07-06 15:07:08,942][gdb0][WARNING] command_check_call: Running command: /bin/dd if=/dev/zero of=/dev/xvdb2 bs=1M count=10
[2017-07-06 15:07:08,942][gdb0][WARNING] 10+0 records in
[2017-07-06 15:07:08,943][gdb0][WARNING] 10+0 records out
[2017-07-06 15:07:08,943][gdb0][WARNING] 10485760 bytes (10 MB, 10 MiB) copied, 0.00465681 s, 2.3 GB/s
[2017-07-06 15:07:08,943][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-07-06 15:07:08,943][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-07-06 15:07:08,943][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-07-06 15:07:08,943][gdb0][WARNING] backup header from main header.
[2017-07-06 15:07:08,943][gdb0][WARNING] 
[2017-07-06 15:07:08,944][gdb0][WARNING] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[2017-07-06 15:07:08,944][gdb0][WARNING] on the recovery & transformation menu to examine the two tables.
[2017-07-06 15:07:08,944][gdb0][WARNING] 
[2017-07-06 15:07:08,944][gdb0][WARNING] Warning! One or more CRCs don't match. You should repair the disk!
[2017-07-06 15:07:08,944][gdb0][WARNING] 
[2017-07-06 15:07:10,011][gdb0][DEBUG ] ****************************************************************************
[2017-07-06 15:07:10,011][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-07-06 15:07:10,011][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-07-06 15:07:10,011][gdb0][DEBUG ] ****************************************************************************
[2017-07-06 15:07:10,012][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-07-06 15:07:10,012][gdb0][DEBUG ] other utilities.
[2017-07-06 15:07:10,012][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-07-06 15:07:11,029][gdb0][DEBUG ] Creating new GPT entries.
[2017-07-06 15:07:11,029][gdb0][DEBUG ] The operation has completed successfully.
[2017-07-06 15:07:11,029][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-07-06 15:07:11,029][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:07:11,029][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 15:07:11,061][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:07:11,063][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:07:11,063][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-07-06 15:07:11,063][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:07:11,063][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-07-06 15:07:11,063][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:07:11,064][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-07-06 15:07:11,064][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:00840453-8b24-41c0-9ae1-a00e162af2a1 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-07-06 15:07:12,081][gdb0][DEBUG ] Setting name!
[2017-07-06 15:07:12,081][gdb0][DEBUG ] partNum is 0
[2017-07-06 15:07:12,081][gdb0][DEBUG ] REALLY setting name!
[2017-07-06 15:07:12,081][gdb0][DEBUG ] The operation has completed successfully.
[2017-07-06 15:07:12,081][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-07-06 15:07:12,081][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:07:12,145][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 15:07:12,309][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:07:12,374][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:07:12,374][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:07:12,374][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-07-06 15:07:12,374][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-07-06 15:07:12,374][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-07-06 15:07:13,140][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8388543 blks
[2017-07-06 15:07:13,141][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-07-06 15:07:13,141][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-07-06 15:07:13,141][gdb0][DEBUG ] data     =                       bsize=4096   blocks=33554171, imaxpct=25
[2017-07-06 15:07:13,141][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-07-06 15:07:13,141][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-07-06 15:07:13,141][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=16383, version=2
[2017-07-06 15:07:13,141][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-07-06 15:07:13,141][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-07-06 15:07:13,141][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.1hFB4V with options noatime,inode64
[2017-07-06 15:07:13,141][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.1hFB4V
[2017-07-06 15:07:13,141][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.1hFB4V
[2017-07-06 15:07:13,142][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.1hFB4V/ceph_fsid.3120.tmp
[2017-07-06 15:07:13,142][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.1hFB4V/fsid.3120.tmp
[2017-07-06 15:07:13,142][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.1hFB4V/magic.3120.tmp
[2017-07-06 15:07:13,142][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.1hFB4V
[2017-07-06 15:07:13,143][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.1hFB4V
[2017-07-06 15:07:13,143][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.1hFB4V
[2017-07-06 15:07:13,175][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:07:13,175][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-07-06 15:07:14,192][gdb0][DEBUG ] The operation has completed successfully.
[2017-07-06 15:07:14,192][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-07-06 15:07:14,193][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:07:14,407][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 15:07:14,521][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:07:14,586][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-07-06 15:07:14,588][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 15:07:19,710][gdb0][INFO  ] checking OSD status...
[2017-07-06 15:07:19,711][gdb0][DEBUG ] find the location of an executable
[2017-07-06 15:07:19,713][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 15:07:19,828][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-07-06 15:07:19,995][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:07:19,995][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1 gdb3
[2017-07-06 15:07:19,995][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:07:19,995][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:07:19,995][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:07:19,996][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 15:07:19,996][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:07:19,996][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4e1942e5a8>
[2017-07-06 15:07:19,996][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:07:19,996][ceph_deploy.cli][INFO  ]  client                        : ['gdb1', 'gdb3']
[2017-07-06 15:07:19,996][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f4e19d45938>
[2017-07-06 15:07:19,996][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:07:19,996][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:07:19,996][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-07-06 15:07:20,257][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 15:07:20,491][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 15:07:20,492][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 15:07:20,508][gdb1][DEBUG ] detect machine type
[2017-07-06 15:07:20,512][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:07:20,513][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-07-06 15:07:20,514][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 15:07:20,529][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 15:07:20,543][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 15:07:20,543][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 15:07:20,560][gdb3][DEBUG ] detect machine type
[2017-07-06 15:07:20,562][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:07:20,564][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-07-06 15:08:36,609][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:08:36,609][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb3
[2017-07-06 15:08:36,609][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:08:36,609][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:08:36,609][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:08:36,610][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 15:08:36,610][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:08:36,610][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1a1d0465a8>
[2017-07-06 15:08:36,610][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:08:36,610][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb3']
[2017-07-06 15:08:36,610][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f1a1d95d938>
[2017-07-06 15:08:36,610][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:08:36,610][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:08:36,610][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-07-06 15:08:36,854][gdb0][DEBUG ] connection detected need for sudo
[2017-07-06 15:08:37,090][gdb0][DEBUG ] connected to host: gdb0 
[2017-07-06 15:08:37,091][gdb0][DEBUG ] detect platform information from remote host
[2017-07-06 15:08:37,107][gdb0][DEBUG ] detect machine type
[2017-07-06 15:08:37,111][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:08:37,114][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 15:08:37,129][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 15:08:37,143][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 15:08:37,143][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 15:08:37,160][gdb3][DEBUG ] detect machine type
[2017-07-06 15:08:37,162][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:09:20,073][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:09:20,073][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-07-06 15:09:20,073][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:09:20,073][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:09:20,073][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 15:09:20,073][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-07-06 15:09:20,073][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 15:09:20,073][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8eca0b3998>
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8eca309aa0>
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:09:20,074][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-07-06 15:09:20,075][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-07-06 15:09:20,314][gdb0][DEBUG ] connection detected need for sudo
[2017-07-06 15:09:20,542][gdb0][DEBUG ] connected to host: gdb0 
[2017-07-06 15:09:20,542][gdb0][DEBUG ] detect platform information from remote host
[2017-07-06 15:09:20,558][gdb0][DEBUG ] detect machine type
[2017-07-06 15:09:20,562][gdb0][DEBUG ] find the location of an executable
[2017-07-06 15:09:20,563][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 15:09:20,563][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-07-06 15:09:20,563][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:09:20,566][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-07-06 15:09:20,566][gdb0][DEBUG ] find the location of an executable
[2017-07-06 15:09:20,568][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-07-06 15:09:20,688][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 15:09:20,696][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 15:09:20,712][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 15:09:20,719][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 15:09:20,735][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 15:09:20,751][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:09:20,751][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:09:20,751][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:09:20,751][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-07-06 15:09:20,755][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-07-06 15:09:20,763][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-07-06 15:09:20,778][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-07-06 15:09:20,780][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:09:20,780][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-07-06 15:09:20,780][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:09:20,780][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-07-06 15:09:20,796][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-07-06 15:09:20,796][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-07-06 15:09:20,796][gdb0][WARNING] backup header from main header.
[2017-07-06 15:09:20,796][gdb0][WARNING] 
[2017-07-06 15:09:21,813][gdb0][DEBUG ] ****************************************************************************
[2017-07-06 15:09:21,813][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-07-06 15:09:21,813][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-07-06 15:09:21,813][gdb0][DEBUG ] ****************************************************************************
[2017-07-06 15:09:21,813][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-07-06 15:09:21,813][gdb0][DEBUG ] other utilities.
[2017-07-06 15:09:21,813][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-07-06 15:09:22,830][gdb0][DEBUG ] Creating new GPT entries.
[2017-07-06 15:09:22,831][gdb0][DEBUG ] The operation has completed successfully.
[2017-07-06 15:09:22,831][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-07-06 15:09:22,831][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:09:22,831][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 15:09:22,863][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:09:22,878][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:09:22,879][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-07-06 15:09:22,879][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:09:22,879][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-07-06 15:09:22,879][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:09:22,879][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-07-06 15:09:22,879][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:4937ba48-ae37-491c-a84c-071a26875bfc --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-07-06 15:09:23,896][gdb0][DEBUG ] Setting name!
[2017-07-06 15:09:23,896][gdb0][DEBUG ] partNum is 0
[2017-07-06 15:09:23,896][gdb0][DEBUG ] REALLY setting name!
[2017-07-06 15:09:23,896][gdb0][DEBUG ] The operation has completed successfully.
[2017-07-06 15:09:23,897][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-07-06 15:09:23,897][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:09:23,961][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 15:09:24,075][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:09:24,139][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:09:24,139][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:09:24,139][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-07-06 15:09:24,139][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-07-06 15:09:24,139][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-07-06 15:09:24,855][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8388543 blks
[2017-07-06 15:09:24,855][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-07-06 15:09:24,856][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-07-06 15:09:24,856][gdb0][DEBUG ] data     =                       bsize=4096   blocks=33554171, imaxpct=25
[2017-07-06 15:09:24,856][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-07-06 15:09:24,856][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-07-06 15:09:24,856][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=16383, version=2
[2017-07-06 15:09:24,856][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-07-06 15:09:24,856][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-07-06 15:09:24,856][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.2EMk4p with options noatime,inode64
[2017-07-06 15:09:24,856][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.2EMk4p
[2017-07-06 15:09:24,888][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.2EMk4p
[2017-07-06 15:09:24,888][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.2EMk4p/ceph_fsid.3748.tmp
[2017-07-06 15:09:24,888][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.2EMk4p/fsid.3748.tmp
[2017-07-06 15:09:24,888][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.2EMk4p/magic.3748.tmp
[2017-07-06 15:09:24,888][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.2EMk4p
[2017-07-06 15:09:24,888][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.2EMk4p
[2017-07-06 15:09:24,889][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.2EMk4p
[2017-07-06 15:09:24,920][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:09:24,920][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-07-06 15:09:25,937][gdb0][DEBUG ] The operation has completed successfully.
[2017-07-06 15:09:25,938][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-07-06 15:09:25,938][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:09:26,152][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 15:09:26,317][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:09:26,349][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-07-06 15:09:26,367][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 15:09:31,489][gdb0][INFO  ] checking OSD status...
[2017-07-06 15:09:31,490][gdb0][DEBUG ] find the location of an executable
[2017-07-06 15:09:31,493][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 15:09:31,658][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-07-06 15:09:31,820][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb3
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8cc1d395a8>
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb3']
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f8cc2650938>
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:09:31,821][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:09:31,822][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-07-06 15:09:32,063][gdb0][DEBUG ] connection detected need for sudo
[2017-07-06 15:09:32,257][gdb0][DEBUG ] connected to host: gdb0 
[2017-07-06 15:09:32,258][gdb0][DEBUG ] detect platform information from remote host
[2017-07-06 15:09:32,274][gdb0][DEBUG ] detect machine type
[2017-07-06 15:09:32,278][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:09:32,280][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 15:09:32,295][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 15:09:32,309][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 15:09:32,310][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 15:09:32,326][gdb3][DEBUG ] detect machine type
[2017-07-06 15:09:32,329][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:10:41,859][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:10:41,859][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk gdb0:/dev/xvdb
[2017-07-06 15:10:41,859][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:10:41,859][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:10:41,859][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 15:10:41,859][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-07-06 15:10:41,859][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 15:10:41,859][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe650347998>
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe65059daa0>
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:10:41,860][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-07-06 15:10:41,861][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-07-06 15:10:42,102][gdb0][DEBUG ] connection detected need for sudo
[2017-07-06 15:10:42,337][gdb0][DEBUG ] connected to host: gdb0 
[2017-07-06 15:10:42,337][gdb0][DEBUG ] detect platform information from remote host
[2017-07-06 15:10:42,353][gdb0][DEBUG ] detect machine type
[2017-07-06 15:10:42,357][gdb0][DEBUG ] find the location of an executable
[2017-07-06 15:10:42,358][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 15:10:42,358][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-07-06 15:10:42,358][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:10:42,361][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-07-06 15:10:42,361][gdb0][DEBUG ] create a keyring file
[2017-07-06 15:10:42,363][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-07-06 15:10:42,363][gdb0][DEBUG ] find the location of an executable
[2017-07-06 15:10:42,365][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-07-06 15:10:42,485][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 15:10:42,493][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 15:10:42,508][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 15:10:42,524][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 15:10:42,532][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:42,532][gdb0][WARNING] set_type: Will colocate journal with data on /dev/xvdb
[2017-07-06 15:10:42,532][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 15:10:42,548][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:42,548][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:42,548][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:42,548][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-07-06 15:10:42,551][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-07-06 15:10:42,567][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-07-06 15:10:42,568][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-07-06 15:10:42,584][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:42,584][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-07-06 15:10:42,585][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:42,585][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-07-06 15:10:42,585][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-07-06 15:10:42,585][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-07-06 15:10:42,585][gdb0][WARNING] backup header from main header.
[2017-07-06 15:10:42,585][gdb0][WARNING] 
[2017-07-06 15:10:43,652][gdb0][DEBUG ] ****************************************************************************
[2017-07-06 15:10:43,652][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-07-06 15:10:43,653][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-07-06 15:10:43,653][gdb0][DEBUG ] ****************************************************************************
[2017-07-06 15:10:43,653][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-07-06 15:10:43,653][gdb0][DEBUG ] other utilities.
[2017-07-06 15:10:43,653][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-07-06 15:10:44,620][gdb0][DEBUG ] Creating new GPT entries.
[2017-07-06 15:10:44,620][gdb0][DEBUG ] The operation has completed successfully.
[2017-07-06 15:10:44,620][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-07-06 15:10:44,620][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:10:44,636][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 15:10:44,668][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:10:44,683][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:44,683][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:44,684][gdb0][WARNING] ptype_tobe_for_name: name = journal
[2017-07-06 15:10:44,684][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:44,684][gdb0][WARNING] create_partition: Creating journal partition num 2 size 5120 on /dev/xvdb
[2017-07-06 15:10:44,684][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:21e870fa-fa21-42be-89f2-fc4cc18f5faa --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-07-06 15:10:45,701][gdb0][DEBUG ] Setting name!
[2017-07-06 15:10:45,701][gdb0][DEBUG ] partNum is 1
[2017-07-06 15:10:45,701][gdb0][DEBUG ] REALLY setting name!
[2017-07-06 15:10:45,701][gdb0][DEBUG ] The operation has completed successfully.
[2017-07-06 15:10:45,701][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-07-06 15:10:45,701][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:10:45,916][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 15:10:46,030][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:10:46,046][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:46,046][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:46,046][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-07-06 15:10:46,046][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/21e870fa-fa21-42be-89f2-fc4cc18f5faa
[2017-07-06 15:10:46,046][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-07-06 15:10:47,114][gdb0][DEBUG ] The operation has completed successfully.
[2017-07-06 15:10:47,114][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-07-06 15:10:47,114][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:10:47,278][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 15:10:47,443][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:10:47,657][gdb0][WARNING] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/21e870fa-fa21-42be-89f2-fc4cc18f5faa
[2017-07-06 15:10:47,657][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:47,658][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-07-06 15:10:47,658][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:47,658][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-07-06 15:10:47,658][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:47,658][gdb0][WARNING] create_partition: Creating data partition num 1 size 0 on /dev/xvdb
[2017-07-06 15:10:47,658][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:9838060b-d02e-4289-a96d-badf10ffe991 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-07-06 15:10:48,675][gdb0][DEBUG ] Setting name!
[2017-07-06 15:10:48,675][gdb0][DEBUG ] partNum is 0
[2017-07-06 15:10:48,675][gdb0][DEBUG ] REALLY setting name!
[2017-07-06 15:10:48,675][gdb0][DEBUG ] The operation has completed successfully.
[2017-07-06 15:10:48,675][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-07-06 15:10:48,675][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:10:48,890][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 15:10:49,054][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:10:49,269][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:49,269][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:49,269][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-07-06 15:10:49,269][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-07-06 15:10:49,269][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-07-06 15:10:49,985][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=8060863 blks
[2017-07-06 15:10:49,986][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-07-06 15:10:49,986][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-07-06 15:10:49,986][gdb0][DEBUG ] data     =                       bsize=4096   blocks=32243451, imaxpct=25
[2017-07-06 15:10:49,986][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-07-06 15:10:49,986][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-07-06 15:10:49,986][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=15743, version=2
[2017-07-06 15:10:49,986][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-07-06 15:10:49,986][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-07-06 15:10:49,986][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.YmvMJO with options noatime,inode64
[2017-07-06 15:10:49,986][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.YmvMJO
[2017-07-06 15:10:50,002][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.YmvMJO
[2017-07-06 15:10:50,002][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.YmvMJO/ceph_fsid.4357.tmp
[2017-07-06 15:10:50,002][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.YmvMJO/fsid.4357.tmp
[2017-07-06 15:10:50,004][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.YmvMJO/magic.4357.tmp
[2017-07-06 15:10:50,007][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.YmvMJO/journal_uuid.4357.tmp
[2017-07-06 15:10:50,008][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.YmvMJO/journal -> /dev/disk/by-partuuid/21e870fa-fa21-42be-89f2-fc4cc18f5faa
[2017-07-06 15:10:50,008][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.YmvMJO
[2017-07-06 15:10:50,012][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.YmvMJO
[2017-07-06 15:10:50,012][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.YmvMJO
[2017-07-06 15:10:50,044][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-07-06 15:10:50,044][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-07-06 15:10:51,061][gdb0][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-07-06 15:10:51,061][gdb0][DEBUG ] The new table will be used at the next reboot or after you
[2017-07-06 15:10:51,061][gdb0][DEBUG ] run partprobe(8) or kpartx(8)
[2017-07-06 15:10:51,061][gdb0][DEBUG ] The operation has completed successfully.
[2017-07-06 15:10:51,061][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-07-06 15:10:51,061][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:10:51,061][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-07-06 15:10:51,276][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-07-06 15:10:51,308][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-07-06 15:10:51,318][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 15:10:56,440][gdb0][INFO  ] checking OSD status...
[2017-07-06 15:10:56,440][gdb0][DEBUG ] find the location of an executable
[2017-07-06 15:10:56,443][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 15:10:56,608][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-07-06 15:10:56,772][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb3
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f17abcfd5a8>
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb3']
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f17ac614938>
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:10:56,773][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:10:56,774][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-07-06 15:10:57,019][gdb0][DEBUG ] connection detected need for sudo
[2017-07-06 15:10:57,242][gdb0][DEBUG ] connected to host: gdb0 
[2017-07-06 15:10:57,242][gdb0][DEBUG ] detect platform information from remote host
[2017-07-06 15:10:57,258][gdb0][DEBUG ] detect machine type
[2017-07-06 15:10:57,262][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:10:57,265][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 15:10:57,280][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 15:10:57,294][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 15:10:57,295][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 15:10:57,311][gdb3][DEBUG ] detect machine type
[2017-07-06 15:10:57,314][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:35:35,663][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy rgw create gdb0
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ]  rgw                           : [('gdb0', 'rgw.gdb0')]
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff36f2b1830>
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ]  func                          : <function rgw at 0x7ff36f972758>
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:35:35,664][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:35:35,665][ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts gdb0:rgw.gdb0
[2017-07-06 15:35:35,898][gdb0][DEBUG ] connection detected need for sudo
[2017-07-06 15:35:36,126][gdb0][DEBUG ] connected to host: gdb0 
[2017-07-06 15:35:36,126][gdb0][DEBUG ] detect platform information from remote host
[2017-07-06 15:35:36,142][gdb0][DEBUG ] detect machine type
[2017-07-06 15:35:36,146][ceph_deploy.rgw][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 15:35:36,146][ceph_deploy.rgw][DEBUG ] remote host will use systemd
[2017-07-06 15:35:36,146][ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to gdb0
[2017-07-06 15:35:36,146][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:35:36,150][gdb0][WARNING] rgw keyring does not exist yet, creating one
[2017-07-06 15:35:36,150][gdb0][DEBUG ] create a keyring file
[2017-07-06 15:35:36,151][gdb0][DEBUG ] create path recursively if it doesn't exist
[2017-07-06 15:35:36,154][gdb0][INFO  ] Running command: sudo ceph --cluster ceph --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create client.rgw.gdb0 osd allow rwx mon allow rw -o /var/lib/ceph/radosgw/ceph-rgw.gdb0/keyring
[2017-07-06 15:35:36,324][gdb0][INFO  ] Running command: sudo systemctl enable ceph-radosgw@rgw.gdb0
[2017-07-06 15:35:36,336][gdb0][WARNING] Created symlink from /etc/systemd/system/ceph-radosgw.target.wants/ceph-radosgw@rgw.gdb0.service to /lib/systemd/system/ceph-radosgw@.service.
[2017-07-06 15:35:36,402][gdb0][INFO  ] Running command: sudo systemctl start ceph-radosgw@rgw.gdb0
[2017-07-06 15:35:36,470][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 15:35:36,587][ceph_deploy.rgw][INFO  ] The Ceph Object Gateway (RGW) is now running on host gdb0 and default port 7480
[2017-07-06 15:54:28,597][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 15:54:28,597][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy rgw create gdb1
[2017-07-06 15:54:28,597][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 15:54:28,597][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 15:54:28,598][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 15:54:28,598][ceph_deploy.cli][INFO  ]  rgw                           : [('gdb1', 'rgw.gdb1')]
[2017-07-06 15:54:28,598][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 15:54:28,598][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-07-06 15:54:28,598][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 15:54:28,598][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdfa64af830>
[2017-07-06 15:54:28,598][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 15:54:28,598][ceph_deploy.cli][INFO  ]  func                          : <function rgw at 0x7fdfa6b70758>
[2017-07-06 15:54:28,598][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 15:54:28,598][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 15:54:28,598][ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts gdb1:rgw.gdb1
[2017-07-06 15:54:28,859][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 15:54:29,101][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 15:54:29,101][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 15:54:29,119][gdb1][DEBUG ] detect machine type
[2017-07-06 15:54:29,123][ceph_deploy.rgw][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 15:54:29,123][ceph_deploy.rgw][DEBUG ] remote host will use systemd
[2017-07-06 15:54:29,124][ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to gdb1
[2017-07-06 15:54:29,124][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 15:54:29,126][ceph_deploy.rgw][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-07-06 15:54:29,126][ceph_deploy][ERROR ] GenericError: Failed to create 1 RGWs

