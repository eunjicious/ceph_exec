[2017-05-31 16:34:09,790][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:34:09,790][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 16:34:09,790][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:34:09,790][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f407344afc8>
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f4073d5d1b8>
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:34:09,791][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:34:09,791][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 16:34:09,791][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 16:34:09,791][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 16:34:09,791][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 16:34:09,818][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:34:09,833][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:34:09,834][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:34:09,850][gdb3][DEBUG ] detect machine type
[2017-05-31 16:34:09,853][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:34:09,853][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 16:34:09,854][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 16:34:09,893][gdb3][DEBUG ] Reading package lists...
[2017-05-31 16:34:10,057][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 16:34:10,057][gdb3][DEBUG ] Reading state information...
[2017-05-31 16:34:10,171][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 16:34:10,172][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 16:34:10,172][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 16:34:10,172][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 16:34:10,172][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 16:34:10,172][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 16:34:10,173][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 16:34:10,173][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 16:34:10,173][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 16:34:10,174][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 16:34:10,288][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 16:34:10,288][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 16:34:10,356][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 16:34:10,357][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 16:34:10,521][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 16:34:10,635][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 16:34:10,635][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 16:34:10,850][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 16:34:10,964][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 16:34:11,129][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 16:34:11,193][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 16:34:11,209][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 16:34:11,375][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 16:34:11,439][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 16:34:11,455][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 16:34:11,624][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 16:34:11,656][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 16:34:11,821][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 16:34:11,885][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 16:34:11,885][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 16:34:12,000][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 16:34:12,816][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 16:34:12,983][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe8d7689710>
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fe8d7f96230>
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:34:12,984][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:34:12,984][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 16:34:13,011][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:34:13,026][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:34:13,026][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:34:13,043][gdb3][DEBUG ] detect machine type
[2017-05-31 16:34:13,045][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:34:13,061][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:34:13,076][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:34:13,077][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:34:13,093][gdb3][DEBUG ] detect machine type
[2017-05-31 16:34:13,096][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:34:13,096][gdb3][INFO  ] purging data on gdb3
[2017-05-31 16:34:13,097][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:34:13,110][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 16:34:13,281][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f329227f9e0>
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f3292b42848>
[2017-05-31 16:34:13,282][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:34:13,283][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:35:36,302][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:35:36,302][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6a720a6560>
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:35:36,303][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f6a7272a758>
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:35:36,304][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 16:35:36,304][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 16:35:36,305][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 16:35:36,331][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:35:36,345][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:35:36,345][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:35:36,362][gdb3][DEBUG ] detect machine type
[2017-05-31 16:35:36,364][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:35:36,365][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 16:35:36,377][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 16:35:36,383][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 16:35:36,383][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 16:35:36,383][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 16:35:36,384][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 16:35:36,384][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 16:35:36,384][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 16:35:36,384][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 16:35:36,384][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 16:35:36,550][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:35:36,550][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 16:35:36,550][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:35:36,550][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa11d0dce60>
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fa11d0b1b18>
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:35:36,551][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 16:35:36,552][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:35:36,552][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 16:35:36,552][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 16:35:36,579][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:35:36,592][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:35:36,593][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:35:36,609][gdb3][DEBUG ] detect machine type
[2017-05-31 16:35:36,612][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:35:36,612][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 16:35:36,612][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 16:35:36,612][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:35:36,613][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 16:35:36,613][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:35:36,613][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 16:35:36,614][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:35:36,615][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 16:35:36,615][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 16:35:36,616][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 16:35:36,616][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 16:35:36,616][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 16:35:36,617][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 16:35:36,656][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 16:35:36,656][gdb3][DEBUG ] ceph-mon: set fsid to e52d1ea0-de58-48ed-b1fd-547c31561e69
[2017-05-31 16:35:36,663][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 16:35:36,663][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 16:35:36,664][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 16:35:36,664][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 16:35:36,665][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:35:36,733][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 16:35:36,805][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 16:35:38,875][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:35:38,940][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 16:35:38,940][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 16:35:38,940][gdb3][DEBUG ] {
[2017-05-31 16:35:38,940][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]   "features": {
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 16:35:38,941][gdb3][DEBUG ]       "kraken", 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]       "luminous"
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     ], 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 16:35:38,941][gdb3][DEBUG ]       "kraken", 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]       "luminous"
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     ]
[2017-05-31 16:35:38,941][gdb3][DEBUG ]   }, 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]   "monmap": {
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "created": "2017-05-31 16:35:36.641607", 
[2017-05-31 16:35:38,941][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]     "features": {
[2017-05-31 16:35:38,942][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]       "persistent": [
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "kraken", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "luminous"
[2017-05-31 16:35:38,942][gdb3][DEBUG ]       ]
[2017-05-31 16:35:38,942][gdb3][DEBUG ]     }, 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]     "fsid": "e52d1ea0-de58-48ed-b1fd-547c31561e69", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]     "modified": "2017-05-31 16:35:36.894213", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]     "mons": [
[2017-05-31 16:35:38,942][gdb3][DEBUG ]       {
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 16:35:38,942][gdb3][DEBUG ]         "rank": 0
[2017-05-31 16:35:38,942][gdb3][DEBUG ]       }
[2017-05-31 16:35:38,943][gdb3][DEBUG ]     ]
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   }, 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "quorum": [
[2017-05-31 16:35:38,943][gdb3][DEBUG ]     0
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   ], 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 16:35:38,943][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 16:35:38,943][gdb3][DEBUG ] }
[2017-05-31 16:35:38,943][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 16:35:38,943][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 16:35:38,944][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:35:39,009][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 16:35:39,025][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:35:39,039][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:35:39,039][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:35:39,056][gdb3][DEBUG ] detect machine type
[2017-05-31 16:35:39,058][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:35:39,060][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:35:39,125][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 16:35:39,125][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 16:35:39,125][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 16:35:39,127][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmphK0O18
[2017-05-31 16:35:39,143][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:35:39,158][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:35:39,158][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:35:39,175][gdb3][DEBUG ] detect machine type
[2017-05-31 16:35:39,177][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:35:39,177][gdb3][DEBUG ] fetch remote file
[2017-05-31 16:35:39,179][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:35:39,245][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 16:35:39,411][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 16:35:39,577][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 16:35:39,744][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 16:35:39,960][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 16:35:40,127][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 16:35:40,293][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 16:35:40,459][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 16:35:40,625][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 16:35:40,792][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 16:35:40,957][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 16:35:40,957][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 16:35:40,957][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mgr.keyring
[2017-05-31 16:35:40,958][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 16:35:40,958][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 16:35:40,958][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 16:35:40,958][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmphK0O18
[2017-05-31 16:35:41,140][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:35:41,140][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8159351518>
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f8159c68938>
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:35:41,141][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:35:41,141][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 16:35:41,168][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:35:41,182][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:35:41,182][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:35:41,198][gdb3][DEBUG ] detect machine type
[2017-05-31 16:35:41,201][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:36:04,282][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:36:04,282][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff46122a908>
[2017-05-31 16:36:04,283][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:36:04,284][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:36:04,284][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff461480aa0>
[2017-05-31 16:36:04,284][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:36:04,284][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:36:04,284][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:36:04,284][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-31 16:36:04,526][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:36:04,718][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:36:04,719][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:36:04,734][gdb0][DEBUG ] detect machine type
[2017-05-31 16:36:04,738][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:36:04,739][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:36:04,739][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 16:36:04,739][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:36:04,741][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-31 16:36:04,741][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:36:04,743][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:36:04,864][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:36:04,867][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:36:04,867][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:36:04,867][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:36:04,883][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:36:04,884][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:36:04,900][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:36:04,900][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:36:04,900][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:36:04,900][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 16:36:04,901][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 16:36:04,901][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 16:36:04,901][gdb0][WARNING]     args.func(args)
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 16:36:04,901][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2018, in prepare
[2017-05-31 16:36:04,901][gdb0][WARNING]     self.prepare_locked()
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2090, in prepare_locked
[2017-05-31 16:36:04,901][gdb0][WARNING]     self.data.prepare(*to_prepare_list)
[2017-05-31 16:36:04,901][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2816, in prepare
[2017-05-31 16:36:04,902][gdb0][WARNING]     self.prepare_device(*to_prepare_list)
[2017-05-31 16:36:04,902][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2984, in prepare_device
[2017-05-31 16:36:04,902][gdb0][WARNING]     super(PrepareBluestoreData, self).prepare_device(*to_prepare_list)
[2017-05-31 16:36:04,902][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2878, in prepare_device
[2017-05-31 16:36:04,902][gdb0][WARNING]     self.sanity_checks()
[2017-05-31 16:36:04,902][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2841, in sanity_checks
[2017-05-31 16:36:04,902][gdb0][WARNING]     check_partitions=not self.args.dmcrypt)
[2017-05-31 16:36:04,902][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-05-31 16:36:04,902][gdb0][WARNING]     raise Error('Device is mounted', partition)
[2017-05-31 16:36:04,902][gdb0][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-05-31 16:36:04,906][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 16:36:04,906][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:36:04,906][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:39:15,638][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:39:15,639][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7ac7e0b908>
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7ac8061aa0>
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:39:15,640][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:39:15,640][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-31 16:39:15,875][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:39:16,106][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:39:16,106][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:39:16,122][gdb0][DEBUG ] detect machine type
[2017-05-31 16:39:16,126][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:39:16,126][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:39:16,126][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 16:39:16,127][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:39:16,129][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:39:16,129][gdb0][DEBUG ] create a keyring file
[2017-05-31 16:39:16,131][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-31 16:39:16,131][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:39:16,133][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:39:16,254][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:39:16,257][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:16,257][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:39:16,257][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:39:16,265][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:39:16,281][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:39:16,282][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:16,282][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:16,282][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:16,282][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 16:39:16,282][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 16:39:16,283][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 16:39:16,283][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 16:39:16,283][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 16:39:16,283][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 16:39:16,283][gdb0][WARNING]     args.func(args)
[2017-05-31 16:39:16,284][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 16:39:16,284][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 16:39:16,284][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2018, in prepare
[2017-05-31 16:39:16,284][gdb0][WARNING]     self.prepare_locked()
[2017-05-31 16:39:16,284][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2090, in prepare_locked
[2017-05-31 16:39:16,284][gdb0][WARNING]     self.data.prepare(*to_prepare_list)
[2017-05-31 16:39:16,285][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2816, in prepare
[2017-05-31 16:39:16,285][gdb0][WARNING]     self.prepare_device(*to_prepare_list)
[2017-05-31 16:39:16,285][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2984, in prepare_device
[2017-05-31 16:39:16,285][gdb0][WARNING]     super(PrepareBluestoreData, self).prepare_device(*to_prepare_list)
[2017-05-31 16:39:16,285][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2878, in prepare_device
[2017-05-31 16:39:16,285][gdb0][WARNING]     self.sanity_checks()
[2017-05-31 16:39:16,285][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2841, in sanity_checks
[2017-05-31 16:39:16,285][gdb0][WARNING]     check_partitions=not self.args.dmcrypt)
[2017-05-31 16:39:16,285][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 944, in verify_not_in_use
[2017-05-31 16:39:16,285][gdb0][WARNING]     raise Error('Device is mounted', partition)
[2017-05-31 16:39:16,286][gdb0][WARNING] ceph_disk.main.Error: Error: Device is mounted: /dev/xvdb1
[2017-05-31 16:39:16,293][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 16:39:16,293][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:39:16,293][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:39:39,078][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb0:/dev/xvdb
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/dev/xvdb', None)]
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:39:39,079][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5011019908>
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f501126faa0>
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:39:39,080][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:39:39,080][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/dev/xvdb:
[2017-05-31 16:39:39,322][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:39:39,549][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:39:39,550][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:39:39,565][gdb0][DEBUG ] detect machine type
[2017-05-31 16:39:39,569][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:39:39,570][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:39:39,570][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 16:39:39,570][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:39:39,572][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:39:39,572][gdb0][DEBUG ] create a keyring file
[2017-05-31 16:39:39,574][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /dev/xvdb journal None activate True
[2017-05-31 16:39:39,574][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:39:39,576][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:39:39,696][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:39:39,700][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,700][gdb0][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:39:39,700][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:39:39,716][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:39:39,717][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:39:39,733][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,733][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,733][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,733][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-31 16:39:39,734][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-31 16:39:39,742][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-31 16:39:39,758][gdb0][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-31 16:39:39,759][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,759][gdb0][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-31 16:39:39,759][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:39,759][gdb0][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-31 16:39:39,760][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-31 16:39:39,764][gdb0][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-31 16:39:39,764][gdb0][WARNING] backup header from main header.
[2017-05-31 16:39:39,764][gdb0][WARNING] 
[2017-05-31 16:39:40,831][gdb0][DEBUG ] ****************************************************************************
[2017-05-31 16:39:40,831][gdb0][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-31 16:39:40,832][gdb0][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-31 16:39:40,832][gdb0][DEBUG ] ****************************************************************************
[2017-05-31 16:39:40,832][gdb0][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-31 16:39:40,832][gdb0][DEBUG ] other utilities.
[2017-05-31 16:39:40,832][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-31 16:39:41,849][gdb0][DEBUG ] Creating new GPT entries.
[2017-05-31 16:39:41,849][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:39:41,850][gdb0][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-31 16:39:41,850][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:41,853][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:39:41,885][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:41,888][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:41,888][gdb0][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-31 16:39:41,889][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:41,889][gdb0][WARNING] ptype_tobe_for_name: name = data
[2017-05-31 16:39:41,889][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:41,889][gdb0][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-31 16:39:41,889][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:7e7e4d4e-fb27-4154-a27a-e2f06d039b76 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-31 16:39:42,906][gdb0][DEBUG ] Setting name!
[2017-05-31 16:39:42,906][gdb0][DEBUG ] partNum is 0
[2017-05-31 16:39:42,906][gdb0][DEBUG ] REALLY setting name!
[2017-05-31 16:39:42,906][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:39:42,907][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:39:42,907][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:42,971][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:39:43,085][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] ptype_tobe_for_name: name = block
[2017-05-31 16:39:43,117][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:43,117][gdb0][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-31 16:39:43,117][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:8d4465c8-eb33-4fae-bb6c-98aeb5b9cc73 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-31 16:39:44,135][gdb0][DEBUG ] Setting name!
[2017-05-31 16:39:44,135][gdb0][DEBUG ] partNum is 1
[2017-05-31 16:39:44,135][gdb0][DEBUG ] REALLY setting name!
[2017-05-31 16:39:44,135][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:39:44,135][gdb0][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:39:44,135][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:44,350][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:39:44,615][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:44,830][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:44,830][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:44,830][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-31 16:39:44,830][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/8d4465c8-eb33-4fae-bb6c-98aeb5b9cc73
[2017-05-31 16:39:44,830][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-31 16:39:45,847][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:39:45,848][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:39:45,848][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:46,062][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:39:46,277][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:46,492][gdb0][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/8d4465c8-eb33-4fae-bb6c-98aeb5b9cc73
[2017-05-31 16:39:46,492][gdb0][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-31 16:39:46,492][gdb0][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-31 16:39:46,556][gdb0][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-31 16:39:46,556][gdb0][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-31 16:39:46,556][gdb0][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-31 16:39:46,556][gdb0][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-31 16:39:46,556][gdb0][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-31 16:39:46,556][gdb0][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-31 16:39:46,556][gdb0][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-31 16:39:46,557][gdb0][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-31 16:39:46,557][gdb0][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-31 16:39:46,557][gdb0][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.umpsaG with options noatime,inode64
[2017-05-31 16:39:46,557][gdb0][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.umpsaG
[2017-05-31 16:39:46,572][gdb0][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.umpsaG
[2017-05-31 16:39:46,576][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG/ceph_fsid.8078.tmp
[2017-05-31 16:39:46,579][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG/fsid.8078.tmp
[2017-05-31 16:39:46,587][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG/magic.8078.tmp
[2017-05-31 16:39:46,587][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG/block_uuid.8078.tmp
[2017-05-31 16:39:46,588][gdb0][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.umpsaG/block -> /dev/disk/by-partuuid/8d4465c8-eb33-4fae-bb6c-98aeb5b9cc73
[2017-05-31 16:39:46,589][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG/type.8078.tmp
[2017-05-31 16:39:46,591][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.umpsaG
[2017-05-31 16:39:46,594][gdb0][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.umpsaG
[2017-05-31 16:39:46,594][gdb0][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.umpsaG
[2017-05-31 16:39:46,626][gdb0][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:39:46,626][gdb0][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-31 16:39:47,693][gdb0][DEBUG ] The operation has completed successfully.
[2017-05-31 16:39:47,694][gdb0][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:39:47,694][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:47,858][gdb0][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:39:48,073][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:39:48,338][gdb0][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-31 16:39:48,340][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:39:53,462][gdb0][INFO  ] checking OSD status...
[2017-05-31 16:39:53,462][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:39:53,465][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 16:39:53,530][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 16:40:00,896][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create --zap-disk --bluestore gdb1:/dev/xvdb
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/dev/xvdb', None)]
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  bluestore                     : True
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:40:00,897][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6d2b948908>
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6d2bb9eaa0>
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:40:00,898][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 16:40:00,898][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/dev/xvdb:
[2017-05-31 16:40:01,144][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:40:01,379][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:40:01,379][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:40:01,396][gdb1][DEBUG ] detect machine type
[2017-05-31 16:40:01,400][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:40:01,401][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:40:01,401][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 16:40:01,401][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:40:01,403][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:40:01,404][gdb1][DEBUG ] create a keyring file
[2017-05-31 16:40:01,405][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /dev/xvdb journal None activate True
[2017-05-31 16:40:01,405][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:40:01,407][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --zap-disk --bluestore --cluster ceph --fs-type xfs -- /dev/xvdb
[2017-05-31 16:40:01,578][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:40:01,578][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,578][gdb1][WARNING] set_type: Will colocate block with data on /dev/xvdb
[2017-05-31 16:40:01,578][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_size
[2017-05-31 16:40:01,578][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_db_size
[2017-05-31 16:40:01,579][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup bluestore_block_wal_size
[2017-05-31 16:40:01,579][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,579][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,579][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,579][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[2017-05-31 16:40:01,579][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[2017-05-31 16:40:01,586][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[2017-05-31 16:40:01,594][gdb1][WARNING] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[2017-05-31 16:40:01,601][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,602][gdb1][WARNING] zap: Writing zeros to existing partitions on /dev/xvdb
[2017-05-31 16:40:01,602][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:01,602][gdb1][WARNING] zap: Zapping partition table on /dev/xvdb
[2017-05-31 16:40:01,602][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --zap-all -- /dev/xvdb
[2017-05-31 16:40:01,605][gdb1][WARNING] Caution: invalid backup GPT header, but valid main header; regenerating
[2017-05-31 16:40:01,605][gdb1][WARNING] backup header from main header.
[2017-05-31 16:40:01,605][gdb1][WARNING] 
[2017-05-31 16:40:02,673][gdb1][DEBUG ] ****************************************************************************
[2017-05-31 16:40:02,673][gdb1][DEBUG ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[2017-05-31 16:40:02,673][gdb1][DEBUG ] verification and recovery are STRONGLY recommended.
[2017-05-31 16:40:02,673][gdb1][DEBUG ] ****************************************************************************
[2017-05-31 16:40:02,673][gdb1][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or
[2017-05-31 16:40:02,674][gdb1][DEBUG ] other utilities.
[2017-05-31 16:40:02,674][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/xvdb
[2017-05-31 16:40:03,641][gdb1][DEBUG ] Creating new GPT entries.
[2017-05-31 16:40:03,641][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:40:03,641][gdb1][WARNING] update_partition: Calling partprobe on zapped device /dev/xvdb
[2017-05-31 16:40:03,641][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:03,657][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:40:03,689][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:03,692][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:03,692][gdb1][WARNING] set_data_partition: Creating osd partition on /dev/xvdb
[2017-05-31 16:40:03,692][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:03,693][gdb1][WARNING] ptype_tobe_for_name: name = data
[2017-05-31 16:40:03,693][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:03,693][gdb1][WARNING] create_partition: Creating data partition num 1 size 100 on /dev/xvdb
[2017-05-31 16:40:03,693][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --new=1:0:+100M --change-name=1:ceph data --partition-guid=1:03829711-0e84-467b-b718-52c76eb24320 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/xvdb
[2017-05-31 16:40:04,710][gdb1][DEBUG ] Setting name!
[2017-05-31 16:40:04,710][gdb1][DEBUG ] partNum is 0
[2017-05-31 16:40:04,710][gdb1][DEBUG ] REALLY setting name!
[2017-05-31 16:40:04,711][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:40:04,711][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:40:04,711][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:04,775][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:40:04,889][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb1 uuid path is /sys/dev/block/202:17/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] ptype_tobe_for_name: name = block
[2017-05-31 16:40:04,905][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:04,905][gdb1][WARNING] create_partition: Creating block partition num 2 size 0 on /dev/xvdb
[2017-05-31 16:40:04,906][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --largest-new=2 --change-name=2:ceph block --partition-guid=2:ac2260d3-0972-46d4-9786-44e431798028 --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/xvdb
[2017-05-31 16:40:05,923][gdb1][DEBUG ] Setting name!
[2017-05-31 16:40:05,923][gdb1][DEBUG ] partNum is 1
[2017-05-31 16:40:05,923][gdb1][DEBUG ] REALLY setting name!
[2017-05-31 16:40:05,923][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:40:05,924][gdb1][WARNING] update_partition: Calling partprobe on created device /dev/xvdb
[2017-05-31 16:40:05,924][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:06,138][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:40:06,353][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:06,567][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:06,568][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:06,568][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb2 uuid path is /sys/dev/block/202:18/dm/uuid
[2017-05-31 16:40:06,568][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/ac2260d3-0972-46d4-9786-44e431798028
[2017-05-31 16:40:06,568][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=2:cafecafe-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdb
[2017-05-31 16:40:07,585][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:40:07,585][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:40:07,585][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:07,800][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:40:07,965][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:08,179][gdb1][WARNING] prepare_device: Block is GPT partition /dev/disk/by-partuuid/ac2260d3-0972-46d4-9786-44e431798028
[2017-05-31 16:40:08,179][gdb1][WARNING] populate_data_path_device: Creating xfs fs on /dev/xvdb1
[2017-05-31 16:40:08,179][gdb1][WARNING] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/xvdb1
[2017-05-31 16:40:08,243][gdb1][DEBUG ] meta-data=/dev/xvdb1             isize=2048   agcount=4, agsize=6400 blks
[2017-05-31 16:40:08,244][gdb1][DEBUG ]          =                       sectsz=512   attr=2, projid32bit=1
[2017-05-31 16:40:08,244][gdb1][DEBUG ]          =                       crc=1        finobt=1, sparse=0
[2017-05-31 16:40:08,244][gdb1][DEBUG ] data     =                       bsize=4096   blocks=25600, imaxpct=25
[2017-05-31 16:40:08,244][gdb1][DEBUG ]          =                       sunit=0      swidth=0 blks
[2017-05-31 16:40:08,244][gdb1][DEBUG ] naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
[2017-05-31 16:40:08,244][gdb1][DEBUG ] log      =internal log           bsize=4096   blocks=864, version=2
[2017-05-31 16:40:08,244][gdb1][DEBUG ]          =                       sectsz=512   sunit=0 blks, lazy-count=1
[2017-05-31 16:40:08,244][gdb1][DEBUG ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[2017-05-31 16:40:08,244][gdb1][WARNING] mount: Mounting /dev/xvdb1 on /var/lib/ceph/tmp/mnt.KBPkbB with options noatime,inode64
[2017-05-31 16:40:08,244][gdb1][WARNING] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/xvdb1 /var/lib/ceph/tmp/mnt.KBPkbB
[2017-05-31 16:40:08,276][gdb1][WARNING] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.KBPkbB
[2017-05-31 16:40:08,276][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB/ceph_fsid.11280.tmp
[2017-05-31 16:40:08,277][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB/fsid.11280.tmp
[2017-05-31 16:40:08,278][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB/magic.11280.tmp
[2017-05-31 16:40:08,281][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB/block_uuid.11280.tmp
[2017-05-31 16:40:08,282][gdb1][WARNING] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.KBPkbB/block -> /dev/disk/by-partuuid/ac2260d3-0972-46d4-9786-44e431798028
[2017-05-31 16:40:08,283][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB/type.11280.tmp
[2017-05-31 16:40:08,286][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.KBPkbB
[2017-05-31 16:40:08,287][gdb1][WARNING] unmount: Unmounting /var/lib/ceph/tmp/mnt.KBPkbB
[2017-05-31 16:40:08,287][gdb1][WARNING] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.KBPkbB
[2017-05-31 16:40:08,351][gdb1][WARNING] get_dm_uuid: get_dm_uuid /dev/xvdb uuid path is /sys/dev/block/202:16/dm/uuid
[2017-05-31 16:40:08,351][gdb1][WARNING] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdb
[2017-05-31 16:40:09,369][gdb1][DEBUG ] Warning: The kernel is still using the old partition table.
[2017-05-31 16:40:09,369][gdb1][DEBUG ] The new table will be used at the next reboot or after you
[2017-05-31 16:40:09,369][gdb1][DEBUG ] run partprobe(8) or kpartx(8)
[2017-05-31 16:40:09,369][gdb1][DEBUG ] The operation has completed successfully.
[2017-05-31 16:40:09,369][gdb1][WARNING] update_partition: Calling partprobe on prepared device /dev/xvdb
[2017-05-31 16:40:09,369][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:09,369][gdb1][WARNING] command: Running command: /usr/bin/flock -s /dev/xvdb /sbin/partprobe /dev/xvdb
[2017-05-31 16:40:09,534][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[2017-05-31 16:40:09,537][gdb1][WARNING] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match xvdb1
[2017-05-31 16:40:09,555][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:40:14,677][gdb1][INFO  ] checking OSD status...
[2017-05-31 16:40:14,677][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:40:14,680][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 16:40:14,795][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 16:46:47,903][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:46:47,903][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 16:46:47,903][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:46:47,903][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7a17388fc8>
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f7a17c9b1b8>
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:46:47,904][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:46:47,904][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 16:46:47,904][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 16:46:47,904][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 16:46:47,904][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 16:46:47,931][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:46:47,945][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:46:47,946][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:46:47,962][gdb3][DEBUG ] detect machine type
[2017-05-31 16:46:47,965][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:46:47,965][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 16:46:47,966][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 16:46:48,004][gdb3][DEBUG ] Reading package lists...
[2017-05-31 16:46:48,168][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 16:46:48,168][gdb3][DEBUG ] Reading state information...
[2017-05-31 16:46:48,232][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 16:46:48,233][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 16:46:48,233][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 16:46:48,233][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 16:46:48,233][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 16:46:48,297][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 16:46:48,298][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 16:46:48,412][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 16:46:48,412][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 16:46:48,444][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 16:46:48,444][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 16:46:48,608][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 16:46:48,722][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 16:46:48,754][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 16:46:48,972][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 16:46:49,091][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 16:46:49,258][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 16:46:49,322][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 16:46:49,338][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 16:46:49,502][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 16:46:49,617][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 16:46:49,619][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 16:46:49,787][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 16:46:49,787][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 16:46:49,951][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 16:46:50,017][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 16:46:50,017][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 16:46:50,131][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 16:46:50,948][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 16:46:51,114][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbea0df9710>
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fbea1706230>
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:46:51,115][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:46:51,116][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 16:46:51,142][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:46:51,156][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:46:51,157][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:46:51,173][gdb3][DEBUG ] detect machine type
[2017-05-31 16:46:51,176][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:46:51,192][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:46:51,206][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:46:51,207][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:46:51,223][gdb3][DEBUG ] detect machine type
[2017-05-31 16:46:51,226][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:46:51,226][gdb3][INFO  ] purging data on gdb3
[2017-05-31 16:46:51,227][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 16:46:51,240][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 16:46:51,412][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc825dd89e0>
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fc82669b848>
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:46:51,413][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:19,509][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:47:19,509][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7bd7c8b560>
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f7bd830f758>
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:47:19,510][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 16:47:19,511][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:19,511][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 16:47:19,511][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 16:47:19,511][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 16:47:19,537][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:19,551][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:47:19,552][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:19,568][gdb3][DEBUG ] detect machine type
[2017-05-31 16:47:19,571][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:47:19,572][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 16:47:19,583][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 16:47:19,589][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 16:47:19,590][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 16:47:19,758][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f04d9566e60>
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f04d953bb18>
[2017-05-31 16:47:19,759][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:47:19,760][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 16:47:19,760][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:19,760][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 16:47:19,760][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 16:47:19,787][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:19,801][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:47:19,802][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:19,818][gdb3][DEBUG ] detect machine type
[2017-05-31 16:47:19,821][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:47:19,821][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 16:47:19,821][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 16:47:19,821][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:47:19,822][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 16:47:19,822][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:47:19,822][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 16:47:19,823][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:47:19,824][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 16:47:19,824][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 16:47:19,825][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 16:47:19,825][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 16:47:19,825][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 16:47:19,826][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 16:47:19,864][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 16:47:19,865][gdb3][DEBUG ] ceph-mon: set fsid to da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 16:47:19,868][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 16:47:19,871][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 16:47:19,872][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 16:47:19,872][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 16:47:19,873][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:47:19,941][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 16:47:20,013][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 16:47:22,083][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:47:22,148][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 16:47:22,149][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 16:47:22,149][gdb3][DEBUG ] {
[2017-05-31 16:47:22,149][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 16:47:22,149][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]   "features": {
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 16:47:22,150][gdb3][DEBUG ]       "kraken", 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]       "luminous"
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     ], 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 16:47:22,150][gdb3][DEBUG ]       "kraken", 
[2017-05-31 16:47:22,150][gdb3][DEBUG ]       "luminous"
[2017-05-31 16:47:22,150][gdb3][DEBUG ]     ]
[2017-05-31 16:47:22,151][gdb3][DEBUG ]   }, 
[2017-05-31 16:47:22,151][gdb3][DEBUG ]   "monmap": {
[2017-05-31 16:47:22,151][gdb3][DEBUG ]     "created": "2017-05-31 16:47:19.850297", 
[2017-05-31 16:47:22,151][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 16:47:22,151][gdb3][DEBUG ]     "features": {
[2017-05-31 16:47:22,151][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 16:47:22,151][gdb3][DEBUG ]       "persistent": [
[2017-05-31 16:47:22,151][gdb3][DEBUG ]         "kraken", 
[2017-05-31 16:47:22,151][gdb3][DEBUG ]         "luminous"
[2017-05-31 16:47:22,152][gdb3][DEBUG ]       ]
[2017-05-31 16:47:22,152][gdb3][DEBUG ]     }, 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]     "fsid": "da581514-b214-4b7a-baef-020d0e69b258", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]     "modified": "2017-05-31 16:47:20.102387", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]     "mons": [
[2017-05-31 16:47:22,152][gdb3][DEBUG ]       {
[2017-05-31 16:47:22,152][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]         "rank": 0
[2017-05-31 16:47:22,152][gdb3][DEBUG ]       }
[2017-05-31 16:47:22,152][gdb3][DEBUG ]     ]
[2017-05-31 16:47:22,152][gdb3][DEBUG ]   }, 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 16:47:22,152][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 16:47:22,153][gdb3][DEBUG ]   "quorum": [
[2017-05-31 16:47:22,153][gdb3][DEBUG ]     0
[2017-05-31 16:47:22,153][gdb3][DEBUG ]   ], 
[2017-05-31 16:47:22,153][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 16:47:22,153][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 16:47:22,153][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 16:47:22,153][gdb3][DEBUG ] }
[2017-05-31 16:47:22,153][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 16:47:22,153][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 16:47:22,154][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:47:22,219][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 16:47:22,234][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:22,249][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:47:22,249][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:22,266][gdb3][DEBUG ] detect machine type
[2017-05-31 16:47:22,269][gdb3][DEBUG ] find the location of an executable
[2017-05-31 16:47:22,270][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:47:22,335][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 16:47:22,335][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 16:47:22,335][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 16:47:22,337][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpUI2eIB
[2017-05-31 16:47:22,352][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:22,367][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:47:22,367][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:22,384][gdb3][DEBUG ] detect machine type
[2017-05-31 16:47:22,386][gdb3][DEBUG ] get remote short hostname
[2017-05-31 16:47:22,387][gdb3][DEBUG ] fetch remote file
[2017-05-31 16:47:22,388][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 16:47:22,454][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 16:47:22,620][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 16:47:22,786][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 16:47:22,952][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 16:47:23,120][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 16:47:23,286][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 16:47:23,452][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 16:47:23,618][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 16:47:23,785][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 16:47:23,951][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 16:47:24,116][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 16:47:24,116][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 16:47:24,116][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170531164724'
[2017-05-31 16:47:24,117][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 16:47:24,117][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 16:47:24,117][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 16:47:24,117][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpUI2eIB
[2017-05-31 16:47:24,301][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:47:24,301][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 16:47:24,301][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcf888e5518>
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fcf891fc938>
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:47:24,302][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:24,302][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 16:47:24,329][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:24,343][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 16:47:24,344][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:24,360][gdb3][DEBUG ] detect machine type
[2017-05-31 16:47:24,363][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:47:32,107][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create gdb0:/mnt/memstore
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 16:47:32,107][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2b31291908>
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2b314e7aa0>
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:32,108][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 16:47:32,108][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 16:47:32,350][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:32,578][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 16:47:32,578][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:32,594][gdb0][DEBUG ] detect machine type
[2017-05-31 16:47:32,598][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:47:32,598][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:47:32,599][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 16:47:32,599][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:47:32,601][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:47:32,601][gdb0][DEBUG ] create a keyring file
[2017-05-31 16:47:32,603][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate True
[2017-05-31 16:47:32,603][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:47:32,605][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 16:47:32,725][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:47:32,728][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:47:32,744][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:47:32,760][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:47:32,767][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 16:47:32,783][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 16:47:32,791][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.9239.tmp
[2017-05-31 16:47:32,792][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.9239.tmp
[2017-05-31 16:47:32,795][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.9239.tmp
[2017-05-31 16:47:32,813][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:47:37,885][gdb0][INFO  ] checking OSD status...
[2017-05-31 16:47:37,885][gdb0][DEBUG ] find the location of an executable
[2017-05-31 16:47:37,887][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 16:47:37,952][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 16:47:53,911][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create gdb1:/mnt/memstore
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 16:47:53,911][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f15482da908>
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1548530aa0>
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:47:53,912][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 16:47:53,913][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 16:47:54,155][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:47:54,383][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:47:54,383][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:47:54,400][gdb1][DEBUG ] detect machine type
[2017-05-31 16:47:54,404][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:47:54,405][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:47:54,405][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 16:47:54,405][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:47:54,407][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 16:47:54,407][gdb1][DEBUG ] create a keyring file
[2017-05-31 16:47:54,409][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate True
[2017-05-31 16:47:54,409][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:47:54,411][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 16:47:54,582][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 16:47:54,582][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2029, in main
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2025, in factory
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 16:47:54,582][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 16:47:54,583][gdb1][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-05-31 16:47:54,583][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 16:47:54,583][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 16:47:54,583][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 16:48:36,389][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd create gdb1:/mnt/memstore
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa51bbda908>
[2017-05-31 16:48:36,390][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 16:48:36,391][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 16:48:36,391][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa51be30aa0>
[2017-05-31 16:48:36,391][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 16:48:36,391][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 16:48:36,391][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 16:48:36,391][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 16:48:36,632][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 16:48:36,862][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 16:48:36,863][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 16:48:36,879][gdb1][DEBUG ] detect machine type
[2017-05-31 16:48:36,883][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:48:36,884][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 16:48:36,884][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 16:48:36,885][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 16:48:36,887][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate True
[2017-05-31 16:48:36,887][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:48:36,889][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 16:48:37,009][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 16:48:37,025][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:48:37,041][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:48:37,056][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 16:48:37,064][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 16:48:37,080][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 16:48:37,080][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.12243.tmp
[2017-05-31 16:48:37,081][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.12243.tmp
[2017-05-31 16:48:37,084][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.12243.tmp
[2017-05-31 16:48:37,102][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 16:48:42,174][gdb1][INFO  ] checking OSD status...
[2017-05-31 16:48:42,175][gdb1][DEBUG ] find the location of an executable
[2017-05-31 16:48:42,177][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 16:48:42,292][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 17:07:50,298][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd029a82fc8>
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:07:50,298][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 17:07:50,299][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fd02a3951b8>
[2017-05-31 17:07:50,299][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:07:50,299][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:07:50,299][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 17:07:50,299][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 17:07:50,299][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 17:07:50,299][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 17:07:50,325][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:07:50,339][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:07:50,339][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:07:50,356][gdb3][DEBUG ] detect machine type
[2017-05-31 17:07:50,358][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:07:50,358][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 17:07:50,359][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 17:07:50,397][gdb3][DEBUG ] Reading package lists...
[2017-05-31 17:07:50,562][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 17:07:50,562][gdb3][DEBUG ] Reading state information...
[2017-05-31 17:07:50,626][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 17:07:50,626][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 17:07:50,626][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 17:07:50,626][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 17:07:50,627][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 17:07:50,628][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 17:07:50,628][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 17:07:50,660][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 17:07:50,660][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 17:07:50,774][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 17:07:50,774][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 17:07:50,838][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 17:07:50,838][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 17:07:51,003][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 17:07:51,066][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 17:07:51,131][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 17:07:51,346][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 17:07:51,410][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 17:07:51,574][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 17:07:51,639][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 17:07:51,671][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 17:07:51,836][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 17:07:51,900][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 17:07:51,916][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 17:07:52,080][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 17:07:52,112][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 17:07:52,276][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 17:07:52,308][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 17:07:52,316][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 17:07:52,430][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 17:07:53,246][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 17:07:53,410][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:07:53,410][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 17:07:53,410][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:07:53,410][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:07:53,410][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:07:53,410][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f32ed785710>
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f32ee092230>
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:07:53,411][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:07:53,411][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 17:07:53,437][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:07:53,451][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:07:53,451][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:07:53,468][gdb3][DEBUG ] detect machine type
[2017-05-31 17:07:53,470][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:07:53,485][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:07:53,500][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:07:53,500][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:07:53,517][gdb3][DEBUG ] detect machine type
[2017-05-31 17:07:53,519][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:07:53,519][gdb3][INFO  ] purging data on gdb3
[2017-05-31 17:07:53,520][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 17:07:53,533][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 17:07:53,702][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc6007329e0>
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fc600ff5848>
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:07:53,703][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:11,875][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:08:11,875][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe3424ff560>
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fe342b83758>
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:11,876][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 17:08:11,876][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 17:08:11,876][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 17:08:11,903][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:11,917][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:08:11,917][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:11,934][gdb3][DEBUG ] detect machine type
[2017-05-31 17:08:11,936][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:08:11,937][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 17:08:11,949][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 17:08:11,955][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 17:08:11,955][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 17:08:11,955][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 17:08:11,955][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 17:08:11,955][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 17:08:11,955][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 17:08:11,956][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 17:08:11,956][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 17:08:12,122][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:12,122][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 17:08:12,122][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:12,122][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f350509de60>
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f3505072b18>
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 17:08:12,123][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:12,124][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 17:08:12,124][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 17:08:12,150][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:12,164][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:08:12,165][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:12,181][gdb3][DEBUG ] detect machine type
[2017-05-31 17:08:12,183][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:08:12,184][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 17:08:12,184][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 17:08:12,184][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:08:12,184][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 17:08:12,184][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:08:12,185][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 17:08:12,185][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:08:12,186][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 17:08:12,187][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 17:08:12,187][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 17:08:12,187][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 17:08:12,188][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 17:08:12,189][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 17:08:12,227][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 17:08:12,227][gdb3][DEBUG ] ceph-mon: set fsid to ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:08:12,230][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 17:08:12,234][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 17:08:12,234][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 17:08:12,234][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 17:08:12,235][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 17:08:12,305][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 17:08:12,373][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 17:08:14,443][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:08:14,508][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 17:08:14,508][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 17:08:14,508][gdb3][DEBUG ] {
[2017-05-31 17:08:14,508][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 17:08:14,508][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 17:08:14,508][gdb3][DEBUG ]   "features": {
[2017-05-31 17:08:14,508][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "kraken", 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "luminous"
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     ], 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "kraken", 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "luminous"
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     ]
[2017-05-31 17:08:14,509][gdb3][DEBUG ]   }, 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]   "monmap": {
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "created": "2017-05-31 17:08:12.212153", 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]     "features": {
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 17:08:14,509][gdb3][DEBUG ]       "persistent": [
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "kraken", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "luminous"
[2017-05-31 17:08:14,510][gdb3][DEBUG ]       ]
[2017-05-31 17:08:14,510][gdb3][DEBUG ]     }, 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]     "fsid": "ca3e0a81-2fd3-4069-bf21-8407a86a4dd3", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]     "modified": "2017-05-31 17:08:12.455156", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]     "mons": [
[2017-05-31 17:08:14,510][gdb3][DEBUG ]       {
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]         "rank": 0
[2017-05-31 17:08:14,510][gdb3][DEBUG ]       }
[2017-05-31 17:08:14,510][gdb3][DEBUG ]     ]
[2017-05-31 17:08:14,510][gdb3][DEBUG ]   }, 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 17:08:14,510][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 17:08:14,511][gdb3][DEBUG ]   "quorum": [
[2017-05-31 17:08:14,511][gdb3][DEBUG ]     0
[2017-05-31 17:08:14,511][gdb3][DEBUG ]   ], 
[2017-05-31 17:08:14,511][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 17:08:14,511][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 17:08:14,511][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 17:08:14,511][gdb3][DEBUG ] }
[2017-05-31 17:08:14,511][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 17:08:14,511][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 17:08:14,512][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:08:14,577][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 17:08:14,592][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:14,607][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:08:14,607][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:14,624][gdb3][DEBUG ] detect machine type
[2017-05-31 17:08:14,627][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:08:14,628][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:08:14,693][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 17:08:14,693][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 17:08:14,693][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 17:08:14,695][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpU1Oj8a
[2017-05-31 17:08:14,710][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:14,724][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:08:14,724][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:14,740][gdb3][DEBUG ] detect machine type
[2017-05-31 17:08:14,743][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:08:14,743][gdb3][DEBUG ] fetch remote file
[2017-05-31 17:08:14,744][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:08:14,810][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 17:08:14,976][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 17:08:15,142][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 17:08:15,308][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 17:08:15,475][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 17:08:15,641][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 17:08:15,807][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 17:08:15,973][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 17:08:16,139][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 17:08:16,306][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 17:08:16,471][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 17:08:16,471][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 17:08:16,471][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170531170816'
[2017-05-31 17:08:16,472][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 17:08:16,472][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 17:08:16,472][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 17:08:16,472][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpU1Oj8a
[2017-05-31 17:08:16,654][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:16,654][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 17:08:16,654][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb6f184f518>
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fb6f2166938>
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:16,655][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:16,655][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 17:08:16,681][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:16,696][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:08:16,697][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:16,713][gdb3][DEBUG ] detect machine type
[2017-05-31 17:08:16,715][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:08:30,514][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:08:30,514][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0ee519a908>
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0ee53f0aa0>
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:30,515][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:08:30,515][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:08:30,758][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:30,982][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:08:30,982][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:30,998][gdb0][DEBUG ] detect machine type
[2017-05-31 17:08:31,002][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:31,003][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:08:31,003][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:08:31,003][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:08:31,005][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 17:08:31,005][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:08:41,595][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd prepare gdb0:/mnt/memstore
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:08:41,595][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd061813908>
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd061a69aa0>
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:41,596][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:08:41,596][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:08:41,830][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:42,054][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:08:42,054][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:42,070][gdb0][DEBUG ] detect machine type
[2017-05-31 17:08:42,074][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:42,074][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:08:42,075][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:08:42,075][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:08:42,077][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:08:42,077][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:42,079][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:08:42,199][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:08:42,203][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:08:42,219][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:08:42,234][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:08:42,242][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:08:42,258][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:08:47,266][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:08:47,267][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:47,269][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:08:47,334][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:08:58,043][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd activate gdb0:/mnt/memstore
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f59abf02908>
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f59ac158aa0>
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:08:58,044][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:08:58,045][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:08:58,282][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:08:58,469][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:08:58,470][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:08:58,486][gdb0][DEBUG ] detect machine type
[2017-05-31 17:08:58,489][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:58,490][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:08:58,490][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:08:58,490][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:08:58,491][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:08:58,492][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:08:58,612][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:08:58,613][gdb0][WARNING] activate: Cluster uuid is da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:08:58,613][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:08:58,614][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:08:58,614][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:08:58,614][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:08:58,615][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:08:58,615][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:08:58,615][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:08:58,615][gdb0][WARNING]     args.func(args)
[2017-05-31 17:08:58,616][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:08:58,616][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:08:58,616][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:08:58,616][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:08:58,616][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 17:08:58,617][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 17:08:58,617][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:08:58,624][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:08:58,625][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:21:03,164][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd activate gdb0:/mnt/memstore
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fae890dc908>
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:21:03,165][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fae89332aa0>
[2017-05-31 17:21:03,166][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:21:03,166][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:21:03,166][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:21:03,166][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:21:03,406][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:21:03,637][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:21:03,638][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:21:03,654][gdb0][DEBUG ] detect machine type
[2017-05-31 17:21:03,657][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:03,658][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:21:03,658][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:21:03,658][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:21:03,658][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:03,660][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:21:03,780][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:21:03,781][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:21:03,781][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:21:03,782][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:21:03,782][gdb0][WARNING] activate: OSD uuid is 6a42016a-cb50-44b5-a78b-c0660b317c1f
[2017-05-31 17:21:03,782][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:21:03,782][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 6a42016a-cb50-44b5-a78b-c0660b317c1f
[2017-05-31 17:21:03,846][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:21:03,846][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:21:03,846][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:21:03,847][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:21:03,847][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:21:03,847][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:21:03,847][gdb0][WARNING]     args.func(args)
[2017-05-31 17:21:03,847][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:21:03,847][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:21:03,847][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:21:03,847][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:21:03,847][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:21:03,848][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:21:03,849][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:21:03,849][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:21:03,849][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:21:03.832088 7f463d537700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:21:03,849][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:21:03,849][gdb0][WARNING] 
[2017-05-31 17:21:03,856][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:21:03,857][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:21:24,941][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd prepare gdb0:/mnt/memstore
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:21:24,942][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe5b1696908>
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe5b18ecaa0>
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:21:24,943][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:21:24,943][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:21:25,186][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:21:25,410][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:21:25,410][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:21:25,426][gdb0][DEBUG ] detect machine type
[2017-05-31 17:21:25,430][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:25,430][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:21:25,430][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:21:25,431][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:21:25,433][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:21:25,433][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:25,435][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:21:25,555][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:21:25,559][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:21:25,575][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:21:25,590][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:21:25,598][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:21:25,614][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:21:30,626][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:21:30,627][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:30,629][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:21:30,694][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:21:37,853][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd activate gdb0:/mnt/memstore
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:21:37,853][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faaaf868908>
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7faaafabeaa0>
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:21:37,854][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:21:37,854][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:21:38,098][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:21:38,325][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:21:38,326][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:21:38,342][gdb0][DEBUG ] detect machine type
[2017-05-31 17:21:38,345][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:38,346][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:21:38,346][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:21:38,346][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:21:38,346][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:21:38,348][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:21:38,469][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:21:38,469][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:21:38,469][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:21:38,472][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:21:38,472][gdb0][WARNING] activate: OSD uuid is 6a42016a-cb50-44b5-a78b-c0660b317c1f
[2017-05-31 17:21:38,473][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:21:38,473][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 6a42016a-cb50-44b5-a78b-c0660b317c1f
[2017-05-31 17:21:38,537][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:21:38,537][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:21:38,537][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:21:38,537][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:21:38,537][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:21:38,537][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:21:38,537][gdb0][WARNING]     args.func(args)
[2017-05-31 17:21:38,537][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:21:38,537][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:21:38,537][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:21:38,537][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:21:38,538][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:21:38,538][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:21:38,538][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:21:38,538][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:21:38,538][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:21:38.521237 7fad53326700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:21:38,538][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:21:38,538][gdb0][WARNING] 
[2017-05-31 17:21:38,546][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:21:38,546][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:22:33,708][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:22:33,708][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd prepare gdb0:/mnt/memstore
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:22:33,709][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc9a79d4908>
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc9a7c2aaa0>
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:22:33,710][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:22:33,711][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:22:33,711][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:22:33,955][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:22:34,182][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:22:34,183][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:22:34,199][gdb0][DEBUG ] detect machine type
[2017-05-31 17:22:34,203][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:22:34,204][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:22:34,204][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:22:34,204][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:22:34,207][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:22:34,207][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:22:34,209][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:22:34,329][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:22:34,329][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:22:34,329][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:22:34,329][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:22:34,329][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:22:34,329][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:22:34,330][gdb0][WARNING]     args.func(args)
[2017-05-31 17:22:34,330][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:22:34,330][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:22:34,330][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:22:34,330][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:22:34,330][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:22:34,330][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:22:34,330][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:22:34,330][gdb0][WARNING]     self.set_type()
[2017-05-31 17:22:34,330][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:22:34,330][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:22:34,330][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-05-31 17:22:34,334][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:22:34,334][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:22:34,334][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:23:25,827][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd prepare gdb0:/home/ubuntu/memstore/
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/home/ubuntu/memstore/', None)]
[2017-05-31 17:23:25,827][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7ac59bd908>
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7ac5c13aa0>
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:23:25,828][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:23:25,829][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/home/ubuntu/memstore/:
[2017-05-31 17:23:26,067][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:23:26,291][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:23:26,291][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:23:26,307][gdb0][DEBUG ] detect machine type
[2017-05-31 17:23:26,311][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:23:26,312][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:23:26,312][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:23:26,312][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:23:26,315][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /home/ubuntu/memstore/ journal None activate False
[2017-05-31 17:23:26,315][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:23:26,317][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /home/ubuntu/memstore/
[2017-05-31 17:23:26,437][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:23:26,445][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:23:26,460][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:23:26,476][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:23:26,484][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:23:26,500][gdb0][WARNING] populate_data_path: Preparing osd data dir /home/ubuntu/memstore/
[2017-05-31 17:23:26,507][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /home/ubuntu/memstore/ceph_fsid.10517.tmp
[2017-05-31 17:23:26,508][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /home/ubuntu/memstore/fsid.10517.tmp
[2017-05-31 17:23:26,512][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /home/ubuntu/memstore/magic.10517.tmp
[2017-05-31 17:23:31,533][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:23:31,533][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:23:31,535][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:23:31,600][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:23:41,212][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:23:41,212][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd activate gdb0:/home/ubuntu/memstore/
[2017-05-31 17:23:41,212][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9bd0d86908>
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9bd0fdcaa0>
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:23:41,213][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/home/ubuntu/memstore/', None)]
[2017-05-31 17:23:41,213][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/home/ubuntu/memstore/:
[2017-05-31 17:23:41,459][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:23:41,686][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:23:41,687][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:23:41,703][gdb0][DEBUG ] detect machine type
[2017-05-31 17:23:41,707][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:23:41,707][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:23:41,707][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /home/ubuntu/memstore/
[2017-05-31 17:23:41,707][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:23:41,708][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:23:41,710][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /home/ubuntu/memstore/
[2017-05-31 17:23:41,830][gdb0][WARNING] main_activate: path = /home/ubuntu/memstore/
[2017-05-31 17:23:41,830][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:23:41,830][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:23:41,838][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:23:41,838][gdb0][WARNING] activate: OSD uuid is c85613d8-2b0e-4250-af41-052f3f698198
[2017-05-31 17:23:41,838][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:23:41,838][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise c85613d8-2b0e-4250-af41-052f3f698198
[2017-05-31 17:23:41,952][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:23:41,953][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:23:41,953][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:23:41,953][gdb0][WARNING]     args.func(args)
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:23:41,953][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:23:41,953][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:23:41,953][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:23:41,953][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:23:41,954][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:23:41,954][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:23:41,954][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:23:41.891883 7f56b0bc8700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:23:41,954][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:23:41,954][gdb0][WARNING] 
[2017-05-31 17:23:41,954][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:23:41,954][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /home/ubuntu/memstore/

[2017-05-31 17:24:50,365][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd46bc40518>
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fd46c557938>
[2017-05-31 17:24:50,366][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:24:50,367][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:24:50,367][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 17:24:50,611][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:24:50,835][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:24:50,835][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:24:50,851][gdb0][DEBUG ] detect machine type
[2017-05-31 17:24:50,855][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:24:55,871][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:24:55,871][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:24:55,871][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:24:55,871][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f68cdf29518>
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f68ce840938>
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:24:55,872][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:24:55,872][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:24:56,112][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:24:56,339][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:24:56,339][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:24:56,355][gdb1][DEBUG ] detect machine type
[2017-05-31 17:24:56,359][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:24:56,361][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 17:24:56,361][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 17:25:01,810][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:25:01,810][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf osd activate gdb0:/home/ubuntu/memstore/
[2017-05-31 17:25:01,810][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe68c37c908>
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe68c5d2aa0>
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:25:01,811][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/home/ubuntu/memstore/', None)]
[2017-05-31 17:25:01,811][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/home/ubuntu/memstore/:
[2017-05-31 17:25:02,051][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:25:02,283][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:25:02,283][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:25:02,300][gdb0][DEBUG ] detect machine type
[2017-05-31 17:25:02,304][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:25:02,305][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:25:02,305][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /home/ubuntu/memstore/
[2017-05-31 17:25:02,305][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:25:02,305][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:25:02,307][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /home/ubuntu/memstore/
[2017-05-31 17:25:02,428][gdb0][WARNING] main_activate: path = /home/ubuntu/memstore/
[2017-05-31 17:25:02,428][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:25:02,428][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:25:02,436][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:25:02,436][gdb0][WARNING] activate: OSD uuid is c85613d8-2b0e-4250-af41-052f3f698198
[2017-05-31 17:25:02,436][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:25:02,436][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise c85613d8-2b0e-4250-af41-052f3f698198
[2017-05-31 17:25:02,550][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:25:02,550][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:25:02,550][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:25:02,550][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:25:02,551][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:25:02,551][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:25:02,551][gdb0][WARNING]     args.func(args)
[2017-05-31 17:25:02,551][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:25:02,551][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:25:02,551][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:25:02,551][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:25:02,551][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:25:02,551][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:25:02,551][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:25:02,551][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:25:02,551][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:25:02.489541 7f7c2ad08700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:25:02,551][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:25:02,552][gdb0][WARNING] 
[2017-05-31 17:25:02,552][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:25:02,552][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /home/ubuntu/memstore/

[2017-05-31 17:26:09,147][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:09,428][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:26:09,428][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:09,428][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:09,428][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:09,428][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fedc15c8518>
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fedc1edf938>
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:09,429][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:09,429][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:26:09,675][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:09,867][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:26:09,867][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:09,883][gdb1][DEBUG ] detect machine type
[2017-05-31 17:26:09,887][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:26:09,889][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 17:26:09,889][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 17:26:13,872][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:13,872][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 17:26:13,872][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbea7d08518>
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fbea861f938>
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:13,873][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:13,873][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 17:26:14,115][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:14,366][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:26:14,367][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:14,383][gdb0][DEBUG ] detect machine type
[2017-05-31 17:26:14,387][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:26:17,810][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:17,810][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faa512c5518>
[2017-05-31 17:26:17,811][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:17,811][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 17:26:17,811][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7faa51bdc938>
[2017-05-31 17:26:17,811][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:17,811][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:17,811][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 17:26:18,051][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:18,282][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:26:18,283][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:18,299][gdb0][DEBUG ] detect machine type
[2017-05-31 17:26:18,302][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:26:31,577][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f82035a7518>
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:31,577][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:26:31,578][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f8203ebe938>
[2017-05-31 17:26:31,578][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:31,578][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:31,578][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:26:31,819][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:32,047][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:26:32,047][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:32,063][gdb1][DEBUG ] detect machine type
[2017-05-31 17:26:32,067][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:26:32,069][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 17:26:32,069][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 17:26:37,804][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f262edd4518>
[2017-05-31 17:26:37,804][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:37,805][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:26:37,805][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f262f6eb938>
[2017-05-31 17:26:37,805][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:37,805][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:37,805][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:26:38,048][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:38,275][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:26:38,275][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:38,291][gdb1][DEBUG ] detect machine type
[2017-05-31 17:26:38,295][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:26:41,218][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbd2a5e0518>
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fbd2aef7938>
[2017-05-31 17:26:41,219][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:26:41,220][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:26:41,220][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:26:41,463][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:26:41,691][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:26:41,691][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:26:41,708][gdb1][DEBUG ] detect machine type
[2017-05-31 17:26:41,712][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:28:07,417][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:28:07,417][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd create --zap-disk gdb0:/mnt/memstore
[2017-05-31 17:28:07,417][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:28:07,417][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:28:07,417][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff7512f6908>
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff75154caa0>
[2017-05-31 17:28:07,418][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:28:07,419][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:28:07,419][ceph_deploy.cli][INFO  ]  zap_disk                      : True
[2017-05-31 17:28:07,419][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:28:07,660][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:28:07,890][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:28:07,891][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:28:07,907][gdb0][DEBUG ] detect machine type
[2017-05-31 17:28:07,910][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:28:07,911][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:28:07,911][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:28:07,911][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:28:07,913][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate True
[2017-05-31 17:28:07,914][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:28:07,916][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:28:08,036][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:28:08,036][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:28:08,036][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:28:08,037][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:28:08,037][gdb0][WARNING]     args.func(args)
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:28:08,037][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:28:08,037][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:28:08,037][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:28:08,037][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:28:08,037][gdb0][WARNING]     self.set_type()
[2017-05-31 17:28:08,038][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:28:08,038][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:28:08,038][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-05-31 17:28:08,039][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:28:08,039][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --zap-disk --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:28:08,039][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:28:36,224][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd create gdb0:/mnt/memstore
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 17:28:36,225][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f304ebe3908>
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f304ee39aa0>
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:28:36,226][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:28:36,226][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:28:36,467][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:28:36,699][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:28:36,699][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:28:36,716][gdb0][DEBUG ] detect machine type
[2017-05-31 17:28:36,719][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:28:36,720][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:28:36,720][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:28:36,720][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:28:36,722][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate True
[2017-05-31 17:28:36,723][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:28:36,724][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:28:36,845][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:28:36,845][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:28:36,845][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:28:36,845][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:28:36,845][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:28:36,845][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:28:36,845][gdb0][WARNING]     args.func(args)
[2017-05-31 17:28:36,845][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:28:36,845][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:28:36,845][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:28:36,845][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:28:36,846][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:28:36,846][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:28:36,846][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:28:36,846][gdb0][WARNING]     self.set_type()
[2017-05-31 17:28:36,846][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:28:36,846][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:28:36,846][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-05-31 17:28:36,850][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:28:36,850][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:28:36,850][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:29:39,243][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd create gdb0:/mnt/memstore1
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore1', None)]
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:29:39,243][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  subcommand                    : create
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8e4a597908>
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8e4a7edaa0>
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:29:39,244][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:29:39,245][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore1:
[2017-05-31 17:29:39,484][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:29:39,714][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:29:39,715][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:29:39,731][gdb0][DEBUG ] detect machine type
[2017-05-31 17:29:39,734][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:29:39,735][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:29:39,735][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:29:39,736][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:29:39,738][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore1 journal None activate True
[2017-05-31 17:29:39,738][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:29:39,740][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore1
[2017-05-31 17:29:39,860][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:29:39,860][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:29:39,860][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:29:39,861][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:29:39,861][gdb0][WARNING]     args.func(args)
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:29:39,861][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:29:39,861][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:29:39,861][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:29:39,861][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:29:39,861][gdb0][WARNING]     self.set_type()
[2017-05-31 17:29:39,862][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:29:39,862][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:29:39,862][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore1'
[2017-05-31 17:29:39,863][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:29:39,863][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore1
[2017-05-31 17:29:39,863][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:31:18,609][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore0
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore0', None)]
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:31:18,610][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f24ec147908>
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f24ec39daa0>
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:31:18,611][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:31:18,611][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore0:
[2017-05-31 17:31:18,850][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:31:19,077][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:31:19,078][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:31:19,093][gdb0][DEBUG ] detect machine type
[2017-05-31 17:31:19,096][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:19,097][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:31:19,097][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:31:19,097][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:31:19,100][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore0 journal None activate False
[2017-05-31 17:31:19,100][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:19,101][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore0
[2017-05-31 17:31:19,222][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:31:19,222][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:31:19,222][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:31:19,222][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:31:19,222][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:31:19,222][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:31:19,222][gdb0][WARNING]     args.func(args)
[2017-05-31 17:31:19,223][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:31:19,223][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:31:19,223][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:31:19,223][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:31:19,223][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:31:19,223][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:31:19,223][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:31:19,223][gdb0][WARNING]     self.set_type()
[2017-05-31 17:31:19,223][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:31:19,223][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:31:19,223][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore0'
[2017-05-31 17:31:19,224][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:31:19,224][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore0
[2017-05-31 17:31:19,224][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:31:27,340][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:31:27,341][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0a00ca5908>
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0a00efbaa0>
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:31:27,342][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:31:27,342][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:31:27,582][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:31:27,810][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:31:27,810][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:31:27,826][gdb0][DEBUG ] detect machine type
[2017-05-31 17:31:27,829][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:27,830][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:31:27,830][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:31:27,831][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:31:27,833][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:31:27,833][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:27,835][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:31:27,955][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:31:27,955][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:31:27,955][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:31:27,955][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:31:27,956][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:31:27,956][gdb0][WARNING]     args.func(args)
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 17:31:27,956][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 17:31:27,956][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 17:31:27,956][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 17:31:27,956][gdb0][WARNING]     self.set_type()
[2017-05-31 17:31:27,956][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 17:31:27,956][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 17:31:27,957][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-05-31 17:31:27,957][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:31:27,957][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:31:27,957][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 17:31:45,369][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:31:45,369][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:31:45,369][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:31:45,369][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f511e360908>
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f511e5b6aa0>
[2017-05-31 17:31:45,370][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:31:45,371][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:31:45,371][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:31:45,371][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:31:45,606][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:31:45,834][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:31:45,834][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:31:45,850][gdb0][DEBUG ] detect machine type
[2017-05-31 17:31:45,853][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:45,854][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:31:45,854][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:31:45,854][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:31:45,857][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:31:45,857][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:45,859][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:31:45,979][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:31:45,983][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:31:45,999][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:31:46,014][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:31:46,022][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:31:46,038][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 17:31:46,053][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.11500.tmp
[2017-05-31 17:31:46,054][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.11500.tmp
[2017-05-31 17:31:46,054][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.11500.tmp
[2017-05-31 17:31:51,071][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:31:51,071][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:31:51,073][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:31:51,239][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:31:59,940][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:31:59,940][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:31:59,940][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fabd52b2908>
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fabd5508aa0>
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:31:59,941][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:31:59,942][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:32:00,183][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:32:00,410][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:32:00,410][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:32:00,426][gdb0][DEBUG ] detect machine type
[2017-05-31 17:32:00,429][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:32:00,430][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:32:00,430][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:32:00,430][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:32:00,430][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:32:00,432][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:32:00,552][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:32:00,553][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:32:00,553][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:32:00,556][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:32:00,556][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:32:00,556][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:32:00,556][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:32:00,671][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:32:00,671][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:32:00,671][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:32:00,671][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:32:00,671][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:32:00,671][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:32:00,671][gdb0][WARNING]     args.func(args)
[2017-05-31 17:32:00,671][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:32:00,671][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:32:00,671][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:32:00,672][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:32:00,672][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:32:00,672][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:32:00,672][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:32:00,672][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:32:00,672][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:32:00.611189 7fd518948700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:32:00,672][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:32:00,672][gdb0][WARNING] 
[2017-05-31 17:32:00,672][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:32:00,672][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:32:37,303][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:32:37,303][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:32:37,303][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:32:37,303][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:32:37,303][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:32:37,303][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb869074908>
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb8692caaa0>
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:32:37,304][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:32:37,304][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:32:37,546][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:32:37,773][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:32:37,774][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:32:37,790][gdb0][DEBUG ] detect machine type
[2017-05-31 17:32:37,793][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:32:37,794][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:32:37,794][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:32:37,794][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:32:37,794][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:32:37,796][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:32:37,916][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:32:37,917][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:32:37,917][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:32:37,920][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:32:37,920][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:32:37,920][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:32:37,920][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:32:37,984][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:32:37,984][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:32:37,985][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:32:37,985][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:32:37,985][gdb0][WARNING]     args.func(args)
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:32:37,985][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:32:37,985][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:32:37,985][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:32:37,985][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:32:37,986][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:32:37,986][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:32:37.970899 7f301d34a700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:32:37,986][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:32:37,986][gdb0][WARNING] 
[2017-05-31 17:32:37,993][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:32:37,994][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:33:01,166][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f80bcdce908>
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f80bd024aa0>
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:33:01,167][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:33:01,168][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:33:01,407][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:33:01,630][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:33:01,631][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:33:01,647][gdb0][DEBUG ] detect machine type
[2017-05-31 17:33:01,651][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:33:01,652][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:33:01,652][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:33:01,652][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:33:01,652][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:33:01,654][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:33:01,775][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:33:01,775][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:33:01,775][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:33:01,778][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:33:01,779][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:33:01,779][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:33:01,779][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:33:01,843][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:33:01,843][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:33:01,843][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:33:01,843][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:33:01,843][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:33:01,844][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:33:01,844][gdb0][WARNING]     args.func(args)
[2017-05-31 17:33:01,844][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:33:01,844][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:33:01,844][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:33:01,844][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:33:01,845][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:33:01,846][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:33:01,846][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:33:01,846][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:33:01,846][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:33:01.830834 7f8a312d6700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:33:01,846][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:33:01,846][gdb0][WARNING] 
[2017-05-31 17:33:01,862][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:33:01,862][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:35:20,726][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:35:20,726][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f31fc0bf908>
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f31fc315aa0>
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:35:20,727][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:35:20,727][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:35:20,966][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:35:21,190][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:35:21,190][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:35:21,206][gdb0][DEBUG ] detect machine type
[2017-05-31 17:35:21,209][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:21,210][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:35:21,210][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:35:21,210][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:35:21,210][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:21,213][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:35:21,333][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:35:21,333][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:35:21,333][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:35:21,335][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:35:21,335][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:21,335][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:35:21,335][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:21,449][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:35:21,450][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:35:21,450][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:35:21,450][gdb0][WARNING]     args.func(args)
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:35:21,450][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:35:21,450][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:35:21,450][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:35:21,450][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:35:21,451][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:35:21,451][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:35:21,451][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:35:21.391200 7fcb3940a700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:35:21,451][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:35:21,451][gdb0][WARNING] 
[2017-05-31 17:35:21,451][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:35:21,451][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:35:43,815][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6ae6d2f908>
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6ae6f85aa0>
[2017-05-31 17:35:43,816][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:35:43,817][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:35:43,817][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:35:43,817][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:35:44,054][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:35:44,281][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:35:44,282][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:35:44,298][gdb0][DEBUG ] detect machine type
[2017-05-31 17:35:44,301][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:44,302][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:35:44,302][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:35:44,302][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:35:44,302][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:44,304][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:35:44,424][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:35:44,424][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:35:44,425][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:35:44,426][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:35:44,426][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:44,426][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:35:44,426][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:44,490][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:35:44,490][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:35:44,491][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:35:44,491][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:35:44,491][gdb0][WARNING]     args.func(args)
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:35:44,491][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:35:44,491][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:35:44,491][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:35:44,491][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:35:44,495][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:35:44,495][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:35:44.478096 7f0159824700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:35:44,495][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:35:44,495][gdb0][WARNING] 
[2017-05-31 17:35:44,503][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:35:44,503][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:35:47,483][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:35:47,484][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f21d1e41908>
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f21d2097aa0>
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:35:47,485][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:35:47,485][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:35:47,722][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:35:47,949][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:35:47,950][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:35:47,965][gdb0][DEBUG ] detect machine type
[2017-05-31 17:35:47,969][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:47,970][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:35:47,970][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:35:47,970][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:35:47,972][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:35:47,972][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:47,974][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:35:48,095][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:35:48,098][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:35:48,114][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:35:48,130][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:35:48,137][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:35:48,145][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:35:53,166][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:35:53,166][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:53,168][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:35:53,333][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:35:57,208][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb58bf81908>
[2017-05-31 17:35:57,208][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:35:57,209][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb58c1d7aa0>
[2017-05-31 17:35:57,209][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:35:57,209][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:35:57,209][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:35:57,209][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:35:57,450][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:35:57,677][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:35:57,678][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:35:57,693][gdb0][DEBUG ] detect machine type
[2017-05-31 17:35:57,697][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:57,698][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:35:57,698][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:35:57,698][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:35:57,698][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:35:57,700][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:35:57,820][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:35:57,821][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:35:57,821][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:35:57,821][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:35:57,821][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:57,822][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:35:57,822][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:35:57,937][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:35:57,937][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:35:57,937][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:35:57,937][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:35:57,937][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:35:57,937][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:35:57,937][gdb0][WARNING]     args.func(args)
[2017-05-31 17:35:57,937][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:35:57,937][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:35:57,937][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:35:57,938][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:35:57,938][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:35:57,938][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:35:57,938][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:35:57,938][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:35:57,938][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:35:57.877212 7f9978f6b700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:35:57,938][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:35:57,938][gdb0][WARNING] 
[2017-05-31 17:35:57,938][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:35:57,938][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:37:04,926][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:37:04,926][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f67b3b90908>
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f67b3de6aa0>
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:37:04,927][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:37:04,928][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:37:05,166][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:05,393][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:37:05,394][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:05,409][gdb0][DEBUG ] detect machine type
[2017-05-31 17:37:05,412][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:05,413][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:37:05,413][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:37:05,413][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:37:05,416][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:37:05,416][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:05,418][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:37:05,538][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:37:05,541][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:05,557][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:05,573][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:05,580][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:37:05,596][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:37:10,605][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:37:10,605][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:10,607][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:37:10,773][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:37:14,106][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fde7b9cf908>
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fde7bc25aa0>
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:37:14,107][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:37:14,108][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:37:14,346][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:14,577][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:37:14,578][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:14,594][gdb0][DEBUG ] detect machine type
[2017-05-31 17:37:14,597][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:14,598][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:37:14,598][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:37:14,598][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:37:14,598][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:14,600][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:37:14,721][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:37:14,721][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:37:14,721][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:37:14,722][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:37:14,722][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:37:14,722][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:37:14,722][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:37:14,786][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:37:14,786][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:37:14,787][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:37:14,787][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:37:14,787][gdb0][WARNING]     args.func(args)
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:37:14,787][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:37:14,787][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:37:14,787][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:37:14,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:37:14,788][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:37:14,788][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:37:14.773746 7fd04a885700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:37:14,788][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:37:14,788][gdb0][WARNING] 
[2017-05-31 17:37:14,795][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:37:14,795][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:37:26,260][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb120b14e60>
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fb120ae9b18>
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:37:26,261][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 17:37:26,262][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:37:26,262][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 17:37:26,262][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 17:37:26,289][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:26,303][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:37:26,304][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:26,320][gdb3][DEBUG ] detect machine type
[2017-05-31 17:37:26,323][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:37:26,323][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 17:37:26,323][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 17:37:26,323][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:37:26,324][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 17:37:26,324][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:37:26,324][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 17:37:26,325][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:37:26,326][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 17:37:26,326][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 17:37:26,327][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 17:37:26,327][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 17:37:26,328][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 17:37:26,401][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 17:37:26,469][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 17:37:28,482][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:37:28,547][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 17:37:28,547][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 17:37:28,548][gdb3][DEBUG ] {
[2017-05-31 17:37:28,548][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]   "features": {
[2017-05-31 17:37:28,548][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 17:37:28,548][gdb3][DEBUG ]       "kraken", 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]       "luminous"
[2017-05-31 17:37:28,548][gdb3][DEBUG ]     ], 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 17:37:28,548][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 17:37:28,549][gdb3][DEBUG ]       "kraken", 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]       "luminous"
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     ]
[2017-05-31 17:37:28,549][gdb3][DEBUG ]   }, 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]   "monmap": {
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     "created": "2017-05-31 17:08:12.212153", 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     "features": {
[2017-05-31 17:37:28,549][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]       "persistent": [
[2017-05-31 17:37:28,549][gdb3][DEBUG ]         "kraken", 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]         "luminous"
[2017-05-31 17:37:28,549][gdb3][DEBUG ]       ]
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     }, 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     "fsid": "ca3e0a81-2fd3-4069-bf21-8407a86a4dd3", 
[2017-05-31 17:37:28,549][gdb3][DEBUG ]     "modified": "2017-05-31 17:08:12.455156", 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]     "mons": [
[2017-05-31 17:37:28,550][gdb3][DEBUG ]       {
[2017-05-31 17:37:28,550][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]         "rank": 0
[2017-05-31 17:37:28,550][gdb3][DEBUG ]       }
[2017-05-31 17:37:28,550][gdb3][DEBUG ]     ]
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   }, 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   "quorum": [
[2017-05-31 17:37:28,550][gdb3][DEBUG ]     0
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   ], 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 17:37:28,550][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 17:37:28,551][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 17:37:28,551][gdb3][DEBUG ] }
[2017-05-31 17:37:28,551][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 17:37:28,551][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 17:37:28,552][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:37:28,617][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 17:37:28,634][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:28,649][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:37:28,649][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:28,666][gdb3][DEBUG ] detect machine type
[2017-05-31 17:37:28,668][gdb3][DEBUG ] find the location of an executable
[2017-05-31 17:37:28,669][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:37:28,734][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 17:37:28,735][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 17:37:28,735][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 17:37:28,737][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpVN2kfG
[2017-05-31 17:37:28,752][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:28,766][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 17:37:28,767][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:28,783][gdb3][DEBUG ] detect machine type
[2017-05-31 17:37:28,785][gdb3][DEBUG ] get remote short hostname
[2017-05-31 17:37:28,786][gdb3][DEBUG ] fetch remote file
[2017-05-31 17:37:28,787][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 17:37:28,853][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 17:37:29,019][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 17:37:29,185][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 17:37:29,351][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 17:37:29,518][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 17:37:29,683][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.client.admin.keyring' already exists
[2017-05-31 17:37:29,683][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mds.keyring' already exists
[2017-05-31 17:37:29,683][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-mgr.keyring' already exists
[2017-05-31 17:37:29,684][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 17:37:29,684][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-osd.keyring' already exists
[2017-05-31 17:37:29,684][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.bootstrap-rgw.keyring' already exists
[2017-05-31 17:37:29,684][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpVN2kfG
[2017-05-31 17:37:37,308][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:37:37,308][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 17:37:37,308][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:37:37,308][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:37:37,308][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:37:37,308][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb432d88518>
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fb43369f938>
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:37:37,309][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:37:37,309][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 17:37:37,550][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:37,774][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:37:37,774][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:37,790][gdb0][DEBUG ] detect machine type
[2017-05-31 17:37:37,793][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:37:52,886][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:37:52,886][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f78bf918908>
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f78bfb6eaa0>
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:37:52,887][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:37:52,888][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:37:53,127][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:37:53,354][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:37:53,354][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:37:53,370][gdb0][DEBUG ] detect machine type
[2017-05-31 17:37:53,374][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:53,375][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:37:53,375][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:37:53,375][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:37:53,377][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:37:53,377][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:53,379][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:37:53,499][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:37:53,507][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:53,523][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:53,538][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:37:53,546][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:37:53,562][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:37:58,570][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:37:58,571][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:37:58,573][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:37:58,738][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:38:04,191][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:38:04,191][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd5b2840908>
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd5b2a96aa0>
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:38:04,192][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:38:04,193][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:38:04,434][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:38:04,666][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:38:04,666][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:38:04,682][gdb0][DEBUG ] detect machine type
[2017-05-31 17:38:04,685][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:04,686][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:38:04,686][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:38:04,686][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:38:04,686][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:04,688][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:38:04,808][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:38:04,808][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:38:04,809][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:38:04,812][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:38:04,812][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:04,812][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:38:04,812][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:04,876][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:38:04,876][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:38:04,876][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:38:04,877][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:38:04,877][gdb0][WARNING]     args.func(args)
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:38:04,877][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:38:04,877][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:38:04,877][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:38:04,877][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:38:04,877][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:38:04,878][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:38:04.863263 7fe323196700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:38:04,878][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:38:04,878][gdb0][WARNING] 
[2017-05-31 17:38:04,885][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:38:04,885][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:38:38,724][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:38:38,724][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0cb5e89908>
[2017-05-31 17:38:38,725][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:38:38,725][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0cb60dfaa0>
[2017-05-31 17:38:38,725][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:38:38,725][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:38:38,725][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:38:38,725][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:38:38,962][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:38:39,190][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:38:39,190][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:38:39,206][gdb0][DEBUG ] detect machine type
[2017-05-31 17:38:39,210][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:39,211][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:38:39,211][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:38:39,211][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:38:39,211][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:39,213][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:38:39,333][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:38:39,333][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:38:39,333][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:38:39,337][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:38:39,337][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:39,337][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:38:39,337][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:39,401][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:38:39,401][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:38:39,401][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:38:39,401][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:38:39,401][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:38:39,402][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:38:39,402][gdb0][WARNING]     args.func(args)
[2017-05-31 17:38:39,402][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:38:39,402][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:38:39,402][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:38:39,402][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 17:38:39,402][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:38:39,402][gdb0][WARNING]     keyring=keyring,
[2017-05-31 17:38:39,402][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:38:39,402][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 17:38:39,402][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:38:39.386582 7f2459254700 -1 auth: unable to find a keyring on /var/lib/ceph/bootstrap-osd/ceph.keyring: (2) No such file or directory
[2017-05-31 17:38:39,402][gdb0][WARNING] 2017-05-31 17:38:39.386595 7f2459254700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication
[2017-05-31 17:38:39,402][gdb0][WARNING] 2017-05-31 17:38:39.386596 7f2459254700  0 librados: client.bootstrap-osd initialization error (2) No such file or directory
[2017-05-31 17:38:39,403][gdb0][WARNING] error connecting to the cluster
[2017-05-31 17:38:39,403][gdb0][WARNING] 
[2017-05-31 17:38:39,410][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:38:39,410][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:38:42,692][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:38:42,692][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:38:42,692][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa8cdb3a908>
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:38:42,693][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa8cdd90aa0>
[2017-05-31 17:38:42,694][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:38:42,694][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:38:42,694][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:38:42,694][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:38:42,934][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:38:43,161][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:38:43,162][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:38:43,178][gdb0][DEBUG ] detect machine type
[2017-05-31 17:38:43,181][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:43,182][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:38:43,182][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:38:43,182][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:38:43,185][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 17:38:43,185][gdb0][DEBUG ] create a keyring file
[2017-05-31 17:38:43,187][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:38:43,187][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:43,189][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:38:43,309][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:38:43,313][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:38:43,328][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:38:43,344][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:38:43,352][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:38:43,359][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:38:48,380][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:38:48,380][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:48,383][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:38:48,548][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:38:58,883][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:38:58,884][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcc7933f908>
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcc79595aa0>
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:38:58,885][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:38:58,886][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:38:59,126][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:38:59,358][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:38:59,358][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:38:59,374][gdb0][DEBUG ] detect machine type
[2017-05-31 17:38:59,378][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:59,378][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:38:59,378][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:38:59,379][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:38:59,379][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:38:59,381][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:38:59,501][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:38:59,501][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:38:59,501][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:38:59,505][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:38:59,505][gdb0][WARNING] activate: OSD uuid is 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:59,505][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:38:59,505][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 1b3b5a35-7de7-489c-8108-1493f28b991b
[2017-05-31 17:38:59,670][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.13365.tmp
[2017-05-31 17:38:59,670][gdb0][WARNING] activate: OSD id is 0
[2017-05-31 17:38:59,670][gdb0][WARNING] activate: Initializing OSD...
[2017-05-31 17:38:59,670][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 17:38:59,784][gdb0][WARNING] got monmap epoch 2
[2017-05-31 17:38:59,800][gdb0][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid 1b3b5a35-7de7-489c-8108-1493f28b991b --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 17:38:59,832][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 17:38:59,832][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 17:38:59,832][gdb0][WARNING] activate: Authorizing OSD key...
[2017-05-31 17:38:59,832][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 17:38:59,997][gdb0][WARNING] added key for osd.0
[2017-05-31 17:38:59,997][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.13365.tmp
[2017-05-31 17:38:59,997][gdb0][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-05-31 17:38:59,997][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 17:38:59,997][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 17:38:59,998][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:38:59,998][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:38:59,998][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 17:38:59,998][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:38:59,998][gdb0][WARNING]     args.func(args)
[2017-05-31 17:38:59,998][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:38:59,998][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 17:38:59,998][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3543, in activate_dir
[2017-05-31 17:38:59,999][gdb0][WARNING]     old = os.readlink(canonical)
[2017-05-31 17:38:59,999][gdb0][WARNING] OSError: [Errno 22] Invalid argument: '/var/lib/ceph/osd/ceph-0'
[2017-05-31 17:39:00,006][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:39:00,007][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:41:18,574][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:41:18,574][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc749669908>
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:41:18,575][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:41:18,576][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc7498bfaa0>
[2017-05-31 17:41:18,576][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:41:18,576][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:41:18,576][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:41:18,576][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:41:18,818][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:41:19,037][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:41:19,038][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:41:19,053][gdb0][DEBUG ] detect machine type
[2017-05-31 17:41:19,057][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:19,058][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:41:19,058][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 17:41:19,058][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:41:19,060][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 17:41:19,060][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:19,062][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:41:19,183][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:41:19,186][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:41:19,202][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:41:19,218][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:41:19,225][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:41:19,241][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 17:41:19,241][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.13634.tmp
[2017-05-31 17:41:19,242][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.13634.tmp
[2017-05-31 17:41:19,246][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.13634.tmp
[2017-05-31 17:41:24,266][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:41:24,267][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:24,269][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:41:24,435][gdb0][WARNING] there is 1 OSD down
[2017-05-31 17:41:24,435][gdb0][WARNING] there is 1 OSD out
[2017-05-31 17:41:24,435][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 17:41:36,573][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5dc56f3908>
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5dc5949aa0>
[2017-05-31 17:41:36,574][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:41:36,575][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:41:36,575][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:41:36,575][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:41:36,814][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:41:37,046][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:41:37,046][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:41:37,062][gdb0][DEBUG ] detect machine type
[2017-05-31 17:41:37,065][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:37,066][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:41:37,066][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:41:37,066][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:41:37,067][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:37,068][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:41:37,189][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:41:37,189][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:41:37,189][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:41:37,192][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:41:37,193][gdb0][WARNING] activate: OSD uuid is a824912d-c2ff-4ce5-b541-4675a0141fef
[2017-05-31 17:41:37,193][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:41:37,193][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise a824912d-c2ff-4ce5-b541-4675a0141fef
[2017-05-31 17:41:37,407][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.13755.tmp
[2017-05-31 17:41:37,407][gdb0][WARNING] activate: OSD id is 1
[2017-05-31 17:41:37,407][gdb0][WARNING] activate: Initializing OSD...
[2017-05-31 17:41:37,408][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 17:41:37,572][gdb0][WARNING] got monmap epoch 2
[2017-05-31 17:41:37,572][gdb0][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 1 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid a824912d-c2ff-4ce5-b541-4675a0141fef --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 17:41:37,572][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 17:41:37,572][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 17:41:37,572][gdb0][WARNING] activate: Authorizing OSD key...
[2017-05-31 17:41:37,572][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.1 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 17:41:37,737][gdb0][WARNING] added key for osd.1
[2017-05-31 17:41:37,737][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.13755.tmp
[2017-05-31 17:41:37,737][gdb0][WARNING] activate: ceph osd.1 data dir is ready at /mnt/memstore
[2017-05-31 17:41:37,737][gdb0][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-1 -> /mnt/memstore
[2017-05-31 17:41:37,737][gdb0][WARNING] start_daemon: Starting ceph osd.1...
[2017-05-31 17:41:37,737][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1
[2017-05-31 17:41:37,737][gdb0][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2017-05-31 17:41:37,801][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1 --runtime
[2017-05-31 17:41:37,833][gdb0][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@1
[2017-05-31 17:41:37,833][gdb0][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 17:41:37,897][gdb0][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@1
[2017-05-31 17:41:42,967][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:41:42,967][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:41:42,969][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:41:43,135][gdb0][WARNING] there is 1 OSD down
[2017-05-31 17:41:43,135][gdb0][WARNING] there is 1 OSD out
[2017-05-31 17:41:43,137][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 17:54:07,021][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:54:07,021][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa99b391908>
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa99b5e7aa0>
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:54:07,022][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 17:54:07,023][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 17:54:07,263][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 17:54:07,490][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 17:54:07,490][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 17:54:07,506][gdb0][DEBUG ] detect machine type
[2017-05-31 17:54:07,510][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:54:07,511][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:54:07,511][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 17:54:07,511][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:54:07,511][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:54:07,513][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:54:07,634][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:54:07,634][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:54:07,634][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:54:07,638][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 17:54:07,638][gdb0][WARNING] activate: OSD uuid is a824912d-c2ff-4ce5-b541-4675a0141fef
[2017-05-31 17:54:07,638][gdb0][WARNING] activate: OSD id is 1
[2017-05-31 17:54:07,638][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 17:54:07,638][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 17:54:07,639][gdb0][WARNING] activate: ceph osd.1 data dir is ready at /mnt/memstore
[2017-05-31 17:54:07,639][gdb0][WARNING] start_daemon: Starting ceph osd.1...
[2017-05-31 17:54:07,639][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1
[2017-05-31 17:54:07,655][gdb0][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2017-05-31 17:54:07,719][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1 --runtime
[2017-05-31 17:54:07,751][gdb0][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@1
[2017-05-31 17:54:07,751][gdb0][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 17:54:07,815][gdb0][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@1
[2017-05-31 17:54:12,821][gdb0][INFO  ] checking OSD status...
[2017-05-31 17:54:12,821][gdb0][DEBUG ] find the location of an executable
[2017-05-31 17:54:12,823][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:54:12,989][gdb0][WARNING] there is 1 OSD down
[2017-05-31 17:54:12,989][gdb0][WARNING] there is 1 OSD out
[2017-05-31 17:54:12,991][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 17:54:19,274][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:54:19,274][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0a43521908>
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0a43777aa0>
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:54:19,275][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:54:19,276][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:54:19,523][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:54:19,755][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:54:19,755][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:54:19,772][gdb1][DEBUG ] detect machine type
[2017-05-31 17:54:19,776][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:54:19,776][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:54:19,777][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 17:54:19,777][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:54:19,779][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 17:54:19,779][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:54:19,782][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:54:19,952][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:54:19,952][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:54:19,952][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:54:19,953][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:54:19,960][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:54:19,968][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:54:24,989][gdb1][INFO  ] checking OSD status...
[2017-05-31 17:54:24,989][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:54:24,991][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:54:25,157][gdb1][WARNING] there is 1 OSD down
[2017-05-31 17:54:25,157][gdb1][WARNING] there is 1 OSD out
[2017-05-31 17:54:25,157][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 17:55:02,565][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:55:02,565][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4a8d45f908>
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f4a8d6b5aa0>
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:55:02,566][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:55:02,567][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:55:02,807][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:55:03,030][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:55:03,031][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:55:03,047][gdb1][DEBUG ] detect machine type
[2017-05-31 17:55:03,051][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:03,051][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:55:03,051][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:55:03,052][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:55:03,052][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:03,053][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:55:03,174][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:55:03,174][gdb1][WARNING] activate: Cluster uuid is da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:03,174][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:55:03,190][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 17:55:03,190][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:55:03,190][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3609, in activate
[2017-05-31 17:55:03,190][gdb1][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:03,192][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:55:03,192][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:55:25,577][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7b0446a518>
[2017-05-31 17:55:25,577][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:55:25,578][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:55:25,578][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f7b04d81938>
[2017-05-31 17:55:25,578][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:55:25,578][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:55:25,578][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:55:25,819][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:55:26,051][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:55:26,052][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:55:26,068][gdb1][DEBUG ] detect machine type
[2017-05-31 17:55:26,072][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:55:27,598][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2e1b214908>
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2e1b46aaa0>
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:55:27,599][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:55:27,600][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:55:27,839][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:55:28,067][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:55:28,068][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:55:28,084][gdb1][DEBUG ] detect machine type
[2017-05-31 17:55:28,088][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:28,089][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:55:28,089][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:55:28,089][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:55:28,089][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:28,091][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:55:28,212][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:55:28,212][gdb1][WARNING] activate: Cluster uuid is da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:28,212][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:55:28,228][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 17:55:28,228][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:55:28,228][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3609, in activate
[2017-05-31 17:55:28,229][gdb1][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:28,236][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:55:28,236][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:55:42,010][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:55:42,010][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd366180908>
[2017-05-31 17:55:42,011][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:55:42,012][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:55:42,012][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd3663d6aa0>
[2017-05-31 17:55:42,012][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:55:42,012][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:55:42,012][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:55:42,012][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:55:42,259][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:55:42,450][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:55:42,451][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:55:42,467][gdb1][DEBUG ] detect machine type
[2017-05-31 17:55:42,471][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:42,471][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:55:42,472][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 17:55:42,472][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:55:42,474][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 17:55:42,474][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:42,476][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:55:42,647][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:55:42,647][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:55:42,647][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:55:42,647][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:55:42,651][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:55:42,666][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:55:47,679][gdb1][INFO  ] checking OSD status...
[2017-05-31 17:55:47,679][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:47,682][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:55:47,847][gdb1][WARNING] there is 1 OSD down
[2017-05-31 17:55:47,847][gdb1][WARNING] there is 1 OSD out
[2017-05-31 17:55:47,848][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 17:55:56,173][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:55:56,173][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efc390cd908>
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7efc39323aa0>
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:55:56,174][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:55:56,174][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:55:56,419][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:55:56,619][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:55:56,620][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:55:56,636][gdb1][DEBUG ] detect machine type
[2017-05-31 17:55:56,640][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:56,641][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:55:56,641][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:55:56,641][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:55:56,641][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:55:56,643][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:55:56,813][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:55:56,814][gdb1][WARNING] activate: Cluster uuid is da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:56,814][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:55:56,814][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 17:55:56,814][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:55:56,814][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3609, in activate
[2017-05-31 17:55:56,814][gdb1][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid da581514-b214-4b7a-baef-020d0e69b258
[2017-05-31 17:55:56,815][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:55:56,815][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:57:12,883][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:57:12,883][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 17:57:12,883][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:57:12,883][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f98fa502908>
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f98fa758aa0>
[2017-05-31 17:57:12,884][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:57:12,885][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:57:12,885][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:57:12,885][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:57:13,127][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:57:13,363][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:57:13,363][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:57:13,380][gdb1][DEBUG ] detect machine type
[2017-05-31 17:57:13,384][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:57:13,384][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:57:13,384][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 17:57:13,385][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:57:13,387][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 17:57:13,387][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:57:13,389][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:57:13,509][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:57:13,525][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:57:13,541][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:57:13,548][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:57:13,564][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:57:13,580][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 17:57:13,580][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.13708.tmp
[2017-05-31 17:57:13,580][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.13708.tmp
[2017-05-31 17:57:13,583][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.13708.tmp
[2017-05-31 17:57:18,604][gdb1][INFO  ] checking OSD status...
[2017-05-31 17:57:18,604][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:57:18,607][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:57:18,772][gdb1][WARNING] there is 1 OSD down
[2017-05-31 17:57:18,773][gdb1][WARNING] there is 1 OSD out
[2017-05-31 17:57:18,773][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 17:57:23,589][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:57:23,589][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1db2621908>
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1db2877aa0>
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:57:23,590][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:57:23,591][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:57:23,831][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:57:24,062][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:57:24,063][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:57:24,079][gdb1][DEBUG ] detect machine type
[2017-05-31 17:57:24,083][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:57:24,084][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:57:24,084][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:57:24,084][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:57:24,084][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:57:24,086][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:57:24,257][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:57:24,257][gdb1][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:57:24,257][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:57:24,257][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 17:57:24,257][gdb1][WARNING] activate: OSD uuid is 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:57:24,257][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:57:24,258][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:57:24,289][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 17:57:24,290][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:57:24,290][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:57:24,290][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:57:24.280299 7fa85e850700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:57:24,290][gdb1][WARNING] error connecting to the cluster
[2017-05-31 17:57:24,290][gdb1][WARNING] 
[2017-05-31 17:57:24,298][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:57:24,298][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:58:07,650][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:58:07,650][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa3a7716908>
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa3a796caa0>
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:58:07,651][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:58:07,651][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:58:07,887][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:58:08,082][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:58:08,083][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:58:08,099][gdb1][DEBUG ] detect machine type
[2017-05-31 17:58:08,103][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:58:08,103][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:58:08,103][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:58:08,103][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:58:08,104][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:58:08,105][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:58:08,226][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:58:08,226][gdb1][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:58:08,226][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:58:08,242][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 17:58:08,242][gdb1][WARNING] activate: OSD uuid is 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:58:08,242][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:58:08,242][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:58:08,306][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 17:58:08,306][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 17:58:08,306][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 17:58:08,307][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 17:58:08,307][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 17:58:08.298544 7f68f30e8700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 17:58:08,307][gdb1][WARNING] error connecting to the cluster
[2017-05-31 17:58:08,307][gdb1][WARNING] 
[2017-05-31 17:58:08,315][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 17:58:08,315][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 17:59:27,881][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:59:27,881][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4023cc6518>
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f40245dd938>
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:59:27,882][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:59:27,882][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 17:59:28,123][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:59:28,346][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:59:28,347][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:59:28,363][gdb1][DEBUG ] detect machine type
[2017-05-31 17:59:28,367][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:59:34,857][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:59:34,857][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 17:59:34,857][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:59:34,857][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:59:34,857][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ffbf28de908>
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ffbf2b34aa0>
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:59:34,858][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 17:59:34,859][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:59:35,099][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:59:35,330][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:59:35,331][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:59:35,347][gdb1][DEBUG ] detect machine type
[2017-05-31 17:59:35,350][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:59:35,351][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:59:35,351][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 17:59:35,352][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 17:59:35,354][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 17:59:35,354][gdb1][DEBUG ] create a keyring file
[2017-05-31 17:59:35,356][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 17:59:35,356][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:59:35,358][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 17:59:35,479][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:59:35,494][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:59:35,510][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:59:35,518][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 17:59:35,533][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 17:59:35,541][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 17:59:40,562][gdb1][INFO  ] checking OSD status...
[2017-05-31 17:59:40,562][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:59:40,565][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 17:59:40,730][gdb1][WARNING] there is 1 OSD down
[2017-05-31 17:59:40,730][gdb1][WARNING] there is 1 OSD out
[2017-05-31 17:59:40,730][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 17:59:53,587][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 17:59:53,587][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1e2cdae908>
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1e2d004aa0>
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 17:59:53,588][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 17:59:53,588][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 17:59:53,832][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 17:59:54,058][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 17:59:54,059][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 17:59:54,075][gdb1][DEBUG ] detect machine type
[2017-05-31 17:59:54,079][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:59:54,080][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 17:59:54,080][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 17:59:54,080][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 17:59:54,080][gdb1][DEBUG ] find the location of an executable
[2017-05-31 17:59:54,082][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 17:59:54,202][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 17:59:54,202][gdb1][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 17:59:54,202][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 17:59:54,218][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 17:59:54,218][gdb1][WARNING] activate: OSD uuid is 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:59:54,218][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 17:59:54,218][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 17:59:54,884][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.14229.tmp
[2017-05-31 17:59:54,884][gdb1][WARNING] activate: OSD id is 2
[2017-05-31 17:59:54,885][gdb1][WARNING] activate: Initializing OSD...
[2017-05-31 17:59:54,885][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 17:59:54,999][gdb1][WARNING] got monmap epoch 2
[2017-05-31 17:59:55,006][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 2 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid 101ab77c-5335-474c-915b-a82ed5ca89ba --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 17:59:55,038][gdb1][WARNING] activate: Marking with init system systemd
[2017-05-31 17:59:55,038][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 17:59:55,040][gdb1][WARNING] activate: Authorizing OSD key...
[2017-05-31 17:59:55,040][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.2 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 17:59:55,204][gdb1][WARNING] added key for osd.2
[2017-05-31 17:59:55,204][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.14229.tmp
[2017-05-31 17:59:55,204][gdb1][WARNING] activate: ceph osd.2 data dir is ready at /mnt/memstore
[2017-05-31 17:59:55,204][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-2 -> /mnt/memstore
[2017-05-31 17:59:55,204][gdb1][WARNING] start_daemon: Starting ceph osd.2...
[2017-05-31 17:59:55,205][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@2
[2017-05-31 17:59:55,212][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@2.service.
[2017-05-31 17:59:55,276][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@2 --runtime
[2017-05-31 17:59:55,308][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@2
[2017-05-31 17:59:55,309][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@2.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 17:59:55,373][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@2
[2017-05-31 18:00:00,493][gdb1][INFO  ] checking OSD status...
[2017-05-31 18:00:00,493][gdb1][DEBUG ] find the location of an executable
[2017-05-31 18:00:00,495][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:00:00,661][gdb1][WARNING] there is 1 OSD down
[2017-05-31 18:00:00,661][gdb1][WARNING] there is 1 OSD out
[2017-05-31 18:00:00,663][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 18:45:31,589][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:45:31,589][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 18:45:31,589][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:45:31,589][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:45:31,589][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f84a3f46fc8>
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f84a48591b8>
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:45:31,590][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:45:31,590][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 18:45:31,590][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 18:45:31,590][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 18:45:31,590][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 18:45:31,617][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:31,631][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:31,631][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:31,648][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:31,650][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:45:31,650][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 18:45:31,652][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 18:45:31,690][gdb3][DEBUG ] Reading package lists...
[2017-05-31 18:45:31,854][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 18:45:31,854][gdb3][DEBUG ] Reading state information...
[2017-05-31 18:45:31,969][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 18:45:31,969][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 18:45:31,969][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 18:45:31,969][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 18:45:31,969][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 18:45:31,969][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 18:45:31,969][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 18:45:31,969][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 18:45:31,970][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 18:45:31,970][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 18:45:31,970][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 18:45:31,970][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 18:45:31,970][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 18:45:31,970][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 18:45:31,970][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 18:45:32,084][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 18:45:32,084][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 18:45:32,149][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 18:45:32,149][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 18:45:32,313][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 18:45:32,427][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 18:45:32,459][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 18:45:32,675][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 18:45:32,791][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 18:45:32,960][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 18:45:33,026][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 18:45:33,029][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 18:45:33,200][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 18:45:33,264][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 18:45:33,279][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 18:45:33,447][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 18:45:33,478][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 18:45:33,643][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 18:45:33,675][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 18:45:33,690][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 18:45:33,805][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 18:45:34,671][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 18:45:34,838][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f106bbfe710>
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f106c50b230>
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:45:34,839][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:45:34,839][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 18:45:34,866][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:34,880][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:34,881][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:34,897][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:34,900][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:45:34,916][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:34,931][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:34,931][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:34,948][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:34,950][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:45:34,950][gdb3][INFO  ] purging data on gdb3
[2017-05-31 18:45:34,951][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 18:45:34,964][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 18:45:35,137][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0d360b29e0>
[2017-05-31 18:45:35,137][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:45:35,138][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f0d36975848>
[2017-05-31 18:45:35,138][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:45:35,138][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:45:56,148][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:45:56,148][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 18:45:56,148][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:45:56,148][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc886d8f560>
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fc887413758>
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:45:56,149][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 18:45:56,149][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 18:45:56,150][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 18:45:56,176][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:56,190][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:56,191][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:56,207][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:56,210][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:45:56,211][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 18:45:56,222][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 18:45:56,229][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 18:45:56,229][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 18:45:56,394][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6db4199e60>
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f6db416eb18>
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:45:56,395][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 18:45:56,396][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:45:56,396][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 18:45:56,396][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 18:45:56,422][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:56,437][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:56,437][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:56,454][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:56,457][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:45:56,457][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 18:45:56,457][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 18:45:56,457][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:45:56,457][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 18:45:56,458][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:45:56,458][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 18:45:56,458][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:45:56,460][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 18:45:56,460][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 18:45:56,460][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 18:45:56,461][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 18:45:56,461][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 18:45:56,462][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 18:45:56,500][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 18:45:56,500][gdb3][DEBUG ] ceph-mon: set fsid to c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:45:56,504][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 18:45:56,505][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 18:45:56,506][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 18:45:56,506][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 18:45:56,507][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 18:45:56,577][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 18:45:56,644][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 18:45:58,715][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:45:58,780][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 18:45:58,780][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 18:45:58,781][gdb3][DEBUG ] {
[2017-05-31 18:45:58,781][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 18:45:58,781][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 18:45:58,782][gdb3][DEBUG ]   "features": {
[2017-05-31 18:45:58,782][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 18:45:58,782][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 18:45:58,782][gdb3][DEBUG ]       "kraken", 
[2017-05-31 18:45:58,782][gdb3][DEBUG ]       "luminous"
[2017-05-31 18:45:58,782][gdb3][DEBUG ]     ], 
[2017-05-31 18:45:58,782][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 18:45:58,782][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 18:45:58,782][gdb3][DEBUG ]       "kraken", 
[2017-05-31 18:45:58,783][gdb3][DEBUG ]       "luminous"
[2017-05-31 18:45:58,783][gdb3][DEBUG ]     ]
[2017-05-31 18:45:58,783][gdb3][DEBUG ]   }, 
[2017-05-31 18:45:58,783][gdb3][DEBUG ]   "monmap": {
[2017-05-31 18:45:58,783][gdb3][DEBUG ]     "created": "2017-05-31 18:45:56.486262", 
[2017-05-31 18:45:58,783][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 18:45:58,783][gdb3][DEBUG ]     "features": {
[2017-05-31 18:45:58,783][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 18:45:58,784][gdb3][DEBUG ]       "persistent": [
[2017-05-31 18:45:58,784][gdb3][DEBUG ]         "kraken", 
[2017-05-31 18:45:58,784][gdb3][DEBUG ]         "luminous"
[2017-05-31 18:45:58,784][gdb3][DEBUG ]       ]
[2017-05-31 18:45:58,784][gdb3][DEBUG ]     }, 
[2017-05-31 18:45:58,784][gdb3][DEBUG ]     "fsid": "c676bc10-14a8-4f7c-93e4-c9f0324506d5", 
[2017-05-31 18:45:58,784][gdb3][DEBUG ]     "modified": "2017-05-31 18:45:56.730163", 
[2017-05-31 18:45:58,784][gdb3][DEBUG ]     "mons": [
[2017-05-31 18:45:58,784][gdb3][DEBUG ]       {
[2017-05-31 18:45:58,784][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]         "rank": 0
[2017-05-31 18:45:58,785][gdb3][DEBUG ]       }
[2017-05-31 18:45:58,785][gdb3][DEBUG ]     ]
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   }, 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "quorum": [
[2017-05-31 18:45:58,785][gdb3][DEBUG ]     0
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   ], 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 18:45:58,785][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 18:45:58,785][gdb3][DEBUG ] }
[2017-05-31 18:45:58,785][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 18:45:58,786][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 18:45:58,787][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:45:58,852][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 18:45:58,868][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:58,882][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:58,883][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:58,899][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:58,902][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:45:58,903][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:45:58,968][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 18:45:58,969][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 18:45:58,969][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 18:45:58,970][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpDGdpWF
[2017-05-31 18:45:58,986][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:45:59,001][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:45:59,002][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:45:59,018][gdb3][DEBUG ] detect machine type
[2017-05-31 18:45:59,021][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:45:59,021][gdb3][DEBUG ] fetch remote file
[2017-05-31 18:45:59,023][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:45:59,089][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 18:45:59,255][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 18:45:59,421][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 18:45:59,587][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 18:45:59,754][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 18:45:59,920][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 18:46:00,086][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 18:46:00,253][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 18:46:00,419][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 18:46:00,585][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 18:46:00,751][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 18:46:00,751][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 18:46:00,751][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170531184600'
[2017-05-31 18:46:00,751][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 18:46:00,751][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 18:46:00,752][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 18:46:00,752][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpDGdpWF
[2017-05-31 18:46:00,936][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:46:00,936][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 18:46:00,936][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:46:00,936][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc602b75518>
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fc60348c938>
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:46:00,937][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:46:00,937][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 18:46:00,964][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:46:00,979][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:46:00,979][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:46:00,996][gdb3][DEBUG ] detect machine type
[2017-05-31 18:46:00,999][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:47:29,974][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:29,974][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9869f33518>
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f986a84a938>
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:29,975][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:29,975][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:47:30,222][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:30,449][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:47:30,450][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:30,466][gdb0][DEBUG ] detect machine type
[2017-05-31 18:47:30,469][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:47:30,471][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:47:30,471][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 18:47:30,649][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:30,650][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faca145a908>
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7faca16b0aa0>
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:30,651][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:47:30,651][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:47:30,886][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:31,118][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:47:31,118][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:31,134][gdb0][DEBUG ] detect machine type
[2017-05-31 18:47:31,138][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:47:31,139][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:47:31,139][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:47:31,139][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:47:31,141][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:47:31,141][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 18:47:31,315][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:31,315][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:47:31,315][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0e1cda0908>
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0e1cff6aa0>
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:31,316][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:47:31,317][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:47:31,554][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:31,782][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:47:31,782][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:31,799][gdb0][DEBUG ] detect machine type
[2017-05-31 18:47:31,802][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:47:31,803][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:47:31,803][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:47:31,803][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:47:31,803][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:47:31,806][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:47:31,926][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:47:31,926][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 18:47:31,926][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:47:31,928][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 18:47:31,928][gdb0][WARNING] activate: OSD uuid is a824912d-c2ff-4ce5-b541-4675a0141fef
[2017-05-31 18:47:31,928][gdb0][WARNING] activate: OSD id is 1
[2017-05-31 18:47:31,928][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 18:47:31,929][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 18:47:31,932][gdb0][WARNING] activate: ceph osd.1 data dir is ready at /mnt/memstore
[2017-05-31 18:47:31,932][gdb0][WARNING] start_daemon: Starting ceph osd.1...
[2017-05-31 18:47:31,932][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1
[2017-05-31 18:47:31,936][gdb0][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2017-05-31 18:47:32,000][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1 --runtime
[2017-05-31 18:47:32,064][gdb0][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@1
[2017-05-31 18:47:32,064][gdb0][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 18:47:32,096][gdb0][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@1
[2017-05-31 18:47:37,215][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:47:37,216][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:47:37,218][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:47:37,335][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 18:47:55,868][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0173bb3518>
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 18:47:55,869][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f01744ca938>
[2017-05-31 18:47:55,870][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:55,870][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:55,870][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 18:47:56,115][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:56,346][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 18:47:56,347][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:56,363][gdb1][DEBUG ] detect machine type
[2017-05-31 18:47:56,366][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:47:56,368][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:47:56,369][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 18:47:56,548][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:56,548][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb06843d908>
[2017-05-31 18:47:56,549][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:56,550][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:47:56,550][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb068693aa0>
[2017-05-31 18:47:56,550][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:56,550][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:56,550][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:47:56,550][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 18:47:56,795][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:57,026][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 18:47:57,027][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:57,043][gdb1][DEBUG ] detect machine type
[2017-05-31 18:47:57,046][gdb1][DEBUG ] find the location of an executable
[2017-05-31 18:47:57,047][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:47:57,047][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 18:47:57,048][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:47:57,049][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:47:57,049][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 18:47:57,222][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:47:57,222][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 18:47:57,222][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feedcb72908>
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7feedcdc8aa0>
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:47:57,223][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 18:47:57,224][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 18:47:57,463][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 18:47:57,695][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 18:47:57,696][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 18:47:57,712][gdb1][DEBUG ] detect machine type
[2017-05-31 18:47:57,716][gdb1][DEBUG ] find the location of an executable
[2017-05-31 18:47:57,716][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:47:57,717][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 18:47:57,717][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:47:57,717][gdb1][DEBUG ] find the location of an executable
[2017-05-31 18:47:57,719][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:47:57,839][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:47:57,840][gdb1][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 18:47:57,840][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:47:57,855][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 18:47:57,856][gdb1][WARNING] activate: OSD uuid is 101ab77c-5335-474c-915b-a82ed5ca89ba
[2017-05-31 18:47:57,856][gdb1][WARNING] activate: OSD id is 2
[2017-05-31 18:47:57,856][gdb1][WARNING] activate: Marking with init system systemd
[2017-05-31 18:47:57,856][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 18:47:57,856][gdb1][WARNING] activate: ceph osd.2 data dir is ready at /mnt/memstore
[2017-05-31 18:47:57,856][gdb1][WARNING] start_daemon: Starting ceph osd.2...
[2017-05-31 18:47:57,856][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@2
[2017-05-31 18:47:57,859][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@2.service.
[2017-05-31 18:47:57,923][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@2 --runtime
[2017-05-31 18:47:58,038][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@2
[2017-05-31 18:47:58,038][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@2.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 18:47:58,102][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@2
[2017-05-31 18:48:03,222][gdb1][INFO  ] checking OSD status...
[2017-05-31 18:48:03,222][gdb1][DEBUG ] find the location of an executable
[2017-05-31 18:48:03,224][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:48:03,342][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 18:49:54,207][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0 gdb1
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fceed5d2518>
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  client                        : ['gdb0', 'gdb1']
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fceedee9938>
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:49:54,208][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:49:54,208][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:49:54,446][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:49:54,678][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:49:54,678][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:49:54,694][gdb0][DEBUG ] detect machine type
[2017-05-31 18:49:54,697][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:49:54,699][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:49:54,699][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 18:49:54,931][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 18:49:55,160][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 18:49:55,160][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 18:49:55,177][gdb1][DEBUG ] detect machine type
[2017-05-31 18:49:55,181][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:49:55,182][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:49:55,183][ceph_deploy][ERROR ] GenericError: Failed to configure 2 admin hosts

[2017-05-31 18:50:28,701][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:50:28,701][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-05-31 18:50:28,701][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:50:28,701][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7d0e56e518>
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f7d0ee85938>
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:50:28,702][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:50:28,702][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:50:28,942][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:50:29,170][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:50:29,170][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:50:29,186][gdb0][DEBUG ] detect machine type
[2017-05-31 18:50:29,190][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:50:29,365][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:50:29,366][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe3e2b4a908>
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe3e2da0aa0>
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:50:29,367][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:50:29,368][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:50:29,610][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:50:29,838][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:50:29,838][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:50:29,854][gdb0][DEBUG ] detect machine type
[2017-05-31 18:50:29,858][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:50:29,859][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:50:29,859][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:50:29,859][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:50:29,861][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:50:29,861][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:50:29,863][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:50:29,984][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:50:29,987][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:50:30,003][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:50:30,018][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:50:30,026][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:50:30,042][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 18:50:35,048][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:50:35,049][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:50:35,051][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:50:35,217][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:50:35,382][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:50:35,382][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:50:35,382][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc329b70908>
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc329dc6aa0>
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:50:35,383][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:50:35,383][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:50:35,630][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:50:35,826][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:50:35,826][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:50:35,842][gdb0][DEBUG ] detect machine type
[2017-05-31 18:50:35,846][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:50:35,847][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:50:35,847][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:50:35,847][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:50:35,847][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:50:35,849][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:50:35,970][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:50:35,970][gdb0][WARNING] activate: Cluster uuid is ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 18:50:35,970][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:50:35,974][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:50:35,974][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:50:35,974][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:50:35,974][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:50:35,974][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:50:35,975][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:50:35,975][gdb0][WARNING]     args.func(args)
[2017-05-31 18:50:35,975][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:50:35,975][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:50:35,975][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:50:35,975][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:50:35,975][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:50:35,975][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:50:35,976][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid ca3e0a81-2fd3-4069-bf21-8407a86a4dd3
[2017-05-31 18:50:35,983][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:50:35,983][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:51:44,351][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:51:44,351][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-05-31 18:51:44,351][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:51:44,351][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f80e4e73518>
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f80e578a938>
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:51:44,352][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:51:44,352][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:51:44,594][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:51:44,826][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:51:44,826][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:51:44,843][gdb0][DEBUG ] detect machine type
[2017-05-31 18:51:44,846][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:52:04,467][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:52:04,467][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memsto
[2017-05-31 18:52:04,467][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:52:04,467][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:52:04,467][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:52:04,467][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memsto', None)]
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fceef85e908>
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fceefab4aa0>
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:52:04,468][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:52:04,469][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memsto:
[2017-05-31 18:52:04,702][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:52:04,930][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:52:04,931][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:52:04,947][gdb0][DEBUG ] detect machine type
[2017-05-31 18:52:04,950][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:04,951][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:52:04,951][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:52:04,951][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:52:04,954][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memsto journal None activate False
[2017-05-31 18:52:04,954][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:04,956][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memsto
[2017-05-31 18:52:05,076][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:52:05,076][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:52:05,076][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:52:05,076][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:52:05,077][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:52:05,077][gdb0][WARNING]     args.func(args)
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-05-31 18:52:05,077][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-05-31 18:52:05,077][gdb0][WARNING]     return PrepareFilestore(args)
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-05-31 18:52:05,077][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-05-31 18:52:05,077][gdb0][WARNING]     self.set_type()
[2017-05-31 18:52:05,077][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-05-31 18:52:05,077][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-05-31 18:52:05,077][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memsto'
[2017-05-31 18:52:05,078][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:52:05,078][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memsto
[2017-05-31 18:52:05,078][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 18:52:07,388][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:52:07,388][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:52:07,388][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd0298fd908>
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:52:07,389][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd029b53aa0>
[2017-05-31 18:52:07,390][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:52:07,390][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:52:07,390][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:52:07,390][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:52:07,630][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:52:07,822][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:52:07,822][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:52:07,838][gdb0][DEBUG ] detect machine type
[2017-05-31 18:52:07,842][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:07,843][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:52:07,843][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:52:07,843][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:52:07,845][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:52:07,846][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:07,847][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:52:07,968][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:52:07,971][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:07,987][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:08,003][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:08,018][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:52:08,026][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 18:52:08,034][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.15711.tmp
[2017-05-31 18:52:08,037][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.15711.tmp
[2017-05-31 18:52:08,045][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.15711.tmp
[2017-05-31 18:52:13,057][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:52:13,057][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:13,060][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:52:13,225][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:52:21,817][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:52:21,817][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:52:21,817][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:52:21,817][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:52:21,817][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa09dd7f908>
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa09dfd5aa0>
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:52:21,818][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:52:21,818][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:52:22,050][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:52:22,278][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:52:22,278][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:52:22,294][gdb0][DEBUG ] detect machine type
[2017-05-31 18:52:22,298][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:22,298][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:52:22,299][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:52:22,299][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:52:22,299][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:22,301][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:52:22,421][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:52:22,421][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:52:22,421][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:52:22,425][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 18:52:22,425][gdb0][WARNING] activate: OSD uuid is c0f127b4-e253-45f0-9f51-71ce307580a4
[2017-05-31 18:52:22,425][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 18:52:22,425][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise c0f127b4-e253-45f0-9f51-71ce307580a4
[2017-05-31 18:52:22,489][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:52:22,489][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:52:22,489][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:52:22,489][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:52:22,489][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:52:22,489][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:52:22,490][gdb0][WARNING]     args.func(args)
[2017-05-31 18:52:22,490][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:52:22,490][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:52:22,490][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:52:22,490][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:52:22,490][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3625, in activate
[2017-05-31 18:52:22,490][gdb0][WARNING]     keyring=keyring,
[2017-05-31 18:52:22,490][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 18:52:22,490][gdb0][WARNING]     raise Error('ceph osd create failed', e, e.output)
[2017-05-31 18:52:22,490][gdb0][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 18:52:22.476913 7f087fdec700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 18:52:22,490][gdb0][WARNING] error connecting to the cluster
[2017-05-31 18:52:22,490][gdb0][WARNING] 
[2017-05-31 18:52:22,498][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:52:22,498][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:52:40,085][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:52:40,086][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb19a84b908>
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb19aaa1aa0>
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:52:40,087][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:52:40,087][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:52:40,326][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:52:40,558][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:52:40,558][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:52:40,574][gdb0][DEBUG ] detect machine type
[2017-05-31 18:52:40,578][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:40,579][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:52:40,579][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:52:40,579][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:52:40,581][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 18:52:40,581][gdb0][DEBUG ] create a keyring file
[2017-05-31 18:52:40,583][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:52:40,583][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:40,585][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:52:40,706][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:52:40,709][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:40,725][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:40,741][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:52:40,748][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:52:40,764][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 18:52:45,772][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:52:45,773][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:45,775][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:52:45,940][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:52:48,018][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:52:48,018][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:52:48,018][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:52:48,018][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:52:48,018][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb44c146908>
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb44c39caa0>
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:52:48,019][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:52:48,019][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:52:48,258][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:52:48,454][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:52:48,455][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:52:48,471][gdb0][DEBUG ] detect machine type
[2017-05-31 18:52:48,474][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:48,475][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:52:48,475][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:52:48,475][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:52:48,475][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:52:48,477][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:52:48,598][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:52:48,598][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:52:48,598][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:52:48,606][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 18:52:48,606][gdb0][WARNING] activate: OSD uuid is c0f127b4-e253-45f0-9f51-71ce307580a4
[2017-05-31 18:52:48,606][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 18:52:48,606][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise c0f127b4-e253-45f0-9f51-71ce307580a4
[2017-05-31 18:52:48,770][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.16053.tmp
[2017-05-31 18:52:48,771][gdb0][WARNING] activate: OSD id is 0
[2017-05-31 18:52:48,771][gdb0][WARNING] activate: Initializing OSD...
[2017-05-31 18:52:48,771][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 18:52:48,885][gdb0][WARNING] got monmap epoch 2
[2017-05-31 18:52:48,893][gdb0][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid c0f127b4-e253-45f0-9f51-71ce307580a4 --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 18:52:48,924][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 18:52:48,925][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 18:52:48,928][gdb0][WARNING] activate: Authorizing OSD key...
[2017-05-31 18:52:48,928][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 18:52:49,092][gdb0][WARNING] added key for osd.0
[2017-05-31 18:52:49,093][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.16053.tmp
[2017-05-31 18:52:49,093][gdb0][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-05-31 18:52:49,093][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:52:49,093][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:52:49,093][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:52:49,093][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:52:49,093][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:52:49,093][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:52:49,093][gdb0][WARNING]     args.func(args)
[2017-05-31 18:52:49,093][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:52:49,093][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:52:49,094][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3543, in activate_dir
[2017-05-31 18:52:49,094][gdb0][WARNING]     old = os.readlink(canonical)
[2017-05-31 18:52:49,094][gdb0][WARNING] OSError: [Errno 22] Invalid argument: '/var/lib/ceph/osd/ceph-0'
[2017-05-31 18:52:49,094][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:52:49,094][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:53:23,583][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:53:23,583][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3854a70fc8>
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f38553831b8>
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:53:23,584][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:53:23,584][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 18:53:23,584][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 18:53:23,584][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-05-31 18:53:23,585][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-05-31 18:53:23,610][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:53:23,624][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:53:23,625][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:53:23,641][gdb3][DEBUG ] detect machine type
[2017-05-31 18:53:23,644][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:53:23,644][gdb3][INFO  ] Purging Ceph on gdb3
[2017-05-31 18:53:23,645][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 18:53:23,683][gdb3][DEBUG ] Reading package lists...
[2017-05-31 18:53:23,847][gdb3][DEBUG ] Building dependency tree...
[2017-05-31 18:53:23,847][gdb3][DEBUG ] Reading state information...
[2017-05-31 18:53:23,911][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 18:53:23,911][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 18:53:23,911][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 18:53:23,912][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 18:53:23,912][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 18:53:23,976][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-05-31 18:53:23,976][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 18:53:24,091][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 18:53:24,091][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 18:53:24,123][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104522 files and directories currently installed.)
[2017-05-31 18:53:24,123][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 18:53:24,287][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 18:53:24,402][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 18:53:24,405][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 18:53:24,621][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 18:53:24,737][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 18:53:24,905][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 18:53:24,969][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-05-31 18:53:25,000][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 18:53:25,167][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 18:53:25,232][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-05-31 18:53:25,233][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 18:53:25,397][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 18:53:25,429][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 18:53:25,593][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 18:53:25,657][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 18:53:25,665][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 18:53:25,779][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 18:53:26,595][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 18:53:26,761][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:53:26,761][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-05-31 18:53:26,761][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:53:26,761][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:53:26,761][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6c68cf3710>
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f6c69600230>
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:53:26,762][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:53:26,762][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-05-31 18:53:26,788][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:53:26,802][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:53:26,802][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:53:26,819][gdb3][DEBUG ] detect machine type
[2017-05-31 18:53:26,821][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:53:26,837][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:53:26,850][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:53:26,851][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:53:26,867][gdb3][DEBUG ] detect machine type
[2017-05-31 18:53:26,869][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:53:26,869][gdb3][INFO  ] purging data on gdb3
[2017-05-31 18:53:26,870][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 18:53:26,883][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 18:53:27,052][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa6a31b89e0>
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fa6a3a7b848>
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:53:27,053][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:53:59,636][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:53:59,636][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1af7e04560>
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f1af8488758>
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:53:59,637][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-05-31 18:53:59,638][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:53:59,638][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-05-31 18:53:59,638][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-05-31 18:53:59,638][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-05-31 18:53:59,664][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:53:59,678][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:53:59,678][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:53:59,695][gdb3][DEBUG ] detect machine type
[2017-05-31 18:53:59,697][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:53:59,698][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-05-31 18:53:59,710][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-05-31 18:53:59,716][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-05-31 18:53:59,716][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-05-31 18:53:59,716][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-05-31 18:53:59,716][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-05-31 18:53:59,716][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-05-31 18:53:59,716][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-05-31 18:53:59,717][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-05-31 18:53:59,717][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-05-31 18:53:59,881][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f73b3696e60>
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f73b366bb18>
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-05-31 18:53:59,882][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:53:59,883][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-05-31 18:53:59,883][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-05-31 18:53:59,909][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:53:59,923][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:53:59,924][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:53:59,940][gdb3][DEBUG ] detect machine type
[2017-05-31 18:53:59,942][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:53:59,943][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-05-31 18:53:59,943][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-05-31 18:53:59,943][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:53:59,943][gdb3][DEBUG ] deploying mon to gdb3
[2017-05-31 18:53:59,943][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:53:59,944][gdb3][DEBUG ] remote hostname: gdb3
[2017-05-31 18:53:59,944][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:53:59,946][gdb3][DEBUG ] create the mon path if it does not exist
[2017-05-31 18:53:59,946][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 18:53:59,946][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-05-31 18:53:59,947][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 18:53:59,947][gdb3][DEBUG ] create the monitor keyring file
[2017-05-31 18:53:59,948][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-05-31 18:53:59,986][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-05-31 18:53:59,986][gdb3][DEBUG ] ceph-mon: set fsid to cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 18:53:59,988][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-05-31 18:53:59,989][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-05-31 18:53:59,989][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-05-31 18:53:59,990][gdb3][DEBUG ] create the init path if it does not exist
[2017-05-31 18:53:59,991][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 18:54:00,060][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-05-31 18:54:00,129][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-05-31 18:54:02,171][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:54:02,236][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 18:54:02,236][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-05-31 18:54:02,237][gdb3][DEBUG ] {
[2017-05-31 18:54:02,237][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]   "features": {
[2017-05-31 18:54:02,237][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]     "quorum_mon": [
[2017-05-31 18:54:02,237][gdb3][DEBUG ]       "kraken", 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]       "luminous"
[2017-05-31 18:54:02,237][gdb3][DEBUG ]     ], 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]     "required_mon": [
[2017-05-31 18:54:02,237][gdb3][DEBUG ]       "kraken", 
[2017-05-31 18:54:02,237][gdb3][DEBUG ]       "luminous"
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     ]
[2017-05-31 18:54:02,238][gdb3][DEBUG ]   }, 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]   "monmap": {
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "created": "2017-05-31 18:53:59.971511", 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "epoch": 2, 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "features": {
[2017-05-31 18:54:02,238][gdb3][DEBUG ]       "optional": [], 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]       "persistent": [
[2017-05-31 18:54:02,238][gdb3][DEBUG ]         "kraken", 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]         "luminous"
[2017-05-31 18:54:02,238][gdb3][DEBUG ]       ]
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     }, 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "fsid": "cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab", 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "modified": "2017-05-31 18:54:00.211295", 
[2017-05-31 18:54:02,238][gdb3][DEBUG ]     "mons": [
[2017-05-31 18:54:02,238][gdb3][DEBUG ]       {
[2017-05-31 18:54:02,238][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]         "name": "gdb3", 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]         "rank": 0
[2017-05-31 18:54:02,239][gdb3][DEBUG ]       }
[2017-05-31 18:54:02,239][gdb3][DEBUG ]     ]
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   }, 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "name": "gdb3", 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "quorum": [
[2017-05-31 18:54:02,239][gdb3][DEBUG ]     0
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   ], 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "rank": 0, 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "state": "leader", 
[2017-05-31 18:54:02,239][gdb3][DEBUG ]   "sync_provider": []
[2017-05-31 18:54:02,239][gdb3][DEBUG ] }
[2017-05-31 18:54:02,239][gdb3][DEBUG ] ********************************************************************************
[2017-05-31 18:54:02,239][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-05-31 18:54:02,240][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:54:02,306][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-05-31 18:54:02,320][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:54:02,334][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:54:02,335][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:54:02,351][gdb3][DEBUG ] detect machine type
[2017-05-31 18:54:02,353][gdb3][DEBUG ] find the location of an executable
[2017-05-31 18:54:02,354][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:54:02,420][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-05-31 18:54:02,420][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-05-31 18:54:02,420][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-05-31 18:54:02,422][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpSV9u_P
[2017-05-31 18:54:02,437][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:54:02,451][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:54:02,452][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:54:02,468][gdb3][DEBUG ] detect machine type
[2017-05-31 18:54:02,470][gdb3][DEBUG ] get remote short hostname
[2017-05-31 18:54:02,471][gdb3][DEBUG ] fetch remote file
[2017-05-31 18:54:02,472][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-05-31 18:54:02,538][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-05-31 18:54:02,704][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-05-31 18:54:02,870][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-05-31 18:54:03,036][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-05-31 18:54:03,253][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-05-31 18:54:03,419][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-05-31 18:54:03,586][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-05-31 18:54:03,752][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-05-31 18:54:03,918][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-05-31 18:54:04,084][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-05-31 18:54:04,249][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170531185404'
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-05-31 18:54:04,250][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpSV9u_P
[2017-05-31 18:54:04,431][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:54:04,431][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 18:54:04,431][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc3e268f518>
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fc3e2fa6938>
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:54:04,432][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:54:04,432][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 18:54:04,459][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:54:04,473][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:54:04,473][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:54:04,490][gdb3][DEBUG ] detect machine type
[2017-05-31 18:54:04,492][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:54:45,775][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:54:45,775][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f25debd2518>
[2017-05-31 18:54:45,776][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:54:45,776][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 18:54:45,776][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f25df4e9938>
[2017-05-31 18:54:45,776][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:54:45,776][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:54:45,776][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 18:54:45,803][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:54:45,818][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:54:45,818][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:54:45,835][gdb3][DEBUG ] detect machine type
[2017-05-31 18:54:45,838][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:55:44,967][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f83cffec518>
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f83d0903938>
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:55:44,968][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:55:44,969][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:55:45,210][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:55:45,442][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:55:45,442][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:55:45,458][gdb0][DEBUG ] detect machine type
[2017-05-31 18:55:45,461][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:55:45,463][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 18:55:45,464][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 18:55:56,020][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:55:56,020][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7551d43518>
[2017-05-31 18:55:56,021][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:55:56,021][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:55:56,021][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f755265a938>
[2017-05-31 18:55:56,021][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:55:56,021][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:55:56,021][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:55:56,262][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:55:56,494][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:55:56,494][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:55:56,510][gdb0][DEBUG ] detect machine type
[2017-05-31 18:55:56,513][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:56:09,705][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:56:09,705][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:56:09,705][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:56:09,705][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:56:09,705][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:56:09,705][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feadbb8b908>
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7feadbde1aa0>
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:56:09,706][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:56:09,707][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:56:09,942][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:56:10,174][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:56:10,174][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:56:10,190][gdb0][DEBUG ] detect machine type
[2017-05-31 18:56:10,194][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:10,194][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:56:10,194][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:56:10,195][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:56:10,197][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 18:56:10,197][gdb0][DEBUG ] create a keyring file
[2017-05-31 18:56:10,199][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:56:10,199][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:10,201][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:56:10,322][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:56:10,323][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:56:10,339][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:56:10,355][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:56:10,362][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:56:10,378][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 18:56:15,387][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:56:15,387][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:15,389][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:56:15,555][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:56:24,144][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:56:24,144][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:56:24,144][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:56:24,144][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:56:24,144][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f896b4a4908>
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f896b6faaa0>
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:56:24,145][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:56:24,145][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:56:24,386][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:56:24,610][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:56:24,610][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:56:24,626][gdb0][DEBUG ] detect machine type
[2017-05-31 18:56:24,629][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:24,630][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:56:24,630][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:56:24,630][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:56:24,631][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:24,632][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:56:24,753][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:56:24,753][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:56:24,753][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:56:24,757][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:56:24,757][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:56:24,757][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:56:24,757][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:56:24,757][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:56:24,758][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:56:24,758][gdb0][WARNING]     args.func(args)
[2017-05-31 18:56:24,758][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:56:24,758][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:56:24,758][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:56:24,758][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:56:24,758][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:56:24,758][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:56:24,758][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:56:24,766][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:56:24,766][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:56:52,810][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:56:52,810][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff039de8518>
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7ff03a6ff938>
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:56:52,811][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:56:52,811][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:56:53,054][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:56:53,278][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:56:53,279][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:56:53,295][gdb0][DEBUG ] detect machine type
[2017-05-31 18:56:53,298][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:56:57,584][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:56:57,584][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0178db6908>
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f017900caa0>
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:56:57,585][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:56:57,586][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:56:57,826][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:56:58,053][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:56:58,054][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:56:58,069][gdb0][DEBUG ] detect machine type
[2017-05-31 18:56:58,073][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:58,074][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:56:58,074][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:56:58,074][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:56:58,075][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:56:58,076][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:56:58,197][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:56:58,197][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:56:58,197][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:56:58,198][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:56:58,198][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:56:58,198][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:56:58,199][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:56:58,199][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:56:58,199][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:56:58,199][gdb0][WARNING]     args.func(args)
[2017-05-31 18:56:58,199][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:56:58,200][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:56:58,200][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:56:58,200][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:56:58,200][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:56:58,200][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:56:58,200][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:56:58,216][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:56:58,216][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:57:09,851][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:57:09,851][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7c2bbe7518>
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f7c2c4fe938>
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:57:09,852][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:57:09,852][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-05-31 18:57:09,878][gdb3][DEBUG ] connection detected need for sudo
[2017-05-31 18:57:09,892][gdb3][DEBUG ] connected to host: gdb3 
[2017-05-31 18:57:09,893][gdb3][DEBUG ] detect platform information from remote host
[2017-05-31 18:57:09,909][gdb3][DEBUG ] detect machine type
[2017-05-31 18:57:09,911][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:57:16,149][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa7d3366908>
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa7d35bcaa0>
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:57:16,150][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:57:16,151][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:57:16,386][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:57:16,613][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:57:16,614][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:57:16,629][gdb0][DEBUG ] detect machine type
[2017-05-31 18:57:16,633][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:16,633][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:57:16,634][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:57:16,634][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:57:16,634][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:16,636][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:57:16,756][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:57:16,756][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:57:16,757][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:57:16,760][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:57:16,760][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:57:16,760][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:57:16,761][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:57:16,761][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:57:16,761][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:57:16,761][gdb0][WARNING]     args.func(args)
[2017-05-31 18:57:16,761][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:57:16,761][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:57:16,761][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:57:16,761][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:57:16,761][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:57:16,761][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:57:16,761][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:57:16,769][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:57:16,769][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:57:21,647][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:57:21,647][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:57:21,647][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:57:21,647][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efcda9f6908>
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7efcdac4caa0>
[2017-05-31 18:57:21,648][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:57:21,649][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:57:21,649][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:57:21,649][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:57:21,890][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:57:22,118][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:57:22,118][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:57:22,134][gdb0][DEBUG ] detect machine type
[2017-05-31 18:57:22,137][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:22,138][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:57:22,138][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:57:22,138][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:57:22,140][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:57:22,141][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:22,142][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:57:22,263][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:57:22,266][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:57:22,282][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:57:22,297][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:57:22,305][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:57:22,321][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 18:57:27,329][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:57:27,330][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:27,332][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:57:27,497][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:57:29,326][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:57:29,326][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:57:29,326][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:57:29,326][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:57:29,326][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:57:29,326][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efd3714f908>
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7efd373a5aa0>
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:57:29,327][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:57:29,327][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:57:29,562][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:57:29,786][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:57:29,786][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:57:29,802][gdb0][DEBUG ] detect machine type
[2017-05-31 18:57:29,805][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:29,806][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:57:29,806][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:57:29,806][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:57:29,806][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:57:29,808][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:57:29,929][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:57:29,929][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:57:29,929][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:57:29,930][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:57:29,930][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:57:29,931][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:57:29,931][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:57:29,931][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:57:29,931][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:57:29,931][gdb0][WARNING]     args.func(args)
[2017-05-31 18:57:29,931][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:57:29,931][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:57:29,932][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:57:29,932][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:57:29,932][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:57:29,932][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:57:29,932][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:57:29,948][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:57:29,948][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 18:58:02,594][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f37bf68c518>
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f37bffa3938>
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:58:02,595][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:58:02,596][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 18:58:02,838][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:58:03,062][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:58:03,062][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:58:03,078][gdb0][DEBUG ] detect machine type
[2017-05-31 18:58:03,082][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:58:05,043][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:58:05,043][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 18:58:05,043][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9982fa1908>
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f99831f7aa0>
[2017-05-31 18:58:05,044][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:58:05,045][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:58:05,045][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 18:58:05,045][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:58:05,286][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:58:05,509][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:58:05,510][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:58:05,526][gdb0][DEBUG ] detect machine type
[2017-05-31 18:58:05,529][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:58:05,530][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:58:05,530][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 18:58:05,530][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 18:58:05,533][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 18:58:05,533][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:58:05,534][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 18:58:05,655][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:58:05,658][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:58:05,674][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:58:05,690][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 18:58:05,697][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 18:58:05,713][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 18:58:10,722][gdb0][INFO  ] checking OSD status...
[2017-05-31 18:58:10,722][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:58:10,724][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 18:58:10,889][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 18:58:14,417][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 18:58:14,417][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fea3fd2a908>
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fea3ff80aa0>
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 18:58:14,418][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 18:58:14,418][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 18:58:14,658][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 18:58:14,881][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 18:58:14,882][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 18:58:14,898][gdb0][DEBUG ] detect machine type
[2017-05-31 18:58:14,901][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:58:14,902][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 18:58:14,902][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 18:58:14,902][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 18:58:14,902][gdb0][DEBUG ] find the location of an executable
[2017-05-31 18:58:14,904][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 18:58:15,025][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 18:58:15,025][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:58:15,025][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 18:58:15,028][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 18:58:15,028][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 18:58:15,029][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 18:58:15,029][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 18:58:15,029][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 18:58:15,029][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 18:58:15,029][gdb0][WARNING]     args.func(args)
[2017-05-31 18:58:15,029][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 18:58:15,029][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 18:58:15,030][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 18:58:15,030][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 18:58:15,030][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 18:58:15,030][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 18:58:15,030][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 18:58:15,046][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 18:58:15,046][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:00:23,548][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:00:23,548][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2fbebd7908>
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2fbee2daa0>
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:00:23,549][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:00:23,550][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:00:23,791][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:00:24,022][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:00:24,022][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:00:24,038][gdb0][DEBUG ] detect machine type
[2017-05-31 19:00:24,042][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:00:24,042][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:00:24,043][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 19:00:24,043][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:00:24,045][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 19:00:24,045][gdb0][DEBUG ] create a keyring file
[2017-05-31 19:00:24,047][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 19:00:24,047][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:00:24,049][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:00:24,169][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:00:24,171][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:00:24,186][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:00:24,202][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:00:24,210][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:00:24,225][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 19:00:29,234][gdb0][INFO  ] checking OSD status...
[2017-05-31 19:00:29,234][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:00:29,237][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:00:29,402][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 19:00:34,139][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:00:34,139][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff7156d0908>
[2017-05-31 19:00:34,140][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:00:34,140][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff715926aa0>
[2017-05-31 19:00:34,140][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:00:34,140][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:00:34,140][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:00:34,140][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:00:34,378][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:00:34,605][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:00:34,606][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:00:34,621][gdb0][DEBUG ] detect machine type
[2017-05-31 19:00:34,625][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:00:34,626][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:00:34,626][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 19:00:34,626][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:00:34,626][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:00:34,628][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:00:34,748][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:00:34,749][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 19:00:34,749][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:00:34,752][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 19:00:34,752][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 19:00:34,753][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:00:34,753][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:00:34,753][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 19:00:34,753][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:00:34,753][gdb0][WARNING]     args.func(args)
[2017-05-31 19:00:34,753][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:00:34,753][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 19:00:34,754][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:00:34,754][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 19:00:34,754][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 19:00:34,754][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 19:00:34,754][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 19:00:34,762][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:00:34,762][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:01:30,099][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:01:30,100][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb097308908>
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb09755eaa0>
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:01:30,101][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:01:30,101][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:01:30,348][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:01:30,579][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:01:30,579][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:01:30,596][gdb1][DEBUG ] detect machine type
[2017-05-31 19:01:30,600][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:01:30,600][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:01:30,601][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 19:01:30,601][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:01:30,603][ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 19:01:30,603][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-05-31 19:01:40,792][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:01:40,792][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb1
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fbbd048a518>
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fbbd0da1938>
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:01:40,793][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:01:40,793][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 19:01:41,031][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:01:41,263][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:01:41,263][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:01:41,280][gdb1][DEBUG ] detect machine type
[2017-05-31 19:01:41,283][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:01:41,285][ceph_deploy.admin][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[2017-05-31 19:01:41,285][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-05-31 19:01:48,568][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0433916518>
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f043422d938>
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:01:48,569][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:01:48,569][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 19:01:48,811][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:01:49,047][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:01:49,047][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:01:49,063][gdb1][DEBUG ] detect machine type
[2017-05-31 19:01:49,067][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:01:52,711][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:01:52,711][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7a9172a908>
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7a91980aa0>
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:01:52,712][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:01:52,713][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:01:52,959][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:01:53,190][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:01:53,191][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:01:53,207][gdb1][DEBUG ] detect machine type
[2017-05-31 19:01:53,211][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:01:53,211][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:01:53,211][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 19:01:53,212][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:01:53,214][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 19:01:53,214][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:01:53,216][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:01:53,336][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:01:53,352][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:01:53,368][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:01:53,383][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:01:53,391][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:01:53,407][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 19:01:53,407][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.16092.tmp
[2017-05-31 19:01:53,407][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.16092.tmp
[2017-05-31 19:01:53,411][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.16092.tmp
[2017-05-31 19:01:58,432][gdb1][INFO  ] checking OSD status...
[2017-05-31 19:01:58,432][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:01:58,434][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:01:58,600][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 19:02:05,404][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:02:05,404][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 19:02:05,404][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:02:05,404][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fef4e246908>
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fef4e49caa0>
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:02:05,405][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:02:05,405][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:02:05,643][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:02:05,874][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:02:05,875][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:02:05,891][gdb1][DEBUG ] detect machine type
[2017-05-31 19:02:05,895][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:05,895][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:02:05,895][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 19:02:05,896][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:02:05,896][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:05,898][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:02:06,018][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:02:06,018][gdb1][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:02:06,018][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:02:06,034][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 19:02:06,034][gdb1][WARNING] activate: OSD uuid is 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:06,034][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:02:06,034][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:06,099][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 19:02:06,099][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 19:02:06,099][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:02:06,099][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:02:06,099][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:02:06,099][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:02:06,100][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:02:06,100][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 19:02:06,100][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 19:02:06,100][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 19:02:06.089590 7f162bcf1700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 19:02:06,100][gdb1][WARNING] error connecting to the cluster
[2017-05-31 19:02:06,100][gdb1][WARNING] 
[2017-05-31 19:02:06,108][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:02:06,108][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:02:26,603][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:02:26,603][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 19:02:26,603][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:02:26,603][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:02:26,603][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:02:26,603][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f31d72c1908>
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f31d7517aa0>
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:02:26,604][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:02:26,604][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:02:26,851][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:02:27,083][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:02:27,084][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:02:27,100][gdb1][DEBUG ] detect machine type
[2017-05-31 19:02:27,104][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:27,105][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:02:27,105][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 19:02:27,105][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:02:27,105][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:27,107][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:02:27,278][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:02:27,278][gdb1][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:02:27,278][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:02:27,278][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 19:02:27,278][gdb1][WARNING] activate: OSD uuid is 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:27,278][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:02:27,278][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:27,310][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 19:02:27,311][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 19:02:27,311][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:02:27,311][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:02:27,311][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:02:27,312][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:02:27,312][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:02:27,312][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 19:02:27,312][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 19:02:27,312][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 19:02:27.301230 7fc80c4df700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 19:02:27,312][gdb1][WARNING] error connecting to the cluster
[2017-05-31 19:02:27,312][gdb1][WARNING] 
[2017-05-31 19:02:27,320][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:02:27,320][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:02:38,060][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:02:38,060][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f14bb70e908>
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:02:38,061][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:02:38,062][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f14bb964aa0>
[2017-05-31 19:02:38,062][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:02:38,062][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:02:38,062][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:02:38,062][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:02:38,303][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:02:38,535][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:02:38,535][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:02:38,551][gdb1][DEBUG ] detect machine type
[2017-05-31 19:02:38,555][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:38,556][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:02:38,556][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 19:02:38,556][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:02:38,558][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 19:02:38,558][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:38,560][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:02:38,680][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:02:38,696][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:02:38,712][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:02:38,728][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:02:38,735][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:02:38,751][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 19:02:43,760][gdb1][INFO  ] checking OSD status...
[2017-05-31 19:02:43,760][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:43,762][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:02:43,928][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 19:02:45,629][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:02:45,629][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 19:02:45,629][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:02:45,629][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:02:45,629][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:02:45,629][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8b71056908>
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8b712acaa0>
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:02:45,630][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:02:45,630][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:02:45,867][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:02:46,099][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:02:46,099][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:02:46,115][gdb1][DEBUG ] detect machine type
[2017-05-31 19:02:46,119][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:46,120][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:02:46,120][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 19:02:46,120][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:02:46,120][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:02:46,122][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:02:46,243][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:02:46,243][gdb1][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:02:46,243][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:02:46,259][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 19:02:46,259][gdb1][WARNING] activate: OSD uuid is 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:46,259][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:02:46,259][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:02:46,373][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 19:02:46,373][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 19:02:46,373][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 19:02:46,374][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 19:02:46,374][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 19:02:46.315195 7ff02b813700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 19:02:46,374][gdb1][WARNING] error connecting to the cluster
[2017-05-31 19:02:46,374][gdb1][WARNING] 
[2017-05-31 19:02:46,374][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:02:46,374][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:03:30,541][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:03:30,541][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 19:03:30,541][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:03:30,541][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:03:30,541][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcc71562908>
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcc717b8aa0>
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:03:30,542][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:03:30,543][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:03:30,783][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:03:31,015][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:03:31,015][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:03:31,031][gdb1][DEBUG ] detect machine type
[2017-05-31 19:03:31,035][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:03:31,035][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:03:31,036][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 19:03:31,036][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:03:31,038][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 19:03:31,038][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:03:31,040][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:03:31,160][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:03:31,176][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:03:31,192][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:03:31,208][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:03:31,215][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:03:31,231][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 19:03:36,239][gdb1][INFO  ] checking OSD status...
[2017-05-31 19:03:36,240][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:03:36,242][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:03:36,408][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 19:03:41,172][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:03:41,172][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7d45f48908>
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7d4619eaa0>
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:03:41,173][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:03:41,174][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:03:41,415][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:03:41,647][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:03:41,647][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:03:41,664][gdb1][DEBUG ] detect machine type
[2017-05-31 19:03:41,667][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:03:41,668][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:03:41,668][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 19:03:41,668][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:03:41,668][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:03:41,670][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:03:41,790][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:03:41,791][gdb1][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:03:41,791][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:03:41,806][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 19:03:41,806][gdb1][WARNING] activate: OSD uuid is 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:03:41,807][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:03:41,807][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:03:41,871][gdb1][WARNING] Traceback (most recent call last):
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-05-31 19:03:41,871][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-05-31 19:03:41,871][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-05-31 19:03:41,872][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-05-31 19:03:41.861160 7f9a5c447700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-05-31 19:03:41,872][gdb1][WARNING] error connecting to the cluster
[2017-05-31 19:03:41,872][gdb1][WARNING] 
[2017-05-31 19:03:41,879][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:03:41,880][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:04:13,941][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:04:13,941][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-05-31 19:04:13,941][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f24f8549518>
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f24f8e60938>
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:04:13,942][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:04:13,942][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-05-31 19:04:14,184][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:04:14,415][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:04:14,415][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:04:14,432][gdb1][DEBUG ] detect machine type
[2017-05-31 19:04:14,435][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:04:18,227][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:04:18,228][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa794a1a908>
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa794c70aa0>
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:04:18,229][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:04:18,229][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:04:18,471][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:04:18,703][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:04:18,704][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:04:18,720][gdb1][DEBUG ] detect machine type
[2017-05-31 19:04:18,724][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:18,725][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:04:18,725][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-05-31 19:04:18,725][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:04:18,727][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 19:04:18,727][gdb1][DEBUG ] create a keyring file
[2017-05-31 19:04:18,729][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-05-31 19:04:18,729][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:18,731][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:04:18,901][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:04:18,902][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:04:18,902][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:04:18,902][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:04:18,909][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:04:18,925][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 19:04:23,934][gdb1][INFO  ] checking OSD status...
[2017-05-31 19:04:23,934][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:23,936][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:04:24,102][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-05-31 19:04:27,131][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:04:27,131][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f10404af908>
[2017-05-31 19:04:27,132][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:04:27,132][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1040705aa0>
[2017-05-31 19:04:27,132][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:04:27,132][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:04:27,132][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-05-31 19:04:27,132][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-05-31 19:04:27,375][gdb1][DEBUG ] connection detected need for sudo
[2017-05-31 19:04:27,606][gdb1][DEBUG ] connected to host: gdb1 
[2017-05-31 19:04:27,607][gdb1][DEBUG ] detect platform information from remote host
[2017-05-31 19:04:27,623][gdb1][DEBUG ] detect machine type
[2017-05-31 19:04:27,627][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:27,628][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:04:27,628][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-05-31 19:04:27,628][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:04:27,628][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:27,630][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:04:27,751][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:04:27,751][gdb1][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:04:27,751][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:04:27,767][gdb1][WARNING] activate: Cluster name is ceph
[2017-05-31 19:04:27,767][gdb1][WARNING] activate: OSD uuid is 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:04:27,767][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:04:27,767][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 7c61a5ba-3130-4674-af68-e74e94fe90f6
[2017-05-31 19:04:27,931][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.17048.tmp
[2017-05-31 19:04:27,931][gdb1][WARNING] activate: OSD id is 0
[2017-05-31 19:04:27,931][gdb1][WARNING] activate: Initializing OSD...
[2017-05-31 19:04:27,931][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 19:04:28,096][gdb1][WARNING] got monmap epoch 2
[2017-05-31 19:04:28,096][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid 7c61a5ba-3130-4674-af68-e74e94fe90f6 --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 19:04:28,128][gdb1][WARNING] activate: Marking with init system systemd
[2017-05-31 19:04:28,128][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 19:04:28,128][gdb1][WARNING] activate: Authorizing OSD key...
[2017-05-31 19:04:28,128][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 19:04:28,292][gdb1][WARNING] added key for osd.0
[2017-05-31 19:04:28,293][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.17048.tmp
[2017-05-31 19:04:28,293][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-05-31 19:04:28,293][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-0 -> /mnt/memstore
[2017-05-31 19:04:28,293][gdb1][WARNING] start_daemon: Starting ceph osd.0...
[2017-05-31 19:04:28,293][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-05-31 19:04:28,293][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-05-31 19:04:28,357][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-05-31 19:04:28,389][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-05-31 19:04:28,392][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 19:04:28,456][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-05-31 19:04:33,576][gdb1][INFO  ] checking OSD status...
[2017-05-31 19:04:33,576][gdb1][DEBUG ] find the location of an executable
[2017-05-31 19:04:33,578][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:04:33,746][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-05-31 19:04:48,602][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2e86347518>
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:04:48,602][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 19:04:48,603][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f2e86c5e938>
[2017-05-31 19:04:48,603][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:04:48,603][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:04:48,603][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 19:04:48,842][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:04:49,070][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:04:49,070][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:04:49,086][gdb0][DEBUG ] detect machine type
[2017-05-31 19:04:49,090][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:05:02,677][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:05:02,677][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f56878ce908>
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5687b24aa0>
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:05:02,678][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:05:02,679][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:05:02,919][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:05:03,150][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:05:03,151][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:05:03,167][gdb0][DEBUG ] detect machine type
[2017-05-31 19:05:03,170][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:05:03,171][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:05:03,171][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 19:05:03,171][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:05:03,174][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 19:05:03,174][gdb0][DEBUG ] create a keyring file
[2017-05-31 19:05:03,176][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 19:05:03,176][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:05:03,178][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:05:03,298][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:05:03,306][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:05:03,321][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:05:03,329][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:05:03,345][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:05:03,352][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-05-31 19:05:08,373][gdb0][INFO  ] checking OSD status...
[2017-05-31 19:05:08,373][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:05:08,376][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:05:08,541][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 19:05:18,167][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:05:18,167][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 19:05:18,167][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:05:18,167][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f46d5fcb908>
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f46d6221aa0>
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:05:18,168][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:05:18,168][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:05:18,406][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:05:18,630][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:05:18,630][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:05:18,646][gdb0][DEBUG ] detect machine type
[2017-05-31 19:05:18,649][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:05:18,650][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:05:18,650][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 19:05:18,650][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:05:18,651][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:05:18,652][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:05:18,773][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:05:18,773][gdb0][WARNING] activate: Cluster uuid is c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 19:05:18,773][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:05:18,776][gdb0][WARNING] Traceback (most recent call last):
[2017-05-31 19:05:18,776][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-05-31 19:05:18,776][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-05-31 19:05:18,777][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-05-31 19:05:18,777][gdb0][WARNING]     main(sys.argv[1:])
[2017-05-31 19:05:18,777][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-05-31 19:05:18,777][gdb0][WARNING]     args.func(args)
[2017-05-31 19:05:18,777][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-05-31 19:05:18,777][gdb0][WARNING]     init=args.mark_init,
[2017-05-31 19:05:18,779][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-05-31 19:05:18,779][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-05-31 19:05:18,779][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-05-31 19:05:18,779][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-05-31 19:05:18,779][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid c676bc10-14a8-4f7c-93e4-c9f0324506d5
[2017-05-31 19:05:18,787][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-05-31 19:05:18,787][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-05-31 19:08:02,744][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:08:02,744][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb0
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa2089aefc8>
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  host                          : ['gdb0']
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fa2092c11b8>
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:08:02,745][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:08:02,745][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-05-31 19:08:02,745][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-05-31 19:08:02,745][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb0
[2017-05-31 19:08:02,745][ceph_deploy.install][DEBUG ] Detecting platform for host gdb0 ...
[2017-05-31 19:08:03,057][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:08:03,290][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:08:03,290][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:08:03,321][gdb0][DEBUG ] detect machine type
[2017-05-31 19:08:03,324][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:08:03,324][gdb0][INFO  ] Purging Ceph on gdb0
[2017-05-31 19:08:03,326][gdb0][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-05-31 19:08:03,647][gdb0][DEBUG ] Reading package lists...
[2017-05-31 19:08:03,761][gdb0][DEBUG ] Building dependency tree...
[2017-05-31 19:08:03,761][gdb0][DEBUG ] Reading state information...
[2017-05-31 19:08:03,825][gdb0][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-05-31 19:08:03,825][gdb0][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-05-31 19:08:03,825][gdb0][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-05-31 19:08:03,825][gdb0][DEBUG ]   ceph-fuse javascript-common libaio1 libcephfs2 libgoogle-perftools4
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   libibverbs1 libjs-jquery libleveldb1v5 liblttng-ust-ctl2 liblttng-ust0
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   libnspr4 libnss3 libnss3-nssdb libopts25 libpython2.7 librados2
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   libradosstriper1 librbd1 librgw2 libsnappy1v5 libtcmalloc-minimal4
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   libunwind8 liburcu4 linux-aws-headers-4.4.0-1013
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   linux-headers-4.4.0-1013-aws linux-image-4.4.0-1013-aws ntp python-blinker
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-05-31 19:08:03,826][gdb0][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-05-31 19:08:03,826][gdb0][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-05-31 19:08:03,858][gdb0][DEBUG ] The following packages will be REMOVED:
[2017-05-31 19:08:03,858][gdb0][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-05-31 19:08:04,173][gdb0][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 35 not upgraded.
[2017-05-31 19:08:04,173][gdb0][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-05-31 19:08:04,538][gdb0][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 104462 files and directories currently installed.)
[2017-05-31 19:08:04,539][gdb0][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-05-31 19:08:04,753][gdb0][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-05-31 19:08:04,817][gdb0][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-05-31 19:08:04,881][gdb0][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-05-31 19:08:05,146][gdb0][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-05-31 19:08:05,260][gdb0][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-05-31 19:08:05,425][gdb0][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-05-31 19:08:05,539][gdb0][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-05-31 19:08:05,703][gdb0][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-05-31 19:08:05,818][gdb0][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-05-31 19:08:05,982][gdb0][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-05-31 19:08:05,998][gdb0][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/bootstrap-osd' not empty so not removed
[2017-05-31 19:08:05,998][gdb0][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-05-31 19:08:06,062][gdb0][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-05-31 19:08:06,176][gdb0][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-05-31 19:08:06,290][gdb0][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-05-31 19:08:06,291][gdb0][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-05-31 19:08:06,455][gdb0][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-05-31 19:08:07,522][gdb0][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-05-31 19:08:33,625][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:08:33,625][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb0
[2017-05-31 19:08:33,625][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:08:33,625][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:08:33,625][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff824f26710>
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  host                          : ['gdb0']
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7ff825833230>
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:08:33,626][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:08:33,626][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb0
[2017-05-31 19:08:33,866][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:08:34,058][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:08:34,058][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:08:34,074][gdb0][DEBUG ] detect machine type
[2017-05-31 19:08:34,077][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:08:34,302][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:08:34,529][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:08:34,530][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:08:34,545][gdb0][DEBUG ] detect machine type
[2017-05-31 19:08:34,549][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:08:34,549][gdb0][INFO  ] purging data on gdb0
[2017-05-31 19:08:34,550][gdb0][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-05-31 19:08:34,564][gdb0][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-05-31 19:09:11,389][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:09:11,389][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb0
[2017-05-31 19:09:11,389][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:09:11,389][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:09:11,389][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb55bdce518>
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fb55c6e5938>
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:09:11,390][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:09:11,390][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-05-31 19:09:11,626][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:09:11,853][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:09:11,854][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:09:11,869][gdb0][DEBUG ] detect machine type
[2017-05-31 19:09:11,873][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:09:32,698][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:09:32,698][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-05-31 19:09:32,698][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fadccfdd908>
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-05-31 19:09:32,699][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fadcd233aa0>
[2017-05-31 19:09:32,700][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:09:32,700][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:09:32,700][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-05-31 19:09:32,700][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:09:32,938][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:09:33,162][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:09:33,162][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:09:33,178][gdb0][DEBUG ] detect machine type
[2017-05-31 19:09:33,182][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:33,182][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:09:33,182][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-05-31 19:09:33,183][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-05-31 19:09:33,185][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-05-31 19:09:33,185][gdb0][DEBUG ] create a keyring file
[2017-05-31 19:09:33,187][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-05-31 19:09:33,187][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:33,189][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-05-31 19:09:33,410][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:09:33,417][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:09:33,433][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:09:33,449][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-05-31 19:09:33,456][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-05-31 19:09:33,472][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-05-31 19:09:33,488][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.3293.tmp
[2017-05-31 19:09:33,489][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.3293.tmp
[2017-05-31 19:09:33,492][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.3293.tmp
[2017-05-31 19:09:38,513][gdb0][INFO  ] checking OSD status...
[2017-05-31 19:09:38,513][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:38,516][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:09:38,882][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-05-31 19:09:44,982][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  username                      : None
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe13f609908>
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-05-31 19:09:44,982][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe13f85faa0>
[2017-05-31 19:09:44,983][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-05-31 19:09:44,983][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-05-31 19:09:44,983][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-05-31 19:09:44,983][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-05-31 19:09:45,226][gdb0][DEBUG ] connection detected need for sudo
[2017-05-31 19:09:45,453][gdb0][DEBUG ] connected to host: gdb0 
[2017-05-31 19:09:45,454][gdb0][DEBUG ] detect platform information from remote host
[2017-05-31 19:09:45,470][gdb0][DEBUG ] detect machine type
[2017-05-31 19:09:45,473][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:45,474][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-05-31 19:09:45,474][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-05-31 19:09:45,474][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-05-31 19:09:45,474][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:45,477][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-05-31 19:09:45,597][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-05-31 19:09:45,597][gdb0][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-05-31 19:09:45,597][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-05-31 19:09:45,601][gdb0][WARNING] activate: Cluster name is ceph
[2017-05-31 19:09:45,601][gdb0][WARNING] activate: OSD uuid is a397040a-60e7-423c-bb3e-a90280753962
[2017-05-31 19:09:45,601][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-05-31 19:09:45,601][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise a397040a-60e7-423c-bb3e-a90280753962
[2017-05-31 19:09:46,518][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.3415.tmp
[2017-05-31 19:09:46,518][gdb0][WARNING] activate: OSD id is 1
[2017-05-31 19:09:46,518][gdb0][WARNING] activate: Initializing OSD...
[2017-05-31 19:09:46,518][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-05-31 19:09:46,683][gdb0][WARNING] got monmap epoch 2
[2017-05-31 19:09:46,683][gdb0][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 1 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid a397040a-60e7-423c-bb3e-a90280753962 --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-05-31 19:09:46,698][gdb0][WARNING] activate: Marking with init system systemd
[2017-05-31 19:09:46,699][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-05-31 19:09:46,699][gdb0][WARNING] activate: Authorizing OSD key...
[2017-05-31 19:09:46,699][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.1 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-05-31 19:09:46,863][gdb0][WARNING] added key for osd.1
[2017-05-31 19:09:46,863][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.3415.tmp
[2017-05-31 19:09:46,863][gdb0][WARNING] activate: ceph osd.1 data dir is ready at /mnt/memstore
[2017-05-31 19:09:46,863][gdb0][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-1 -> /mnt/memstore
[2017-05-31 19:09:46,863][gdb0][WARNING] start_daemon: Starting ceph osd.1...
[2017-05-31 19:09:46,863][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1
[2017-05-31 19:09:46,867][gdb0][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2017-05-31 19:09:46,931][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1 --runtime
[2017-05-31 19:09:46,995][gdb0][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@1
[2017-05-31 19:09:46,995][gdb0][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /lib/systemd/system/ceph-osd@.service.
[2017-05-31 19:09:47,027][gdb0][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@1
[2017-05-31 19:09:52,096][gdb0][INFO  ] checking OSD status...
[2017-05-31 19:09:52,096][gdb0][DEBUG ] find the location of an executable
[2017-05-31 19:09:52,099][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-05-31 19:09:52,266][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 00:09:24,868][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8dbf6f80e0>
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f8dc00051b8>
[2017-06-11 00:09:24,870][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:09:24,871][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:09:24,871][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-11 00:09:24,871][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-11 00:09:24,871][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-11 00:09:24,871][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-11 00:09:24,907][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:09:24,934][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:09:24,935][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:09:24,951][gdb3][DEBUG ] detect machine type
[2017-06-11 00:09:24,953][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:09:24,953][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-11 00:09:24,955][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-11 00:09:24,994][gdb3][DEBUG ] Reading package lists...
[2017-06-11 00:09:25,159][gdb3][DEBUG ] Building dependency tree...
[2017-06-11 00:09:25,159][gdb3][DEBUG ] Reading state information...
[2017-06-11 00:09:25,223][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-11 00:09:25,223][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-11 00:09:25,223][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-11 00:09:25,223][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-11 00:09:25,223][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-11 00:09:25,223][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws ntp python-blinker python-cephfs
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-06-11 00:09:25,224][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-06-11 00:09:25,224][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-11 00:09:25,288][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-11 00:09:25,288][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-11 00:09:25,553][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 40 not upgraded.
[2017-06-11 00:09:25,553][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-11 00:09:25,918][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 129562 files and directories currently installed.)
[2017-06-11 00:09:25,918][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-11 00:09:26,134][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-11 00:09:26,198][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-11 00:09:26,262][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-11 00:09:26,477][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-11 00:09:26,591][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-11 00:09:26,759][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-11 00:09:26,823][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-11 00:09:26,823][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-11 00:09:26,991][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-11 00:09:27,105][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-11 00:09:27,106][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-11 00:09:27,270][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-11 00:09:27,270][gdb3][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-06-11 00:09:27,278][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-11 00:09:27,445][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-11 00:09:27,476][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-11 00:09:27,492][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-11 00:09:27,656][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-06-11 00:09:28,473][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-11 00:09:28,641][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0cac286758>
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f0cacb93230>
[2017-06-11 00:09:28,642][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:09:28,643][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:09:28,643][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-11 00:09:28,669][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:09:28,683][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:09:28,683][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:09:28,700][gdb3][DEBUG ] detect machine type
[2017-06-11 00:09:28,702][gdb3][DEBUG ] find the location of an executable
[2017-06-11 00:09:28,718][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:09:28,731][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:09:28,732][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:09:28,748][gdb3][DEBUG ] detect machine type
[2017-06-11 00:09:28,751][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:09:28,751][gdb3][INFO  ] purging data on gdb3
[2017-06-11 00:09:28,752][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-11 00:09:28,765][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-11 00:09:28,934][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:09:28,935][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6dcc48ea28>
[2017-06-11 00:09:28,936][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:09:28,936][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f6dccd51848>
[2017-06-11 00:09:28,936][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:09:28,936][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:10:12,282][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:10:12,282][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-11 00:10:12,282][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:10:12,282][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:10:12,282][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:10:12,282][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3b19d035a8>
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f3b1a387758>
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:10:12,283][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-11 00:10:12,283][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-11 00:10:12,284][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-11 00:10:12,309][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:10:12,323][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:10:12,323][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:10:12,340][gdb3][DEBUG ] detect machine type
[2017-06-11 00:10:12,342][gdb3][DEBUG ] find the location of an executable
[2017-06-11 00:10:12,344][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-11 00:10:12,355][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-11 00:10:12,361][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-11 00:10:12,361][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-11 00:10:12,361][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-11 00:10:12,361][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-11 00:10:12,362][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-11 00:10:12,362][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-11 00:10:12,362][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-11 00:10:12,362][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-11 00:10:12,528][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:10:12,528][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-11 00:10:12,528][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:10:12,528][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f08add22ea8>
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f08adcf6b18>
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-11 00:10:12,529][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:10:12,530][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-11 00:10:12,530][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-11 00:10:12,556][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:10:12,570][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:10:12,571][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:10:12,587][gdb3][DEBUG ] detect machine type
[2017-06-11 00:10:12,590][gdb3][DEBUG ] find the location of an executable
[2017-06-11 00:10:12,590][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-11 00:10:12,591][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-11 00:10:12,591][gdb3][DEBUG ] get remote short hostname
[2017-06-11 00:10:12,591][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-11 00:10:12,591][gdb3][DEBUG ] get remote short hostname
[2017-06-11 00:10:12,591][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-11 00:10:12,592][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:10:12,593][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-11 00:10:12,594][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 00:10:12,594][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 00:10:12,594][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 00:10:12,594][gdb3][DEBUG ] create the monitor keyring file
[2017-06-11 00:10:12,596][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-11 00:10:12,666][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-11 00:10:12,666][gdb3][DEBUG ] ceph-mon: set fsid to f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:10:12,666][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-11 00:10:12,666][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 00:10:12,667][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-11 00:10:12,667][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-11 00:10:12,668][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 00:10:12,741][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-11 00:10:12,813][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-11 00:10:14,851][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 00:10:14,966][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 00:10:14,966][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-11 00:10:14,966][gdb3][DEBUG ] {
[2017-06-11 00:10:14,966][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-11 00:10:14,966][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]   "features": {
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-11 00:10:14,967][gdb3][DEBUG ]       "kraken", 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]       "luminous"
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     ], 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     "required_mon": [
[2017-06-11 00:10:14,967][gdb3][DEBUG ]       "kraken", 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]       "luminous"
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     ]
[2017-06-11 00:10:14,967][gdb3][DEBUG ]   }, 
[2017-06-11 00:10:14,967][gdb3][DEBUG ]   "monmap": {
[2017-06-11 00:10:14,967][gdb3][DEBUG ]     "created": "2017-06-11 00:10:12.642384", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     "features": {
[2017-06-11 00:10:14,968][gdb3][DEBUG ]       "optional": [], 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]       "persistent": [
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "kraken", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "luminous"
[2017-06-11 00:10:14,968][gdb3][DEBUG ]       ]
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     }, 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     "fsid": "f4f36e31-c61f-42db-97ec-96a4b3bd98b7", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     "modified": "2017-06-11 00:10:12.894445", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]     "mons": [
[2017-06-11 00:10:14,968][gdb3][DEBUG ]       {
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-11 00:10:14,968][gdb3][DEBUG ]         "rank": 0
[2017-06-11 00:10:14,969][gdb3][DEBUG ]       }
[2017-06-11 00:10:14,969][gdb3][DEBUG ]     ]
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   }, 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "quorum": [
[2017-06-11 00:10:14,969][gdb3][DEBUG ]     0
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   ], 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "rank": 0, 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "state": "leader", 
[2017-06-11 00:10:14,969][gdb3][DEBUG ]   "sync_provider": []
[2017-06-11 00:10:14,969][gdb3][DEBUG ] }
[2017-06-11 00:10:14,969][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 00:10:14,969][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-11 00:10:14,970][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 00:10:15,035][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-11 00:10:15,051][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:10:15,064][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:10:15,065][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:10:15,081][gdb3][DEBUG ] detect machine type
[2017-06-11 00:10:15,084][gdb3][DEBUG ] find the location of an executable
[2017-06-11 00:10:15,085][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 00:10:15,150][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-11 00:10:15,150][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-11 00:10:15,150][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-11 00:10:15,152][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmp2yNmqZ
[2017-06-11 00:10:15,167][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:10:15,181][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:10:15,181][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:10:15,197][gdb3][DEBUG ] detect machine type
[2017-06-11 00:10:15,200][gdb3][DEBUG ] get remote short hostname
[2017-06-11 00:10:15,200][gdb3][DEBUG ] fetch remote file
[2017-06-11 00:10:15,201][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 00:10:15,267][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-11 00:10:15,433][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-11 00:10:15,599][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-11 00:10:15,765][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-11 00:10:15,932][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-11 00:10:16,098][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-11 00:10:16,264][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-11 00:10:16,430][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-11 00:10:16,596][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-11 00:10:16,762][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-11 00:10:16,928][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-11 00:10:16,928][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-11 00:10:16,929][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170611001016'
[2017-06-11 00:10:16,929][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-11 00:10:16,929][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-11 00:10:16,929][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-11 00:10:16,929][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp2yNmqZ
[2017-06-11 00:10:17,112][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:10:17,112][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff2a565f560>
[2017-06-11 00:10:17,113][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:10:17,113][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-11 00:10:17,113][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7ff2a5f76938>
[2017-06-11 00:10:17,113][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:10:17,113][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:10:17,113][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-11 00:10:17,139][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 00:10:17,153][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 00:10:17,154][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 00:10:17,170][gdb3][DEBUG ] detect machine type
[2017-06-11 00:10:17,173][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:12:58,042][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:12:58,042][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fab03053560>
[2017-06-11 00:12:58,043][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:12:58,043][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:12:58,043][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fab0396a938>
[2017-06-11 00:12:58,043][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:12:58,043][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:12:58,043][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:12:58,293][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:12:58,514][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:12:58,514][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:12:58,532][gdb0][DEBUG ] detect machine type
[2017-06-11 00:12:58,536][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:12:58,707][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:12:58,708][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe64d0ea950>
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe64d340aa0>
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:12:58,709][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:12:58,709][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:12:58,950][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:12:59,177][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:12:59,178][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:12:59,194][gdb0][DEBUG ] detect machine type
[2017-06-11 00:12:59,198][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:12:59,199][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:12:59,199][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:12:59,199][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:12:59,202][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:12:59,202][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:12:59,204][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:12:59,325][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:12:59,332][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:12:59,348][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:12:59,356][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:12:59,371][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:12:59,379][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-11 00:13:04,400][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:13:04,400][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:13:04,403][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:13:04,618][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-11 00:13:04,785][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:13:04,785][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fece470a950>
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fece4960aa0>
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:13:04,786][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:13:04,787][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:13:05,030][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:13:05,262][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:13:05,263][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:13:05,278][gdb0][DEBUG ] detect machine type
[2017-06-11 00:13:05,282][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:13:05,283][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:13:05,283][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:13:05,283][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:13:05,283][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:13:05,285][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:13:05,406][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:13:05,406][gdb0][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-06-11 00:13:05,406][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:13:05,414][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:13:05,414][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:13:05,414][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:13:05,414][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:13:05,414][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:13:05,415][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:13:05,415][gdb0][WARNING]     args.func(args)
[2017-06-11 00:13:05,415][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-06-11 00:13:05,415][gdb0][WARNING]     init=args.mark_init,
[2017-06-11 00:13:05,415][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-11 00:13:05,415][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-06-11 00:13:05,415][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-06-11 00:13:05,415][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-06-11 00:13:05,415][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-06-11 00:13:05,424][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:13:05,424][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:14:15,105][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5b4b05c560>
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:14:15,106][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f5b4b973938>
[2017-06-11 00:14:15,107][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:15,107][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:15,107][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:14:15,342][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:15,538][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:15,539][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:15,554][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:15,558][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:14:15,730][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:15,730][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f61228b6950>
[2017-06-11 00:14:15,731][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:15,732][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:14:15,732][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6122b0caa0>
[2017-06-11 00:14:15,732][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:15,732][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:15,732][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:14:15,732][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:14:15,979][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:16,214][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:16,215][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:16,231][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:16,235][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:16,236][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:14:16,236][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:14:16,236][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:14:16,238][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-06-11 00:14:16,238][gdb0][DEBUG ] create a keyring file
[2017-06-11 00:14:16,240][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:14:16,240][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:16,242][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:14:16,362][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:14:16,370][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:14:16,386][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:14:16,401][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:14:16,409][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:14:16,425][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-11 00:14:21,433][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:14:21,434][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:21,436][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:14:21,602][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-11 00:14:21,767][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:21,767][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:14:21,767][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:21,767][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f87f2ff2950>
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f87f3248aa0>
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:21,768][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:14:21,768][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:14:22,011][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:22,242][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:22,242][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:22,258][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:22,262][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:22,263][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:14:22,264][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:14:22,264][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:14:22,264][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:22,267][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:14:22,387][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:14:22,387][gdb0][WARNING] activate: Cluster uuid is cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-06-11 00:14:22,387][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:14:22,395][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:14:22,395][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:14:22,395][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:14:22,395][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:14:22,395][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:14:22,395][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:14:22,395][gdb0][WARNING]     args.func(args)
[2017-06-11 00:14:22,396][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-06-11 00:14:22,396][gdb0][WARNING]     init=args.mark_init,
[2017-06-11 00:14:22,396][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-11 00:14:22,396][gdb0][WARNING]     (osd_id, cluster) = activate(path, activate_key_template, init)
[2017-06-11 00:14:22,396][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3609, in activate
[2017-06-11 00:14:22,396][gdb0][WARNING]     ' with fsid %s' % ceph_fsid)
[2017-06-11 00:14:22,396][gdb0][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid cb3c2cda-b4dd-4a09-9b7a-a7dc594196ab
[2017-06-11 00:14:22,404][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:14:22,404][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:14:32,489][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc337d4e560>
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fc338665938>
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:32,490][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:32,490][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:14:32,734][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:32,966][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:32,966][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:32,982][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:32,986][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:14:33,166][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:14:33,166][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efdceb02950>
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7efdced58aa0>
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:33,167][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:14:33,168][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:14:33,411][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:33,639][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:33,639][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:33,655][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:33,659][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:33,660][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:14:33,660][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:14:33,661][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:14:33,663][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:14:33,663][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:33,665][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:14:33,786][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:14:33,786][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:14:33,786][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:14:33,786][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:14:33,786][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:14:33,786][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:14:33,786][gdb0][WARNING]     args.func(args)
[2017-06-11 00:14:33,786][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-06-11 00:14:33,787][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-06-11 00:14:33,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-06-11 00:14:33,787][gdb0][WARNING]     return PrepareFilestore(args)
[2017-06-11 00:14:33,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-06-11 00:14:33,787][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-06-11 00:14:33,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-06-11 00:14:33,787][gdb0][WARNING]     self.set_type()
[2017-06-11 00:14:33,787][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-06-11 00:14:33,787][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-06-11 00:14:33,787][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-06-11 00:14:33,788][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:14:33,788][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:14:33,788][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-06-11 00:14:33,961][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:14:33,961][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:14:33,961][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f82706f4950>
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f827094aaa0>
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:14:33,962][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:14:33,963][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:14:34,203][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:14:34,433][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:14:34,434][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:14:34,450][gdb0][DEBUG ] detect machine type
[2017-06-11 00:14:34,454][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:34,455][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:14:34,455][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:14:34,455][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:14:34,455][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:14:34,457][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:14:34,577][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:14:34,578][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:14:34,578][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:14:34,578][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:14:34,578][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:14:34,578][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:14:34,578][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:14:34,578][gdb0][WARNING]     args.func(args)
[2017-06-11 00:14:34,578][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3684, in main_activate
[2017-06-11 00:14:34,578][gdb0][WARNING]     raise Error('%s does not exist' % args.path)
[2017-06-11 00:14:34,578][gdb0][WARNING] ceph_disk.main.Error: Error: /mnt/memstore does not exist
[2017-06-11 00:14:34,579][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:14:34,579][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:15:11,999][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:11,999][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:15:11,999][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f02dd89e560>
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f02de1b5938>
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:12,000][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:12,000][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:15:12,243][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:12,474][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:12,475][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:12,491][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:12,495][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:15:12,667][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa670dcb950>
[2017-06-11 00:15:12,668][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:12,669][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:15:12,669][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa671021aa0>
[2017-06-11 00:15:12,669][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:12,669][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:12,669][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:15:12,669][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:15:12,911][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:13,142][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:13,143][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:13,159][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:13,163][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:13,164][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:15:13,164][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:15:13,164][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:15:13,167][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:15:13,167][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:13,169][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:15:13,289][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:15:13,290][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:15:13,290][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:15:13,290][gdb0][WARNING]     args.func(args)
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-06-11 00:15:13,290][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-06-11 00:15:13,290][gdb0][WARNING]     return PrepareFilestore(args)
[2017-06-11 00:15:13,290][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-06-11 00:15:13,291][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-06-11 00:15:13,291][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-06-11 00:15:13,291][gdb0][WARNING]     self.set_type()
[2017-06-11 00:15:13,291][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-06-11 00:15:13,291][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-06-11 00:15:13,291][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-06-11 00:15:13,291][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:15:13,291][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:15:13,292][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-06-11 00:15:13,464][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:13,464][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:15:13,464][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:13,464][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8a8a923950>
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f8a8ab79aa0>
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:13,465][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:15:13,465][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:15:13,711][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:13,942][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:13,943][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:13,959][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:13,963][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:13,963][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:15:13,964][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:15:13,964][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:15:13,964][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:13,966][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:15:14,086][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:15:14,087][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:15:14,087][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:15:14,087][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:15:14,087][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:15:14,087][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:15:14,087][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:15:14,087][gdb0][WARNING]     args.func(args)
[2017-06-11 00:15:14,087][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3684, in main_activate
[2017-06-11 00:15:14,087][gdb0][WARNING]     raise Error('%s does not exist' % args.path)
[2017-06-11 00:15:14,087][gdb0][WARNING] ceph_disk.main.Error: Error: /mnt/memstore does not exist
[2017-06-11 00:15:14,089][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:15:14,089][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:15:33,532][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:33,532][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0a4f92d560>
[2017-06-11 00:15:33,533][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:33,533][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:15:33,533][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f0a50244938>
[2017-06-11 00:15:33,533][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:33,533][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:33,533][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:15:33,775][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:34,002][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:34,003][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:34,019][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:34,022][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:15:34,194][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:15:34,195][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f030023f950>
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0300495aa0>
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:34,196][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:15:34,196][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:15:34,439][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:34,670][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:34,671][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:34,687][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:34,690][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:34,691][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:15:34,691][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:15:34,692][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:15:34,694][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-06-11 00:15:34,694][gdb0][DEBUG ] create a keyring file
[2017-06-11 00:15:34,696][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:15:34,696][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:34,698][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:15:34,818][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:15:34,818][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:15:34,818][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:15:34,818][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:15:34,819][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:15:34,819][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:15:34,819][gdb0][WARNING]     args.func(args)
[2017-06-11 00:15:34,819][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2029, in main
[2017-06-11 00:15:34,819][gdb0][WARNING]     Prepare.factory(args).prepare()
[2017-06-11 00:15:34,819][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2025, in factory
[2017-06-11 00:15:34,819][gdb0][WARNING]     return PrepareFilestore(args)
[2017-06-11 00:15:34,819][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2037, in __init__
[2017-06-11 00:15:34,819][gdb0][WARNING]     self.data = PrepareFilestoreData(args)
[2017-06-11 00:15:34,819][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2739, in __init__
[2017-06-11 00:15:34,820][gdb0][WARNING]     self.set_type()
[2017-06-11 00:15:34,820][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 2747, in set_type
[2017-06-11 00:15:34,820][gdb0][WARNING]     dmode = os.stat(self.args.data).st_mode
[2017-06-11 00:15:34,820][gdb0][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-06-11 00:15:34,820][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:15:34,820][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:15:34,820][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-06-11 00:15:34,993][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:15:34,993][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6aa5e11950>
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6aa6067aa0>
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:15:34,994][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:15:34,994][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:15:35,239][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:15:35,462][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:15:35,463][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:15:35,479][gdb0][DEBUG ] detect machine type
[2017-06-11 00:15:35,483][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:35,483][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:15:35,484][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:15:35,484][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:15:35,484][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:15:35,486][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:15:35,606][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:15:35,606][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:15:35,606][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:15:35,606][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:15:35,606][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:15:35,606][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:15:35,607][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:15:35,607][gdb0][WARNING]     args.func(args)
[2017-06-11 00:15:35,607][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3684, in main_activate
[2017-06-11 00:15:35,607][gdb0][WARNING]     raise Error('%s does not exist' % args.path)
[2017-06-11 00:15:35,607][gdb0][WARNING] ceph_disk.main.Error: Error: /mnt/memstore does not exist
[2017-06-11 00:15:35,607][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:15:35,607][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:16:29,599][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:16:29,599][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:16:29,599][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:16:29,599][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb9eadbd560>
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fb9eb6d4938>
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:16:29,600][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:16:29,600][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:16:29,839][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:16:30,071][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:16:30,071][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:16:30,088][gdb0][DEBUG ] detect machine type
[2017-06-11 00:16:30,092][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:16:30,265][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:16:30,265][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f68f0624950>
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f68f087aaa0>
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:16:30,266][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:16:30,267][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:16:30,507][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:16:30,734][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:16:30,735][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:16:30,751][gdb0][DEBUG ] detect machine type
[2017-06-11 00:16:30,755][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:16:30,756][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:16:30,756][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:16:30,756][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:16:30,758][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:16:30,759][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:16:30,761][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:16:30,881][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:16:30,889][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:16:30,904][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:16:30,912][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:16:30,928][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:16:30,943][gdb0][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-06-11 00:16:30,943][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.5726.tmp
[2017-06-11 00:16:30,944][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.5726.tmp
[2017-06-11 00:16:30,951][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.5726.tmp
[2017-06-11 00:16:35,964][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:16:35,964][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:16:35,967][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:16:36,132][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-11 00:16:36,297][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:16:36,297][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:16:36,297][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:16:36,297][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:16:36,297][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:16:36,297][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fedbb592950>
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fedbb7e8aa0>
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:16:36,298][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:16:36,298][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:16:36,543][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:16:36,774][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:16:36,775][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:16:36,791][gdb0][DEBUG ] detect machine type
[2017-06-11 00:16:36,795][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:16:36,796][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:16:36,796][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:16:36,796][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:16:36,796][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:16:36,798][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:16:36,918][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:16:36,918][gdb0][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:16:36,919][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:16:36,926][gdb0][WARNING] activate: Cluster name is ceph
[2017-06-11 00:16:36,926][gdb0][WARNING] activate: OSD uuid is db0fe94b-dc3b-488f-b03f-a33e5706669c
[2017-06-11 00:16:36,926][gdb0][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-11 00:16:36,926][gdb0][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise db0fe94b-dc3b-488f-b03f-a33e5706669c
[2017-06-11 00:16:37,091][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.5843.tmp
[2017-06-11 00:16:37,091][gdb0][WARNING] activate: OSD id is 0
[2017-06-11 00:16:37,091][gdb0][WARNING] activate: Initializing OSD...
[2017-06-11 00:16:37,091][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-06-11 00:16:37,255][gdb0][WARNING] got monmap epoch 2
[2017-06-11 00:16:37,256][gdb0][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid db0fe94b-dc3b-488f-b03f-a33e5706669c --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-06-11 00:16:37,271][gdb0][WARNING] activate: Marking with init system systemd
[2017-06-11 00:16:37,271][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-06-11 00:16:37,273][gdb0][WARNING] activate: Authorizing OSD key...
[2017-06-11 00:16:37,273][gdb0][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-06-11 00:16:37,437][gdb0][WARNING] added key for osd.0
[2017-06-11 00:16:37,438][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.5843.tmp
[2017-06-11 00:16:37,438][gdb0][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-06-11 00:16:37,438][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:16:37,438][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:16:37,438][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:16:37,438][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:16:37,438][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:16:37,438][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:16:37,438][gdb0][WARNING]     args.func(args)
[2017-06-11 00:16:37,438][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-06-11 00:16:37,438][gdb0][WARNING]     init=args.mark_init,
[2017-06-11 00:16:37,438][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3543, in activate_dir
[2017-06-11 00:16:37,439][gdb0][WARNING]     old = os.readlink(canonical)
[2017-06-11 00:16:37,439][gdb0][WARNING] OSError: [Errno 22] Invalid argument: '/var/lib/ceph/osd/ceph-0'
[2017-06-11 00:16:37,446][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:16:37,446][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:17:02,077][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:17:02,077][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:17:02,077][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:17:02,077][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:17:02,077][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:17:02,077][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faa27ff3560>
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7faa2890a938>
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:17:02,078][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:17:02,078][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:17:02,319][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:17:02,551][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:17:02,552][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:17:02,568][gdb0][DEBUG ] detect machine type
[2017-06-11 00:17:02,572][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:17:02,745][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:17:02,745][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:17:02,745][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:17:02,745][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:17:02,745][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9f7325f950>
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f9f734b5aa0>
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:17:02,746][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:17:02,747][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:17:02,987][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:17:03,214][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:17:03,214][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:17:03,230][gdb0][DEBUG ] detect machine type
[2017-06-11 00:17:03,234][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:17:03,235][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:17:03,235][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:17:03,236][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:17:03,238][gdb0][WARNING] osd keyring does not exist yet, creating one
[2017-06-11 00:17:03,238][gdb0][DEBUG ] create a keyring file
[2017-06-11 00:17:03,240][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:17:03,240][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:17:03,242][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:17:03,363][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:17:03,370][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:17:03,386][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:17:03,402][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:17:03,409][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:17:03,425][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-11 00:17:08,432][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:17:08,432][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:17:08,435][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:17:08,600][gdb0][WARNING] there is 1 OSD down
[2017-06-11 00:17:08,600][gdb0][WARNING] there is 1 OSD out
[2017-06-11 00:17:08,600][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-11 00:17:08,766][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:17:08,766][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:17:08,766][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f48d6cca950>
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f48d6f20aa0>
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:17:08,767][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:17:08,768][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:17:09,007][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:17:09,238][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:17:09,239][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:17:09,255][gdb0][DEBUG ] detect machine type
[2017-06-11 00:17:09,259][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:17:09,260][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:17:09,260][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:17:09,260][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:17:09,260][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:17:09,262][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:17:09,382][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:17:09,383][gdb0][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:17:09,383][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:17:09,390][gdb0][WARNING] activate: Cluster name is ceph
[2017-06-11 00:17:09,391][gdb0][WARNING] activate: OSD uuid is db0fe94b-dc3b-488f-b03f-a33e5706669c
[2017-06-11 00:17:09,391][gdb0][WARNING] activate: OSD id is 0
[2017-06-11 00:17:09,391][gdb0][WARNING] activate: Marking with init system systemd
[2017-06-11 00:17:09,391][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-06-11 00:17:09,391][gdb0][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-06-11 00:17:09,392][gdb0][WARNING] Traceback (most recent call last):
[2017-06-11 00:17:09,392][gdb0][WARNING]   File "/usr/sbin/ceph-disk", line 9, in <module>
[2017-06-11 00:17:09,392][gdb0][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:17:09,392][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:17:09,392][gdb0][WARNING]     main(sys.argv[1:])
[2017-06-11 00:17:09,392][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:17:09,392][gdb0][WARNING]     args.func(args)
[2017-06-11 00:17:09,393][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3713, in main_activate
[2017-06-11 00:17:09,393][gdb0][WARNING]     init=args.mark_init,
[2017-06-11 00:17:09,393][gdb0][WARNING]   File "/usr/lib/python2.7/dist-packages/ceph_disk/main.py", line 3543, in activate_dir
[2017-06-11 00:17:09,393][gdb0][WARNING]     old = os.readlink(canonical)
[2017-06-11 00:17:09,393][gdb0][WARNING] OSError: [Errno 22] Invalid argument: '/var/lib/ceph/osd/ceph-0'
[2017-06-11 00:17:09,401][gdb0][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:17:09,401][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:19:30,321][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:19:30,321][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-11 00:19:30,321][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5a94088560>
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f5a9499f938>
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:19:30,322][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:19:30,322][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-11 00:19:30,564][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:19:30,795][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:19:30,795][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:19:30,811][gdb0][DEBUG ] detect machine type
[2017-06-11 00:19:30,815][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:19:30,987][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:19:30,987][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb54454f950>
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:19:30,988][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:19:30,989][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fb5447a5aa0>
[2017-06-11 00:19:30,989][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:19:30,989][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:19:30,989][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:19:30,989][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:19:31,227][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:19:31,458][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:19:31,458][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:19:31,474][gdb0][DEBUG ] detect machine type
[2017-06-11 00:19:31,478][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:31,479][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:19:31,479][ceph_deploy.osd][DEBUG ] Deploying osd to gdb0
[2017-06-11 00:19:31,479][gdb0][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:19:31,482][ceph_deploy.osd][DEBUG ] Preparing host gdb0 disk /mnt/memstore journal None activate False
[2017-06-11 00:19:31,482][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:31,485][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:19:31,605][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:19:31,613][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:19:31,628][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:19:31,636][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:19:31,651][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:19:31,667][gdb0][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-11 00:19:36,676][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:19:36,676][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:36,679][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:19:36,844][gdb0][WARNING] there is 1 OSD down
[2017-06-11 00:19:36,844][gdb0][WARNING] there is 1 OSD out
[2017-06-11 00:19:36,844][ceph_deploy.osd][DEBUG ] Host gdb0 is now ready for osd use.
[2017-06-11 00:19:37,009][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd7485dd950>
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:19:37,010][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd748833aa0>
[2017-06-11 00:19:37,011][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:19:37,011][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:19:37,011][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-11 00:19:37,011][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-11 00:19:37,255][gdb0][DEBUG ] connection detected need for sudo
[2017-06-11 00:19:37,482][gdb0][DEBUG ] connected to host: gdb0 
[2017-06-11 00:19:37,482][gdb0][DEBUG ] detect platform information from remote host
[2017-06-11 00:19:37,499][gdb0][DEBUG ] detect machine type
[2017-06-11 00:19:37,502][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:37,503][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:19:37,503][ceph_deploy.osd][DEBUG ] activating host gdb0 disk /mnt/memstore
[2017-06-11 00:19:37,504][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:19:37,504][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:37,506][gdb0][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:19:37,626][gdb0][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:19:37,626][gdb0][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:19:37,627][gdb0][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:19:37,634][gdb0][WARNING] activate: Cluster name is ceph
[2017-06-11 00:19:37,634][gdb0][WARNING] activate: OSD uuid is db0fe94b-dc3b-488f-b03f-a33e5706669c
[2017-06-11 00:19:37,634][gdb0][WARNING] activate: OSD id is 0
[2017-06-11 00:19:37,634][gdb0][WARNING] activate: Marking with init system systemd
[2017-06-11 00:19:37,634][gdb0][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-06-11 00:19:37,634][gdb0][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-06-11 00:19:37,634][gdb0][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-0 -> /mnt/memstore
[2017-06-11 00:19:37,635][gdb0][WARNING] start_daemon: Starting ceph osd.0...
[2017-06-11 00:19:37,635][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-06-11 00:19:37,638][gdb0][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-06-11 00:19:37,702][gdb0][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-06-11 00:19:37,766][gdb0][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-06-11 00:19:37,766][gdb0][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-06-11 00:19:37,830][gdb0][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-06-11 00:19:42,900][gdb0][INFO  ] checking OSD status...
[2017-06-11 00:19:42,900][gdb0][DEBUG ] find the location of an executable
[2017-06-11 00:19:42,903][gdb0][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:19:43,070][gdb0][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 00:21:23,142][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f36ae470560>
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f36aed87938>
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:21:23,143][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:21:23,143][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 00:21:23,444][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:21:23,679][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:21:23,679][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:21:23,697][gdb1][DEBUG ] detect machine type
[2017-06-11 00:21:23,700][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:21:23,871][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:21:23,871][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-06-11 00:21:23,871][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fabd4586950>
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:21:23,872][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:21:23,873][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fabd47dcaa0>
[2017-06-11 00:21:23,873][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:21:23,873][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:21:23,873][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:21:23,873][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-06-11 00:21:24,119][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:21:24,314][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:21:24,314][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:21:24,331][gdb1][DEBUG ] detect machine type
[2017-06-11 00:21:24,335][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:21:24,335][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:21:24,335][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-11 00:21:24,336][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:21:24,339][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-06-11 00:21:24,339][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:21:24,341][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:21:24,511][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:21:24,512][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:21:24,519][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:21:24,535][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:21:24,542][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:21:24,558][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-06-11 00:21:24,558][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.918.tmp
[2017-06-11 00:21:24,561][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.918.tmp
[2017-06-11 00:21:24,577][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.918.tmp
[2017-06-11 00:21:29,598][gdb1][INFO  ] checking OSD status...
[2017-06-11 00:21:29,598][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:21:29,601][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:21:29,917][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-11 00:21:30,082][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f74b7ac8950>
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f74b7d1eaa0>
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:21:30,083][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-11 00:21:30,084][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-06-11 00:21:30,327][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:21:30,554][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:21:30,555][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:21:30,571][gdb1][DEBUG ] detect machine type
[2017-06-11 00:21:30,575][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:21:30,576][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:21:30,576][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-06-11 00:21:30,576][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:21:30,576][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:21:30,578][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:21:30,749][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:21:30,749][gdb1][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:21:30,749][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:21:30,749][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-11 00:21:30,749][gdb1][WARNING] activate: OSD uuid is 83154121-02fd-4e57-a1d6-f080dd469712
[2017-06-11 00:21:30,749][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-11 00:21:30,749][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 83154121-02fd-4e57-a1d6-f080dd469712
[2017-06-11 00:21:30,781][gdb1][WARNING] Traceback (most recent call last):
[2017-06-11 00:21:30,781][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-11 00:21:30,781][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-11 00:21:30,781][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-11 00:21:30,782][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-11 00:21:30,782][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-06-11 00:21:30,782][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-11 00:21:30,782][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3625, in activate
[2017-06-11 00:21:30,782][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 1075, in allocate_osd_id
[2017-06-11 00:21:30,782][gdb1][WARNING] ceph_disk.main.Error: Error: ceph osd create failed: Command '/usr/bin/ceph' returned non-zero exit status 1: 2017-06-11 00:21:30.770633 7f770c53c700  0 librados: client.bootstrap-osd authentication error (1) Operation not permitted
[2017-06-11 00:21:30,782][gdb1][WARNING] error connecting to the cluster
[2017-06-11 00:21:30,782][gdb1][WARNING] 
[2017-06-11 00:21:30,790][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-11 00:21:30,790][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-11 00:21:58,875][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:21:58,875][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-11 00:21:58,875][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:21:58,875][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:21:58,875][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:21:58,875][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff9b6787560>
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7ff9b709e938>
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:21:58,876][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:21:58,876][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-11 00:21:59,119][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:21:59,347][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:21:59,347][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:21:59,363][gdb1][DEBUG ] detect machine type
[2017-06-11 00:21:59,367][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:21:59,542][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-11 00:21:59,542][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1bdb3b9950>
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1bdb60faa0>
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:21:59,543][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-11 00:21:59,544][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-06-11 00:21:59,787][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:22:00,023][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:22:00,023][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:22:00,040][gdb1][DEBUG ] detect machine type
[2017-06-11 00:22:00,044][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:00,045][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:22:00,045][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-11 00:22:00,045][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 00:22:00,047][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-11 00:22:00,048][gdb1][DEBUG ] create a keyring file
[2017-06-11 00:22:00,049][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-06-11 00:22:00,049][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:00,051][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-11 00:22:00,222][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:22:00,222][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:22:00,222][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:22:00,222][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-11 00:22:00,226][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-11 00:22:00,241][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-11 00:22:05,250][gdb1][INFO  ] checking OSD status...
[2017-06-11 00:22:05,250][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:05,253][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:22:05,418][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-11 00:22:05,583][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2dce406950>
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f2dce65caa0>
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 00:22:05,584][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-11 00:22:05,585][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-06-11 00:22:05,823][gdb1][DEBUG ] connection detected need for sudo
[2017-06-11 00:22:06,051][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-11 00:22:06,051][gdb1][DEBUG ] detect platform information from remote host
[2017-06-11 00:22:06,068][gdb1][DEBUG ] detect machine type
[2017-06-11 00:22:06,072][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:06,073][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 00:22:06,073][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-06-11 00:22:06,073][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-11 00:22:06,073][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:06,075][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-11 00:22:06,245][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-06-11 00:22:06,246][gdb1][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-11 00:22:06,246][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-11 00:22:06,246][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-11 00:22:06,246][gdb1][WARNING] activate: OSD uuid is 83154121-02fd-4e57-a1d6-f080dd469712
[2017-06-11 00:22:06,246][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-11 00:22:06,246][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 83154121-02fd-4e57-a1d6-f080dd469712
[2017-06-11 00:22:06,511][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.1382.tmp
[2017-06-11 00:22:06,511][gdb1][WARNING] activate: OSD id is 1
[2017-06-11 00:22:06,511][gdb1][WARNING] activate: Initializing OSD...
[2017-06-11 00:22:06,511][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-06-11 00:22:06,625][gdb1][WARNING] got monmap epoch 2
[2017-06-11 00:22:06,625][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 1 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid 83154121-02fd-4e57-a1d6-f080dd469712 --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-06-11 00:22:06,657][gdb1][WARNING] activate: Marking with init system systemd
[2017-06-11 00:22:06,657][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-06-11 00:22:06,657][gdb1][WARNING] activate: Authorizing OSD key...
[2017-06-11 00:22:06,657][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.1 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-06-11 00:22:06,822][gdb1][WARNING] added key for osd.1
[2017-06-11 00:22:06,822][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.1382.tmp
[2017-06-11 00:22:06,822][gdb1][WARNING] activate: ceph osd.1 data dir is ready at /mnt/memstore
[2017-06-11 00:22:06,822][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-1 -> /mnt/memstore
[2017-06-11 00:22:06,822][gdb1][WARNING] start_daemon: Starting ceph osd.1...
[2017-06-11 00:22:06,822][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1
[2017-06-11 00:22:06,823][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service.
[2017-06-11 00:22:06,987][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@1 --runtime
[2017-06-11 00:22:07,019][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@1
[2017-06-11 00:22:07,019][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /lib/systemd/system/ceph-osd@.service.
[2017-06-11 00:22:07,083][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@1
[2017-06-11 00:22:12,152][gdb1][INFO  ] checking OSD status...
[2017-06-11 00:22:12,152][gdb1][DEBUG ] find the location of an executable
[2017-06-11 00:22:12,155][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-11 00:22:12,322][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 20:16:32,118][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:16:32,118][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1a4165cfc8>
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f1a41f6f1b8>
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:16:32,119][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:16:32,119][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-11 20:16:32,119][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-11 20:16:32,119][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-11 20:16:32,119][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-11 20:16:32,146][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:16:32,160][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:16:32,161][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:16:32,177][gdb3][DEBUG ] detect machine type
[2017-06-11 20:16:32,180][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:16:32,180][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-11 20:16:32,181][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-11 20:16:32,219][gdb3][DEBUG ] Reading package lists...
[2017-06-11 20:16:32,383][gdb3][DEBUG ] Building dependency tree...
[2017-06-11 20:16:32,384][gdb3][DEBUG ] Reading state information...
[2017-06-11 20:16:32,448][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-11 20:16:32,448][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-11 20:16:32,448][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-11 20:16:32,448][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-11 20:16:32,448][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-11 20:16:32,448][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws ntp python-blinker python-cephfs
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-06-11 20:16:32,449][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-06-11 20:16:32,450][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-11 20:16:32,481][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-11 20:16:32,482][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-11 20:16:32,646][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 40 not upgraded.
[2017-06-11 20:16:32,646][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-11 20:16:32,654][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 129562 files and directories currently installed.)
[2017-06-11 20:16:32,657][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-11 20:16:32,822][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-11 20:16:32,936][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-11 20:16:32,968][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-11 20:16:33,182][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-11 20:16:33,296][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-11 20:16:33,461][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-11 20:16:33,524][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-11 20:16:33,532][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-11 20:16:33,696][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-11 20:16:33,761][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-11 20:16:33,777][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-11 20:16:33,943][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-11 20:16:34,007][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-11 20:16:34,121][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-11 20:16:34,235][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-11 20:16:34,235][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-11 20:16:34,299][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu7) ...
[2017-06-11 20:16:35,166][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-11 20:16:35,332][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:16:35,332][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe142a53710>
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fe143360230>
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:16:35,333][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:16:35,333][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-11 20:16:35,359][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:16:35,373][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:16:35,374][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:16:35,390][gdb3][DEBUG ] detect machine type
[2017-06-11 20:16:35,393][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:16:35,408][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:16:35,423][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:16:35,423][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:16:35,440][gdb3][DEBUG ] detect machine type
[2017-06-11 20:16:35,442][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-11 20:16:35,442][gdb3][INFO  ] purging data on gdb3
[2017-06-11 20:16:35,443][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-11 20:16:35,456][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-11 20:16:35,627][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc0f7f579e0>
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7fc0f881a848>
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:16:35,628][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:17:04,392][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:17:04,392][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7feb54d2b560>
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7feb553af758>
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:17:04,393][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-11 20:17:04,393][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-11 20:17:04,393][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-11 20:17:04,420][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:17:04,434][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:17:04,435][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:17:04,451][gdb3][DEBUG ] detect machine type
[2017-06-11 20:17:04,454][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:17:04,455][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-11 20:17:04,466][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-11 20:17:04,473][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-11 20:17:04,473][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-11 20:17:04,474][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-11 20:17:04,639][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe6754a9e60>
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fe67547eb18>
[2017-06-11 20:17:04,640][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:17:04,641][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-11 20:17:04,641][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:17:04,641][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-11 20:17:04,641][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-11 20:17:04,668][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:17:04,682][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:17:04,683][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:17:04,700][gdb3][DEBUG ] detect machine type
[2017-06-11 20:17:04,702][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:17:04,702][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-11 20:17:04,703][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-11 20:17:04,703][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:17:04,703][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-11 20:17:04,703][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:17:04,703][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-11 20:17:04,704][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-11 20:17:04,705][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-11 20:17:04,706][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 20:17:04,706][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-11 20:17:04,706][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 20:17:04,707][gdb3][DEBUG ] create the monitor keyring file
[2017-06-11 20:17:04,708][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-11 20:17:04,746][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-11 20:17:04,746][gdb3][DEBUG ] ceph-mon: set fsid to 9ccb7614-4874-4746-9634-d8f0080c96da
[2017-06-11 20:17:04,749][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-11 20:17:04,751][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-11 20:17:04,751][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-11 20:17:04,752][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-11 20:17:04,753][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-11 20:17:04,824][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-11 20:17:04,897][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-11 20:17:06,935][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:17:07,000][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:17:07,000][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-11 20:17:07,000][gdb3][DEBUG ] {
[2017-06-11 20:17:07,000][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-11 20:17:07,000][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-11 20:17:07,000][gdb3][DEBUG ]   "features": {
[2017-06-11 20:17:07,000][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-11 20:17:07,000][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-11 20:17:07,000][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:17:07,000][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     ], 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     "required_mon": [
[2017-06-11 20:17:07,001][gdb3][DEBUG ]       "kraken", 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]       "luminous"
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     ]
[2017-06-11 20:17:07,001][gdb3][DEBUG ]   }, 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]   "monmap": {
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     "created": "2017-06-11 20:17:04.732069", 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     "features": {
[2017-06-11 20:17:07,001][gdb3][DEBUG ]       "optional": [], 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]       "persistent": [
[2017-06-11 20:17:07,001][gdb3][DEBUG ]         "kraken", 
[2017-06-11 20:17:07,001][gdb3][DEBUG ]         "luminous"
[2017-06-11 20:17:07,001][gdb3][DEBUG ]       ]
[2017-06-11 20:17:07,001][gdb3][DEBUG ]     }, 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]     "fsid": "9ccb7614-4874-4746-9634-d8f0080c96da", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]     "modified": "2017-06-11 20:17:04.974670", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]     "mons": [
[2017-06-11 20:17:07,002][gdb3][DEBUG ]       {
[2017-06-11 20:17:07,002][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]         "rank": 0
[2017-06-11 20:17:07,002][gdb3][DEBUG ]       }
[2017-06-11 20:17:07,002][gdb3][DEBUG ]     ]
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   }, 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   "quorum": [
[2017-06-11 20:17:07,002][gdb3][DEBUG ]     0
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   ], 
[2017-06-11 20:17:07,002][gdb3][DEBUG ]   "rank": 0, 
[2017-06-11 20:17:07,003][gdb3][DEBUG ]   "state": "leader", 
[2017-06-11 20:17:07,003][gdb3][DEBUG ]   "sync_provider": []
[2017-06-11 20:17:07,003][gdb3][DEBUG ] }
[2017-06-11 20:17:07,003][gdb3][DEBUG ] ********************************************************************************
[2017-06-11 20:17:07,003][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-11 20:17:07,004][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:17:07,069][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-11 20:17:07,086][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:17:07,100][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:17:07,101][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:17:07,118][gdb3][DEBUG ] detect machine type
[2017-06-11 20:17:07,120][gdb3][DEBUG ] find the location of an executable
[2017-06-11 20:17:07,121][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:17:07,186][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-11 20:17:07,187][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-11 20:17:07,187][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-11 20:17:07,189][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpl8OhV4
[2017-06-11 20:17:07,204][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:17:07,218][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:17:07,219][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:17:07,235][gdb3][DEBUG ] detect machine type
[2017-06-11 20:17:07,237][gdb3][DEBUG ] get remote short hostname
[2017-06-11 20:17:07,238][gdb3][DEBUG ] fetch remote file
[2017-06-11 20:17:07,239][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-11 20:17:07,305][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-11 20:17:07,471][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-11 20:17:07,688][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-11 20:17:07,854][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-11 20:17:08,020][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-11 20:17:08,186][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-11 20:17:08,353][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-11 20:17:08,519][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-11 20:17:08,685][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-11 20:17:08,851][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-11 20:17:09,017][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-11 20:17:09,017][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-11 20:17:09,017][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170611201709'
[2017-06-11 20:17:09,018][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-11 20:17:09,018][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-11 20:17:09,018][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-11 20:17:09,018][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpl8OhV4
[2017-06-11 20:17:09,198][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2dac57c518>
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-11 20:17:09,199][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f2dace93938>
[2017-06-11 20:17:09,200][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-11 20:17:09,200][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-11 20:17:09,200][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-11 20:17:09,226][gdb3][DEBUG ] connection detected need for sudo
[2017-06-11 20:17:09,240][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-11 20:17:09,241][gdb3][DEBUG ] detect platform information from remote host
[2017-06-11 20:17:09,257][gdb3][DEBUG ] detect machine type
[2017-06-11 20:17:09,259][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 20:59:57,344][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb32b9bc0e0>
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fb32c2c91b8>
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 20:59:57,346][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 20:59:57,346][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-20 20:59:57,347][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-20 20:59:57,347][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-20 20:59:57,347][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-20 20:59:57,384][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 20:59:57,397][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 20:59:57,398][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 20:59:57,414][gdb3][DEBUG ] detect machine type
[2017-06-20 20:59:57,417][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 20:59:57,417][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-20 20:59:57,418][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-20 20:59:57,457][gdb3][DEBUG ] Reading package lists...
[2017-06-20 20:59:57,622][gdb3][DEBUG ] Building dependency tree...
[2017-06-20 20:59:57,622][gdb3][DEBUG ] Reading state information...
[2017-06-20 20:59:57,686][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-20 20:59:57,686][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-20 20:59:57,686][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-20 20:59:57,686][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-20 20:59:57,686][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-20 20:59:57,687][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-20 20:59:57,687][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-06-20 20:59:57,687][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-headers-4.4.0-1016-aws
[2017-06-20 20:59:57,687][gdb3][DEBUG ]   linux-headers-4.4.0-1017-aws linux-image-4.4.0-1013-aws
[2017-06-20 20:59:57,687][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws ntp python-blinker
[2017-06-20 20:59:57,687][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-06-20 20:59:57,687][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-06-20 20:59:57,687][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-06-20 20:59:57,688][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-06-20 20:59:57,688][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-06-20 20:59:57,688][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-20 20:59:57,704][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-20 20:59:57,704][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-20 20:59:57,818][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 45 not upgraded.
[2017-06-20 20:59:57,818][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-20 20:59:57,882][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 154602 files and directories currently installed.)
[2017-06-20 20:59:57,884][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-20 20:59:58,049][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-20 20:59:58,163][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-20 20:59:58,195][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-20 20:59:58,460][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-20 20:59:58,526][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-20 20:59:58,691][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-20 20:59:58,755][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-20 20:59:58,787][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-20 20:59:58,953][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-20 20:59:59,017][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-20 20:59:59,049][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-20 20:59:59,214][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-20 20:59:59,214][gdb3][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-06-20 20:59:59,222][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-20 20:59:59,386][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-20 20:59:59,452][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-20 20:59:59,452][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-20 20:59:59,566][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-06-20 21:00:00,382][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-20 21:00:00,548][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5efc468758>
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f5efcd75230>
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 21:00:00,549][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 21:00:00,549][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-20 21:00:00,576][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 21:00:00,590][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 21:00:00,590][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 21:00:00,607][gdb3][DEBUG ] detect machine type
[2017-06-20 21:00:00,609][gdb3][DEBUG ] find the location of an executable
[2017-06-20 21:00:00,625][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 21:00:00,640][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 21:00:00,640][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 21:00:00,657][gdb3][DEBUG ] detect machine type
[2017-06-20 21:00:00,660][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 21:00:00,660][gdb3][INFO  ] purging data on gdb3
[2017-06-20 21:00:00,661][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-20 21:00:00,674][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-20 21:00:00,845][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 21:00:00,845][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-20 21:00:00,846][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 21:00:00,846][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 21:00:00,846][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 21:00:00,846][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 21:00:00,846][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 21:00:00,846][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3571901a28>
[2017-06-20 21:00:00,846][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 21:00:00,846][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f35721c4848>
[2017-06-20 21:00:00,846][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 21:00:00,846][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 21:00:19,908][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 21:00:19,908][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-20 21:00:19,908][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 21:00:19,908][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 21:00:19,908][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 21:00:19,908][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 21:00:19,908][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 21:00:19,908][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd4178735a8>
[2017-06-20 21:00:19,909][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 21:00:19,909][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-20 21:00:19,909][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-20 21:00:19,909][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fd417ef7758>
[2017-06-20 21:00:19,909][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-20 21:00:19,909][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 21:00:19,909][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-20 21:00:19,909][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 21:00:19,909][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-20 21:00:19,909][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-20 21:00:19,909][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-20 21:00:19,935][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 21:00:19,949][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 21:00:19,950][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 21:00:19,966][gdb3][DEBUG ] detect machine type
[2017-06-20 21:00:19,969][gdb3][DEBUG ] find the location of an executable
[2017-06-20 21:00:19,970][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-20 21:00:19,981][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-20 21:00:19,987][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-20 21:00:19,987][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-20 21:00:19,987][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-20 21:00:19,987][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-20 21:00:19,987][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-20 21:00:19,987][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-20 21:00:19,988][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-20 21:00:19,988][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-20 21:00:20,155][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 21:00:20,155][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-20 21:00:20,155][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 21:00:20,155][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 21:00:20,155][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 21:00:20,155][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 21:00:20,155][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-20 21:00:20,155][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 21:00:20,155][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcf22250ea8>
[2017-06-20 21:00:20,155][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 21:00:20,156][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7fcf22224b18>
[2017-06-20 21:00:20,156][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 21:00:20,156][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-20 21:00:20,156][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 21:00:20,156][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-20 21:00:20,156][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-20 21:00:20,182][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 21:00:20,196][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 21:00:20,197][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 21:00:20,213][gdb3][DEBUG ] detect machine type
[2017-06-20 21:00:20,215][gdb3][DEBUG ] find the location of an executable
[2017-06-20 21:00:20,216][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-20 21:00:20,216][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-20 21:00:20,216][gdb3][DEBUG ] get remote short hostname
[2017-06-20 21:00:20,216][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-20 21:00:20,216][gdb3][DEBUG ] get remote short hostname
[2017-06-20 21:00:20,217][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-20 21:00:20,217][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 21:00:20,219][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-20 21:00:20,219][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-20 21:00:20,219][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-20 21:00:20,220][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-20 21:00:20,220][gdb3][DEBUG ] create the monitor keyring file
[2017-06-20 21:00:20,221][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-20 21:00:20,259][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-20 21:00:20,259][gdb3][DEBUG ] ceph-mon: set fsid to 9e56305c-0f98-46a9-9f1a-45e2c1ff15da
[2017-06-20 21:00:20,262][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-20 21:00:20,266][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-20 21:00:20,266][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-20 21:00:20,267][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-20 21:00:20,268][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-20 21:00:20,335][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-20 21:00:20,403][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-20 21:00:22,442][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 21:00:22,507][gdb3][DEBUG ] ********************************************************************************
[2017-06-20 21:00:22,508][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-20 21:00:22,508][gdb3][DEBUG ] {
[2017-06-20 21:00:22,508][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-20 21:00:22,508][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-20 21:00:22,508][gdb3][DEBUG ]   "features": {
[2017-06-20 21:00:22,508][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-20 21:00:22,508][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-20 21:00:22,508][gdb3][DEBUG ]       "kraken", 
[2017-06-20 21:00:22,508][gdb3][DEBUG ]       "luminous"
[2017-06-20 21:00:22,508][gdb3][DEBUG ]     ], 
[2017-06-20 21:00:22,508][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-20 21:00:22,508][gdb3][DEBUG ]     "required_mon": [
[2017-06-20 21:00:22,508][gdb3][DEBUG ]       "kraken", 
[2017-06-20 21:00:22,509][gdb3][DEBUG ]       "luminous"
[2017-06-20 21:00:22,509][gdb3][DEBUG ]     ]
[2017-06-20 21:00:22,509][gdb3][DEBUG ]   }, 
[2017-06-20 21:00:22,509][gdb3][DEBUG ]   "monmap": {
[2017-06-20 21:00:22,509][gdb3][DEBUG ]     "created": "2017-06-20 21:00:20.243794", 
[2017-06-20 21:00:22,509][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-20 21:00:22,509][gdb3][DEBUG ]     "features": {
[2017-06-20 21:00:22,509][gdb3][DEBUG ]       "optional": [], 
[2017-06-20 21:00:22,509][gdb3][DEBUG ]       "persistent": [
[2017-06-20 21:00:22,509][gdb3][DEBUG ]         "kraken", 
[2017-06-20 21:00:22,509][gdb3][DEBUG ]         "luminous"
[2017-06-20 21:00:22,509][gdb3][DEBUG ]       ]
[2017-06-20 21:00:22,509][gdb3][DEBUG ]     }, 
[2017-06-20 21:00:22,509][gdb3][DEBUG ]     "fsid": "9e56305c-0f98-46a9-9f1a-45e2c1ff15da", 
[2017-06-20 21:00:22,509][gdb3][DEBUG ]     "modified": "2017-06-20 21:00:20.484930", 
[2017-06-20 21:00:22,509][gdb3][DEBUG ]     "mons": [
[2017-06-20 21:00:22,509][gdb3][DEBUG ]       {
[2017-06-20 21:00:22,510][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-20 21:00:22,510][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-20 21:00:22,510][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-20 21:00:22,510][gdb3][DEBUG ]         "rank": 0
[2017-06-20 21:00:22,510][gdb3][DEBUG ]       }
[2017-06-20 21:00:22,510][gdb3][DEBUG ]     ]
[2017-06-20 21:00:22,510][gdb3][DEBUG ]   }, 
[2017-06-20 21:00:22,510][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-20 21:00:22,510][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-20 21:00:22,510][gdb3][DEBUG ]   "quorum": [
[2017-06-20 21:00:22,510][gdb3][DEBUG ]     0
[2017-06-20 21:00:22,510][gdb3][DEBUG ]   ], 
[2017-06-20 21:00:22,510][gdb3][DEBUG ]   "rank": 0, 
[2017-06-20 21:00:22,510][gdb3][DEBUG ]   "state": "leader", 
[2017-06-20 21:00:22,510][gdb3][DEBUG ]   "sync_provider": []
[2017-06-20 21:00:22,510][gdb3][DEBUG ] }
[2017-06-20 21:00:22,510][gdb3][DEBUG ] ********************************************************************************
[2017-06-20 21:00:22,511][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-20 21:00:22,511][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 21:00:22,576][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-20 21:00:22,591][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 21:00:22,606][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 21:00:22,607][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 21:00:22,623][gdb3][DEBUG ] detect machine type
[2017-06-20 21:00:22,626][gdb3][DEBUG ] find the location of an executable
[2017-06-20 21:00:22,627][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 21:00:22,692][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-20 21:00:22,692][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-20 21:00:22,692][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-20 21:00:22,694][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpF7VL0x
[2017-06-20 21:00:22,709][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 21:00:22,723][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 21:00:22,723][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 21:00:22,740][gdb3][DEBUG ] detect machine type
[2017-06-20 21:00:22,742][gdb3][DEBUG ] get remote short hostname
[2017-06-20 21:00:22,742][gdb3][DEBUG ] fetch remote file
[2017-06-20 21:00:22,743][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 21:00:22,809][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-20 21:00:22,975][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-20 21:00:23,142][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-20 21:00:23,308][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-20 21:00:23,524][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-20 21:00:23,690][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-20 21:00:23,856][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-20 21:00:24,023][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-20 21:00:24,189][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-20 21:00:24,355][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-20 21:00:24,520][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-20 21:00:24,521][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-20 21:00:24,521][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170620210024'
[2017-06-20 21:00:24,522][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-20 21:00:24,522][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-20 21:00:24,522][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-20 21:00:24,522][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpF7VL0x
[2017-06-20 21:00:24,707][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 21:00:24,707][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-20 21:00:24,707][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 21:00:24,707][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 21:00:24,708][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 21:00:24,708][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 21:00:24,708][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 21:00:24,708][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f98bb559560>
[2017-06-20 21:00:24,708][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 21:00:24,708][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-20 21:00:24,708][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f98bbe70938>
[2017-06-20 21:00:24,708][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 21:00:24,708][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 21:00:24,708][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-20 21:00:24,734][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 21:00:24,748][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 21:00:24,749][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 21:00:24,765][gdb3][DEBUG ] detect machine type
[2017-06-20 21:00:24,767][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:02:12,586][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb0
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8cecb93560>
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ]  client                        : ['gdb0']
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f8ced4aa938>
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:02:12,587][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:02:12,588][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb0
[2017-06-20 22:02:15,613][ceph_deploy.admin][ERROR ] connecting to host: gdb0 resulted in errors: HostNotFound gdb0
[2017-06-20 22:02:15,613][ceph_deploy][ERROR ] GenericError: Failed to configure 1 admin hosts

[2017-06-20 22:02:15,783][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:02:15,783][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb0:/mnt/memstore
[2017-06-20 22:02:15,783][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:02:15,783][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:02:15,783][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-20 22:02:15,783][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-20 22:02:15,783][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-20 22:02:15,783][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3b287d7950>
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3b28a2daa0>
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:02:15,784][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-20 22:02:15,785][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb0:/mnt/memstore:
[2017-06-20 22:02:18,613][ceph_deploy.osd][ERROR ] connecting to host: gdb0 resulted in errors: HostNotFound gdb0
[2017-06-20 22:02:18,614][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-06-20 22:02:18,779][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:02:18,779][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb0:/mnt/memstore
[2017-06-20 22:02:18,779][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:02:18,779][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:02:18,779][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:02:18,780][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:02:18,780][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-20 22:02:18,780][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:02:18,780][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f469f50c950>
[2017-06-20 22:02:18,780][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:02:18,780][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f469f762aa0>
[2017-06-20 22:02:18,780][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:02:18,780][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:02:18,780][ceph_deploy.cli][INFO  ]  disk                          : [('gdb0', '/mnt/memstore', None)]
[2017-06-20 22:02:18,780][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb0:/mnt/memstore:
[2017-06-20 22:02:21,613][ceph_deploy][ERROR ] RuntimeError: connecting to host: gdb0 resulted in errors: HostNotFound gdb0

[2017-06-20 22:04:14,299][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:04:14,299][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-20 22:04:14,299][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:04:14,299][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:04:14,299][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:04:14,299][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:04:14,299][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:04:14,299][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f08e1132560>
[2017-06-20 22:04:14,299][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:04:14,300][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-20 22:04:14,300][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f08e1a49938>
[2017-06-20 22:04:14,300][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:04:14,300][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:04:14,300][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-20 22:04:14,652][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:04:14,879][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:04:14,880][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:04:14,919][gdb1][DEBUG ] detect machine type
[2017-06-20 22:04:14,923][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:04:15,097][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:04:15,097][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-06-20 22:04:15,097][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:04:15,097][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:04:15,097][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-20 22:04:15,097][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-20 22:04:15,097][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-20 22:04:15,097][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:04:15,097][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-20 22:04:15,097][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-20 22:04:15,098][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:04:15,098][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-20 22:04:15,098][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-20 22:04:15,098][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:04:15,098][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff8b9b57950>
[2017-06-20 22:04:15,098][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:04:15,098][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-20 22:04:15,098][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7ff8b9dadaa0>
[2017-06-20 22:04:15,098][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:04:15,098][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:04:15,098][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-20 22:04:15,098][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-06-20 22:04:15,345][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:04:15,572][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:04:15,573][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:04:15,589][gdb1][DEBUG ] detect machine type
[2017-06-20 22:04:15,593][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:04:15,594][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:04:15,594][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-20 22:04:15,595][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:04:15,600][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-06-20 22:04:15,600][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:04:15,602][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-20 22:04:15,873][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:04:16,038][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:04:16,045][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:04:16,061][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:04:16,069][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-20 22:04:16,084][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-20 22:04:21,097][gdb1][INFO  ] checking OSD status...
[2017-06-20 22:04:21,097][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:04:21,100][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-20 22:04:21,416][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-20 22:04:21,581][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:04:21,581][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-06-20 22:04:21,581][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:04:21,581][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:04:21,581][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:04:21,581][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:04:21,581][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-20 22:04:21,581][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:04:21,581][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f121a2d1950>
[2017-06-20 22:04:21,581][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:04:21,582][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f121a527aa0>
[2017-06-20 22:04:21,582][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:04:21,582][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:04:21,582][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-20 22:04:21,582][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-06-20 22:04:21,820][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:04:22,052][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:04:22,052][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:04:22,069][gdb1][DEBUG ] detect machine type
[2017-06-20 22:04:22,073][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:04:22,074][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:04:22,074][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-06-20 22:04:22,074][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-20 22:04:22,074][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:04:22,077][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-20 22:04:22,197][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-06-20 22:04:22,198][gdb1][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-20 22:04:22,198][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:04:22,214][gdb1][WARNING] Traceback (most recent call last):
[2017-06-20 22:04:22,214][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-20 22:04:22,214][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-20 22:04:22,215][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-20 22:04:22,215][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-20 22:04:22,215][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-06-20 22:04:22,215][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-20 22:04:22,215][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3609, in activate
[2017-06-20 22:04:22,215][gdb1][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-20 22:04:22,219][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-20 22:04:22,219][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-20 22:28:29,290][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:28:29,290][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-06-20 22:28:29,291][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:28:29,291][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:28:29,291][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:28:29,291][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:28:29,291][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:28:29,291][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9c49f540e0>
[2017-06-20 22:28:29,291][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:28:29,291][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-20 22:28:29,291][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f9c4a8611b8>
[2017-06-20 22:28:29,291][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:28:29,291][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:28:29,291][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-06-20 22:28:29,291][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-06-20 22:28:29,291][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-06-20 22:28:29,291][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-06-20 22:28:29,317][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:28:29,331][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:28:29,331][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:28:29,347][gdb3][DEBUG ] detect machine type
[2017-06-20 22:28:29,350][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:28:29,350][gdb3][INFO  ] Purging Ceph on gdb3
[2017-06-20 22:28:29,351][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-06-20 22:28:29,389][gdb3][DEBUG ] Reading package lists...
[2017-06-20 22:28:29,553][gdb3][DEBUG ] Building dependency tree...
[2017-06-20 22:28:29,554][gdb3][DEBUG ] Reading state information...
[2017-06-20 22:28:29,618][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-06-20 22:28:29,618][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-06-20 22:28:29,618][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-06-20 22:28:29,618][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-06-20 22:28:29,618][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-06-20 22:28:29,619][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-06-20 22:28:29,619][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-06-20 22:28:29,619][gdb3][DEBUG ]   linux-headers-4.4.0-1013-aws linux-headers-4.4.0-1016-aws
[2017-06-20 22:28:29,619][gdb3][DEBUG ]   linux-headers-4.4.0-1017-aws linux-image-4.4.0-1013-aws
[2017-06-20 22:28:29,619][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws ntp python-blinker
[2017-06-20 22:28:29,619][gdb3][DEBUG ]   python-cephfs python-cffi-backend python-chardet python-cryptography
[2017-06-20 22:28:29,619][gdb3][DEBUG ]   python-enum34 python-flask python-idna python-ipaddress python-itsdangerous
[2017-06-20 22:28:29,620][gdb3][DEBUG ]   python-jinja2 python-markupsafe python-ndg-httpsclient python-openssl
[2017-06-20 22:28:29,620][gdb3][DEBUG ]   python-pyasn1 python-pyinotify python-rados python-rbd python-requests
[2017-06-20 22:28:29,620][gdb3][DEBUG ]   python-rgw python-six python-urllib3 python-werkzeug
[2017-06-20 22:28:29,620][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-06-20 22:28:29,652][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-06-20 22:28:29,652][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-06-20 22:28:29,766][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 45 not upgraded.
[2017-06-20 22:28:29,766][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-06-20 22:28:29,830][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 154602 files and directories currently installed.)
[2017-06-20 22:28:29,831][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-06-20 22:28:29,995][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-06-20 22:28:30,109][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-06-20 22:28:30,141][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-06-20 22:28:30,405][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-06-20 22:28:30,470][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-06-20 22:28:30,637][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-06-20 22:28:30,701][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-06-20 22:28:30,733][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-06-20 22:28:30,899][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-06-20 22:28:30,963][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-06-20 22:28:30,995][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-06-20 22:28:31,166][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-06-20 22:28:31,182][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-06-20 22:28:31,346][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-06-20 22:28:31,412][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-06-20 22:28:31,420][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-06-20 22:28:31,534][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-06-20 22:28:32,350][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-06-20 22:28:32,516][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:28:32,516][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-06-20 22:28:32,517][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:28:32,517][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:28:32,517][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:28:32,517][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:28:32,517][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:28:32,517][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2c64f48758>
[2017-06-20 22:28:32,517][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:28:32,517][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-06-20 22:28:32,517][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f2c65855230>
[2017-06-20 22:28:32,517][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:28:32,517][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:28:32,517][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-06-20 22:28:32,543][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:28:32,557][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:28:32,558][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:28:32,575][gdb3][DEBUG ] detect machine type
[2017-06-20 22:28:32,577][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:28:32,593][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:28:32,608][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:28:32,608][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:28:32,625][gdb3][DEBUG ] detect machine type
[2017-06-20 22:28:32,628][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:28:32,628][gdb3][INFO  ] purging data on gdb3
[2017-06-20 22:28:32,629][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-06-20 22:28:32,642][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-06-20 22:28:32,813][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:28:32,813][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-06-20 22:28:32,813][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:28:32,813][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:28:32,814][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:28:32,814][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:28:32,814][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:28:32,814][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f780739fa28>
[2017-06-20 22:28:32,814][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:28:32,814][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f7807c62848>
[2017-06-20 22:28:32,814][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:28:32,814][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:28:50,628][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:28:50,628][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-06-20 22:28:50,628][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fabc84ed5a8>
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7fabc8b71758>
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:28:50,629][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-06-20 22:28:50,630][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-06-20 22:28:50,630][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-06-20 22:28:50,656][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:28:50,670][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:28:50,670][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:28:50,687][gdb3][DEBUG ] detect machine type
[2017-06-20 22:28:50,689][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:28:50,690][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-06-20 22:28:50,701][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-06-20 22:28:50,708][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-06-20 22:28:50,708][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-06-20 22:28:50,708][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-06-20 22:28:50,708][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-06-20 22:28:50,708][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-06-20 22:28:50,708][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-06-20 22:28:50,708][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-06-20 22:28:50,709][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-06-20 22:28:50,875][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:28:50,875][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-06-20 22:28:50,875][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:28:50,875][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:28:50,875][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:28:50,876][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:28:50,876][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-06-20 22:28:50,876][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:28:50,876][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f96c553bea8>
[2017-06-20 22:28:50,876][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:28:50,876][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f96c550fb18>
[2017-06-20 22:28:50,876][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:28:50,876][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-06-20 22:28:50,876][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:28:50,877][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-06-20 22:28:50,877][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-06-20 22:28:50,903][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:28:50,918][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:28:50,918][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:28:50,935][gdb3][DEBUG ] detect machine type
[2017-06-20 22:28:50,937][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:28:50,938][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-06-20 22:28:50,938][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-06-20 22:28:50,938][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:28:50,938][gdb3][DEBUG ] deploying mon to gdb3
[2017-06-20 22:28:50,938][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:28:50,939][gdb3][DEBUG ] remote hostname: gdb3
[2017-06-20 22:28:50,939][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:28:50,941][gdb3][DEBUG ] create the mon path if it does not exist
[2017-06-20 22:28:50,941][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-20 22:28:50,941][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-06-20 22:28:50,942][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-20 22:28:50,942][gdb3][DEBUG ] create the monitor keyring file
[2017-06-20 22:28:50,943][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-06-20 22:28:50,981][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-06-20 22:28:50,981][gdb3][DEBUG ] ceph-mon: set fsid to 382eeac0-7e10-4bdc-98d7-53d22201d56d
[2017-06-20 22:28:50,985][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-06-20 22:28:50,988][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-06-20 22:28:50,989][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-06-20 22:28:50,989][gdb3][DEBUG ] create the init path if it does not exist
[2017-06-20 22:28:50,990][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-06-20 22:28:51,058][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-06-20 22:28:51,125][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-06-20 22:28:53,196][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:28:53,261][gdb3][DEBUG ] ********************************************************************************
[2017-06-20 22:28:53,265][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-06-20 22:28:53,265][gdb3][DEBUG ] {
[2017-06-20 22:28:53,265][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-06-20 22:28:53,265][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-06-20 22:28:53,265][gdb3][DEBUG ]   "features": {
[2017-06-20 22:28:53,265][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-06-20 22:28:53,265][gdb3][DEBUG ]     "quorum_mon": [
[2017-06-20 22:28:53,265][gdb3][DEBUG ]       "kraken", 
[2017-06-20 22:28:53,265][gdb3][DEBUG ]       "luminous"
[2017-06-20 22:28:53,265][gdb3][DEBUG ]     ], 
[2017-06-20 22:28:53,266][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-06-20 22:28:53,266][gdb3][DEBUG ]     "required_mon": [
[2017-06-20 22:28:53,266][gdb3][DEBUG ]       "kraken", 
[2017-06-20 22:28:53,266][gdb3][DEBUG ]       "luminous"
[2017-06-20 22:28:53,266][gdb3][DEBUG ]     ]
[2017-06-20 22:28:53,266][gdb3][DEBUG ]   }, 
[2017-06-20 22:28:53,266][gdb3][DEBUG ]   "monmap": {
[2017-06-20 22:28:53,266][gdb3][DEBUG ]     "created": "2017-06-20 22:28:50.966279", 
[2017-06-20 22:28:53,266][gdb3][DEBUG ]     "epoch": 2, 
[2017-06-20 22:28:53,266][gdb3][DEBUG ]     "features": {
[2017-06-20 22:28:53,266][gdb3][DEBUG ]       "optional": [], 
[2017-06-20 22:28:53,266][gdb3][DEBUG ]       "persistent": [
[2017-06-20 22:28:53,266][gdb3][DEBUG ]         "kraken", 
[2017-06-20 22:28:53,266][gdb3][DEBUG ]         "luminous"
[2017-06-20 22:28:53,266][gdb3][DEBUG ]       ]
[2017-06-20 22:28:53,266][gdb3][DEBUG ]     }, 
[2017-06-20 22:28:53,266][gdb3][DEBUG ]     "fsid": "382eeac0-7e10-4bdc-98d7-53d22201d56d", 
[2017-06-20 22:28:53,267][gdb3][DEBUG ]     "modified": "2017-06-20 22:28:51.215704", 
[2017-06-20 22:28:53,267][gdb3][DEBUG ]     "mons": [
[2017-06-20 22:28:53,267][gdb3][DEBUG ]       {
[2017-06-20 22:28:53,267][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-06-20 22:28:53,267][gdb3][DEBUG ]         "name": "gdb3", 
[2017-06-20 22:28:53,267][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-06-20 22:28:53,267][gdb3][DEBUG ]         "rank": 0
[2017-06-20 22:28:53,267][gdb3][DEBUG ]       }
[2017-06-20 22:28:53,267][gdb3][DEBUG ]     ]
[2017-06-20 22:28:53,267][gdb3][DEBUG ]   }, 
[2017-06-20 22:28:53,267][gdb3][DEBUG ]   "name": "gdb3", 
[2017-06-20 22:28:53,267][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-06-20 22:28:53,267][gdb3][DEBUG ]   "quorum": [
[2017-06-20 22:28:53,267][gdb3][DEBUG ]     0
[2017-06-20 22:28:53,267][gdb3][DEBUG ]   ], 
[2017-06-20 22:28:53,267][gdb3][DEBUG ]   "rank": 0, 
[2017-06-20 22:28:53,267][gdb3][DEBUG ]   "state": "leader", 
[2017-06-20 22:28:53,268][gdb3][DEBUG ]   "sync_provider": []
[2017-06-20 22:28:53,268][gdb3][DEBUG ] }
[2017-06-20 22:28:53,268][gdb3][DEBUG ] ********************************************************************************
[2017-06-20 22:28:53,268][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-06-20 22:28:53,269][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:28:53,334][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-06-20 22:28:53,349][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:28:53,362][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:28:53,363][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:28:53,379][gdb3][DEBUG ] detect machine type
[2017-06-20 22:28:53,382][gdb3][DEBUG ] find the location of an executable
[2017-06-20 22:28:53,383][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:28:53,448][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-06-20 22:28:53,448][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-06-20 22:28:53,448][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-06-20 22:28:53,450][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpWU4E8L
[2017-06-20 22:28:53,465][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:28:53,479][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:28:53,480][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:28:53,496][gdb3][DEBUG ] detect machine type
[2017-06-20 22:28:53,499][gdb3][DEBUG ] get remote short hostname
[2017-06-20 22:28:53,499][gdb3][DEBUG ] fetch remote file
[2017-06-20 22:28:53,500][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-06-20 22:28:53,566][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-06-20 22:28:53,732][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-06-20 22:28:53,898][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-06-20 22:28:54,065][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-06-20 22:28:54,231][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-06-20 22:28:54,397][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-06-20 22:28:54,563][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-06-20 22:28:54,729][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-06-20 22:28:54,946][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-06-20 22:28:55,112][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-06-20 22:28:55,277][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-06-20 22:28:55,277][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-06-20 22:28:55,277][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170620222855'
[2017-06-20 22:28:55,278][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-06-20 22:28:55,278][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-06-20 22:28:55,278][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-06-20 22:28:55,278][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpWU4E8L
[2017-06-20 22:28:55,458][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:28:55,458][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-06-20 22:28:55,458][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:28:55,458][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:28:55,458][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:28:55,458][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:28:55,458][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:28:55,458][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9901629560>
[2017-06-20 22:28:55,458][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:28:55,459][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-06-20 22:28:55,459][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f9901f40938>
[2017-06-20 22:28:55,459][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:28:55,459][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:28:55,459][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-06-20 22:28:55,485][gdb3][DEBUG ] connection detected need for sudo
[2017-06-20 22:28:55,500][gdb3][DEBUG ] connected to host: gdb3 
[2017-06-20 22:28:55,500][gdb3][DEBUG ] detect platform information from remote host
[2017-06-20 22:28:55,517][gdb3][DEBUG ] detect machine type
[2017-06-20 22:28:55,519][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:29:27,435][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f48d721a560>
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f48d7b31938>
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:29:27,436][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:29:27,436][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-20 22:29:27,685][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:29:27,925][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:29:27,926][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:29:27,943][gdb1][DEBUG ] detect machine type
[2017-06-20 22:29:27,947][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:29:28,120][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:29:28,120][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-06-20 22:29:28,120][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:29:28,120][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:29:28,120][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f78e97a6950>
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f78e99fcaa0>
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:29:28,121][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:29:28,122][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-20 22:29:28,122][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-06-20 22:29:28,373][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:29:28,613][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:29:28,613][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:29:28,630][gdb1][DEBUG ] detect machine type
[2017-06-20 22:29:28,634][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:29:28,635][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:29:28,635][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-20 22:29:28,635][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:29:28,638][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-06-20 22:29:28,638][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:29:28,640][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-20 22:29:28,761][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:29:28,776][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:29:28,792][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:29:28,800][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:29:28,816][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-20 22:29:28,823][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-06-20 22:29:33,860][gdb1][INFO  ] checking OSD status...
[2017-06-20 22:29:33,861][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:29:33,863][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-20 22:29:34,028][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-20 22:29:34,193][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:29:34,193][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6bef132950>
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6bef388aa0>
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:29:34,194][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-20 22:29:34,195][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-06-20 22:29:34,445][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:29:34,656][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:29:34,657][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:29:34,673][gdb1][DEBUG ] detect machine type
[2017-06-20 22:29:34,677][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:29:34,678][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:29:34,678][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-06-20 22:29:34,678][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-20 22:29:34,678][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:29:34,680][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-20 22:29:34,801][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-06-20 22:29:34,801][gdb1][WARNING] activate: Cluster uuid is f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-20 22:29:34,801][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:29:34,833][gdb1][WARNING] Traceback (most recent call last):
[2017-06-20 22:29:34,833][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-20 22:29:34,833][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-20 22:29:34,834][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-20 22:29:34,834][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-20 22:29:34,834][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-06-20 22:29:34,834][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-06-20 22:29:34,834][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3609, in activate
[2017-06-20 22:29:34,834][gdb1][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid f4f36e31-c61f-42db-97ec-96a4b3bd98b7
[2017-06-20 22:29:34,834][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-20 22:29:34,834][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-20 22:35:45,993][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:35:45,993][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-20 22:35:45,993][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:35:45,994][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:35:45,994][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:35:45,994][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:35:45,994][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:35:45,994][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3cdd66f560>
[2017-06-20 22:35:45,994][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:35:45,994][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-20 22:35:45,994][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f3cddf86938>
[2017-06-20 22:35:45,994][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:35:45,994][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:35:45,994][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-20 22:35:46,245][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:35:46,445][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:35:46,445][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:35:46,462][gdb1][DEBUG ] detect machine type
[2017-06-20 22:35:46,466][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:35:46,639][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:35:46,639][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-06-20 22:35:46,639][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:35:46,639][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:35:46,639][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f33cfad7950>
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f33cfd2daa0>
[2017-06-20 22:35:46,640][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:35:46,641][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:35:46,641][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-20 22:35:46,641][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-06-20 22:35:46,889][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:35:47,096][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:35:47,096][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:35:47,113][gdb1][DEBUG ] detect machine type
[2017-06-20 22:35:47,117][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:35:47,117][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:35:47,117][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-20 22:35:47,118][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:35:47,120][gdb1][WARNING] osd keyring does not exist yet, creating one
[2017-06-20 22:35:47,120][gdb1][DEBUG ] create a keyring file
[2017-06-20 22:35:47,122][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-06-20 22:35:47,122][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:35:47,124][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-20 22:35:47,294][gdb1][WARNING] Traceback (most recent call last):
[2017-06-20 22:35:47,295][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-20 22:35:47,295][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-20 22:35:47,295][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-20 22:35:47,295][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-20 22:35:47,295][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2029, in main
[2017-06-20 22:35:47,295][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2025, in factory
[2017-06-20 22:35:47,295][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2037, in __init__
[2017-06-20 22:35:47,295][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2739, in __init__
[2017-06-20 22:35:47,295][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 2747, in set_type
[2017-06-20 22:35:47,295][gdb1][WARNING] OSError: [Errno 2] No such file or directory: '/mnt/memstore'
[2017-06-20 22:35:47,296][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-20 22:35:47,296][ceph_deploy.osd][ERROR ] Failed to execute command: /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-20 22:35:47,296][ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

[2017-06-20 22:35:47,466][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:35:47,466][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-06-20 22:35:47,466][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:35:47,467][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:35:47,467][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:35:47,467][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:35:47,467][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-20 22:35:47,467][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:35:47,467][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f74dbd5e950>
[2017-06-20 22:35:47,467][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:35:47,467][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f74dbfb4aa0>
[2017-06-20 22:35:47,467][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:35:47,467][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:35:47,467][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-20 22:35:47,468][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-06-20 22:35:47,717][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:35:47,957][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:35:47,957][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:35:47,974][gdb1][DEBUG ] detect machine type
[2017-06-20 22:35:47,978][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:35:47,978][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:35:47,978][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-06-20 22:35:47,979][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-20 22:35:47,979][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:35:47,981][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-20 22:35:48,101][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-06-20 22:35:48,102][gdb1][WARNING] Traceback (most recent call last):
[2017-06-20 22:35:48,102][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-06-20 22:35:48,102][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-06-20 22:35:48,102][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-06-20 22:35:48,102][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-06-20 22:35:48,102][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3684, in main_activate
[2017-06-20 22:35:48,102][gdb1][WARNING] ceph_disk.main.Error: Error: /mnt/memstore does not exist
[2017-06-20 22:35:48,134][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-06-20 22:35:48,135][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

[2017-06-20 22:36:20,089][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:36:20,089][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-06-20 22:36:20,089][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:36:20,089][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:36:20,089][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:36:20,089][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-06-20 22:36:20,089][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:36:20,090][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8323ba4560>
[2017-06-20 22:36:20,090][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:36:20,090][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-06-20 22:36:20,090][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f83244bb938>
[2017-06-20 22:36:20,090][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:36:20,090][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:36:20,090][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-06-20 22:36:20,342][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:36:20,581][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:36:20,582][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:36:20,598][gdb1][DEBUG ] detect machine type
[2017-06-20 22:36:20,602][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:36:20,774][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:36:20,774][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-06-20 22:36:20,774][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:36:20,774][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:36:20,774][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-06-20 22:36:20,774][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-20 22:36:20,774][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5e87495950>
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5e876ebaa0>
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:36:20,775][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-06-20 22:36:20,776][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-06-20 22:36:21,025][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:36:21,229][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:36:21,230][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:36:21,246][gdb1][DEBUG ] detect machine type
[2017-06-20 22:36:21,250][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:36:21,251][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:36:21,251][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-06-20 22:36:21,251][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-06-20 22:36:21,254][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-06-20 22:36:21,254][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:36:21,256][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-06-20 22:36:21,376][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:36:21,392][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:36:21,408][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:36:21,416][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-06-20 22:36:21,431][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-06-20 22:36:21,439][gdb1][WARNING] populate_data_path: Preparing osd data dir /mnt/memstore
[2017-06-20 22:36:21,442][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/ceph_fsid.2599.tmp
[2017-06-20 22:36:21,446][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/fsid.2599.tmp
[2017-06-20 22:36:21,462][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/magic.2599.tmp
[2017-06-20 22:36:26,499][gdb1][INFO  ] checking OSD status...
[2017-06-20 22:36:26,499][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:36:26,503][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-20 22:36:26,668][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-06-20 22:36:26,834][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-06-20 22:36:26,834][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-06-20 22:36:26,834][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-06-20 22:36:26,834][ceph_deploy.cli][INFO  ]  username                      : None
[2017-06-20 22:36:26,834][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-06-20 22:36:26,835][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-06-20 22:36:26,835][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-06-20 22:36:26,835][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-06-20 22:36:26,835][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0c796cb950>
[2017-06-20 22:36:26,835][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-06-20 22:36:26,835][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0c79921aa0>
[2017-06-20 22:36:26,835][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-06-20 22:36:26,835][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-06-20 22:36:26,835][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-06-20 22:36:26,835][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-06-20 22:36:27,089][gdb1][DEBUG ] connection detected need for sudo
[2017-06-20 22:36:27,329][gdb1][DEBUG ] connected to host: gdb1 
[2017-06-20 22:36:27,329][gdb1][DEBUG ] detect platform information from remote host
[2017-06-20 22:36:27,346][gdb1][DEBUG ] detect machine type
[2017-06-20 22:36:27,350][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:36:27,351][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-06-20 22:36:27,351][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-06-20 22:36:27,351][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-06-20 22:36:27,351][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:36:27,353][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-06-20 22:36:27,524][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-06-20 22:36:27,524][gdb1][WARNING] activate: Cluster uuid is 382eeac0-7e10-4bdc-98d7-53d22201d56d
[2017-06-20 22:36:27,524][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-06-20 22:36:27,524][gdb1][WARNING] activate: Cluster name is ceph
[2017-06-20 22:36:27,524][gdb1][WARNING] activate: OSD uuid is b1e22da6-1558-4766-87c3-7118d8079782
[2017-06-20 22:36:27,524][gdb1][WARNING] allocate_osd_id: Allocating OSD id...
[2017-06-20 22:36:27,524][gdb1][WARNING] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise b1e22da6-1558-4766-87c3-7118d8079782
[2017-06-20 22:36:27,688][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/whoami.2717.tmp
[2017-06-20 22:36:27,689][gdb1][WARNING] activate: OSD id is 0
[2017-06-20 22:36:27,689][gdb1][WARNING] activate: Initializing OSD...
[2017-06-20 22:36:27,689][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /mnt/memstore/activate.monmap
[2017-06-20 22:36:27,803][gdb1][WARNING] got monmap epoch 2
[2017-06-20 22:36:27,804][gdb1][WARNING] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /mnt/memstore/activate.monmap --osd-data /mnt/memstore --osd-journal /mnt/memstore/journal --osd-uuid b1e22da6-1558-4766-87c3-7118d8079782 --keyring /mnt/memstore/keyring --setuser ceph --setgroup ceph
[2017-06-20 22:36:27,868][gdb1][WARNING] activate: Marking with init system systemd
[2017-06-20 22:36:27,868][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/systemd
[2017-06-20 22:36:27,868][gdb1][WARNING] activate: Authorizing OSD key...
[2017-06-20 22:36:27,868][gdb1][WARNING] command_check_call: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /mnt/memstore/keyring osd allow * mon allow profile osd
[2017-06-20 22:36:28,033][gdb1][WARNING] added key for osd.0
[2017-06-20 22:36:28,033][gdb1][WARNING] command: Running command: /bin/chown -R ceph:ceph /mnt/memstore/active.2717.tmp
[2017-06-20 22:36:28,033][gdb1][WARNING] activate: ceph osd.0 data dir is ready at /mnt/memstore
[2017-06-20 22:36:28,033][gdb1][WARNING] activate_dir: Creating symlink /var/lib/ceph/osd/ceph-0 -> /mnt/memstore
[2017-06-20 22:36:28,033][gdb1][WARNING] start_daemon: Starting ceph osd.0...
[2017-06-20 22:36:28,033][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0
[2017-06-20 22:36:28,033][gdb1][WARNING] Removed symlink /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service.
[2017-06-20 22:36:28,248][gdb1][WARNING] command_check_call: Running command: /bin/systemctl disable ceph-osd@0 --runtime
[2017-06-20 22:36:28,312][gdb1][WARNING] command_check_call: Running command: /bin/systemctl enable ceph-osd@0
[2017-06-20 22:36:28,312][gdb1][WARNING] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /lib/systemd/system/ceph-osd@.service.
[2017-06-20 22:36:28,426][gdb1][WARNING] command_check_call: Running command: /bin/systemctl start ceph-osd@0
[2017-06-20 22:36:33,545][gdb1][INFO  ] checking OSD status...
[2017-06-20 22:36:33,546][gdb1][DEBUG ] find the location of an executable
[2017-06-20 22:36:33,555][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-06-20 22:36:33,923][gdb1][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 02:18:52,098][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:18:52,100][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-07-06 02:18:52,100][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:18:52,100][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:18:52,100][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:18:52,100][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:18:52,100][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:18:52,100][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f01331b6fc8>
[2017-07-06 02:18:52,100][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:18:52,100][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 02:18:52,100][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f0133ac91b8>
[2017-07-06 02:18:52,101][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:18:52,101][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:18:52,101][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-07-06 02:18:52,101][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-07-06 02:18:52,101][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-07-06 02:18:52,101][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-07-06 02:18:52,139][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:18:52,165][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:18:52,167][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:18:52,183][gdb3][DEBUG ] detect machine type
[2017-07-06 02:18:52,185][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 02:18:52,185][gdb3][INFO  ] Purging Ceph on gdb3
[2017-07-06 02:18:52,186][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-07-06 02:18:52,226][gdb3][DEBUG ] Reading package lists...
[2017-07-06 02:18:52,390][gdb3][DEBUG ] Building dependency tree...
[2017-07-06 02:18:52,391][gdb3][DEBUG ] Reading state information...
[2017-07-06 02:18:52,454][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-07-06 02:18:52,455][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-07-06 02:18:52,455][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-07-06 02:18:52,455][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-07-06 02:18:52,455][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-07-06 02:18:52,455][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-07-06 02:18:52,455][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-07-06 02:18:52,455][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1018 linux-headers-4.4.0-1013-aws
[2017-07-06 02:18:52,455][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-headers-4.4.0-1017-aws
[2017-07-06 02:18:52,455][gdb3][DEBUG ]   linux-headers-4.4.0-1018-aws linux-image-4.4.0-1013-aws
[2017-07-06 02:18:52,455][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws
[2017-07-06 02:18:52,455][gdb3][DEBUG ]   linux-image-4.4.0-1018-aws ntp python-blinker python-cephfs
[2017-07-06 02:18:52,455][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-07-06 02:18:52,456][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-07-06 02:18:52,456][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-07-06 02:18:52,456][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-07-06 02:18:52,456][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-07-06 02:18:52,456][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-07-06 02:18:52,488][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-07-06 02:18:52,488][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-07-06 02:18:52,602][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 60 not upgraded.
[2017-07-06 02:18:52,602][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-07-06 02:18:52,666][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 179644 files and directories currently installed.)
[2017-07-06 02:18:52,682][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-07-06 02:18:52,855][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-07-06 02:18:52,969][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-07-06 02:18:52,972][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-07-06 02:18:53,188][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-07-06 02:18:53,309][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-07-06 02:18:53,474][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-07-06 02:18:53,506][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-07-06 02:18:53,538][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-07-06 02:18:53,703][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-07-06 02:18:53,818][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-07-06 02:18:53,819][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-07-06 02:18:53,984][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-07-06 02:18:53,984][gdb3][DEBUG ] dpkg: warning: while removing ceph-base, directory '/var/lib/ceph/tmp' not empty so not removed
[2017-07-06 02:18:54,000][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-07-06 02:18:54,115][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-07-06 02:18:54,229][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-07-06 02:18:54,229][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-07-06 02:18:54,293][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-07-06 02:18:55,109][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-07-06 02:18:55,274][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:18:55,274][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-07-06 02:18:55,274][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:18:55,274][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:18:55,274][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:18:55,274][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:18:55,274][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:18:55,274][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f40e2265710>
[2017-07-06 02:18:55,274][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:18:55,274][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 02:18:55,275][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f40e2b72230>
[2017-07-06 02:18:55,275][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:18:55,275][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:18:55,275][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-07-06 02:18:55,300][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:18:55,314][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:18:55,315][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:18:55,331][gdb3][DEBUG ] detect machine type
[2017-07-06 02:18:55,334][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:18:55,349][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:18:55,362][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:18:55,363][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:18:55,379][gdb3][DEBUG ] detect machine type
[2017-07-06 02:18:55,381][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 02:18:55,382][gdb3][INFO  ] purging data on gdb3
[2017-07-06 02:18:55,382][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-07-06 02:18:55,395][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-07-06 02:18:55,564][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:18:55,564][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-07-06 02:18:55,564][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:18:55,564][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:18:55,564][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:18:55,564][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:18:55,564][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:18:55,564][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f817c7b99e0>
[2017-07-06 02:18:55,565][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:18:55,565][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f817d07c848>
[2017-07-06 02:18:55,565][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:18:55,565][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:19:17,641][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:19:17,642][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-07-06 02:19:17,642][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:19:17,642][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:19:17,642][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:19:17,642][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:19:17,642][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:19:17,642][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5468e4d560>
[2017-07-06 02:19:17,642][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:19:17,642][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-07-06 02:19:17,642][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-07-06 02:19:17,642][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f54694d1758>
[2017-07-06 02:19:17,643][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-07-06 02:19:17,643][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:19:17,643][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-07-06 02:19:17,643][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:19:17,643][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-07-06 02:19:17,643][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-07-06 02:19:17,643][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-07-06 02:19:17,669][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:19:17,683][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:19:17,684][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:19:17,700][gdb3][DEBUG ] detect machine type
[2017-07-06 02:19:17,703][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:19:17,704][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-07-06 02:19:17,715][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-07-06 02:19:17,721][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-07-06 02:19:17,721][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-07-06 02:19:17,722][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-07-06 02:19:17,722][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-07-06 02:19:17,722][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-07-06 02:19:17,722][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-07-06 02:19:17,722][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-07-06 02:19:17,722][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-07-06 02:19:17,887][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3ff2534e60>
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f3ff2509b18>
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-07-06 02:19:17,888][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:19:17,889][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-07-06 02:19:17,889][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-07-06 02:19:17,915][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:19:17,929][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:19:17,930][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:19:17,946][gdb3][DEBUG ] detect machine type
[2017-07-06 02:19:17,948][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:19:17,948][ceph_deploy.mon][ERROR ] ceph needs to be installed in remote host: gdb3
[2017-07-06 02:19:17,949][ceph_deploy][ERROR ] GenericError: Failed to create 1 monitors

[2017-07-06 02:19:18,129][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:19:18,129][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-07-06 02:19:18,129][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:19:18,129][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:19:18,129][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:19:18,129][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:19:18,129][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:19:18,129][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1432e96518>
[2017-07-06 02:19:18,129][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:19:18,129][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-07-06 02:19:18,130][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f14337ad938>
[2017-07-06 02:19:18,130][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:19:18,130][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:19:18,130][ceph_deploy][ERROR ] RuntimeError: ceph.client.admin.keyring not found

[2017-07-06 02:20:39,448][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff004f09560>
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7ff00558d758>
[2017-07-06 02:20:39,449][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-07-06 02:20:39,450][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:20:39,450][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-07-06 02:20:39,450][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:20:39,450][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-07-06 02:20:39,450][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-07-06 02:20:39,450][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-07-06 02:20:39,475][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:20:39,489][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:20:39,490][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:20:39,506][gdb3][DEBUG ] detect machine type
[2017-07-06 02:20:39,508][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:20:39,509][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-07-06 02:20:39,520][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-07-06 02:20:39,527][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-07-06 02:20:39,527][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-07-06 02:20:39,527][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-07-06 02:20:39,527][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-07-06 02:20:39,527][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-07-06 02:20:39,527][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-07-06 02:20:39,527][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-07-06 02:20:39,528][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-07-06 02:20:39,692][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:20:39,693][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-07-06 02:20:39,693][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:20:39,693][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:20:39,693][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:20:39,693][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:20:39,693][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-07-06 02:20:39,693][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:20:39,693][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f036b186e60>
[2017-07-06 02:20:39,694][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:20:39,694][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f036b15bb18>
[2017-07-06 02:20:39,694][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:20:39,694][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-07-06 02:20:39,694][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:20:39,695][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-07-06 02:20:39,695][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-07-06 02:20:39,721][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:20:39,735][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:20:39,736][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:20:39,752][gdb3][DEBUG ] detect machine type
[2017-07-06 02:20:39,754][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:20:39,755][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-07-06 02:20:39,755][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-07-06 02:20:39,755][gdb3][DEBUG ] get remote short hostname
[2017-07-06 02:20:39,755][gdb3][DEBUG ] deploying mon to gdb3
[2017-07-06 02:20:39,755][gdb3][DEBUG ] get remote short hostname
[2017-07-06 02:20:39,756][gdb3][DEBUG ] remote hostname: gdb3
[2017-07-06 02:20:39,756][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 02:20:39,757][gdb3][DEBUG ] create the mon path if it does not exist
[2017-07-06 02:20:39,758][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 02:20:39,758][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 02:20:39,758][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 02:20:39,759][gdb3][DEBUG ] create the monitor keyring file
[2017-07-06 02:20:39,760][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-07-06 02:20:39,798][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-07-06 02:20:39,798][gdb3][DEBUG ] ceph-mon: set fsid to 27c8a69d-df07-4fbe-96ad-e5d06e6d4d49
[2017-07-06 02:20:39,805][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-07-06 02:20:39,809][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 02:20:39,809][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-07-06 02:20:39,810][gdb3][DEBUG ] create the init path if it does not exist
[2017-07-06 02:20:39,811][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 02:20:39,884][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-07-06 02:20:39,952][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-07-06 02:20:41,991][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:20:42,106][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 02:20:42,107][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-07-06 02:20:42,107][gdb3][DEBUG ] {
[2017-07-06 02:20:42,107][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-07-06 02:20:42,107][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-07-06 02:20:42,107][gdb3][DEBUG ]   "features": {
[2017-07-06 02:20:42,107][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-07-06 02:20:42,107][gdb3][DEBUG ]     "quorum_mon": [
[2017-07-06 02:20:42,107][gdb3][DEBUG ]       "kraken", 
[2017-07-06 02:20:42,107][gdb3][DEBUG ]       "luminous"
[2017-07-06 02:20:42,107][gdb3][DEBUG ]     ], 
[2017-07-06 02:20:42,107][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-07-06 02:20:42,108][gdb3][DEBUG ]     "required_mon": [
[2017-07-06 02:20:42,108][gdb3][DEBUG ]       "kraken", 
[2017-07-06 02:20:42,108][gdb3][DEBUG ]       "luminous"
[2017-07-06 02:20:42,108][gdb3][DEBUG ]     ]
[2017-07-06 02:20:42,108][gdb3][DEBUG ]   }, 
[2017-07-06 02:20:42,108][gdb3][DEBUG ]   "monmap": {
[2017-07-06 02:20:42,108][gdb3][DEBUG ]     "created": "2017-07-06 02:20:39.783026", 
[2017-07-06 02:20:42,108][gdb3][DEBUG ]     "epoch": 2, 
[2017-07-06 02:20:42,108][gdb3][DEBUG ]     "features": {
[2017-07-06 02:20:42,108][gdb3][DEBUG ]       "optional": [], 
[2017-07-06 02:20:42,108][gdb3][DEBUG ]       "persistent": [
[2017-07-06 02:20:42,108][gdb3][DEBUG ]         "kraken", 
[2017-07-06 02:20:42,108][gdb3][DEBUG ]         "luminous"
[2017-07-06 02:20:42,108][gdb3][DEBUG ]       ]
[2017-07-06 02:20:42,108][gdb3][DEBUG ]     }, 
[2017-07-06 02:20:42,108][gdb3][DEBUG ]     "fsid": "27c8a69d-df07-4fbe-96ad-e5d06e6d4d49", 
[2017-07-06 02:20:42,108][gdb3][DEBUG ]     "modified": "2017-07-06 02:20:40.031021", 
[2017-07-06 02:20:42,109][gdb3][DEBUG ]     "mons": [
[2017-07-06 02:20:42,109][gdb3][DEBUG ]       {
[2017-07-06 02:20:42,109][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-07-06 02:20:42,109][gdb3][DEBUG ]         "name": "gdb3", 
[2017-07-06 02:20:42,109][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-07-06 02:20:42,109][gdb3][DEBUG ]         "rank": 0
[2017-07-06 02:20:42,109][gdb3][DEBUG ]       }
[2017-07-06 02:20:42,109][gdb3][DEBUG ]     ]
[2017-07-06 02:20:42,109][gdb3][DEBUG ]   }, 
[2017-07-06 02:20:42,109][gdb3][DEBUG ]   "name": "gdb3", 
[2017-07-06 02:20:42,109][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-07-06 02:20:42,109][gdb3][DEBUG ]   "quorum": [
[2017-07-06 02:20:42,109][gdb3][DEBUG ]     0
[2017-07-06 02:20:42,109][gdb3][DEBUG ]   ], 
[2017-07-06 02:20:42,109][gdb3][DEBUG ]   "rank": 0, 
[2017-07-06 02:20:42,109][gdb3][DEBUG ]   "state": "leader", 
[2017-07-06 02:20:42,110][gdb3][DEBUG ]   "sync_provider": []
[2017-07-06 02:20:42,110][gdb3][DEBUG ] }
[2017-07-06 02:20:42,110][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 02:20:42,110][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-07-06 02:20:42,111][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:20:42,176][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-07-06 02:20:42,191][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:20:42,206][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:20:42,206][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:20:42,223][gdb3][DEBUG ] detect machine type
[2017-07-06 02:20:42,225][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:20:42,227][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:20:42,292][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-07-06 02:20:42,292][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-07-06 02:20:42,292][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-07-06 02:20:42,294][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpB_1G2a
[2017-07-06 02:20:42,309][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:20:42,322][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:20:42,323][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:20:42,339][gdb3][DEBUG ] detect machine type
[2017-07-06 02:20:42,342][gdb3][DEBUG ] get remote short hostname
[2017-07-06 02:20:42,342][gdb3][DEBUG ] fetch remote file
[2017-07-06 02:20:42,343][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:20:42,409][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-07-06 02:20:42,575][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-07-06 02:20:42,741][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-07-06 02:20:42,908][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-07-06 02:20:43,124][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-07-06 02:20:43,290][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-07-06 02:20:43,456][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-07-06 02:20:43,623][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-07-06 02:20:43,789][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-07-06 02:20:43,955][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-07-06 02:20:44,120][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-07-06 02:20:44,120][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-07-06 02:20:44,121][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170706022044'
[2017-07-06 02:20:44,122][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-07-06 02:20:44,122][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-07-06 02:20:44,122][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-07-06 02:20:44,122][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpB_1G2a
[2017-07-06 02:20:44,303][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:20:44,303][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-07-06 02:20:44,303][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:20:44,303][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:20:44,303][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:20:44,303][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:20:44,303][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:20:44,303][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd8bde54518>
[2017-07-06 02:20:44,304][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:20:44,304][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-07-06 02:20:44,304][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fd8be76b938>
[2017-07-06 02:20:44,304][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:20:44,304][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:20:44,304][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 02:20:44,330][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:20:44,343][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:20:44,344][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:20:44,360][gdb3][DEBUG ] detect machine type
[2017-07-06 02:20:44,362][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 02:22:45,618][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:22:45,618][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purge gdb3
[2017-07-06 02:22:45,618][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:22:45,618][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:22:45,618][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:22:45,618][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:22:45,618][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:22:45,619][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe577833fc8>
[2017-07-06 02:22:45,619][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:22:45,619][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 02:22:45,619][ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7fe5781461b8>
[2017-07-06 02:22:45,619][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:22:45,619][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:22:45,619][ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[2017-07-06 02:22:45,619][ceph_deploy.install][INFO  ] like: librbd1 and librados2
[2017-07-06 02:22:45,619][ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts gdb3
[2017-07-06 02:22:45,619][ceph_deploy.install][DEBUG ] Detecting platform for host gdb3 ...
[2017-07-06 02:22:45,645][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:22:45,658][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:22:45,659][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:22:45,675][gdb3][DEBUG ] detect machine type
[2017-07-06 02:22:45,677][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 02:22:45,677][gdb3][INFO  ] Purging Ceph on gdb3
[2017-07-06 02:22:45,678][gdb3][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[2017-07-06 02:22:45,716][gdb3][DEBUG ] Reading package lists...
[2017-07-06 02:22:45,881][gdb3][DEBUG ] Building dependency tree...
[2017-07-06 02:22:45,881][gdb3][DEBUG ] Reading state information...
[2017-07-06 02:22:45,945][gdb3][DEBUG ] Package 'radosgw' is not installed, so not removed
[2017-07-06 02:22:45,945][gdb3][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[2017-07-06 02:22:45,945][gdb3][DEBUG ] The following packages were automatically installed and are no longer required:
[2017-07-06 02:22:45,945][gdb3][DEBUG ]   ceph-fuse javascript-common libcephfs2 libgoogle-perftools4 libjs-jquery
[2017-07-06 02:22:45,945][gdb3][DEBUG ]   libleveldb1v5 libopts25 libpython2.7 libradosstriper1 librgw2 libsnappy1v5
[2017-07-06 02:22:45,945][gdb3][DEBUG ]   libtcmalloc-minimal4 libunwind8 linux-aws-headers-4.4.0-1013
[2017-07-06 02:22:45,945][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1016 linux-aws-headers-4.4.0-1017
[2017-07-06 02:22:45,946][gdb3][DEBUG ]   linux-aws-headers-4.4.0-1018 linux-headers-4.4.0-1013-aws
[2017-07-06 02:22:45,946][gdb3][DEBUG ]   linux-headers-4.4.0-1016-aws linux-headers-4.4.0-1017-aws
[2017-07-06 02:22:45,946][gdb3][DEBUG ]   linux-headers-4.4.0-1018-aws linux-image-4.4.0-1013-aws
[2017-07-06 02:22:45,946][gdb3][DEBUG ]   linux-image-4.4.0-1016-aws linux-image-4.4.0-1017-aws
[2017-07-06 02:22:45,946][gdb3][DEBUG ]   linux-image-4.4.0-1018-aws ntp python-blinker python-cephfs
[2017-07-06 02:22:45,946][gdb3][DEBUG ]   python-cffi-backend python-chardet python-cryptography python-enum34
[2017-07-06 02:22:45,946][gdb3][DEBUG ]   python-flask python-idna python-ipaddress python-itsdangerous python-jinja2
[2017-07-06 02:22:45,946][gdb3][DEBUG ]   python-markupsafe python-ndg-httpsclient python-openssl python-pyasn1
[2017-07-06 02:22:45,946][gdb3][DEBUG ]   python-pyinotify python-rados python-rbd python-requests python-rgw
[2017-07-06 02:22:45,946][gdb3][DEBUG ]   python-six python-urllib3 python-werkzeug
[2017-07-06 02:22:45,946][gdb3][DEBUG ] Use 'sudo apt autoremove' to remove them.
[2017-07-06 02:22:45,978][gdb3][DEBUG ] The following packages will be REMOVED:
[2017-07-06 02:22:45,978][gdb3][DEBUG ]   ceph* ceph-base* ceph-common* ceph-mds* ceph-mgr* ceph-mon* ceph-osd*
[2017-07-06 02:22:46,092][gdb3][DEBUG ] 0 upgraded, 0 newly installed, 7 to remove and 60 not upgraded.
[2017-07-06 02:22:46,093][gdb3][DEBUG ] After this operation, 284 MB disk space will be freed.
[2017-07-06 02:22:46,156][gdb3][DEBUG ] (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 179644 files and directories currently installed.)
[2017-07-06 02:22:46,164][gdb3][DEBUG ] Removing ceph-mds (12.0.1-1) ...
[2017-07-06 02:22:46,329][gdb3][DEBUG ] Purging configuration files for ceph-mds (12.0.1-1) ...
[2017-07-06 02:22:46,443][gdb3][DEBUG ] Removing ceph (12.0.1-1) ...
[2017-07-06 02:22:46,475][gdb3][DEBUG ] Removing ceph-osd (12.0.1-1) ...
[2017-07-06 02:22:46,689][gdb3][DEBUG ] Purging configuration files for ceph-osd (12.0.1-1) ...
[2017-07-06 02:22:46,804][gdb3][DEBUG ] Removing ceph-mon (12.0.1-1) ...
[2017-07-06 02:22:46,969][gdb3][DEBUG ] Purging configuration files for ceph-mon (12.0.1-1) ...
[2017-07-06 02:22:47,001][gdb3][DEBUG ] dpkg: warning: while removing ceph-mon, directory '/var/lib/ceph/mon' not empty so not removed
[2017-07-06 02:22:47,032][gdb3][DEBUG ] Removing ceph-mgr (12.0.1-1) ...
[2017-07-06 02:22:47,198][gdb3][DEBUG ] Purging configuration files for ceph-mgr (12.0.1-1) ...
[2017-07-06 02:22:47,262][gdb3][DEBUG ] dpkg: warning: while removing ceph-mgr, directory '/var/lib/ceph/mgr' not empty so not removed
[2017-07-06 02:22:47,277][gdb3][DEBUG ] Removing ceph-base (12.0.1-1) ...
[2017-07-06 02:22:47,442][gdb3][DEBUG ] Purging configuration files for ceph-base (12.0.1-1) ...
[2017-07-06 02:22:47,473][gdb3][DEBUG ] Removing ceph-common (12.0.1-1) ...
[2017-07-06 02:22:47,638][gdb3][DEBUG ] Purging configuration files for ceph-common (12.0.1-1) ...
[2017-07-06 02:22:47,670][gdb3][DEBUG ] dpkg: warning: while removing ceph-common, directory '/var/lib/ceph' not empty so not removed
[2017-07-06 02:22:47,685][gdb3][DEBUG ] Processing triggers for man-db (2.7.5-1) ...
[2017-07-06 02:22:47,799][gdb3][DEBUG ] Processing triggers for libc-bin (2.23-0ubuntu9) ...
[2017-07-06 02:22:48,616][gdb3][WARNING] W: --force-yes is deprecated, use one of the options starting with --allow instead.
[2017-07-06 02:22:48,778][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:22:48,778][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy purgedata gdb3
[2017-07-06 02:22:48,778][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:22:48,778][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:22:48,778][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:22:48,778][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:22:48,778][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:22:48,779][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd6765f5710>
[2017-07-06 02:22:48,779][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:22:48,779][ceph_deploy.cli][INFO  ]  host                          : ['gdb3']
[2017-07-06 02:22:48,779][ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7fd676f02230>
[2017-07-06 02:22:48,779][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:22:48,779][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:22:48,779][ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts gdb3
[2017-07-06 02:22:48,804][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:22:48,818][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:22:48,819][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:22:48,835][gdb3][DEBUG ] detect machine type
[2017-07-06 02:22:48,837][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:22:48,853][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:22:48,867][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:22:48,868][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:22:48,884][gdb3][DEBUG ] detect machine type
[2017-07-06 02:22:48,887][ceph_deploy.install][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 02:22:48,887][gdb3][INFO  ] purging data on gdb3
[2017-07-06 02:22:48,888][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[2017-07-06 02:22:48,901][gdb3][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
[2017-07-06 02:22:49,072][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:22:49,072][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy forgetkeys
[2017-07-06 02:22:49,072][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:22:49,072][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:22:49,072][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:22:49,072][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:22:49,073][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:22:49,073][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6b7dd859e0>
[2017-07-06 02:22:49,073][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:22:49,073][ceph_deploy.cli][INFO  ]  func                          : <function forgetkeys at 0x7f6b7e648848>
[2017-07-06 02:22:49,073][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:22:49,073][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:23:09,561][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:23:09,562][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy new gdb3
[2017-07-06 02:23:09,562][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:23:09,562][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:23:09,562][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:23:09,562][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:23:09,562][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:23:09,562][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7047360560>
[2017-07-06 02:23:09,562][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:23:09,562][ceph_deploy.cli][INFO  ]  ssh_copykey                   : True
[2017-07-06 02:23:09,562][ceph_deploy.cli][INFO  ]  mon                           : ['gdb3']
[2017-07-06 02:23:09,563][ceph_deploy.cli][INFO  ]  func                          : <function new at 0x7f70479e4758>
[2017-07-06 02:23:09,563][ceph_deploy.cli][INFO  ]  public_network                : None
[2017-07-06 02:23:09,563][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:23:09,563][ceph_deploy.cli][INFO  ]  cluster_network               : None
[2017-07-06 02:23:09,563][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:23:09,563][ceph_deploy.cli][INFO  ]  fsid                          : None
[2017-07-06 02:23:09,563][ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[2017-07-06 02:23:09,563][ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[2017-07-06 02:23:09,589][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:23:09,603][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:23:09,603][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:23:09,619][gdb3][DEBUG ] detect machine type
[2017-07-06 02:23:09,622][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:23:09,623][gdb3][INFO  ] Running command: sudo /bin/ip link show
[2017-07-06 02:23:09,634][gdb3][INFO  ] Running command: sudo /bin/ip addr show
[2017-07-06 02:23:09,640][gdb3][DEBUG ] IP addresses found: [u'172.31.22.186']
[2017-07-06 02:23:09,640][ceph_deploy.new][DEBUG ] Resolving host gdb3
[2017-07-06 02:23:09,641][ceph_deploy.new][DEBUG ] Monitor gdb3 at 172.31.22.186
[2017-07-06 02:23:09,641][ceph_deploy.new][DEBUG ] Monitor initial members are ['gdb3']
[2017-07-06 02:23:09,641][ceph_deploy.new][DEBUG ] Monitor addrs are ['172.31.22.186']
[2017-07-06 02:23:09,641][ceph_deploy.new][DEBUG ] Creating a random mon key...
[2017-07-06 02:23:09,641][ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[2017-07-06 02:23:09,641][ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
[2017-07-06 02:23:09,806][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:23:09,806][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy mon create-initial
[2017-07-06 02:23:09,806][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:23:09,806][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:23:09,806][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:23:09,806][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:23:09,806][ceph_deploy.cli][INFO  ]  subcommand                    : create-initial
[2017-07-06 02:23:09,806][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:23:09,806][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9bd5800e60>
[2017-07-06 02:23:09,807][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:23:09,807][ceph_deploy.cli][INFO  ]  func                          : <function mon at 0x7f9bd57d5b18>
[2017-07-06 02:23:09,807][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:23:09,807][ceph_deploy.cli][INFO  ]  keyrings                      : None
[2017-07-06 02:23:09,807][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:23:09,807][ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts gdb3
[2017-07-06 02:23:09,808][ceph_deploy.mon][DEBUG ] detecting platform for host gdb3 ...
[2017-07-06 02:23:09,833][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:23:09,847][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:23:09,847][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:23:09,864][gdb3][DEBUG ] detect machine type
[2017-07-06 02:23:09,866][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:23:09,867][ceph_deploy.mon][INFO  ] distro info: Ubuntu 16.04 xenial
[2017-07-06 02:23:09,867][gdb3][DEBUG ] determining if provided host has same hostname in remote
[2017-07-06 02:23:09,867][gdb3][DEBUG ] get remote short hostname
[2017-07-06 02:23:09,867][gdb3][DEBUG ] deploying mon to gdb3
[2017-07-06 02:23:09,867][gdb3][DEBUG ] get remote short hostname
[2017-07-06 02:23:09,868][gdb3][DEBUG ] remote hostname: gdb3
[2017-07-06 02:23:09,868][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 02:23:09,869][gdb3][DEBUG ] create the mon path if it does not exist
[2017-07-06 02:23:09,870][gdb3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 02:23:09,870][gdb3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-gdb3/done
[2017-07-06 02:23:09,871][gdb3][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 02:23:09,871][gdb3][DEBUG ] create the monitor keyring file
[2017-07-06 02:23:09,872][gdb3][INFO  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i gdb3 --keyring /var/lib/ceph/tmp/ceph-gdb3.mon.keyring --setuser 64045 --setgroup 64045
[2017-07-06 02:23:09,910][gdb3][DEBUG ] ceph-mon: mon.noname-a 172.31.22.186:6789/0 is local, renaming to mon.gdb3
[2017-07-06 02:23:09,910][gdb3][DEBUG ] ceph-mon: set fsid to 5b56fef1-c89a-4db0-9bcf-6963b9199fcb
[2017-07-06 02:23:09,913][gdb3][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-gdb3 for mon.gdb3
[2017-07-06 02:23:09,915][gdb3][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-gdb3.mon.keyring
[2017-07-06 02:23:09,915][gdb3][DEBUG ] create a done file to avoid re-doing the mon deployment
[2017-07-06 02:23:09,915][gdb3][DEBUG ] create the init path if it does not exist
[2017-07-06 02:23:09,916][gdb3][INFO  ] Running command: sudo systemctl enable ceph.target
[2017-07-06 02:23:09,987][gdb3][INFO  ] Running command: sudo systemctl enable ceph-mon@gdb3
[2017-07-06 02:23:10,059][gdb3][INFO  ] Running command: sudo systemctl start ceph-mon@gdb3
[2017-07-06 02:23:12,097][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:23:12,162][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 02:23:12,162][gdb3][DEBUG ] status for monitor: mon.gdb3
[2017-07-06 02:23:12,162][gdb3][DEBUG ] {
[2017-07-06 02:23:12,162][gdb3][DEBUG ]   "election_epoch": 4, 
[2017-07-06 02:23:12,162][gdb3][DEBUG ]   "extra_probe_peers": [], 
[2017-07-06 02:23:12,162][gdb3][DEBUG ]   "features": {
[2017-07-06 02:23:12,162][gdb3][DEBUG ]     "quorum_con": "1152323339925389307", 
[2017-07-06 02:23:12,162][gdb3][DEBUG ]     "quorum_mon": [
[2017-07-06 02:23:12,162][gdb3][DEBUG ]       "kraken", 
[2017-07-06 02:23:12,162][gdb3][DEBUG ]       "luminous"
[2017-07-06 02:23:12,163][gdb3][DEBUG ]     ], 
[2017-07-06 02:23:12,163][gdb3][DEBUG ]     "required_con": "153140804152475648", 
[2017-07-06 02:23:12,163][gdb3][DEBUG ]     "required_mon": [
[2017-07-06 02:23:12,163][gdb3][DEBUG ]       "kraken", 
[2017-07-06 02:23:12,163][gdb3][DEBUG ]       "luminous"
[2017-07-06 02:23:12,163][gdb3][DEBUG ]     ]
[2017-07-06 02:23:12,163][gdb3][DEBUG ]   }, 
[2017-07-06 02:23:12,163][gdb3][DEBUG ]   "monmap": {
[2017-07-06 02:23:12,163][gdb3][DEBUG ]     "created": "2017-07-06 02:23:09.894943", 
[2017-07-06 02:23:12,163][gdb3][DEBUG ]     "epoch": 2, 
[2017-07-06 02:23:12,163][gdb3][DEBUG ]     "features": {
[2017-07-06 02:23:12,163][gdb3][DEBUG ]       "optional": [], 
[2017-07-06 02:23:12,163][gdb3][DEBUG ]       "persistent": [
[2017-07-06 02:23:12,163][gdb3][DEBUG ]         "kraken", 
[2017-07-06 02:23:12,163][gdb3][DEBUG ]         "luminous"
[2017-07-06 02:23:12,163][gdb3][DEBUG ]       ]
[2017-07-06 02:23:12,163][gdb3][DEBUG ]     }, 
[2017-07-06 02:23:12,164][gdb3][DEBUG ]     "fsid": "5b56fef1-c89a-4db0-9bcf-6963b9199fcb", 
[2017-07-06 02:23:12,164][gdb3][DEBUG ]     "modified": "2017-07-06 02:23:10.148909", 
[2017-07-06 02:23:12,164][gdb3][DEBUG ]     "mons": [
[2017-07-06 02:23:12,164][gdb3][DEBUG ]       {
[2017-07-06 02:23:12,164][gdb3][DEBUG ]         "addr": "172.31.22.186:6789/0", 
[2017-07-06 02:23:12,164][gdb3][DEBUG ]         "name": "gdb3", 
[2017-07-06 02:23:12,164][gdb3][DEBUG ]         "public_addr": "172.31.22.186:6789/0", 
[2017-07-06 02:23:12,164][gdb3][DEBUG ]         "rank": 0
[2017-07-06 02:23:12,164][gdb3][DEBUG ]       }
[2017-07-06 02:23:12,164][gdb3][DEBUG ]     ]
[2017-07-06 02:23:12,164][gdb3][DEBUG ]   }, 
[2017-07-06 02:23:12,164][gdb3][DEBUG ]   "name": "gdb3", 
[2017-07-06 02:23:12,164][gdb3][DEBUG ]   "outside_quorum": [], 
[2017-07-06 02:23:12,164][gdb3][DEBUG ]   "quorum": [
[2017-07-06 02:23:12,164][gdb3][DEBUG ]     0
[2017-07-06 02:23:12,164][gdb3][DEBUG ]   ], 
[2017-07-06 02:23:12,165][gdb3][DEBUG ]   "rank": 0, 
[2017-07-06 02:23:12,165][gdb3][DEBUG ]   "state": "leader", 
[2017-07-06 02:23:12,165][gdb3][DEBUG ]   "sync_provider": []
[2017-07-06 02:23:12,165][gdb3][DEBUG ] }
[2017-07-06 02:23:12,165][gdb3][DEBUG ] ********************************************************************************
[2017-07-06 02:23:12,165][gdb3][INFO  ] monitor: mon.gdb3 is running
[2017-07-06 02:23:12,166][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:23:12,231][ceph_deploy.mon][INFO  ] processing monitor mon.gdb3
[2017-07-06 02:23:12,246][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:23:12,260][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:23:12,260][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:23:12,277][gdb3][DEBUG ] detect machine type
[2017-07-06 02:23:12,279][gdb3][DEBUG ] find the location of an executable
[2017-07-06 02:23:12,280][gdb3][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:23:12,345][ceph_deploy.mon][INFO  ] mon.gdb3 monitor has reached quorum!
[2017-07-06 02:23:12,345][ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[2017-07-06 02:23:12,345][ceph_deploy.mon][INFO  ] Running gatherkeys...
[2017-07-06 02:23:12,347][ceph_deploy.gatherkeys][INFO  ] Storing keys in temp directory /tmp/tmpkM5Xxd
[2017-07-06 02:23:12,362][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:23:12,375][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:23:12,376][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:23:12,392][gdb3][DEBUG ] detect machine type
[2017-07-06 02:23:12,394][gdb3][DEBUG ] get remote short hostname
[2017-07-06 02:23:12,395][gdb3][DEBUG ] fetch remote file
[2017-07-06 02:23:12,396][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.gdb3.asok mon_status
[2017-07-06 02:23:12,462][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.admin
[2017-07-06 02:23:12,628][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *
[2017-07-06 02:23:12,794][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mds
[2017-07-06 02:23:12,960][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds
[2017-07-06 02:23:13,126][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-mgr
[2017-07-06 02:23:13,292][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr
[2017-07-06 02:23:13,459][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-osd
[2017-07-06 02:23:13,625][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd
[2017-07-06 02:23:13,791][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get client.bootstrap-rgw
[2017-07-06 02:23:13,957][gdb3][INFO  ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-gdb3/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw
[2017-07-06 02:23:14,173][ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring
[2017-07-06 02:23:14,173][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring
[2017-07-06 02:23:14,173][ceph_deploy.gatherkeys][INFO  ] Replacing 'ceph.bootstrap-mgr.keyring' and backing up old key as 'ceph.bootstrap-mgr.keyring-20170706022314'
[2017-07-06 02:23:14,173][ceph_deploy.gatherkeys][INFO  ] keyring 'ceph.mon.keyring' already exists
[2017-07-06 02:23:14,173][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring
[2017-07-06 02:23:14,174][ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring
[2017-07-06 02:23:14,174][ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmpkM5Xxd
[2017-07-06 02:23:14,355][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:23:14,355][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy admin gdb3
[2017-07-06 02:23:14,355][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:23:14,356][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:23:14,356][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:23:14,356][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:23:14,356][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:23:14,356][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fb7aab5b518>
[2017-07-06 02:23:14,356][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:23:14,356][ceph_deploy.cli][INFO  ]  client                        : ['gdb3']
[2017-07-06 02:23:14,356][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7fb7ab472938>
[2017-07-06 02:23:14,356][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:23:14,356][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:23:14,356][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb3
[2017-07-06 02:23:14,382][gdb3][DEBUG ] connection detected need for sudo
[2017-07-06 02:23:14,396][gdb3][DEBUG ] connected to host: gdb3 
[2017-07-06 02:23:14,396][gdb3][DEBUG ] detect platform information from remote host
[2017-07-06 02:23:14,412][gdb3][DEBUG ] detect machine type
[2017-07-06 02:23:14,415][gdb3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 02:23:22,016][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:23:22,017][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy --overwrite-conf admin gdb1
[2017-07-06 02:23:22,017][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:23:22,017][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:23:22,017][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:23:22,017][ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[2017-07-06 02:23:22,017][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:23:22,017][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1f0c685518>
[2017-07-06 02:23:22,017][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:23:22,017][ceph_deploy.cli][INFO  ]  client                        : ['gdb1']
[2017-07-06 02:23:22,017][ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f1f0cf9c938>
[2017-07-06 02:23:22,017][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:23:22,018][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:23:22,018][ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to gdb1
[2017-07-06 02:23:22,277][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 02:23:22,520][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 02:23:22,520][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 02:23:22,538][gdb1][DEBUG ] detect machine type
[2017-07-06 02:23:22,542][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 02:23:22,718][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:23:22,718][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd prepare gdb1:/mnt/memstore
[2017-07-06 02:23:22,718][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:23:22,718][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:23:22,718][ceph_deploy.cli][INFO  ]  block_db                      : None
[2017-07-06 02:23:22,718][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-07-06 02:23:22,718][ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[2017-07-06 02:23:22,718][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:23:22,718][ceph_deploy.cli][INFO  ]  bluestore                     : None
[2017-07-06 02:23:22,718][ceph_deploy.cli][INFO  ]  block_wal                     : None
[2017-07-06 02:23:22,719][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:23:22,719][ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[2017-07-06 02:23:22,719][ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[2017-07-06 02:23:22,719][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:23:22,719][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f696742b908>
[2017-07-06 02:23:22,719][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:23:22,719][ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[2017-07-06 02:23:22,719][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f6967681aa0>
[2017-07-06 02:23:22,719][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:23:22,719][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:23:22,719][ceph_deploy.cli][INFO  ]  zap_disk                      : False
[2017-07-06 02:23:22,719][ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks gdb1:/mnt/memstore:
[2017-07-06 02:23:22,962][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 02:23:23,156][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 02:23:23,157][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 02:23:23,174][gdb1][DEBUG ] detect machine type
[2017-07-06 02:23:23,178][gdb1][DEBUG ] find the location of an executable
[2017-07-06 02:23:23,179][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 02:23:23,179][ceph_deploy.osd][DEBUG ] Deploying osd to gdb1
[2017-07-06 02:23:23,179][gdb1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[2017-07-06 02:23:23,182][ceph_deploy.osd][DEBUG ] Preparing host gdb1 disk /mnt/memstore journal None activate False
[2017-07-06 02:23:23,182][gdb1][DEBUG ] find the location of an executable
[2017-07-06 02:23:23,184][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /mnt/memstore
[2017-07-06 02:23:23,304][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 02:23:23,320][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 02:23:23,336][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 02:23:23,343][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph --setuser ceph --setgroup ceph
[2017-07-06 02:23:23,359][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[2017-07-06 02:23:23,375][gdb1][WARNING] populate_data_path: Data dir /mnt/memstore already exists
[2017-07-06 02:23:28,381][gdb1][INFO  ] checking OSD status...
[2017-07-06 02:23:28,382][gdb1][DEBUG ] find the location of an executable
[2017-07-06 02:23:28,384][gdb1][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[2017-07-06 02:23:28,599][ceph_deploy.osd][DEBUG ] Host gdb1 is now ready for osd use.
[2017-07-06 02:23:28,762][ceph_deploy.conf][DEBUG ] found configuration file at: /home/ubuntu/.cephdeploy.conf
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ] Invoked (1.5.37): /home/ubuntu/bin/ceph-deploy osd activate gdb1:/mnt/memstore
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ]  username                      : None
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ]  verbose                       : False
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ]  subcommand                    : activate
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ]  quiet                         : False
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f22ddf5a908>
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f22de1b0aa0>
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ]  default_release               : False
[2017-07-06 02:23:28,763][ceph_deploy.cli][INFO  ]  disk                          : [('gdb1', '/mnt/memstore', None)]
[2017-07-06 02:23:28,764][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks gdb1:/mnt/memstore:
[2017-07-06 02:23:29,010][gdb1][DEBUG ] connection detected need for sudo
[2017-07-06 02:23:29,236][gdb1][DEBUG ] connected to host: gdb1 
[2017-07-06 02:23:29,237][gdb1][DEBUG ] detect platform information from remote host
[2017-07-06 02:23:29,253][gdb1][DEBUG ] detect machine type
[2017-07-06 02:23:29,257][gdb1][DEBUG ] find the location of an executable
[2017-07-06 02:23:29,258][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 16.04 xenial
[2017-07-06 02:23:29,258][ceph_deploy.osd][DEBUG ] activating host gdb1 disk /mnt/memstore
[2017-07-06 02:23:29,258][ceph_deploy.osd][DEBUG ] will use init type: systemd
[2017-07-06 02:23:29,259][gdb1][DEBUG ] find the location of an executable
[2017-07-06 02:23:29,261][gdb1][INFO  ] Running command: sudo /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore
[2017-07-06 02:23:29,431][gdb1][WARNING] main_activate: path = /mnt/memstore
[2017-07-06 02:23:29,431][gdb1][WARNING] activate: Cluster uuid is 382eeac0-7e10-4bdc-98d7-53d22201d56d
[2017-07-06 02:23:29,431][gdb1][WARNING] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2017-07-06 02:23:29,431][gdb1][WARNING] Traceback (most recent call last):
[2017-07-06 02:23:29,431][gdb1][WARNING]   File "/usr/local/bin/ceph-disk", line 9, in <module>
[2017-07-06 02:23:29,432][gdb1][WARNING]     load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()
[2017-07-06 02:23:29,432][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5653, in run
[2017-07-06 02:23:29,432][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 5604, in main
[2017-07-06 02:23:29,432][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3713, in main_activate
[2017-07-06 02:23:29,432][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3533, in activate_dir
[2017-07-06 02:23:29,432][gdb1][WARNING]   File "build/bdist.linux-x86_64/egg/ceph_disk/main.py", line 3609, in activate
[2017-07-06 02:23:29,432][gdb1][WARNING] ceph_disk.main.Error: Error: No cluster conf found in /etc/ceph with fsid 382eeac0-7e10-4bdc-98d7-53d22201d56d
[2017-07-06 02:23:29,433][gdb1][ERROR ] RuntimeError: command returned non-zero exit status: 1
[2017-07-06 02:23:29,433][ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/local/bin/ceph-disk -v activate --mark-init systemd --mount /mnt/memstore

